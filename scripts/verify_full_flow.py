#!/usr/bin/env python3
"""
é›†æˆæµ‹è¯•è„šæœ¬ï¼šéªŒè¯åç«¯é€»è¾‘æ˜¯å¦ç¬¦åˆå‰ç«¯é¢„æœŸ

æ¨¡æ‹Ÿå‰ç«¯APIè°ƒç”¨ï¼ŒéªŒè¯JSONå“åº”ç»“æ„ã€‚
"""
import sys
import os
import asyncio
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from gibh_agent.main import GIBHAgent
from gibh_agent.core.orchestrator import AgentOrchestrator


class TestResult:
    """æµ‹è¯•ç»“æœ"""
    def __init__(self, scenario_name: str):
        self.scenario_name = scenario_name
        self.passed = True
        self.errors = []
        self.warnings = []
        self.events = []
    
    def fail(self, message: str):
        self.passed = False
        self.errors.append(message)
        print(f"  âŒ FAIL: {message}")
    
    def warn(self, message: str):
        self.warnings.append(message)
        print(f"  âš ï¸  WARN: {message}")
    
    def success(self, message: str):
        print(f"  âœ… PASS: {message}")
    
    def add_event(self, event_type: str, data: Dict[str, Any]):
        self.events.append({"type": event_type, "data": data})


def parse_sse_stream(stream_text: str) -> List[Dict[str, Any]]:
    """
    è§£æ SSE æµæ–‡æœ¬ï¼Œæå–äº‹ä»¶
    
    Args:
        stream_text: SSE æ ¼å¼çš„æ–‡æœ¬æµ
        
    Returns:
        äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« type å’Œ data
    """
    events = []
    lines = stream_text.split('\n')
    
    current_event_type = None
    current_data_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            # ç©ºè¡Œè¡¨ç¤ºäº‹ä»¶ç»“æŸ
            if current_event_type and current_data_lines:
                try:
                    data_json = '\n'.join(current_data_lines)
                    data = json.loads(data_json)
                    events.append({
                        "type": current_event_type,
                        "data": data
                    })
                except json.JSONDecodeError as e:
                    print(f"  âš ï¸  JSON è§£æé”™è¯¯: {e}, æ•°æ®: {data_json[:100]}")
            current_event_type = None
            current_data_lines = []
            continue
        
        if line.startswith('event: '):
            current_event_type = line[7:].strip()
        elif line.startswith('data: '):
            current_data_lines.append(line[6:])
        elif current_data_lines:
            # å¤šè¡Œæ•°æ®
            current_data_lines.append(line)
    
    # å¤„ç†æœ€åä¸€ä¸ªäº‹ä»¶
    if current_event_type and current_data_lines:
        try:
            data_json = '\n'.join(current_data_lines)
            data = json.loads(data_json)
            events.append({
                "type": current_event_type,
                "data": data
            })
        except json.JSONDecodeError as e:
            print(f"  âš ï¸  JSON è§£æé”™è¯¯: {e}, æ•°æ®: {data_json[:100]}")
    
    return events


async def test_scenario_1_plan_first(agent: GIBHAgent, upload_dir: str) -> TestResult:
    """
    åœºæ™¯1ï¼šPlan-Firstï¼ˆæ— æ–‡ä»¶ï¼‰
    
    è¾“å…¥: query="I want PCA", files=[]
    éªŒè¯: workflow äº‹ä»¶ï¼Œtemplate_mode=Trueï¼Œsteps ä¸ä¸ºç©º
    """
    result = TestResult("Scenario 1: Plan-First (No File)")
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•åœºæ™¯: {result.scenario_name}")
    print(f"{'='*60}")
    
    try:
        # åˆ›å»ºç¼–æ’å™¨
        orchestrator = AgentOrchestrator(agent, upload_dir=upload_dir)
        
        # è°ƒç”¨ stream_process
        query = "I want PCA"
        files = []
        
        print(f"\nğŸ“¤ è¾“å…¥:")
        print(f"  Query: {query}")
        print(f"  Files: {files}")
        
        # æ”¶é›†æ‰€æœ‰ SSE äº‹ä»¶
        stream_chunks = []
        async for chunk in orchestrator.stream_process(
            query=query,
            files=files,
            session_id="test-session-1",
            user_id="test-user"
        ):
            stream_chunks.append(chunk)
        
        # è§£æ SSE æµ
        stream_text = ''.join(stream_chunks)
        events = parse_sse_stream(stream_text)
        
        print(f"\nğŸ“¥ æ”¶åˆ° {len(events)} ä¸ªäº‹ä»¶:")
        for i, event in enumerate(events):
            print(f"  [{i+1}] {event['type']}: {json.dumps(event['data'], ensure_ascii=False)[:100]}...")
            result.add_event(event['type'], event['data'])
        
        # éªŒè¯ workflow äº‹ä»¶
        workflow_events = [e for e in events if e['type'] == 'workflow']
        if not workflow_events:
            result.fail("æœªæ‰¾åˆ° 'workflow' äº‹ä»¶")
        else:
            result.success(f"æ‰¾åˆ° {len(workflow_events)} ä¸ª 'workflow' äº‹ä»¶")
            workflow_data = workflow_events[0]['data']
            
            # éªŒè¯ workflow_config ç»“æ„
            workflow_config = workflow_data.get('workflow_config') or workflow_data.get('workflow_data') or workflow_data
            
            # æ£€æŸ¥ steps
            steps = workflow_config.get('steps') or workflow_config.get('workflow_data', {}).get('steps', [])
            if not steps or len(steps) == 0:
                result.fail("workflow_config.steps ä¸ºç©º")
            else:
                result.success(f"workflow_config.steps åŒ…å« {len(steps)} ä¸ªæ­¥éª¤")
                
                # éªŒè¯æ¯ä¸ªæ­¥éª¤éƒ½æœ‰ step_id
                for i, step in enumerate(steps):
                    step_id = step.get('step_id') or step.get('id') or step.get('tool_id')
                    if not step_id:
                        result.fail(f"æ­¥éª¤ {i} ç¼ºå°‘ step_id/id/tool_id: {step}")
            
            # éªŒè¯ template_mode
            template_mode = workflow_config.get('template_mode') or workflow_data.get('template_mode')
            if template_mode is not True:
                result.fail(f"template_mode åº”ä¸º Trueï¼Œå®é™…ä¸º: {template_mode}")
            else:
                result.success("template_mode = True")
            
            # éªŒè¯ file_path å ä½ç¬¦
            has_placeholder = False
            for step in steps:
                params = step.get('params', {})
                file_path = params.get('file_path') or params.get('adata_path')
                if file_path and ('<å¾…ä¸Šä¼ æ•°æ®>' in str(file_path) or '<PENDING_UPLOAD>' in str(file_path)):
                    has_placeholder = True
                    break
            
            if not has_placeholder:
                result.warn("æœªæ‰¾åˆ° file_path å ä½ç¬¦ï¼ˆ<å¾…ä¸Šä¼ æ•°æ®> æˆ– <PENDING_UPLOAD>ï¼‰")
            else:
                result.success("æ‰¾åˆ° file_path å ä½ç¬¦")
        
        # éªŒè¯ diagnosis äº‹ä»¶
        diagnosis_events = [e for e in events if e['type'] == 'diagnosis']
        if diagnosis_events:
            diagnosis_data = diagnosis_events[0]['data']
            diagnosis_text = diagnosis_data.get('message') or str(diagnosis_data)
            if 'æ–¹æ¡ˆå·²ç”Ÿæˆ' in diagnosis_text or 'Template Ready' in diagnosis_text or 'template_ready' in str(diagnosis_data):
                result.success("è¯Šæ–­æŠ¥å‘ŠåŒ…å« 'Template Ready' ä¿¡æ¯")
            else:
                result.warn(f"è¯Šæ–­æŠ¥å‘Šå¯èƒ½ä¸åŒ…å«æ¨¡æ¿ä¿¡æ¯: {diagnosis_text[:100]}")
        
        # éªŒè¯ result äº‹ä»¶
        result_events = [e for e in events if e['type'] == 'result']
        if result_events:
            result_data = result_events[0]['data']
            
            # æ£€æŸ¥ç»“æ„åµŒå¥—
            if 'workflow_data' in result_data and 'workflow_data' in result_data.get('workflow_data', {}):
                result.fail("æ£€æµ‹åˆ°åµŒå¥—çš„ workflow_data.workflow_dataï¼ˆç»“æ„é”™è¯¯ï¼‰")
            
            # æ£€æŸ¥ diagnosis_report å’Œ workflow_config
            has_diagnosis = 'diagnosis_report' in result_data or 'report_data' in result_data
            has_workflow = 'workflow_config' in result_data or 'report_data' in result_data
            
            if has_diagnosis and has_workflow:
                result.success("result äº‹ä»¶åŒ…å« diagnosis_report å’Œ workflow_config")
            else:
                result.warn(f"result äº‹ä»¶ç»“æ„: diagnosis={has_diagnosis}, workflow={has_workflow}")
        
        # éªŒè¯ done äº‹ä»¶
        done_events = [e for e in events if e['type'] == 'done']
        if not done_events:
            result.warn("æœªæ‰¾åˆ° 'done' äº‹ä»¶")
        else:
            result.success("æ‰¾åˆ° 'done' äº‹ä»¶")
        
    except Exception as e:
        result.fail(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"  è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
    
    return result


async def test_scenario_2_execution(agent: GIBHAgent, upload_dir: str) -> TestResult:
    """
    åœºæ™¯2ï¼šæ‰§è¡Œæ¨¡å¼ï¼ˆæœ‰æ–‡ä»¶ï¼‰
    
    è¾“å…¥: query="Analyze this", files=["/mock/path/cow_diet.csv"]
    éªŒè¯: workflow äº‹ä»¶ï¼Œtemplate_mode=Falseï¼Œfile_path ä¸ºçœŸå®è·¯å¾„
    """
    result = TestResult("Scenario 2: Execution (With File)")
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•åœºæ™¯: {result.scenario_name}")
    print(f"{'='*60}")
    
    try:
        # åˆ›å»ºç¼–æ’å™¨
        orchestrator = AgentOrchestrator(agent, upload_dir=upload_dir)
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ–‡ä»¶
        mock_file_path = Path(upload_dir) / "test_cow_diet.csv"
        mock_file_path.parent.mkdir(parents=True, exist_ok=True)
        mock_file_path.write_text("sample,group,metabolite1,metabolite2\n1,A,1.0,2.0\n2,B,1.5,2.5\n")
        
        # è°ƒç”¨ stream_process
        query = "Analyze this"
        files = [{"name": "test_cow_diet.csv", "path": str(mock_file_path)}]
        
        print(f"\nğŸ“¤ è¾“å…¥:")
        print(f"  Query: {query}")
        print(f"  Files: {files}")
        
        # æ”¶é›†æ‰€æœ‰ SSE äº‹ä»¶
        stream_chunks = []
        async for chunk in orchestrator.stream_process(
            query=query,
            files=files,
            session_id="test-session-2",
            user_id="test-user"
        ):
            stream_chunks.append(chunk)
        
        # è§£æ SSE æµ
        stream_text = ''.join(stream_chunks)
        events = parse_sse_stream(stream_text)
        
        print(f"\nğŸ“¥ æ”¶åˆ° {len(events)} ä¸ªäº‹ä»¶:")
        for i, event in enumerate(events):
            print(f"  [{i+1}] {event['type']}: {json.dumps(event['data'], ensure_ascii=False)[:100]}...")
            result.add_event(event['type'], event['data'])
        
        # éªŒè¯ workflow äº‹ä»¶
        workflow_events = [e for e in events if e['type'] == 'workflow']
        if not workflow_events:
            result.fail("æœªæ‰¾åˆ° 'workflow' äº‹ä»¶")
        else:
            result.success(f"æ‰¾åˆ° {len(workflow_events)} ä¸ª 'workflow' äº‹ä»¶")
            workflow_data = workflow_events[0]['data']
            
            # éªŒè¯ workflow_config ç»“æ„
            workflow_config = workflow_data.get('workflow_config') or workflow_data.get('workflow_data') or workflow_data
            
            # æ£€æŸ¥ steps
            steps = workflow_config.get('steps') or workflow_config.get('workflow_data', {}).get('steps', [])
            if not steps or len(steps) == 0:
                result.fail("workflow_config.steps ä¸ºç©º")
            else:
                result.success(f"workflow_config.steps åŒ…å« {len(steps)} ä¸ªæ­¥éª¤")
            
            # éªŒè¯ template_mode åº”ä¸º False æˆ–ä¸å­˜åœ¨
            template_mode = workflow_config.get('template_mode') or workflow_data.get('template_mode')
            if template_mode is True:
                result.fail(f"template_mode åº”ä¸º False æˆ–ä¸å­˜åœ¨ï¼Œå®é™…ä¸º: {template_mode}")
            else:
                result.success(f"template_mode = {template_mode} (æ­£ç¡®)")
            
            # éªŒè¯ file_path ä¸ºçœŸå®è·¯å¾„ï¼ˆä¸æ˜¯å ä½ç¬¦ï¼‰
            has_real_path = False
            for step in steps:
                params = step.get('params', {})
                file_path = params.get('file_path') or params.get('adata_path')
                if file_path and '<å¾…ä¸Šä¼ æ•°æ®>' not in str(file_path) and '<PENDING_UPLOAD>' not in str(file_path):
                    has_real_path = True
                    break
            
            if not has_real_path:
                result.warn("æœªæ‰¾åˆ°çœŸå® file_pathï¼ˆå¯èƒ½ä»ä¸ºå ä½ç¬¦ï¼‰")
            else:
                result.success("æ‰¾åˆ°çœŸå® file_path")
        
        # éªŒè¯ result äº‹ä»¶
        result_events = [e for e in events if e['type'] == 'result']
        if not result_events:
            result.warn("æœªæ‰¾åˆ° 'result' äº‹ä»¶")
        else:
            result.success("æ‰¾åˆ° 'result' äº‹ä»¶")
        
        # æ¸…ç†æ¨¡æ‹Ÿæ–‡ä»¶
        if mock_file_path.exists():
            mock_file_path.unlink()
        
    except Exception as e:
        result.fail(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"  è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
    
    return result


async def test_scenario_3_frontend_contract(agent: GIBHAgent, upload_dir: str) -> TestResult:
    """
    åœºæ™¯3ï¼šå‰ç«¯å¥‘çº¦æ£€æŸ¥
    
    æ£€æŸ¥ JSON ç»“æ„æ˜¯å¦ç¬¦åˆå‰ç«¯æœŸæœ›
    """
    result = TestResult("Scenario 3: Frontend Contract Check")
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•åœºæ™¯: {result.scenario_name}")
    print(f"{'='*60}")
    
    try:
        # ä½¿ç”¨åœºæ™¯1çš„ç»“æœè¿›è¡ŒéªŒè¯
        scenario1_result = await test_scenario_1_plan_first(agent, upload_dir)
        
        if not scenario1_result.events:
            result.fail("åœºæ™¯1æœªäº§ç”Ÿäº‹ä»¶ï¼Œæ— æ³•è¿›è¡Œå¥‘çº¦æ£€æŸ¥")
            return result
        
        # æ£€æŸ¥æ‰€æœ‰ workflow äº‹ä»¶çš„ç»“æ„
        workflow_events = [e for e in scenario1_result.events if e['type'] == 'workflow']
        
        for event in workflow_events:
            data = event['data']
            
            # æ£€æŸ¥åµŒå¥—ç»“æ„
            if 'workflow_data' in data and isinstance(data.get('workflow_data'), dict):
                if 'workflow_data' in data['workflow_data']:
                    result.fail("æ£€æµ‹åˆ° workflow_data.workflow_data åµŒå¥—ç»“æ„ï¼ˆé”™è¯¯ï¼‰")
                else:
                    result.success("workflow_data ç»“æ„æ­£ç¡®ï¼ˆæ— åµŒå¥—ï¼‰")
            
            # æ£€æŸ¥æ­¥éª¤ç»“æ„
            workflow_config = data.get('workflow_config') or data.get('workflow_data') or data
            steps = workflow_config.get('steps') or workflow_config.get('workflow_data', {}).get('steps', [])
            
            for i, step in enumerate(steps):
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                if not step.get('step_id') and not step.get('id') and not step.get('tool_id'):
                    result.fail(f"æ­¥éª¤ {i} ç¼ºå°‘ step_id/id/tool_id: {step}")
                
                # æ£€æŸ¥ params ç»“æ„
                if 'params' not in step:
                    result.warn(f"æ­¥éª¤ {i} ç¼ºå°‘ params å­—æ®µ")
        
        # æ£€æŸ¥ result äº‹ä»¶ç»“æ„
        result_events = [e for e in scenario1_result.events if e['type'] == 'result']
        for event in result_events:
            data = event['data']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ diagnosis_report å’Œ workflow_config
            has_diagnosis = 'diagnosis_report' in data or 'report_data' in data
            has_workflow = 'workflow_config' in data or 'report_data' in data
            
            if not has_diagnosis:
                result.warn("result äº‹ä»¶ç¼ºå°‘ diagnosis_report")
            if not has_workflow:
                result.warn("result äº‹ä»¶ç¼ºå°‘ workflow_config")
            
            if has_diagnosis and has_workflow:
                result.success("result äº‹ä»¶ç»“æ„å®Œæ•´")
        
    except Exception as e:
        result.fail(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"  è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
    
    return result


async def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("é›†æˆæµ‹è¯•ï¼šéªŒè¯åç«¯é€»è¾‘æ˜¯å¦ç¬¦åˆå‰ç«¯é¢„æœŸ")
    print("="*60)
    
    # åˆå§‹åŒ– Agent
    config_path = "config/settings.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("   ä½¿ç”¨é»˜è®¤é…ç½®...")
        config_path = None
    
    try:
        agent = GIBHAgent(config_path) if config_path else GIBHAgent()
        print("âœ… GIBHAgent åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ GIBHAgent åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # è·å–ä¸Šä¼ ç›®å½•ï¼ˆä½¿ç”¨é¡¹ç›®å†…çš„ç›®å½•é¿å…æƒé™é—®é¢˜ï¼‰
    upload_dir = os.getenv("UPLOAD_DIR", str(project_root / "uploads"))
    print(f"ğŸ“ ä¸Šä¼ ç›®å½•: {upload_dir}")
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(upload_dir).mkdir(parents=True, exist_ok=True)
    
    # è¿è¡Œæµ‹è¯•åœºæ™¯
    results = []
    
    # åœºæ™¯1ï¼šPlan-First
    result1 = await test_scenario_1_plan_first(agent, upload_dir)
    results.append(result1)
    
    # åœºæ™¯2ï¼šExecution
    result2 = await test_scenario_2_execution(agent, upload_dir)
    results.append(result2)
    
    # åœºæ™¯3ï¼šFrontend Contract
    result3 = await test_scenario_3_frontend_contract(agent, upload_dir)
    results.append(result3)
    
    # æ±‡æ€»ç»“æœ
    print(f"\n{'='*60}")
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    
    all_passed = True
    for result in results:
        status = "âœ… PASS" if result.passed else "âŒ FAIL"
        print(f"\n{status} {result.scenario_name}")
        if result.errors:
            print(f"  é”™è¯¯ ({len(result.errors)}):")
            for error in result.errors:
                print(f"    - {error}")
        if result.warnings:
            print(f"  è­¦å‘Š ({len(result.warnings)}):")
            for warning in result.warnings:
                print(f"    - {warning}")
        
        if not result.passed:
            all_passed = False
    
    print(f"\n{'='*60}")
    if all_passed:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

