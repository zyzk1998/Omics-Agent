#!/usr/bin/env python3
"""
æµ‹è¯•LLM APIè¿æ¥
ç”¨äºè¯Šæ–­AIä¸“å®¶åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥çš„åŸå› 
"""
import asyncio
import os
import sys
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

async def test_llm_connection():
    """æµ‹è¯•LLM APIè¿æ¥"""
    print("=" * 80)
    print("ğŸ” æ­¥éª¤3ï¼šæµ‹è¯•LLM APIè¿æ¥")
    print("=" * 80)
    
    try:
        from gibh_agent.core.llm_client import LLMClientFactory
        
        print("\n1. åˆ›å»ºLLMå®¢æˆ·ç«¯...")
        client = LLMClientFactory.create_default()
        print(f"   âœ… LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        print(f"   - base_url: {client.base_url}")
        print(f"   - model: {client.model}")
        print(f"   - api_key: {'å·²è®¾ç½®' if hasattr(client, 'api_key') and client.api_key else 'æœªè®¾ç½®'}")
        
        if hasattr(client, 'api_key') and client.api_key:
            # åªæ˜¾ç¤ºå‰4ä¸ªå­—ç¬¦å’Œå4ä¸ªå­—ç¬¦
            api_key_preview = client.api_key[:4] + "..." + client.api_key[-4:] if len(client.api_key) > 8 else "***"
            print(f"   - api_keyé¢„è§ˆ: {api_key_preview}")
        else:
            print(f"   âŒ APIå¯†é’¥æœªè®¾ç½®ï¼")
            return False
        
        print("\n2. æµ‹è¯•ç®€å•LLMè°ƒç”¨...")
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Say hello in Chinese."}
        ]
        
        print(f"   - å‘é€æ¶ˆæ¯æ•°é‡: {len(messages)}")
        print(f"   - system messageé•¿åº¦: {len(messages[0]['content'])} å­—ç¬¦")
        print(f"   - user messageé•¿åº¦: {len(messages[1]['content'])} å­—ç¬¦")
        
        completion = await client.achat(messages, temperature=0.3, max_tokens=100)
        
        if completion and hasattr(completion, 'choices') and len(completion.choices) > 0:
            response = completion.choices[0].message.content
            print(f"   âœ… LLMè°ƒç”¨æˆåŠŸ")
            print(f"   - å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            print(f"   - å“åº”å†…å®¹: {response[:100]}...")
            return True
        else:
            print(f"   âŒ LLMè°ƒç”¨è¿”å›ç©ºå“åº”")
            return False
            
    except Exception as e:
        print(f"   âŒ LLMè°ƒç”¨å¤±è´¥")
        print(f"   - é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   - é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"\n   å®Œæ•´å †æ ˆ:")
        traceback.print_exc()
        return False

async def test_llm_analysis_summary_simulation():
    """æ¨¡æ‹ŸAIä¸“å®¶åˆ†ææŠ¥å‘Šçš„LLMè°ƒç”¨"""
    print("\n" + "=" * 80)
    print("ğŸ” æ¨¡æ‹ŸAIä¸“å®¶åˆ†ææŠ¥å‘Šçš„LLMè°ƒç”¨")
    print("=" * 80)
    
    try:
        from gibh_agent.core.llm_client import LLMClientFactory
        
        client = LLMClientFactory.create_default()
        
        # æ¨¡æ‹ŸçœŸå®çš„AIä¸“å®¶åˆ†ææŠ¥å‘Šè°ƒç”¨
        key_findings_json = '{"pca_variance_explained": [0.35, 0.28], "n_differential": 15, "top_pathways": ["Pathway1", "Pathway2"]}'
        
        system_prompt = """You are a Senior Bioinformatics Scientist specializing in metabolomics analysis. 
Generate comprehensive biological interpretation reports in Simplified Chinese."""
        
        user_prompt = f"""Based on these analysis metrics: {key_findings_json}

Write a comprehensive biological interpretation report in Simplified Chinese. Include:
1. ç»“æœæ‘˜è¦ (quantitative findings)
2. ç”Ÿç‰©å­¦æœºåˆ¶è§£è¯» (connect metabolites/pathways to biological functions)
3. æ½œåœ¨æ ‡å¿—ç‰© (discuss VIP molecules)
4. ä¸‹ä¸€æ­¥å»ºè®® (validation experiments)

Minimum 500 words. Be scientific and detailed."""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        print(f"\n1. æ„å»ºè¯·æ±‚æ¶ˆæ¯...")
        print(f"   - messagesæ•°é‡: {len(messages)}")
        print(f"   - system messageé•¿åº¦: {len(messages[0]['content'])} å­—ç¬¦")
        print(f"   - user messageé•¿åº¦: {len(messages[1]['content'])} å­—ç¬¦")
        print(f"   - æ€»é•¿åº¦: {len(messages[0]['content']) + len(messages[1]['content'])} å­—ç¬¦")
        
        print(f"\n2. è°ƒç”¨LLM API (temperature=0.3, max_tokens=2500)...")
        completion = await client.achat(messages, temperature=0.3, max_tokens=2500)
        
        print(f"   âœ… LLMè°ƒç”¨å®Œæˆ")
        
        # è§£æå“åº”
        think_content, response = client.extract_think_and_content(completion)
        original_content = completion.choices[0].message.content or ""
        
        print(f"\n3. è§£æå“åº”...")
        print(f"   - åŸå§‹å“åº”é•¿åº¦: {len(original_content)} å­—ç¬¦")
        print(f"   - æå–çš„å“åº”é•¿åº¦: {len(response) if response else 0} å­—ç¬¦")
        print(f"   - æ€è€ƒå†…å®¹é•¿åº¦: {len(think_content) if think_content else 0} å­—ç¬¦")
        
        if response and len(response.strip()) > 100:
            print(f"   âœ… å“åº”æœ‰æ•ˆï¼ˆé•¿åº¦ > 100å­—ç¬¦ï¼‰")
            print(f"   - å“åº”é¢„è§ˆ: {response[:200]}...")
            return True
        else:
            print(f"   âš ï¸ å“åº”è¿‡çŸ­ï¼ˆé•¿åº¦ <= 100å­—ç¬¦ï¼‰")
            if response:
                print(f"   - å“åº”å†…å®¹: {response}")
            return False
            
    except Exception as e:
        print(f"   âŒ æ¨¡æ‹Ÿè°ƒç”¨å¤±è´¥")
        print(f"   - é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   - é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"\n   å®Œæ•´å †æ ˆ:")
        traceback.print_exc()
        return False

async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹LLMè¿æ¥è¯Šæ–­")
    print("=" * 80)
    
    # æµ‹è¯•1ï¼šåŸºæœ¬è¿æ¥
    result1 = await test_llm_connection()
    
    if result1:
        # æµ‹è¯•2ï¼šæ¨¡æ‹ŸAIä¸“å®¶åˆ†ææŠ¥å‘Šè°ƒç”¨
        result2 = await test_llm_analysis_summary_simulation()
        
        if result2:
            print("\n" + "=" * 80)
            print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLM APIè¿æ¥æ­£å¸¸")
            print("=" * 80)
        else:
            print("\n" + "=" * 80)
            print("âš ï¸ åŸºæœ¬è¿æ¥æ­£å¸¸ï¼Œä½†æ¨¡æ‹ŸAIä¸“å®¶åˆ†ææŠ¥å‘Šè°ƒç”¨å¤±è´¥")
            print("=" * 80)
    else:
        print("\n" + "=" * 80)
        print("âŒ LLM APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        print("=" * 80)
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. APIå¯†é’¥æœªè®¾ç½®æˆ–æ— æ•ˆ")
        print("2. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("3. APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        print("4. ç¯å¢ƒå˜é‡æœªæ­£ç¡®åŠ è½½")

if __name__ == "__main__":
    asyncio.run(main())
