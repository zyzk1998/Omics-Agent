#!/usr/bin/env python3
"""
å‰ç«¯RNAåˆ†ææµ‹è¯•è„šæœ¬

æ¨¡æ‹Ÿå‰ç«¯ä¸Šä¼ 10xæ•°æ®å¹¶æ‰§è¡ŒRNAåˆ†ææµç¨‹ï¼Œè®°å½•æ‰€æœ‰é—®é¢˜å¹¶ä¿®å¤
"""
import os
import sys
import asyncio
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import requests
from fastapi.testclient import TestClient

# é…ç½®æ—¥å¿—
log_file = f"frontend_rna_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é—®é¢˜è®°å½•
issues = []


def record_issue(issue_type: str, description: str, fix: str = ""):
    """è®°å½•é—®é¢˜"""
    issues.append({
        "type": issue_type,
        "description": description,
        "fix": fix,
        "timestamp": datetime.now().isoformat()
    })
    logger.error(f"âŒ [{issue_type}] {description}")


def test_10x_file_structure():
    """æµ‹è¯•1: æ£€æŸ¥10xæ–‡ä»¶ç»“æ„"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•1: æ£€æŸ¥10xæ–‡ä»¶ç»“æ„")
    logger.info("=" * 80)
    
    rawdata_dir = project_root / "test_data" / "rawdata"
    
    # æ£€æŸ¥æ–‡ä»¶ç»“æ„
    matrix_dir = rawdata_dir / "matrix.mtx"
    barcodes_dir = rawdata_dir / "barcodes.tsv"
    features_dir = rawdata_dir / "features.tsv"
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•ï¼ˆ10xæ•°æ®å¯èƒ½æ˜¯åµŒå¥—ç»“æ„ï¼‰
    if matrix_dir.is_dir():
        matrix_file = matrix_dir / "matrix.mtx"
        if not matrix_file.exists():
            record_issue("æ–‡ä»¶ç»“æ„", f"matrix.mtxç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ä¸å­˜åœ¨: {matrix_file}")
            return False
    else:
        record_issue("æ–‡ä»¶ç»“æ„", f"matrix.mtxä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {matrix_dir}")
        return False
    
    if barcodes_dir.is_dir():
        barcodes_file = barcodes_dir / "barcodes.tsv"
        if not barcodes_file.exists():
            record_issue("æ–‡ä»¶ç»“æ„", f"barcodes.tsvç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ä¸å­˜åœ¨: {barcodes_file}")
            return False
    else:
        record_issue("æ–‡ä»¶ç»“æ„", f"barcodes.tsvä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {barcodes_dir}")
        return False
    
    if features_dir.is_dir():
        features_file = features_dir / "features.tsv"
        if not features_file.exists():
            record_issue("æ–‡ä»¶ç»“æ„", f"features.tsvç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ä¸å­˜åœ¨: {features_file}")
            return False
    else:
        record_issue("æ–‡ä»¶ç»“æ„", f"features.tsvä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {features_dir}")
        return False
    
    logger.info("âœ… 10xæ–‡ä»¶ç»“æ„æ£€æŸ¥é€šè¿‡")
    logger.info(f"   - matrix.mtx: {matrix_file}")
    logger.info(f"   - barcodes.tsv: {barcodes_file}")
    logger.info(f"   - features.tsv: {features_file}")
    
    return True


async def test_file_upload():
    """æµ‹è¯•2: æ¨¡æ‹Ÿå‰ç«¯æ–‡ä»¶ä¸Šä¼ """
    logger.info("=" * 80)
    logger.info("æµ‹è¯•2: æ¨¡æ‹Ÿå‰ç«¯æ–‡ä»¶ä¸Šä¼ ")
    logger.info("=" * 80)
    
    try:
        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
        base_url = "http://localhost:8000"
        
        try:
            response = requests.get(f"{base_url}/", timeout=2)
            logger.info("âœ… æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
        except requests.exceptions.ConnectionError:
            logger.warning("âš ï¸ æœåŠ¡å™¨æœªè¿è¡Œï¼Œè·³è¿‡ä¸Šä¼ æµ‹è¯•")
            record_issue("æœåŠ¡å™¨", "æœåŠ¡å™¨æœªè¿è¡Œï¼Œæ— æ³•æµ‹è¯•æ–‡ä»¶ä¸Šä¼ ")
            return False
        
        # å‡†å¤‡10xæ–‡ä»¶
        rawdata_dir = project_root / "test_data" / "rawdata"
        matrix_file = rawdata_dir / "matrix.mtx" / "matrix.mtx"
        barcodes_file = rawdata_dir / "barcodes.tsv" / "barcodes.tsv"
        features_file = rawdata_dir / "features.tsv" / "features.tsv"
        
        if not all([matrix_file.exists(), barcodes_file.exists(), features_file.exists()]):
            record_issue("æ–‡ä»¶ä¸Šä¼ ", "10xæ–‡ä»¶ä¸å®Œæ•´ï¼Œæ— æ³•ä¸Šä¼ ")
            return False
        
        # æ¨¡æ‹Ÿä¸Šä¼ ä¸‰ä¸ªæ–‡ä»¶
        files = [
            ("files", ("matrix.mtx", open(matrix_file, "rb"), "text/plain")),
            ("files", ("barcodes.tsv", open(barcodes_file, "rb"), "text/tab-separated-values")),
            ("files", ("features.tsv", open(features_file, "rb"), "text/tab-separated-values"))
        ]
        
        logger.info("ğŸ“¤ ä¸Šä¼ 10xæ•°æ®æ–‡ä»¶...")
        response = requests.post(
            f"{base_url}/api/upload",
            files=files,
            data={"user_id": "test_user", "session_id": "test_session"}
        )
        
        if response.status_code != 200:
            record_issue("æ–‡ä»¶ä¸Šä¼ ", f"ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
            return False
        
        upload_result = response.json()
        logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {upload_result}")
        
        # æ£€æŸ¥è¿”å›çš„æ–‡ä»¶è·¯å¾„
        if "file_paths" not in upload_result:
            record_issue("æ–‡ä»¶ä¸Šä¼ ", "ä¸Šä¼ å“åº”ä¸­ç¼ºå°‘file_pathså­—æ®µ")
            return False
        
        file_paths = upload_result.get("file_paths", [])
        if len(file_paths) == 0:
            record_issue("æ–‡ä»¶ä¸Šä¼ ", "ä¸Šä¼ å“åº”ä¸­file_pathsä¸ºç©º")
            return False
        
        logger.info(f"âœ… ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {file_paths}")
        
        # å…³é—­æ–‡ä»¶
        for _, file_tuple in files:
            file_tuple[1].close()
        
        return True
        
    except Exception as e:
        record_issue("æ–‡ä»¶ä¸Šä¼ ", f"ä¸Šä¼ è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False


async def test_chat_with_files():
    """æµ‹è¯•3: æ¨¡æ‹Ÿå‰ç«¯å‘é€èŠå¤©è¯·æ±‚ï¼ˆå¸¦æ–‡ä»¶ï¼‰"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•3: æ¨¡æ‹Ÿå‰ç«¯å‘é€èŠå¤©è¯·æ±‚ï¼ˆå¸¦æ–‡ä»¶ï¼‰")
    logger.info("=" * 80)
    
    try:
        base_url = "http://localhost:8000"
        
        # å…ˆä¸Šä¼ æ–‡ä»¶è·å–è·¯å¾„
        rawdata_dir = project_root / "test_data" / "rawdata"
        matrix_file = rawdata_dir / "matrix.mtx" / "matrix.mtx"
        barcodes_file = rawdata_dir / "barcodes.tsv" / "barcodes.tsv"
        features_file = rawdata_dir / "features.tsv" / "features.tsv"
        
        files = [
            ("files", ("matrix.mtx", open(matrix_file, "rb"), "text/plain")),
            ("files", ("barcodes.tsv", open(barcodes_file, "rb"), "text/tab-separated-values")),
            ("files", ("features.tsv", open(features_file, "rb"), "text/tab-separated-values"))
        ]
        
        upload_response = requests.post(
            f"{base_url}/api/upload",
            files=files,
            data={"user_id": "test_user", "session_id": "test_session"}
        )
        
        if upload_response.status_code != 200:
            record_issue("èŠå¤©è¯·æ±‚", f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•: {upload_response.status_code}")
            return False
        
        upload_result = upload_response.json()
        file_paths = upload_result.get("file_paths", [])
        
        # å…³é—­æ–‡ä»¶
        for _, file_tuple in files:
            file_tuple[1].close()
        
        if len(file_paths) == 0:
            record_issue("èŠå¤©è¯·æ±‚", "ä¸Šä¼ æ–‡ä»¶åæœªè·å¾—æ–‡ä»¶è·¯å¾„")
            return False
        
        # æ„å»ºèŠå¤©è¯·æ±‚
        # æ³¨æ„ï¼š10xæ•°æ®åº”è¯¥è¢«è¯†åˆ«ä¸ºgroup_dir
        file_info = upload_result.get("file_info", [])
        if file_info and len(file_info) > 0:
            first_file = file_info[0]
            group_dir = first_file.get("group_dir") or first_file.get("path")
            
            payload = {
                "message": "rnaåˆ†æ",
                "history": [],
                "selected_tool": None,
                "uploaded_files": [{
                    "name": "10xæ•°æ® (3 ä¸ªæ–‡ä»¶)",
                    "file_name": "10xæ•°æ® (3 ä¸ªæ–‡ä»¶)",
                    "path": group_dir,
                    "file_path": group_dir,
                    "file_id": group_dir,
                    "is_10x": True,
                    "group_dir": group_dir
                }],
                "tool_params": None,
                "workflow_data": None,
                "use_history_files": False,
                "stream": True
            }
        else:
            record_issue("èŠå¤©è¯·æ±‚", "ä¸Šä¼ å“åº”ä¸­ç¼ºå°‘file_infoå­—æ®µ")
            return False
        
        logger.info(f"ğŸ“¤ å‘é€èŠå¤©è¯·æ±‚: {json.dumps(payload, ensure_ascii=False, indent=2)}")
        
        # å‘é€SSEè¯·æ±‚ï¼ˆæµå¼å“åº”ï¼‰
        response = requests.post(
            f"{base_url}/api/chat",
            json=payload,
            stream=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        if response.status_code != 200:
            record_issue("èŠå¤©è¯·æ±‚", f"èŠå¤©è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
            return False
        
        # è§£æSSEäº‹ä»¶
        events = []
        for line in response.iter_lines():
            if line:
                line_str = line.decode('utf-8')
                if line_str.startswith('data: '):
                    try:
                        event_data = json.loads(line_str[6:])
                        events.append(event_data)
                        logger.info(f"ğŸ“¡ æ”¶åˆ°SSEäº‹ä»¶: {event_data.get('type', 'unknown')}")
                    except json.JSONDecodeError:
                        logger.warning(f"âš ï¸ æ— æ³•è§£æSSEäº‹ä»¶: {line_str}")
        
        logger.info(f"âœ… æ”¶åˆ° {len(events)} ä¸ªSSEäº‹ä»¶")
        
        # æ£€æŸ¥å…³é”®äº‹ä»¶
        has_workflow = any(e.get("type") == "workflow" for e in events)
        has_diagnosis = any(e.get("type") == "diagnosis" for e in events)
        has_result = any(e.get("type") == "result" for e in events)
        
        if not has_workflow:
            record_issue("SSEäº‹ä»¶", "æœªæ”¶åˆ°workflowäº‹ä»¶")
        
        if not has_diagnosis:
            record_issue("SSEäº‹ä»¶", "æœªæ”¶åˆ°diagnosisäº‹ä»¶ï¼ˆæ•°æ®è¯Šæ–­æŠ¥å‘Šï¼‰")
        
        logger.info(f"   - workflowäº‹ä»¶: {has_workflow}")
        logger.info(f"   - diagnosisäº‹ä»¶: {has_diagnosis}")
        logger.info(f"   - resultäº‹ä»¶: {has_result}")
        
        return True
        
    except Exception as e:
        record_issue("èŠå¤©è¯·æ±‚", f"èŠå¤©è¯·æ±‚æµ‹è¯•å¼‚å¸¸: {str(e)}")
        logger.error(f"âŒ èŠå¤©è¯·æ±‚æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False


async def test_workflow_execution():
    """æµ‹è¯•4: æµ‹è¯•å·¥ä½œæµæ‰§è¡Œ"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•4: æµ‹è¯•å·¥ä½œæµæ‰§è¡Œ")
    logger.info("=" * 80)
    
    try:
        from gibh_agent.core.executor import WorkflowExecutor
        from gibh_agent.core.file_inspector import FileInspector
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®è·¯å¾„
        rawdata_dir = project_root / "test_data" / "rawdata"
        
        # æ£€æŸ¥10xæ ¼å¼æ£€æµ‹
        executor = WorkflowExecutor()
        is_10x = executor._is_10x_format(str(rawdata_dir))
        
        logger.info(f"ğŸ” 10xæ ¼å¼æ£€æµ‹ç»“æœ: {is_10x}")
        
        if not is_10x:
            record_issue("10xæ£€æµ‹", f"æœªèƒ½æ­£ç¡®æ£€æµ‹10xæ ¼å¼: {rawdata_dir}")
            # æ£€æŸ¥åŸå› 
            logger.info("ğŸ” æ£€æŸ¥ç›®å½•å†…å®¹...")
            for item in rawdata_dir.iterdir():
                logger.info(f"   - {item.name} ({'ç›®å½•' if item.is_dir() else 'æ–‡ä»¶'})")
        
        # æµ‹è¯•æ–‡ä»¶æ£€æŸ¥
        file_inspector = FileInspector(str(project_root / "uploads"))
        inspection_result = file_inspector.inspect_file(str(rawdata_dir))
        
        logger.info(f"ğŸ“Š æ–‡ä»¶æ£€æŸ¥ç»“æœ: {inspection_result.get('status')}")
        if inspection_result.get("status") != "success":
            record_issue("æ–‡ä»¶æ£€æŸ¥", f"æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {inspection_result.get('error')}")
            return False
        
        logger.info(f"   - æ–‡ä»¶ç±»å‹: {inspection_result.get('file_type')}")
        logger.info(f"   - ç»†èƒæ•°: {inspection_result.get('n_obs', 'N/A')}")
        logger.info(f"   - åŸºå› æ•°: {inspection_result.get('n_vars', 'N/A')}")
        
        return True
        
    except Exception as e:
        record_issue("å·¥ä½œæµæ‰§è¡Œ", f"å·¥ä½œæµæ‰§è¡Œæµ‹è¯•å¼‚å¸¸: {str(e)}")
        logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œæµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False


def generate_summary():
    """ç”Ÿæˆé—®é¢˜æ€»ç»“"""
    logger.info("=" * 80)
    logger.info("ğŸ“Š é—®é¢˜æ€»ç»“")
    logger.info("=" * 80)
    
    summary = f"""
# å‰ç«¯RNAåˆ†ææµ‹è¯•é—®é¢˜æ€»ç»“

## æµ‹è¯•æ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æµ‹è¯•æ•°æ®
- è·¯å¾„: /home/ubuntu/GIBH-AGENT-V2/test_data/rawdata
- æ–‡ä»¶: matrix.mtx, barcodes.tsv, features.tsv

## å‘ç°çš„é—®é¢˜

"""
    
    if not issues:
        summary += "âœ… æœªå‘ç°é—®é¢˜ï¼Œæ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼\n"
    else:
        for i, issue in enumerate(issues, 1):
            summary += f"""
### é—®é¢˜ {i}: {issue['type']}

**æè¿°**: {issue['description']}

**æ—¶é—´**: {issue['timestamp']}

"""
            if issue.get('fix'):
                summary += f"**ä¿®å¤æ–¹æ¡ˆ**: {issue['fix']}\n"
    
    summary += f"""
## æµ‹è¯•æ—¥å¿—
è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹: {log_file}

## æ€»ç»“
å…±å‘ç° {len(issues)} ä¸ªé—®é¢˜
"""
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    summary_file = f"frontend_rna_test_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(summary)
    
    logger.info(summary)
    logger.info(f"âœ… é—®é¢˜æ€»ç»“å·²ä¿å­˜åˆ°: {summary_file}")
    
    return summary


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª å‰ç«¯RNAåˆ†ææµ‹è¯•")
    logger.info("=" * 80)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_10x_file_structure()
    await test_file_upload()
    await test_chat_with_files()
    await test_workflow_execution()
    
    # ç”Ÿæˆæ€»ç»“
    summary = generate_summary()
    
    return summary


if __name__ == "__main__":
    summary = asyncio.run(main())
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    print(summary)
