#!/usr/bin/env python3
"""
éªŒè¯æŠ¥å‘Šè´¨é‡è„šæœ¬
æµ‹è¯•_generate_analysis_summaryæ˜¯å¦ç”Ÿæˆé«˜è´¨é‡çš„ç§‘å­¦è§£é‡ŠæŠ¥å‘Š
"""

import sys
import asyncio
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from gibh_agent.agents.base_agent import BaseAgent
from gibh_agent.core.llm_client import LLMClient
from gibh_agent.core.prompt_manager import PromptManager


async def test_report_generation():
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
    print("=" * 80)
    print("æŠ¥å‘Šè´¨é‡éªŒè¯è„šæœ¬")
    print("=" * 80)
    
    # Mock steps_results data (simulating successful Metabolomics run)
    mock_steps_results = [
        {
            "step_name": "metabolomics_preprocess_data",
            "status": "success",
            "data": {
                "shape": {"rows": 100, "columns": 500},
                "summary": {
                    "n_samples": 100,
                    "n_features": 500,
                    "missing_rate": 0.05
                }
            }
        },
        {
            "step_name": "metabolomics_pca_analysis",
            "status": "success",
            "data": {
                "explained_variance": {
                    "PC1": 0.452,
                    "PC2": 0.187
                },
                "summary": {
                    "pc1_variance": "45.2%",
                    "pc2_variance": "18.7%"
                }
            }
        },
        {
            "step_name": "metabolomics_differential_analysis",
            "status": "success",
            "data": {
                "results": [
                    {
                        "metabolite": "Glutamate",
                        "log2fc": 2.34,
                        "fdr": 0.001,
                        "pvalue": 0.0001
                    },
                    {
                        "metabolite": "Alanine",
                        "log2fc": 1.89,
                        "fdr": 0.003,
                        "pvalue": 0.0005
                    },
                    {
                        "metabolite": "Leucine",
                        "log2fc": 1.67,
                        "fdr": 0.005,
                        "pvalue": 0.001
                    }
                ],
                "summary": {
                    "significant_count": 23,
                    "total_metabolites": 500,
                    "method": "t-test",
                    "case_group": "Treatment",
                    "control_group": "Control"
                }
            }
        },
        {
            "step_name": "metabolomics_plsda",
            "status": "success",
            "data": {
                "vip_scores": [
                    {
                        "metabolite": "Glutamate",
                        "vip_score": 2.45
                    },
                    {
                        "metabolite": "Alanine",
                        "vip_score": 2.12
                    },
                    {
                        "metabolite": "Leucine",
                        "vip_score": 1.98
                    }
                ]
            }
        },
        {
            "step_name": "metabolomics_pathway_enrichment",
            "status": "success",
            "data": {
                "enriched_pathways": [
                    {
                        "pathway": "Amino acid metabolism",
                        "p_value": 0.0001,
                        "enrichment_score": 0.85
                    },
                    {
                        "pathway": "Fatty acid synthesis",
                        "p_value": 0.0005,
                        "enrichment_score": 0.72
                    },
                    {
                        "pathway": "TCA cycle",
                        "p_value": 0.001,
                        "enrichment_score": 0.68
                    }
                ]
            }
        }
    ]
    
    # Initialize LLM client and agent
    print("\nğŸ“‹ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’ŒAgent...")
    try:
        # Get LLM config from environment or use defaults
        import os
        base_url = os.getenv("LLM_BASE_URL", "https://api.deepseek.com/v1")
        api_key = os.getenv("DEEPSEEK_API_KEY", os.getenv("LLM_API_KEY", "EMPTY"))
        model = os.getenv("LLM_MODEL", "deepseek-chat")
        
        print(f"  ä½¿ç”¨ LLM: {base_url} (model: {model})")
        
        llm_client = LLMClient(
            base_url=base_url,
            api_key=api_key,
            model=model
        )
        prompt_manager = PromptManager()
        
        # Create a mock agent (we'll use MetabolomicsAgent)
        try:
            from gibh_agent.agents.specialists.metabolomics_agent import MetabolomicsAgent
            agent = MetabolomicsAgent(llm_client, prompt_manager)
            print("âœ… ä½¿ç”¨ MetabolomicsAgent")
        except ImportError as e:
            print(f"âš ï¸ æ— æ³•å¯¼å…¥ MetabolomicsAgent: {e}")
            # Create a minimal mock agent that has _generate_analysis_summary
            class MockAgent(BaseAgent):
                async def process_query(self, query, **kwargs):
                    return {"type": "chat", "response": "Mock"}
            agent = MockAgent(llm_client, prompt_manager, "metabolomics_expert")
            print("âœ… ä½¿ç”¨ MockAgent")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Call _generate_analysis_summary
    print("\nğŸ“ è°ƒç”¨ _generate_analysis_summary...")
    try:
            summary_context = {
                "has_failures": False,
                "has_warnings": False,
                "failed_steps": [],
                "warning_steps": [],
                "successful_steps": mock_steps_results,
                "workflow_status": "success"
            }
            
            # Create results dict in the format expected by _generate_analysis_summary
            # The method expects steps_results to be a list of step result dicts
            results = {
                "steps_results": mock_steps_results,  # This should be a list of dicts
                "status": "success",
                "steps_details": [
                    {
                        "step_id": f"step_{i}",
                        "step_name": step["step_name"],
                        "status": step["status"],
                        "step_result": step  # Wrap in step_result
                    }
                    for i, step in enumerate(mock_steps_results)
                ]
            }
            
            # Extract steps_results from results dict (as _generate_analysis_summary expects a list)
            steps_results_list = results.get("steps_results", mock_steps_results)
            
            report = await agent._generate_analysis_summary(
                steps_results_list,  # Pass list of step results
                "Metabolomics",
                summary_context=summary_context
            )
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    if not report:
        print("âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼šè¿”å› None")
        return False
    
    print("\n" + "=" * 80)
    print("ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ï¼š")
    print("=" * 80)
    print(report)
    print("=" * 80)
    
    # Assert Content: Check for keywords
    print("\nğŸ” æ£€æŸ¥æŠ¥å‘Šè´¨é‡...")
    
    # Keywords that indicate scientific interpretation
    scientific_keywords = [
        "åˆ†æ", "åˆ†æç»“æœ", "ç»“æœæ‘˜è¦",
        "ç”Ÿç‰©å­¦", "æœºåˆ¶", "æœºåˆ¶è§£è¯»", "ç”Ÿç‰©å­¦æœºåˆ¶",
        "æ˜¾è‘—", "å·®å¼‚", "å·®å¼‚åˆ†æ",
        "é€šè·¯", "ä»£è°¢é€šè·¯", "pathway",
        "æ ‡å¿—ç‰©", "æ½œåœ¨æ ‡å¿—ç‰©", "biomarker",
        "å»ºè®®", "ä¸‹ä¸€æ­¥", "éªŒè¯",
        "PCA", "ä¸»æˆåˆ†", "PC1", "PC2",
        "VIP", "å·®å¼‚è¡¨è¾¾", "å¯Œé›†"
    ]
    
    found_keywords = []
    report_lower = report.lower()
    for keyword in scientific_keywords:
        if keyword.lower() in report_lower:
            found_keywords.append(keyword)
    
    print(f"  âœ… æ‰¾åˆ° {len(found_keywords)} ä¸ªç§‘å­¦å…³é”®è¯: {found_keywords[:10]}")
    
    # Assert Length: Should be substantial
    report_length = len(report)
    print(f"  ğŸ“ æŠ¥å‘Šé•¿åº¦: {report_length} å­—ç¬¦")
    
    if report_length < 200:
        print(f"  âŒ æŠ¥å‘Šè¿‡çŸ­ï¼ˆ< 200å­—ç¬¦ï¼‰ï¼Œå¯èƒ½åªæ˜¯æ­¥éª¤åˆ—è¡¨")
        return False
    
    # Check for fallback indicators (bad signs)
    fallback_indicators = [
        "âœ… æˆåŠŸæ­¥éª¤",
        "æˆåŠŸæ­¥éª¤",
        "å¤±è´¥æ­¥éª¤",
        "è·³è¿‡æ­¥éª¤",
        "æ­¥éª¤åˆ—è¡¨",
        "LLM ç”Ÿæˆå¤±è´¥",
        "Error",
        "é”™è¯¯ä¿¡æ¯"
    ]
    
    has_fallback = any(indicator in report for indicator in fallback_indicators)
    if has_fallback:
        print(f"  âš ï¸  æ£€æµ‹åˆ°fallbackæˆ–é”™è¯¯ä¿¡æ¯")
        # Check if it's an error message (which is OK) or a lazy fallback (which is bad)
        if "LLM ç”Ÿæˆå¤±è´¥" in report or "é”™è¯¯ä¿¡æ¯" in report:
            print(f"  âš ï¸  è¿™æ˜¯LLMå¤±è´¥çš„é”™è¯¯ä¿¡æ¯ï¼ˆå¯ä»¥æ¥å—ï¼Œä½†éœ€è¦ä¿®å¤ï¼‰")
        else:
            print(f"  âŒ è¿™æ˜¯lazy fallbackï¼ˆä¸åº”è¯¥å‡ºç°ï¼‰")
            return False
    
    # Check for narrative structure (good signs)
    narrative_indicators = [
        "é€šè¿‡",
        "å‘ç°",
        "è¡¨æ˜",
        "æ˜¾ç¤º",
        "è¯†åˆ«",
        "å¯Œé›†",
        "ç›¸å…³",
        "å¯èƒ½",
        "å»ºè®®"
    ]
    
    has_narrative = any(indicator in report for indicator in narrative_indicators)
    if has_narrative:
        print(f"  âœ… æ£€æµ‹åˆ°å™è¿°æ€§æ–‡æœ¬ç»“æ„ï¼ˆç§‘å­¦è§£é‡Šï¼‰")
    else:
        print(f"  âš ï¸  ç¼ºå°‘å™è¿°æ€§æ–‡æœ¬ç»“æ„")
    
    # Final verdict
    print("\n" + "=" * 80)
    if report_length >= 200 and len(found_keywords) >= 5 and has_narrative:
        print("âœ… æŠ¥å‘Šè´¨é‡éªŒè¯é€šè¿‡ï¼")
        print("   - é•¿åº¦å……è¶³")
        print("   - åŒ…å«ç§‘å­¦å…³é”®è¯")
        print("   - å…·æœ‰å™è¿°æ€§ç»“æ„")
        return True
    else:
        print("âŒ æŠ¥å‘Šè´¨é‡éªŒè¯å¤±è´¥")
        print(f"   - é•¿åº¦: {report_length} {'âœ…' if report_length >= 200 else 'âŒ'}")
        print(f"   - å…³é”®è¯: {len(found_keywords)} {'âœ…' if len(found_keywords) >= 5 else 'âŒ'}")
        print(f"   - å™è¿°æ€§: {'âœ…' if has_narrative else 'âŒ'}")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    success = await test_report_generation()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())

