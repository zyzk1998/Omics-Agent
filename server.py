"""
GIBH-AGENT-V2 æµ‹è¯•æœåŠ¡å™¨
æä¾›ç®€å•çš„ Web æ¥å£ç”¨äºæµ‹è¯•åŠŸèƒ½
"""
import os
import sys
import json
import logging
import asyncio
import re
import secrets
from pathlib import Path
from typing import List, Optional, Set
from datetime import datetime
from collections import deque

from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from typing import Optional
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from gibh_agent import create_agent
from gibh_agent.core.file_inspector import FileInspector

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('gibh_agent.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="GIBH-AGENT-V2 Test Server")

# ğŸ”¥ Step 2: Tool-RAG æ¶æ„ - Vector Database Integration
# åˆå§‹åŒ–å·¥å…·æ£€ç´¢å™¨ï¼ˆåœ¨å¯åŠ¨æ—¶åŒæ­¥å·¥å…·ï¼‰
tool_retriever = None
workflow_planner = None
try:
    from gibh_agent.core.tool_retriever import ToolRetriever
    # ğŸ”¥ Step 4: æ¨¡å—åŒ–å·¥å…·ç³»ç»Ÿ - è‡ªåŠ¨å‘ç°å’ŒåŠ è½½æ‰€æœ‰å·¥å…·
    from gibh_agent.tools import load_all_tools
    
    # åˆå§‹åŒ–å·¥å…·æ£€ç´¢å™¨
    chroma_dir = os.getenv("CHROMA_PERSIST_DIR", "./data/chroma_tools")
    embedding_model = os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text")
    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    logger.info(f"ğŸ”§ åˆå§‹åŒ–å·¥å…·æ£€ç´¢å™¨...")
    logger.info(f"   ChromaDB ç›®å½•: {chroma_dir}")
    logger.info(f"   Embedding æ¨¡å‹: {embedding_model}")
    logger.info(f"   Ollama URL: {ollama_url}")
    
    tool_retriever = ToolRetriever(
        persist_directory=chroma_dir,
        embedding_model=embedding_model,
        ollama_base_url=ollama_url
    )
    
    logger.info("âœ… å·¥å…·æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
except ImportError as e:
    logger.warning(f"âš ï¸ å·¥å…·æ£€ç´¢å™¨ä¾èµ–æœªå®‰è£…: {e}")
    logger.warning("   è·³è¿‡å·¥å…·æ£€ç´¢å™¨åˆå§‹åŒ–ï¼ˆéœ€è¦: pip install langchain-chroma langchain-ollamaï¼‰")
except Exception as e:
    logger.error(f"âŒ å·¥å…·æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
    logger.warning("   ç»§ç»­å¯åŠ¨ï¼Œä½†å·¥å…·æ£€ç´¢åŠŸèƒ½å°†ä¸å¯ç”¨")

# ğŸ”¥ Step 3: åˆå§‹åŒ–å·¥ä½œæµè§„åˆ’å™¨ï¼ˆéœ€è¦ agent åˆå§‹åŒ–åæ‰èƒ½è·å– LLM clientï¼‰
# è¿™å°†åœ¨ agent åˆå§‹åŒ–åè®¾ç½®

# é…ç½® CORSï¼ˆå®‰å…¨é…ç½®ï¼‰
# ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶ä¸ºç‰¹å®šåŸŸå
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "*").split(",")
if ALLOWED_ORIGINS == ["*"]:
    logger.warning("âš ï¸  CORS é…ç½®å…è®¸æ‰€æœ‰æ¥æºï¼Œç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶ä¸ºç‰¹å®šåŸŸå")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization"],
)

# åˆ›å»ºä¸Šä¼ ç›®å½•ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œå…¼å®¹å®¹å™¨ç¯å¢ƒï¼‰
# ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨å®¹å™¨å†…çš„é»˜è®¤è·¯å¾„
# æ³¨æ„ï¼šåœ¨å®¹å™¨ç¯å¢ƒä¸­ï¼Œé»˜è®¤è·¯å¾„åº”è¯¥æ˜¯ /app/uploadsï¼Œè€Œä¸æ˜¯ç›¸å¯¹è·¯å¾„
UPLOAD_DIR = Path(os.getenv("UPLOAD_DIR", "/app/uploads"))
RESULTS_DIR = Path(os.getenv("RESULTS_DIR", "/app/results"))

# ç¡®ä¿ç›®å½•å­˜åœ¨ä¸”å¯å†™
try:
    UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥ç›®å½•æƒé™
    if not os.access(UPLOAD_DIR, os.W_OK):
        logger.warning(f"âš ï¸ ä¸Šä¼ ç›®å½•ä¸å¯å†™: {UPLOAD_DIR}")
    if not os.access(RESULTS_DIR, os.W_OK):
        logger.warning(f"âš ï¸ ç»“æœç›®å½•ä¸å¯å†™: {RESULTS_DIR}")
    
    logger.info(f"ğŸ“ ä¸Šä¼ ç›®å½•: {UPLOAD_DIR.absolute()} (å¯å†™: {os.access(UPLOAD_DIR, os.W_OK)})")
    logger.info(f"ğŸ“ ç»“æœç›®å½•: {RESULTS_DIR.absolute()} (å¯å†™: {os.access(RESULTS_DIR, os.W_OK)})")
except Exception as e:
    logger.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}", exc_info=True)
    raise

# å®‰å…¨é…ç½®
MAX_FILE_SIZE = int(os.getenv("MAX_FILE_SIZE", 100 * 1024 * 1024))  # é»˜è®¤ 100MB
ALLOWED_EXTENSIONS = {'.h5ad', '.mtx', '.tsv', '.csv', '.txt', '.gz', '.tar', '.zip'}
ALLOWED_MIME_TYPES = {
    'text/csv', 'text/tab-separated-values', 'text/plain',
    'application/gzip', 'application/x-gzip',
    'application/zip', 'application/x-tar'
}

def sanitize_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    
    Args:
        filename: åŸå§‹æ–‡ä»¶å
    
    Returns:
        æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
    """
    if not filename:
        # å¦‚æœæ–‡ä»¶åä¸ºç©ºï¼Œç”Ÿæˆéšæœºåç§°
        return f"file_{secrets.token_hex(8)}"
    
    # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œå±é™©å­—ç¬¦
    filename = os.path.basename(filename)  # ç§»é™¤è·¯å¾„éƒ¨åˆ†
    filename = re.sub(r'[<>:"|?*\x00-\x1f]', '', filename)  # ç§»é™¤å±é™©å­—ç¬¦
    filename = filename.strip('. ')  # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç‚¹/ç©ºæ ¼
    
    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œç”Ÿæˆéšæœºåç§°
    if not filename:
        filename = f"file_{secrets.token_hex(8)}"
    
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦
    if len(filename) > 255:
        name, ext = os.path.splitext(filename)
        filename = name[:255-len(ext)] + ext
    
    return filename

def validate_file_path(file_path: Path, base_dir: Path) -> Path:
    """
    éªŒè¯æ–‡ä»¶è·¯å¾„æ˜¯å¦åœ¨å…è®¸çš„ç›®å½•å†…ï¼ˆé˜²æ­¢è·¯å¾„éå†ï¼‰
    
    Args:
        file_path: è¦éªŒè¯çš„è·¯å¾„
        base_dir: åŸºç¡€ç›®å½•
    
    Returns:
        è§„èŒƒåŒ–çš„å®‰å…¨è·¯å¾„
    
    Raises:
        HTTPException: å¦‚æœè·¯å¾„ä¸å®‰å…¨
    """
    try:
        # è§£æå¹¶è§„èŒƒåŒ–è·¯å¾„
        resolved_path = file_path.resolve()
        resolved_base = base_dir.resolve()
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨åŸºç¡€ç›®å½•å†…
        if not str(resolved_path).startswith(str(resolved_base)):
            raise HTTPException(
                status_code=403,
                detail="æ–‡ä»¶è·¯å¾„ä¸å®‰å…¨ï¼šä¸å…è®¸è®¿é—®åŸºç¡€ç›®å½•å¤–çš„æ–‡ä»¶"
            )
        
        return resolved_path
    except (ValueError, OSError) as e:
        raise HTTPException(
            status_code=400,
            detail=f"æ— æ•ˆçš„æ–‡ä»¶è·¯å¾„: {str(e)}"
        )

# åˆå§‹åŒ–æ–‡ä»¶æ£€æµ‹å™¨
file_inspector = FileInspector(str(UPLOAD_DIR))

# æ·»åŠ é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆç”¨äºè®¿é—®ç»“æœå›¾ç‰‡ï¼‰
from fastapi.staticfiles import StaticFiles
app.mount("/results", StaticFiles(directory="results"), name="results")
app.mount("/uploads", StaticFiles(directory="uploads"), name="uploads")

# åˆå§‹åŒ–æ™ºèƒ½ä½“
agent = None
try:
    # å°è¯•ä»å½“å‰ç›®å½•åŠ è½½é…ç½®
    config_path = Path(__file__).parent / "gibh_agent" / "config" / "settings.yaml"
    logger.info(f"ğŸ” æŸ¥æ‰¾é…ç½®æ–‡ä»¶: {config_path}")
    logger.info(f"ğŸ“‚ é…ç½®æ–‡ä»¶å­˜åœ¨: {config_path.exists()}")
    
    if not config_path.exists():
        # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–è·¯å¾„
        alt_path = Path(__file__).parent / "config" / "settings.yaml"
        logger.info(f"ğŸ” å°è¯•å¤‡ç”¨è·¯å¾„: {alt_path}")
        if alt_path.exists():
            config_path = alt_path
        else:
            config_path = "gibh_agent/config/settings.yaml"
            logger.info(f"ğŸ” ä½¿ç”¨é»˜è®¤è·¯å¾„: {config_path}")
    
    logger.info(f"ğŸ“„ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    
    # è®¾ç½® scanpy å·¥å…·çš„é»˜è®¤è¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
    import os
    scanpy_output_dir = os.path.join(os.getcwd(), "results")
    logger.info(f"ğŸ“ Scanpy è¾“å‡ºç›®å½•: {scanpy_output_dir}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = create_agent(str(config_path))
    
    # æ›´æ–° scanpy å·¥å…·çš„è¾“å‡ºç›®å½•
    if agent and hasattr(agent, 'agents') and 'rna_agent' in agent.agents:
        rna_agent = agent.agents['rna_agent']
        if hasattr(rna_agent, 'scanpy_tool'):
            rna_agent.scanpy_tool.output_dir = scanpy_output_dir
            os.makedirs(scanpy_output_dir, exist_ok=True)
            logger.info(f"âœ… å·²è®¾ç½® Scanpy è¾“å‡ºç›®å½•: {scanpy_output_dir}")
    
    logger.info("âœ… GIBH-AGENT åˆå§‹åŒ–æˆåŠŸ")
    
    # ğŸ”¥ Step 3: åˆå§‹åŒ–å·¥ä½œæµè§„åˆ’å™¨ï¼ˆéœ€è¦ agent å’Œ tool_retrieverï¼‰
    if agent and tool_retriever:
        try:
            from gibh_agent.core.planner import WorkflowPlanner
            # è·å– LLM clientï¼ˆä» agent çš„æŸä¸ªæ™ºèƒ½ä½“ä¸­è·å–ï¼‰
            llm_client = None
            if hasattr(agent, 'agents') and agent.agents:
                # å°è¯•ä»ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“è·å– LLM client
                first_agent = list(agent.agents.values())[0]
                if hasattr(first_agent, 'llm_client'):
                    llm_client = first_agent.llm_client
            
            if llm_client:
                workflow_planner = WorkflowPlanner(
                    tool_retriever=tool_retriever,
                    llm_client=llm_client
                )
                logger.info("âœ… å·¥ä½œæµè§„åˆ’å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æ— æ³•è·å– LLM clientï¼Œè·³è¿‡å·¥ä½œæµè§„åˆ’å™¨åˆå§‹åŒ–")
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµè§„åˆ’å™¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            logger.warning("   ç»§ç»­å¯åŠ¨ï¼Œä½†åŠ¨æ€è§„åˆ’åŠŸèƒ½å°†ä¸å¯ç”¨")
    
except Exception as e:
    import traceback
    error_msg = f"âŒ GIBH-AGENT åˆå§‹åŒ–å¤±è´¥: {e}"
    logger.error(error_msg, exc_info=True)
    logger.error(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
    agent = None

# ğŸ”¥ Step 2: å¯åŠ¨æ—¶åŒæ­¥å·¥å…·åˆ° ChromaDB
@app.on_event("startup")
async def sync_tools_on_startup():
    """
    å¯åŠ¨æ—¶åŒæ­¥å·¥å…·åˆ° Vector Database
    
    ç¡®ä¿ ChromaDB ä¸­çš„å·¥å…·å®šä¹‰ä¸ä»£ç ä¸­çš„ @register è£…é¥°å™¨ä¿æŒä¸€è‡´ã€‚
    """
    # ğŸ”¥ Step 4: é¦–å…ˆåŠ è½½æ‰€æœ‰å·¥å…·æ¨¡å—ï¼ˆè‡ªåŠ¨å‘ç°ï¼‰
    try:
        logger.info("ğŸ” å¯åŠ¨æ—¶è‡ªåŠ¨å‘ç°å’ŒåŠ è½½å·¥å…·æ¨¡å—...")
        load_result = load_all_tools()
        logger.info(f"âœ… å·¥å…·æ¨¡å—åŠ è½½å®Œæˆ: {load_result['loaded']} ä¸ªæˆåŠŸ, {load_result['failed']} ä¸ªå¤±è´¥")
    except Exception as e:
        logger.error(f"âŒ å·¥å…·æ¨¡å—åŠ è½½å¤±è´¥: {e}", exc_info=True)
        logger.warning("   ç»§ç»­å¯åŠ¨ï¼Œä½†å·¥å…·å¯èƒ½æœªå®Œå…¨åŠ è½½")
    
    # ç„¶ååŒæ­¥åˆ° ChromaDB
    if tool_retriever is None:
        logger.warning("âš ï¸ å·¥å…·æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å·¥å…·åŒæ­¥")
        return
    
    try:
        logger.info("ğŸ”„ å¯åŠ¨æ—¶åŒæ­¥å·¥å…·åˆ° ChromaDB...")
        synced_count = tool_retriever.sync_tools(clear_existing=True)
        logger.info(f"âœ… å·¥å…·åŒæ­¥å®Œæˆ: {synced_count} ä¸ªå·¥å…·å·²åŒæ­¥åˆ° ChromaDB")
    except Exception as e:
        logger.error(f"âŒ å·¥å…·åŒæ­¥å¤±è´¥: {e}", exc_info=True)
        logger.warning("   ç»§ç»­å¯åŠ¨ï¼Œä½†å·¥å…·æ£€ç´¢åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")


# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str = ""
    history: List[dict] = []
    uploaded_files: List[dict] = []
    workflow_data: Optional[dict] = None
    test_dataset_id: Optional[str] = None


# æ—¥å¿—ç¼“å†²åŒºï¼ˆä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
log_buffer = deque(maxlen=1000)
log_listeners: Set[asyncio.Queue] = set()


def log_handler(record):
    """æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—å‘é€åˆ°æ‰€æœ‰ç›‘å¬è€…"""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "level": record.levelname,
        "message": record.getMessage(),
        "module": record.name
    }
    log_buffer.append(log_entry)
    
    # é€šçŸ¥æ‰€æœ‰ç›‘å¬è€…
    for listener in list(log_listeners):
        try:
            listener.put_nowait(log_entry)
        except:
            # å¦‚æœé˜Ÿåˆ—å·²æ»¡æˆ–å·²å…³é—­ï¼Œç§»é™¤ç›‘å¬è€…
            log_listeners.discard(listener)


# æ·»åŠ è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨
class StreamLogHandler(logging.Handler):
    def emit(self, record):
        try:
            # ç¡®ä¿è®°å½•è¢«æ ¼å¼åŒ–
            self.format(record)
            log_handler(record)
        except Exception as e:
            # é¿å…æ—¥å¿—å¤„ç†å™¨æœ¬èº«å‡ºé”™ï¼Œä½†è®°å½•é”™è¯¯
            print(f"æ—¥å¿—å¤„ç†å™¨é”™è¯¯: {e}")


stream_handler = StreamLogHandler()
stream_handler.setLevel(logging.DEBUG)  # é™ä½çº§åˆ«ä»¥æ•è·æ›´å¤šæ—¥å¿—
stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))

# æ·»åŠ åˆ°æ ¹æ—¥å¿—è®°å½•å™¨ï¼Œæ•è·æ‰€æœ‰æ¨¡å—çš„æ—¥å¿—
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)  # é™ä½çº§åˆ«
# ç§»é™¤ç°æœ‰çš„å¤„ç†å™¨ï¼Œé¿å…é‡å¤
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)
root_logger.addHandler(stream_handler)

# ä¹Ÿæ·»åŠ åˆ°å½“å‰logger
if stream_handler not in logger.handlers:
    logger.addHandler(stream_handler)

# æµ‹è¯•æ—¥å¿—
logger.info("ğŸ“‹ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
logger.info("ğŸ” æµ‹è¯•æ—¥å¿—è¾“å‡º - è¿™åº”è¯¥å‡ºç°åœ¨å‰ç«¯")


@app.get("/", response_class=HTMLResponse)
async def index():
    """è¿”å›å‰ç«¯é¡µé¢"""
    # ğŸ”¥ ä¼˜å…ˆè¯»å–å¤–éƒ¨ HTML æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    html_file_path = Path(__file__).parent / "services" / "nginx" / "html" / "index.html"
    if html_file_path.exists():
        try:
            with open(html_file_path, "r", encoding="utf-8") as f:
                html_content = f.read()
            logger.info(f"âœ… å·²åŠ è½½å¤–éƒ¨å‰ç«¯æ–‡ä»¶: {html_file_path}")
            return HTMLResponse(content=html_content)
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–å¤–éƒ¨ HTML æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨å†…åµŒç‰ˆæœ¬: {e}")
    
    # å¦‚æœå¤–éƒ¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…åµŒçš„ HTML
    html_content = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GIBH-AGENT-V2 æµ‹è¯•ç•Œé¢</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #f5f5f5;
            padding: 20px;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            flex-direction: column;
            gap: 20px;
            height: calc(100vh - 40px);
        }
        .panel {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            display: flex;
            flex-direction: column;
        }
        .panel h2 {
            margin-bottom: 15px;
            color: #333;
            border-bottom: 2px solid #4CAF50;
            padding-bottom: 10px;
        }
        .chat-panel {
            flex: 1;
        }
        .chat-area {
            flex: 1;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            overflow-y: auto;
            overflow-x: hidden;
            margin-bottom: 15px;
            background: #fafafa;
            min-height: 300px;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        .message {
            margin-bottom: 10px;
            padding: 8px;
            border-radius: 4px;
            word-wrap: break-word;
            overflow-wrap: break-word;
            max-width: 100%;
            box-sizing: border-box;
            overflow-x: hidden;
        }
        .message.user {
            background: #e3f2fd;
            text-align: right;
        }
        .message.assistant {
            background: #f1f8e9;
        }
        .message.error {
            background: #ffebee;
            color: #c62828;
        }
        .input-area {
            display: flex;
            gap: 10px;
        }
        input[type="text"], input[type="file"] {
            flex: 1;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 14px;
        }
        button {
            padding: 10px 20px;
            background: #4CAF50;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
        }
        button:hover {
            background: #45a049;
        }
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        .file-info {
            margin-top: 10px;
            padding: 10px;
            background: #fff3cd;
            border-radius: 4px;
            font-size: 12px;
        }
        .analysis-result {
            background: #f1f8e9 !important;
            word-wrap: break-word;
            overflow-wrap: break-word;
            max-width: 100%;
            box-sizing: border-box;
            overflow-x: hidden;
        }
        .analysis-summary {
            padding: 15px;
            max-width: 100%;
            box-sizing: border-box;
            overflow-x: hidden;
        }
        .analysis-summary h3 {
            margin-top: 0;
            color: #4CAF50;
        }
        .analysis-summary h4 {
            margin-top: 15px;
            margin-bottom: 10px;
            color: #333;
            border-bottom: 1px solid #ddd;
            padding-bottom: 5px;
        }
        .analysis-summary ul {
            margin: 10px 0;
            padding-left: 20px;
            max-width: 100%;
            box-sizing: border-box;
        }
        .analysis-summary li {
            margin: 5px 0;
            word-wrap: break-word;
            overflow-wrap: break-word;
            max-width: 100%;
        }
        .qc-metrics, .steps-details, .visualization, .step-plots, .markers-table, .diagnosis {
            max-width: 100%;
            box-sizing: border-box;
            overflow-x: hidden;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        .diagnosis div {
            max-width: 100%;
            word-wrap: break-word;
            overflow-wrap: break-word;
            white-space: pre-wrap;
        }
        .visualization, .step-plots {
            max-width: 100%;
            box-sizing: border-box;
            overflow-x: hidden;
        }
        .visualization img, .step-plots img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
            margin: 10px 0;
            display: block;
        }
        .step-plots > div {
            max-width: 100%;
            box-sizing: border-box;
            overflow-x: hidden;
            word-wrap: break-word;
        }
        .markers-table {
            overflow-x: auto;
            max-width: 100%;
            box-sizing: border-box;
            margin: 10px 0;
        }
        .markers-table table {
            width: 100%;
            max-width: 100%;
            border-collapse: collapse;
            margin: 0;
            table-layout: auto;
        }
        .markers-table th, .markers-table td {
            word-wrap: break-word;
            overflow-wrap: break-word;
            max-width: 200px;
        }
        .markers-table th, .markers-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        .markers-table th {
            background: #f5f5f5;
            font-weight: bold;
        }
        .think-card {
            background: #f1f8e9 !important;
        }
        .think-process {
            margin-bottom: 10px;
        }
        .think-header {
            background: #e8f5e9;
            padding: 10px 15px;
            border-radius: 4px;
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 10px;
            user-select: none;
            transition: background 0.2s;
        }
        .think-header:hover {
            background: #c8e6c9;
        }
        .think-icon {
            font-size: 18px;
        }
        .think-title {
            flex: 1;
            font-weight: bold;
            color: #2e7d32;
        }
        .think-toggle {
            color: #666;
            font-size: 12px;
        }
        .think-content {
            margin-top: 10px;
            padding: 15px;
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 4px;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 13px;
            line-height: 1.6;
            color: #333;
            max-height: 500px;
            overflow-y: auto;
        }
        .final-answer {
            margin-top: 10px;
            padding: 10px;
        }
        .test-data-selection {
            background: #f1f8e9 !important;
            max-width: 100%;
            box-sizing: border-box;
            overflow-x: hidden;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        .test-data-selection h3 {
            margin-top: 0;
            color: #4CAF50;
            word-wrap: break-word;
        }
        .test-data-selection div[onclick] {
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            cursor: pointer;
            transition: background 0.2s;
            max-width: 100%;
            box-sizing: border-box;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        .test-data-selection div[onclick]:hover {
            background: #f5f5f5;
            border-color: #4CAF50;
        }
        .dataset-card {
            max-width: 100%;
            box-sizing: border-box;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="panel chat-panel">
            <h2>ğŸ’¬ å¯¹è¯ç•Œé¢</h2>
            <div id="chatArea" class="chat-area"></div>
            <div class="input-area">
                <input type="text" id="messageInput" placeholder="è¾“å…¥æ¶ˆæ¯æˆ–ä¸Šä¼ æ–‡ä»¶è¿›è¡Œåˆ†æ..." />
                <input type="file" id="fileInput" accept=".h5ad,.mtx,.tsv,.csv" multiple />
                <button id="sendBtn" onclick="sendMessage()">å‘é€</button>
            </div>
            <div id="fileInfo" class="file-info" style="display:none;"></div>
        </div>
    </div>

    <script>
        // æ–‡ä»¶ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆè®°ä½å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
        let uploadedFilesContext = [];
        
        // æ–‡ä»¶é€‰æ‹©ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰
        let selectedFiles = [];
        document.getElementById('fileInput').addEventListener('change', function(e) {
            const files = Array.from(e.target.files);
            if (files.length > 0) {
                selectedFiles = files;
                const fileList = files.map(f => `${f.name} (${(f.size / 1024 / 1024).toFixed(2)} MB)`).join('<br>');
                document.getElementById('fileInfo').style.display = 'block';
                document.getElementById('fileInfo').innerHTML = `ğŸ“ å·²é€‰æ‹© ${files.length} ä¸ªæ–‡ä»¶:<br>${fileList}`;
            }
        });

        // å‘é€æ¶ˆæ¯
        async function sendMessage() {
            const input = document.getElementById('messageInput');
            const message = input.value.trim();
            const btn = document.getElementById('sendBtn');
            
            if (!message && selectedFiles.length === 0) {
                alert('è¯·è¾“å…¥æ¶ˆæ¯æˆ–é€‰æ‹©æ–‡ä»¶');
                return;
            }

            btn.disabled = true;
            const fileNames = selectedFiles.length > 0 ? selectedFiles.map(f => f.name).join(', ') : '';
            addMessage('user', message || (fileNames ? `ä¸Šä¼ æ–‡ä»¶: ${fileNames}` : ''));

            try {
                let uploadedFiles = [];
                
                // å¦‚æœæœ‰æ–°é€‰æ‹©çš„æ–‡ä»¶ï¼Œå…ˆä¸Šä¼ æ‰€æœ‰æ–‡ä»¶
                if (selectedFiles.length > 0) {
                    for (const file of selectedFiles) {
                        const formData = new FormData();
                        formData.append('file', file);
                        
                        const uploadRes = await fetch('/api/upload', {
                            method: 'POST',
                            body: formData
                        });
                        
                        if (!uploadRes.ok) {
                            throw new Error(`æ–‡ä»¶ä¸Šä¼ å¤±è´¥: ${file.name}`);
                        }
                        
                        const uploadData = await uploadRes.json();
                        uploadedFiles.push(uploadData);
                        // æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
                        uploadedFilesContext.push(uploadData);
                        addMessage('assistant', `âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: ${uploadData.file_name}`);
                    }
                } else if (uploadedFilesContext.length > 0) {
                    // å¦‚æœæ²¡æœ‰æ–°æ–‡ä»¶ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„æ–‡ä»¶
                    uploadedFiles = uploadedFilesContext;
                    addMessage('assistant', `ğŸ“ ä½¿ç”¨å·²ä¸Šä¼ çš„æ–‡ä»¶: ${uploadedFiles.map(f => f.file_name).join(', ')}`);
                }

                // å‘é€èŠå¤©è¯·æ±‚
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        message: message || (uploadedFiles.length > 0 ? 'åˆ†æè¿™ä¸ªæ–‡ä»¶' : ''),
                        history: [],
                        uploaded_files: uploadedFiles
                    })
                });

                if (!response.ok) {
                    throw new Error(`è¯·æ±‚å¤±è´¥: ${response.status}`);
                }

                const contentType = response.headers.get('content-type');
                
                if (contentType && contentType.includes('application/json')) {
                    const data = await response.json();
                    
                    if (data.type === 'workflow_config') {
                        // æ‰§è¡Œå·¥ä½œæµ
                        addMessage('assistant', 'ğŸš€ å¼€å§‹æ‰§è¡Œåˆ†ææµç¨‹...');
                        await executeWorkflow(data.workflow_data, data.file_paths);
                    } else {
                        addMessage('assistant', JSON.stringify(data, null, 2));
                    }
                } else {
                    // æµå¼å“åº”ï¼ˆæ”¯æŒ think è¿‡ç¨‹æå–ï¼‰
                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();
                    let fullText = '';
                    let thinkBuffer = '';
                    let isThinking = false;
                    let hasThinkBlock = false;
                    let finalAnswer = '';
                    let thinkStartIndex = -1;
                    let datasetsJson = null;
                    
                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;
                        
                        const chunk = decoder.decode(value);
                        fullText += chunk;
                        
                        // æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°æ®é›† JSONï¼ˆæµ‹è¯•æ•°æ®é€‰æ‹©å“åº”ï¼‰
                        // ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œä½†éœ€è¦åŒ¹é…å¤šè¡Œï¼ˆå› ä¸º JSON å¯èƒ½è·¨è¡Œï¼‰
                        const datasetsMatch = fullText.match(/<!-- DATASETS_JSON: (\[[\s\S]*?\]) -->/);
                        if (datasetsMatch && !datasetsJson) {
                            try {
                                // JSON ä¸­çš„æ¢è¡Œç¬¦å·²è¢«æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œç›´æ¥è§£æå³å¯
                                datasetsJson = JSON.parse(datasetsMatch[1]);
                            } catch (e) {
                                console.error('è§£ææ•°æ®é›†JSONå¤±è´¥:', e, datasetsMatch[1].substring(0, 100));
                            }
                        }
                        
                        // æ£€æµ‹ think å¼€å§‹æ ‡ç­¾ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                        const thinkStartPatterns = [
                            /<think>/i,
                            /<think>/i,
                            /<reasoning>/i,
                            /<thought>/i,
                            /<thinking>/i
                        ];
                        
                        for (const pattern of thinkStartPatterns) {
                            const match = fullText.match(pattern);
                            if (match && !hasThinkBlock) {
                            isThinking = true;
                            hasThinkBlock = true;
                                thinkStartIndex = match.index + match[0].length;
                            // åˆ›å»º think å¡ç‰‡
                            if (!document.querySelector('.think-card:last-child .think-process')) {
                                createThinkCard();
                                }
                                break;
                            }
                        }
                        
                        // æ£€æµ‹ think ç»“æŸæ ‡ç­¾
                        const thinkEndPatterns = [
                            /<\/think>/i,
                            /<\/redacted_reasoning>/i,
                            /<\/reasoning>/i,
                            /<\/thought>/i,
                            /<\/thinking>/i
                        ];
                        
                        for (const pattern of thinkEndPatterns) {
                            const match = fullText.match(pattern);
                            if (match && isThinking) {
                                // æå– think å†…å®¹
                                thinkBuffer = fullText.substring(thinkStartIndex, match.index);
                            updateThinkContent(thinkBuffer);
                            isThinking = false;
                            
                                // æå– think æ ‡ç­¾ä¹‹åçš„å†…å®¹ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
                                const afterThinkIndex = match.index + match[0].length;
                                finalAnswer = fullText.substring(afterThinkIndex);
                                if (finalAnswer.trim()) {
                                    updateLastMessage('assistant', finalAnswer.trim());
                            }
                                break;
                        }
                        }
                        
                        // æ›´æ–°æ˜¾ç¤º
                        if (isThinking) {
                            // åœ¨ think å—ä¸­ï¼Œæ›´æ–° think å†…å®¹
                            if (thinkStartIndex >= 0) {
                                thinkBuffer = fullText.substring(thinkStartIndex);
                            updateThinkContent(thinkBuffer);
                            }
                        } else if (hasThinkBlock && !isThinking) {
                            // think å—å·²ç»“æŸï¼Œæ›´æ–°æœ€ç»ˆç­”æ¡ˆ
                            if (finalAnswer) {
                                updateLastMessage('assistant', finalAnswer);
                            }
                        } else {
                            // æ²¡æœ‰ think å—ï¼Œç›´æ¥æ›´æ–°æ¶ˆæ¯
                            // åœ¨æµå¼å“åº”è¿‡ç¨‹ä¸­ï¼Œå…ˆæ˜¾ç¤ºæ–‡æœ¬å†…å®¹ï¼ˆå»é™¤ JSON æ³¨é‡Šï¼‰
                            const cleanText = fullText.replace(/<!-- DATASETS_JSON: \[[\s\S]*?\] -->/g, '').trim();
                            updateLastMessage('assistant', cleanText);
                        }
                    }
                    
                    // æµå¼å“åº”ç»“æŸåï¼Œå¦‚æœæ£€æµ‹åˆ°æ•°æ®é›†ä¿¡æ¯ï¼Œæ›¿æ¢ä¸ºé€‰æ‹©ç•Œé¢
                    if (datasetsJson && datasetsJson.length > 0) {
                        // ç§»é™¤ JSON æ³¨é‡Šï¼Œåªä¿ç•™ç”¨æˆ·å‹å¥½çš„æ–‡æœ¬
                        const cleanText = fullText.replace(/<!-- DATASETS_JSON: \[[\s\S]*?\] -->/g, '').trim();
                        // ç§»é™¤ä¹‹å‰çš„æ™®é€šæ¶ˆæ¯
                        const lastMessage = chatArea.querySelector('.message.assistant:last-child');
                        if (lastMessage && !lastMessage.classList.contains('test-data-selection')) {
                            lastMessage.remove();
                        }
                        // æ˜¾ç¤ºé€‰æ‹©ç•Œé¢
                        if (!document.querySelector('.test-data-selection')) {
                            displayTestDataSelection(cleanText, datasetsJson);
                        }
                    }
                }
            } catch (error) {
                addMessage('error', `âŒ é”™è¯¯: ${error.message}`);
                console.error(error);
            } finally {
                btn.disabled = false;
                input.value = '';
                // ä¸æ¸…ç©º selectedFilesï¼Œä¿ç•™æ–‡ä»¶é€‰æ‹©
                // ä½†æ¸…ç©ºæ–‡ä»¶è¾“å…¥æ¡†ï¼Œå…è®¸ç”¨æˆ·é‡æ–°é€‰æ‹©
                document.getElementById('fileInput').value = '';
                // å¦‚æœæœ‰ä¸Šä¸‹æ–‡æ–‡ä»¶ï¼Œæ˜¾ç¤ºæç¤º
                if (uploadedFilesContext.length > 0) {
                    document.getElementById('fileInfo').style.display = 'block';
                    document.getElementById('fileInfo').innerHTML = `ğŸ“ å·²ä¸Šä¼  ${uploadedFilesContext.length} ä¸ªæ–‡ä»¶ï¼Œå¯ç›´æ¥è¾“å…¥éœ€æ±‚ç»§ç»­åˆ†æ`;
                } else {
                    document.getElementById('fileInfo').style.display = 'none';
                }
            }
        }

        // æ‰§è¡Œå·¥ä½œæµ
        async function executeWorkflow(workflowData, filePaths) {
            try {
                const response = await fetch('/api/execute', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        workflow_data: workflowData,
                        file_paths: filePaths
                    })
                });

                const data = await response.json();
                
                if (data.status === 'success') {
                    // ç¾åŒ–æ˜¾ç¤ºåˆ†æç»“æœ
                    displayAnalysisResult(data);
                } else {
                    addMessage('error', `âŒ åˆ†æå¤±è´¥: ${data.error || 'æœªçŸ¥é”™è¯¯'}`);
                }
            } catch (error) {
                addMessage('error', `âŒ æ‰§è¡Œé”™è¯¯: ${error.message}`);
            }
        }
        
        // ç¾åŒ–æ˜¾ç¤ºåˆ†æç»“æœ
        function displayAnalysisResult(data) {
            const resultDiv = document.createElement('div');
            resultDiv.className = 'message assistant analysis-result';
            
            let html = '<div class="analysis-summary">';
            html += '<h3>âœ… åˆ†æå®Œæˆ</h3>';
            
            // QC æŒ‡æ ‡
            if (data.qc_metrics) {
                html += '<div class="qc-metrics">';
                html += '<h4>ğŸ“Š è´¨é‡æ§åˆ¶æŒ‡æ ‡</h4>';
                html += '<ul>';
                html += `<li>åŸå§‹ç»†èƒæ•°: <strong>${data.qc_metrics.raw_cells || 'N/A'}</strong></li>`;
                html += `<li>åŸå§‹åŸºå› æ•°: <strong>${data.qc_metrics.raw_genes || 'N/A'}</strong></li>`;
                if (data.qc_metrics.filtered_cells) {
                    html += `<li>è¿‡æ»¤åç»†èƒæ•°: <strong>${data.qc_metrics.filtered_cells}</strong></li>`;
                }
                if (data.qc_metrics.filtered_genes) {
                    html += `<li>è¿‡æ»¤ååŸºå› æ•°: <strong>${data.qc_metrics.filtered_genes}</strong></li>`;
                }
                html += '</ul>';
                html += '</div>';
            }
            
            // æ­¥éª¤è¯¦æƒ…
            if (data.steps_details && data.steps_details.length > 0) {
                html += '<div class="steps-details">';
                html += '<h4>ğŸ“‹ æ‰§è¡Œæ­¥éª¤</h4>';
                html += '<ul>';
                data.steps_details.forEach(step => {
                    const stepName = step.name || step.tool_id || 'æœªçŸ¥æ­¥éª¤';
                    const stepSummary = step.summary || 'å®Œæˆ';
                    html += `<li><strong>${stepName}</strong>: ${stepSummary}</li>`;
                });
                html += '</ul>';
                html += '</div>';
            }
            
            // å¯è§†åŒ–å›¾ç‰‡ï¼ˆåªæ˜¾ç¤ºæ­¥éª¤çš„å›¾ç‰‡ï¼Œé¿å…ä¸ final_plot é‡å¤ï¼‰
            if (data.steps_details) {
                const plotSteps = data.steps_details.filter(s => s.plot);
                if (plotSteps.length > 0) {
                    html += '<div class="step-plots">';
                    html += '<h4>ğŸ“ˆ å¯è§†åŒ–ç»“æœ</h4>';
                    plotSteps.forEach(step => {
                        let plotUrl = step.plot;
                        if (!plotUrl.startsWith('http') && !plotUrl.startsWith('/')) {
                            // å¦‚æœè·¯å¾„åŒ…å« resultsï¼Œç›´æ¥ä½¿ç”¨
                            if (plotUrl.includes('results/')) {
                                plotUrl = '/' + plotUrl;
                            } else {
                                plotUrl = '/results/' + plotUrl;
                            }
                        }
                        html += `<div style="margin: 10px 0;">`;
                        html += `<strong>${step.name || step.tool_id}</strong><br>`;
                        html += `<img src="${plotUrl}" alt="${step.name}" style="max-width: 100%; border-radius: 4px; margin-top: 10px;" onerror="this.style.display='none';">`;
                        html += `</div>`;
                    });
                    html += '</div>';
                } else if (data.final_plot) {
                    // å¦‚æœæ²¡æœ‰æ­¥éª¤å›¾ç‰‡ï¼Œä½¿ç”¨ final_plotï¼ˆå‘åå…¼å®¹ï¼‰
                    html += '<div class="visualization">';
                    html += '<h4>ğŸ“ˆ å¯è§†åŒ–ç»“æœ</h4>';
                    let plotUrl = data.final_plot;
                    if (!plotUrl.startsWith('http') && !plotUrl.startsWith('/')) {
                        if (plotUrl.includes('results/')) {
                            plotUrl = '/' + plotUrl;
                        } else {
                            plotUrl = '/results/' + plotUrl;
                        }
                    }
                    html += `<img src="${plotUrl}" alt="Visualization" style="max-width: 100%; border-radius: 4px; margin-top: 10px;" onerror="this.style.display='none'; this.nextElementSibling.style.display='block';"><p style="display:none; color: #999;">å›¾ç‰‡åŠ è½½å¤±è´¥: ${plotUrl}</p>`;
                    html += '</div>';
                }
            } else if (data.final_plot) {
                // å¦‚æœæ²¡æœ‰ steps_detailsï¼Œä½¿ç”¨ final_plot
                html += '<div class="visualization">';
                html += '<h4>ğŸ“ˆ å¯è§†åŒ–ç»“æœ</h4>';
                let plotUrl = data.final_plot;
                if (!plotUrl.startsWith('http') && !plotUrl.startsWith('/')) {
                    if (plotUrl.includes('results/')) {
                        plotUrl = '/' + plotUrl;
                    } else {
                        plotUrl = '/results/' + plotUrl;
                    }
                }
                html += `<img src="${plotUrl}" alt="Visualization" style="max-width: 100%; border-radius: 4px; margin-top: 10px;" onerror="this.style.display='none'; this.nextElementSibling.style.display='block';"><p style="display:none; color: #999;">å›¾ç‰‡åŠ è½½å¤±è´¥: ${plotUrl}</p>`;
                html += '</div>';
            }
            
            // Marker åŸºå› è¡¨æ ¼ï¼ˆå¦‚æœæœ‰ï¼‰
            const markersStep = data.steps_details?.find(s => s.name === 'local_markers' || s.tool_id === 'local_markers');
            if (markersStep && markersStep.details) {
                html += '<div class="markers-table">';
                html += '<h4>ğŸ§¬ Marker åŸºå› </h4>';
                // ç›´æ¥æ˜¾ç¤º HTML è¡¨æ ¼
                html += markersStep.details;
                html += '</div>';
            }
            
            // è¯Šæ–­ä¿¡æ¯
            if (data.diagnosis) {
                html += '<div class="diagnosis">';
                html += '<h4>ğŸ’¡ åˆ†æè¯Šæ–­</h4>';
                html += `<div style="white-space: pre-wrap;">${data.diagnosis}</div>`;
                html += '</div>';
            }
            
            html += '</div>';
            resultDiv.innerHTML = html;
            
            const chatArea = document.getElementById('chatArea');
            chatArea.appendChild(resultDiv);
            chatArea.scrollTop = chatArea.scrollHeight;
        }

        // æ·»åŠ æ¶ˆæ¯
        function addMessage(role, content) {
            const chatArea = document.getElementById('chatArea');
            const msgDiv = document.createElement('div');
            msgDiv.className = `message ${role}`;
            msgDiv.textContent = content;
            chatArea.appendChild(msgDiv);
            chatArea.scrollTop = chatArea.scrollHeight;
        }

        // æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯
        function updateLastMessage(role, content) {
            const chatArea = document.getElementById('chatArea');
            const messages = chatArea.querySelectorAll('.message');
            if (messages.length > 0 && messages[messages.length - 1].classList.contains(role)) {
                const lastMsg = messages[messages.length - 1];
                // å¦‚æœå·²ç»æœ‰ think å¡ç‰‡ï¼Œæ›´æ–°æœ€ç»ˆç­”æ¡ˆéƒ¨åˆ†
                const finalAnswerDiv = lastMsg.querySelector('.final-answer');
                if (finalAnswerDiv) {
                    finalAnswerDiv.textContent = content;
                } else {
                    lastMsg.textContent = content;
                }
            } else {
                addMessage(role, content);
            }
            chatArea.scrollTop = chatArea.scrollHeight;
        }
        
        // åˆ›å»º think å¡ç‰‡
        function createThinkCard() {
            const chatArea = document.getElementById('chatArea');
            const thinkCard = document.createElement('div');
            thinkCard.className = 'message assistant think-card';
            thinkCard.innerHTML = `
                <div class="think-process">
                    <div class="think-header" onclick="toggleThink(this)">
                        <span class="think-icon">ğŸ¤”</span>
                        <span class="think-title">æ€è€ƒè¿‡ç¨‹</span>
                        <span class="think-toggle">â–¼</span>
                    </div>
                    <div class="think-content" style="display: none;"></div>
                </div>
                <div class="final-answer"></div>
            `;
            chatArea.appendChild(thinkCard);
            chatArea.scrollTop = chatArea.scrollHeight;
        }
        
        // æ›´æ–° think å†…å®¹
        function updateThinkContent(content) {
            const chatArea = document.getElementById('chatArea');
            const thinkCards = chatArea.querySelectorAll('.think-card');
            if (thinkCards.length > 0) {
                const lastCard = thinkCards[thinkCards.length - 1];
                const thinkContentDiv = lastCard.querySelector('.think-content');
                if (thinkContentDiv) {
                    thinkContentDiv.textContent = content;
                }
            }
        }
        
        // åˆ‡æ¢ think å¡ç‰‡å±•å¼€/æŠ˜å 
        function toggleThink(header) {
            const thinkCard = header.closest('.think-process');
            const content = thinkCard.querySelector('.think-content');
            const toggle = header.querySelector('.think-toggle');
            
            if (content.style.display === 'none') {
                content.style.display = 'block';
                toggle.textContent = 'â–²';
            } else {
                content.style.display = 'none';
                toggle.textContent = 'â–¼';
            }
        }
        
        // æ˜¾ç¤ºæµ‹è¯•æ•°æ®é€‰æ‹©ç•Œé¢
        function displayTestDataSelection(messageText, datasets) {
            const chatArea = document.getElementById('chatArea');
            
            // æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¾ç¤ºè¿‡é€‰æ‹©ç•Œé¢
            const existing = document.querySelector('.test-data-selection');
            if (existing) {
                // å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°å®ƒè€Œä¸æ˜¯åˆ›å»ºæ–°çš„
                return;
            }
            
            // ç§»é™¤ä¹‹å‰çš„æ™®é€šæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            const lastMessage = chatArea.querySelector('.message.assistant:last-child');
            if (lastMessage && !lastMessage.classList.contains('test-data-selection')) {
                lastMessage.remove();
            }
            
            const selectionDiv = document.createElement('div');
            selectionDiv.className = 'message assistant test-data-selection';
            
            let html = '<div style="padding: 15px; max-width: 100%; box-sizing: border-box;">';
            html += '<h3 style="margin-top: 0; color: #4CAF50; margin-bottom: 10px; word-wrap: break-word;">ğŸ“Š é€‰æ‹©æµ‹è¯•æ•°æ®é›†</h3>';
            
            // æ˜¾ç¤ºæ¶ˆæ¯æ–‡æœ¬ï¼ˆå»é™¤æ•°æ®é›†åˆ—è¡¨éƒ¨åˆ†ï¼‰
            const lines = messageText.split('\\n');
            const messageLine = lines.find(line => line.includes('æ£€æµ‹åˆ°') || line.includes('è¯·é€‰æ‹©'));
            if (messageLine) {
                html += '<p style="margin-bottom: 15px; color: #333; word-wrap: break-word; max-width: 100%;">' + messageLine + '</p>';
            }
            
            // æ˜¾ç¤ºæ•°æ®é›†é€‰æ‹©å¡ç‰‡
            html += '<div style="display: flex; flex-direction: column; gap: 10px; margin-bottom: 15px; max-width: 100%; box-sizing: border-box;">';
            datasets.forEach(dataset => {
                html += `<div class="dataset-card" 
                             style="border: 2px solid #ddd; border-radius: 8px; padding: 15px; cursor: pointer; transition: all 0.2s; background: white; max-width: 100%; box-sizing: border-box; word-wrap: break-word; overflow-wrap: break-word;" 
                             onmouseover="this.style.borderColor='#4CAF50'; this.style.boxShadow='0 2px 8px rgba(76,175,80,0.2)'" 
                             onmouseout="this.style.borderColor='#ddd'; this.style.boxShadow='none'"
                             onclick="selectTestDataset('${dataset.id}', '${dataset.name}')">`;
                html += `<div style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px; max-width: 100%; flex-wrap: wrap;">`;
                html += `<span style="font-size: 24px; flex-shrink: 0;">ğŸ“¦</span>`;
                html += `<strong style="color: #4CAF50; font-size: 18px; word-wrap: break-word; flex: 1; min-width: 0;">${dataset.name}</strong>`;
                html += `</div>`;
                html += `<p style="margin: 0; color: #666; font-size: 14px; word-wrap: break-word; max-width: 100%;">${dataset.description}</p>`;
                html += `<div style="margin-top: 8px; font-size: 12px; color: #999; word-wrap: break-word; max-width: 100%;">ID: <code style="word-break: break-all;">${dataset.id}</code></div>`;
                html += '</div>';
            });
            html += '</div>';
            
            html += '<p style="margin-top: 10px; color: #666; font-size: 14px; font-style: italic; word-wrap: break-word; max-width: 100%;">ğŸ’¡ ç‚¹å‡»ä¸Šé¢çš„æ•°æ®é›†å¡ç‰‡é€‰æ‹©ï¼Œæˆ–ä¸Šä¼ æ‚¨è‡ªå·±çš„æ•°æ®æ–‡ä»¶ã€‚</p>';
            html += '</div>';
            
            selectionDiv.innerHTML = html;
            chatArea.appendChild(selectionDiv);
            chatArea.scrollTop = chatArea.scrollHeight;
        }
        
        // é€‰æ‹©æµ‹è¯•æ•°æ®é›†
        async function selectTestDataset(datasetId, datasetName) {
            addMessage('user', `ä½¿ç”¨æµ‹è¯•æ•°æ®é›†: ${datasetName} (${datasetId})`);
            addMessage('assistant', `æ­£åœ¨ä½¿ç”¨æµ‹è¯•æ•°æ®é›† ${datasetName} æ‰§è¡Œåˆ†æ...`);
            
            try {
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        message: `ä½¿ç”¨æµ‹è¯•æ•°æ®é›† ${datasetId} æ‰§è¡Œå®Œæ•´çš„å•ç»†èƒè½¬å½•ç»„åˆ†ææµç¨‹`,
                        history: [],
                        uploaded_files: [],
                        test_dataset_id: datasetId
                    })
                });
                
                // å¤„ç†å“åº”
                const contentType = response.headers.get('content-type');
                if (contentType && contentType.includes('application/json')) {
                    const data = await response.json();
                    if (data.type === 'workflow_config') {
                        addMessage('assistant', 'ğŸš€ å¼€å§‹æ‰§è¡Œåˆ†ææµç¨‹...');
                        await executeWorkflow(data.workflow_data, data.file_paths || []);
                    } else {
                        addMessage('assistant', JSON.stringify(data, null, 2));
                    }
                } else {
                    // æµå¼å“åº”
                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();
                    let fullText = '';
                    
                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;
                        const chunk = decoder.decode(value);
                        fullText += chunk;
                        updateLastMessage('assistant', fullText);
                    }
                }
            } catch (error) {
                addMessage('error', `âŒ é”™è¯¯: ${error.message}`);
                console.error(error);
            }
        }
        
        // å…¨å±€å‡½æ•°ï¼Œä¾› HTML è°ƒç”¨
        window.toggleThink = toggleThink;
        window.selectTestDataset = selectTestDataset;

        // æ—¥å¿—åŠŸèƒ½å·²ç§»é™¤

        // å›è½¦å‘é€
        document.getElementById('messageInput').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });

        // åˆå§‹åŒ–å®Œæˆ
    </script>
</body>
</html>
    """
    return HTMLResponse(content=html_content)


@app.post("/api/upload")
async def upload_file(files: List[UploadFile] = File(...)):
    """æ–‡ä»¶ä¸Šä¼ æ¥å£ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ä¸Šä¼ ï¼‰"""
    try:
        if not files or len(files) == 0:
            raise HTTPException(status_code=400, detail="No files provided")
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡
        if len(files) > 20:
            raise HTTPException(status_code=400, detail="ä¸€æ¬¡æœ€å¤šä¸Šä¼ 20ä¸ªæ–‡ä»¶")
        
        logger.info(f"ğŸ“¤ æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ : {len(files)} ä¸ªæ–‡ä»¶")
        
        # æ£€æµ‹æ˜¯å¦æ˜¯10x Genomicsæ–‡ä»¶ï¼ˆmatrix.mtx, barcodes.tsv, features.tsvï¼‰
        is_10x_data = False
        tenx_files = []
        other_files = []
        
        for file in files:
            # ğŸ”’ å®‰å…¨ï¼šæ¸…ç†æ–‡ä»¶å
            if not file.filename:
                raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
            
            original_filename = file.filename
            safe_filename = sanitize_filename(original_filename)
            
            # ğŸ”’ å®‰å…¨ï¼šéªŒè¯æ–‡ä»¶æ‰©å±•å
            file_ext = Path(safe_filename).suffix.lower()
            if file_ext and file_ext not in ALLOWED_EXTENSIONS:
                raise HTTPException(
                    status_code=400,
                    detail=f"ä¸å…è®¸çš„æ–‡ä»¶ç±»å‹: {file_ext}ã€‚å…è®¸çš„ç±»å‹: {', '.join(ALLOWED_EXTENSIONS)}"
                )
            
            # æ›´æ–°æ–‡ä»¶åä¸ºå®‰å…¨ç‰ˆæœ¬
            file.filename = safe_filename
            
            filename_lower = safe_filename.lower()
            if any(pattern in filename_lower for pattern in ['matrix.mtx', 'barcodes.tsv', 'features.tsv']):
                is_10x_data = True
                tenx_files.append(file)
            else:
                other_files.append(file)
        
        uploaded_results = []
        
        # å¦‚æœæ˜¯10xæ•°æ®ï¼Œåˆ›å»ºå­ç›®å½•å¹¶ä¿å­˜
        if is_10x_data and len(tenx_files) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯matrix + barcodes/featuresï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            tenx_dir = UPLOAD_DIR / f"10x_data_{timestamp}"
            tenx_dir.mkdir(exist_ok=True)
            
            logger.info(f"ğŸ“ æ£€æµ‹åˆ°10xæ•°æ®ï¼Œåˆ›å»ºç›®å½•: {tenx_dir}")
            
            # ä¿å­˜10xæ–‡ä»¶åˆ°å­ç›®å½•
            for file in tenx_files:
                # ğŸ”’ å®‰å…¨ï¼šéªŒè¯æ–‡ä»¶è·¯å¾„
                file_path = tenx_dir / file.filename
                try:
                    file_path = validate_file_path(file_path, UPLOAD_DIR)
                except HTTPException as e:
                    logger.error(f"âŒ æ–‡ä»¶è·¯å¾„éªŒè¯å¤±è´¥: {file.filename} -> {e.detail}")
                    raise
                
                # ğŸ”’ å®‰å…¨ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°
                content = await file.read()
                if len(content) > MAX_FILE_SIZE:
                    raise HTTPException(
                        status_code=413,
                        detail=f"æ–‡ä»¶ {file.filename} è¶…è¿‡æœ€å¤§å¤§å°é™åˆ¶ ({MAX_FILE_SIZE / 1024 / 1024:.0f}MB)"
                    )
                
                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
                file_path.parent.mkdir(parents=True, exist_ok=True)
                
                try:
                    with open(file_path, "wb") as f:
                        f.write(content)
                except PermissionError as e:
                    logger.error(f"âŒ æ–‡ä»¶å†™å…¥æƒé™é”™è¯¯: {file_path} -> {e}")
                    raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼šæƒé™ä¸è¶³ ({file.filename})")
                except OSError as e:
                    logger.error(f"âŒ æ–‡ä»¶å†™å…¥ç³»ç»Ÿé”™è¯¯: {file_path} -> {e}")
                    raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)} ({file.filename})")
                
                logger.info(f"âœ… 10xæ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
                
                # ç”Ÿæˆå…ƒæ•°æ®
                try:
                    metadata = file_inspector.generate_metadata(str(file_path.relative_to(UPLOAD_DIR)))
                except Exception as e:
                    logger.warning(f"âš ï¸ ç”Ÿæˆæ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {e}")
                    metadata = None
                
                uploaded_results.append({
                    "file_id": str(tenx_dir.relative_to(UPLOAD_DIR)),
                    "file_name": file.filename,
                    "file_path": str(file_path),
                    "file_size": len(content),
                    "metadata": metadata,
                    "is_10x": True,
                    "group_dir": str(tenx_dir.relative_to(UPLOAD_DIR))
                })
            
            # è¿”å›10xç›®å½•è·¯å¾„ï¼ˆè€Œä¸æ˜¯å•ä¸ªæ–‡ä»¶è·¯å¾„ï¼‰
            file_paths = [str(tenx_dir.relative_to(UPLOAD_DIR))]
            return {
                "status": "success",
                "is_10x_data": True,
                "group_dir": str(tenx_dir.relative_to(UPLOAD_DIR)),
                "files": uploaded_results,
                "file_paths": file_paths,  # ğŸ”¥ æ·»åŠ  file_paths æ•°ç»„
                "message": f"10xæ•°æ®å·²ä¿å­˜åˆ°: {tenx_dir.relative_to(UPLOAD_DIR)}"
            }
        
        # å¤„ç†å…¶ä»–æ–‡ä»¶ï¼ˆé10xæˆ–å•ç‹¬çš„10xæ–‡ä»¶ï¼‰
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœåªæœ‰1ä¸ª10xæ–‡ä»¶ï¼Œä¹Ÿå½“ä½œæ™®é€šæ–‡ä»¶å¤„ç†
        files_to_process = other_files
        if is_10x_data and len(tenx_files) == 1:
            # åªæœ‰1ä¸ª10xæ–‡ä»¶ï¼Œå½“ä½œæ™®é€šæ–‡ä»¶å¤„ç†
            logger.info(f"âš ï¸ åªæœ‰1ä¸ª10xæ–‡ä»¶ï¼Œå½“ä½œæ™®é€šæ–‡ä»¶å¤„ç†: {tenx_files[0].filename}")
            files_to_process = other_files + tenx_files
        elif not is_10x_data:
            # ä¸æ˜¯10xæ•°æ®ï¼Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
            files_to_process = other_files + tenx_files
        
        for file in files_to_process:
            # ğŸ”’ å®‰å…¨ï¼šéªŒè¯æ–‡ä»¶è·¯å¾„
            file_path = UPLOAD_DIR / file.filename
            try:
                file_path = validate_file_path(file_path, UPLOAD_DIR)
            except HTTPException as e:
                logger.error(f"âŒ æ–‡ä»¶è·¯å¾„éªŒè¯å¤±è´¥: {file.filename} -> {e.detail}")
                raise
            
            # ğŸ”’ å®‰å…¨ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°
            content = await file.read()
            if len(content) > MAX_FILE_SIZE:
                raise HTTPException(
                    status_code=413,
                    detail=f"æ–‡ä»¶ {file.filename} è¶…è¿‡æœ€å¤§å¤§å°é™åˆ¶ ({MAX_FILE_SIZE / 1024 / 1024:.0f}MB)"
                )
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                with open(file_path, "wb") as f:
                    f.write(content)
            except PermissionError as e:
                logger.error(f"âŒ æ–‡ä»¶å†™å…¥æƒé™é”™è¯¯: {file_path} -> {e}")
                raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼šæƒé™ä¸è¶³ ({file.filename})")
            except OSError as e:
                logger.error(f"âŒ æ–‡ä»¶å†™å…¥ç³»ç»Ÿé”™è¯¯: {file_path} -> {e}")
                raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)} ({file.filename})")
            
            logger.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
            
            # ç”Ÿæˆå…ƒæ•°æ®
            try:
                metadata = file_inspector.generate_metadata(file.filename)
                if metadata:
                    logger.info(f"ğŸ“Š æ–‡ä»¶å…ƒæ•°æ®å·²ç”Ÿæˆ: {metadata.get('file_type', 'unknown')}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç”Ÿæˆæ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {e}")
                metadata = None
            
            uploaded_results.append({
                "file_id": file.filename,
                "file_name": file.filename,
                "file_path": str(file_path),
                "file_size": len(content),
                "metadata": metadata,
                "is_10x": False
            })
        
        # ğŸ”¥ ç»Ÿä¸€è¿”å›æ ¼å¼ï¼šå§‹ç»ˆè¿”å› file_paths æ•°ç»„å’Œ file_info æ•°ç»„ï¼ˆç”¨äºå‰ç«¯å‘é€èŠå¤©è¯·æ±‚ï¼‰
        # æ³¨æ„ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œå› ä¸ºå‰ç«¯éœ€è¦ç›¸å¯¹äº UPLOAD_DIR çš„è·¯å¾„
        file_paths = []
        file_info = []
        for result in uploaded_results:
            # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº UPLOAD_DIRï¼‰
            file_path_abs = result["file_path"]
            if isinstance(file_path_abs, str) and str(UPLOAD_DIR) in file_path_abs:
                # æå–ç›¸å¯¹è·¯å¾„
                rel_path = str(Path(file_path_abs).relative_to(UPLOAD_DIR))
            else:
                rel_path = result["file_id"]  # å›é€€åˆ° file_id
            file_paths.append(rel_path)
            
            # æ„å»º file_info æ¡ç›®
            file_info.append({
                "name": result["file_name"],
                "size": result["file_size"],
                "path": rel_path  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            })
        
        # ğŸ”¥ ç»Ÿä¸€è¿”å›æ ¼å¼ï¼šå§‹ç»ˆè¿”å›ä¸€è‡´çš„ JSON ç»“æ„
        response = {
            "status": "success",
            "file_paths": file_paths,  # æ–‡ä»¶è·¯å¾„æ•°ç»„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
            "file_info": file_info,    # æ–‡ä»¶ä¿¡æ¯æ•°ç»„
            "count": len(uploaded_results)
        }
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œæ·»åŠ å•ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå‘åå…¼å®¹ï¼‰
        if len(uploaded_results) == 1:
            result = uploaded_results[0]
            response.update({
                "file_id": result["file_id"],
                "file_name": result["file_name"],
                "file_path": result["file_path"],  # ç»å¯¹è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
                "file_size": result["file_size"],
                "metadata": result["metadata"]
            })
        else:
            # å¤šä¸ªæ–‡ä»¶æ—¶ï¼Œæ·»åŠ  files æ•°ç»„ï¼ˆå‘åå…¼å®¹ï¼‰
            response["files"] = uploaded_results
        
        return response
        
    except HTTPException:
        # é‡æ–°æŠ›å‡º HTTP å¼‚å¸¸ï¼ˆä¿æŒçŠ¶æ€ç å’Œè¯¦ç»†ä¿¡æ¯ï¼‰
        raise
    except Exception as e:
        # ğŸ”§ æ”¹è¿›ï¼šè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œä½†è¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
        import traceback
        error_detail = f"{type(e).__name__}: {str(e)}"
        logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {error_detail}", exc_info=True)
        logger.error(f"è¯¦ç»†å †æ ˆ:\n{traceback.format_exc()}")
        
        # æ ¹æ®é”™è¯¯ç±»å‹è¿”å›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        if "Permission" in error_detail or "permission" in error_detail.lower():
            raise HTTPException(status_code=500, detail="æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼šæƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        elif "No such file" in error_detail or "directory" in error_detail.lower():
            raise HTTPException(status_code=500, detail="æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼šç›®å½•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        elif "disk" in error_detail.lower() or "space" in error_detail.lower():
            raise HTTPException(status_code=500, detail="æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼šç£ç›˜ç©ºé—´ä¸è¶³")
        else:
            # å¼€å‘ç¯å¢ƒè¿”å›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œç”Ÿäº§ç¯å¢ƒè¿”å›é€šç”¨é”™è¯¯
            import os
            if os.getenv("DEBUG", "false").lower() == "true":
                raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {error_detail}")
            else:
                raise HTTPException(status_code=500, detail="æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")


@app.post("/api/chat")
async def chat_endpoint(req: ChatRequest):
    """èŠå¤©æ¥å£"""
    # #region debug log - entry point
    import json
    import traceback
    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å®¹å™¨å†…çš„æ—¥å¿—è·¯å¾„ï¼ˆç»Ÿä¸€ä½¿ç”¨ /app/debug.logï¼‰
    debug_log_path = Path("/app/debug.log")
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
        with open(debug_log_path, 'a') as f:
            f.write(json.dumps({"location":"server.py:1112","message":"chat_endpoint entry","data":{"agent_is_none":agent is None,"req_message":req.message[:100] if req.message else None},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"ENTRY"})+"\n")
    except Exception as log_err:
        pass  # å³ä½¿æ—¥å¿—å†™å…¥å¤±è´¥ä¹Ÿä¸å½±å“ä¸»æµç¨‹
    # #endregion
    
    if not agent:
        error_msg = "æ™ºèƒ½ä½“æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œæ—¥å¿—ã€‚å¯èƒ½çš„åŸå› ï¼š1) é…ç½®æ–‡ä»¶è·¯å¾„é”™è¯¯ 2) API Keyæœªè®¾ç½® 3) ä¾èµ–åŒ…ç¼ºå¤±"
        logger.error(error_msg)
        logger.error("è¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—ä¸­çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        return JSONResponse(
            status_code=500,
            content={
                "type": "error",
                "error": error_msg,
                "message": "æ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯"
            }
        )
    
    try:
        logger.info(f"ğŸ’¬ æ”¶åˆ°èŠå¤©è¯·æ±‚: {req.message}")
        logger.info(f"ğŸ“ ä¸Šä¼ æ–‡ä»¶æ•°: {len(req.uploaded_files)}")
        logger.info(f"ğŸ”„ å·¥ä½œæµæ•°æ®: {req.workflow_data is not None}")
        
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœåŒ…å«å·¥ä½œæµæ•°æ®ï¼Œç›´æ¥æ‰§è¡Œå·¥ä½œæµï¼ˆè€Œä¸æ˜¯é€šè¿‡ agent.process_queryï¼‰
        if req.workflow_data:
            logger.info("ğŸš€ æ£€æµ‹åˆ°å·¥ä½œæµæ‰§è¡Œè¯·æ±‚ï¼Œç›´æ¥è°ƒç”¨ execute_workflow")
            try:
                # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ workflow_data ä¸­çš„ file_pathsï¼ˆå‰ç«¯å·²ç»è®¾ç½®å¥½ï¼‰
                file_paths = req.workflow_data.get("file_paths", [])
                logger.info(f"ğŸ“ ä» workflow_data è·å–çš„æ–‡ä»¶è·¯å¾„: {file_paths}")
                
                # å¦‚æœ workflow_data ä¸­æ²¡æœ‰ file_pathsï¼Œå†ä» uploaded_files ä¸­æå–
                if not file_paths:
                    logger.info("âš ï¸ workflow_data ä¸­æ²¡æœ‰ file_pathsï¼Œä» uploaded_files ä¸­æå–")
                    for file_info in req.uploaded_files:
                        file_name = file_info.get("file_name", "")
                        file_path_str = file_info.get("file_path", "")
                        
                        if file_path_str:
                            file_path = Path(file_path_str)
                        else:
                            file_path = UPLOAD_DIR / file_name if file_name else None
                        
                        if file_path and file_path.exists():
                            file_paths.append(str(file_path))
                
                logger.info(f"ğŸ“‚ æœ€ç»ˆæ–‡ä»¶è·¯å¾„åˆ—è¡¨: {file_paths}")
                
                # éªŒè¯æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨
                valid_file_paths = []
                for fp in file_paths:
                    fp_path = Path(fp)
                    if fp_path.exists():
                        valid_file_paths.append(str(fp_path))
                    else:
                        logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {fp}")
                
                if not valid_file_paths:
                    raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶å·²æ­£ç¡®ä¸Šä¼ ã€‚")
                
                # ç›´æ¥è°ƒç”¨ execute_workflow å‡½æ•°ï¼ˆä¸é€šè¿‡ HTTPï¼‰
                execute_request = {
                    "workflow_data": req.workflow_data,
                    "file_paths": valid_file_paths
                }
                # è°ƒç”¨ execute_workflow å‡½æ•°ï¼ˆå®šä¹‰åœ¨ä¸‹æ–¹ï¼‰
                result = await execute_workflow(execute_request)
                return result
            except Exception as e:
                logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                return JSONResponse(
                    status_code=500,
                    content={
                        "type": "error",
                        "error": str(e),
                        "message": f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
                    }
                )
        
        # ğŸ”¥ Step 3: å°è¯•ä½¿ç”¨åŠ¨æ€è§„åˆ’å™¨ï¼ˆå¦‚æœå¯ç”¨ä¸”æŸ¥è¯¢çœ‹èµ·æ¥æ˜¯å·¥ä½œæµè§„åˆ’è¯·æ±‚ï¼‰
        if workflow_planner and not req.workflow_data:
            # ç®€å•çš„å¯å‘å¼æ£€æµ‹ï¼šå¦‚æœæŸ¥è¯¢åŒ…å«åˆ†æç›¸å…³çš„å…³é”®è¯ï¼Œå°è¯•ä½¿ç”¨è§„åˆ’å™¨
            query_lower = req.message.lower()
            workflow_keywords = [
                "analyze", "analysis", "pca", "differential", "preprocess",
                "åˆ†æ", "å¤„ç†", "é™ç»´", "å·®å¼‚", "é¢„å¤„ç†"
            ]
            
            # å¦‚æœæœ‰ä¸Šä¼ æ–‡ä»¶æˆ–åŒ…å«å…³é”®è¯ï¼Œå°è¯•ä½¿ç”¨è§„åˆ’å™¨
            has_files = len(req.uploaded_files) > 0
            has_keywords = any(keyword in query_lower for keyword in workflow_keywords)
            
            if has_files or has_keywords:
                try:
                    logger.info("ğŸ§  å°è¯•ä½¿ç”¨åŠ¨æ€è§„åˆ’å™¨ç”Ÿæˆå·¥ä½œæµ...")
                    
                    # æå–æ–‡ä»¶è·¯å¾„ï¼ˆå…ˆè½¬æ¢ uploaded_filesï¼‰
                    file_paths = []
                    for file_info in req.uploaded_files:
                        file_path = file_info.get("path") or file_info.get("file_name")
                        if file_path:
                            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                            if not Path(file_path).is_absolute():
                                file_path = str(UPLOAD_DIR / Path(file_path).name)
                            file_paths.append(file_path)
                    
                    # æ£€æµ‹ç±»åˆ«ï¼ˆç®€å•å¯å‘å¼ï¼‰
                    category_filter = None
                    if any(keyword in query_lower for keyword in ["metabolite", "ä»£è°¢", "metabolomics"]):
                        category_filter = "Metabolomics"
                    elif any(keyword in query_lower for keyword in ["rna", "gene", "transcript", "è½¬å½•"]):
                        category_filter = "scRNA-seq"
                    
                    # è°ƒç”¨è§„åˆ’å™¨
                    plan_result = await workflow_planner.plan(
                        user_query=req.message,
                        context_files=file_paths,
                        category_filter=category_filter
                    )
                    
                    # å¦‚æœè§„åˆ’æˆåŠŸï¼Œè¿”å›ç»“æœ
                    if plan_result.get("type") == "workflow_config":
                        logger.info("âœ… åŠ¨æ€è§„åˆ’å™¨æˆåŠŸç”Ÿæˆå·¥ä½œæµ")
                        return JSONResponse(content=plan_result)
                    else:
                        logger.info(f"âš ï¸ åŠ¨æ€è§„åˆ’å™¨è¿”å›: {plan_result.get('type')}ï¼Œç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæµç¨‹")
                        # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæµç¨‹
                except Exception as planner_err:
                    logger.warning(f"âš ï¸ åŠ¨æ€è§„åˆ’å™¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæµç¨‹: {planner_err}")
                    # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæµç¨‹
        
        # ğŸ”¥ è½¬æ¢æ–‡ä»¶è·¯å¾„ï¼šæ”¯æŒå¤šç§å‰ç«¯æ ¼å¼
        uploaded_files = []
        logger.info(f"ğŸ“¥ æ”¶åˆ° uploaded_files: {len(req.uploaded_files)} ä¸ªæ–‡ä»¶")
        
        for file_info in req.uploaded_files:
            # æ”¯æŒå¤šç§å­—æ®µåï¼šfile_name/name, file_path/path
            file_name = file_info.get("file_name") or file_info.get("name", "")
            file_path_str = file_info.get("file_path") or file_info.get("path", "")
            
            # ğŸ”’ å®‰å…¨ï¼šæ¸…ç†æ–‡ä»¶å
            if file_name:
                file_name = sanitize_filename(file_name)
            
            # ğŸ”¥ æ„å»ºæ–‡ä»¶è·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨ file_pathï¼Œå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„åˆ™æ‹¼æ¥ UPLOAD_DIR
            if file_path_str:
                file_path = Path(file_path_str)
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œæ‹¼æ¥ UPLOAD_DIR
                if not file_path.is_absolute():
                    file_path = UPLOAD_DIR / file_path
            elif file_name:
                # å¦‚æœæ²¡æœ‰è·¯å¾„ï¼Œä½¿ç”¨æ–‡ä»¶ååœ¨ UPLOAD_DIR ä¸­æŸ¥æ‰¾
                file_path = UPLOAD_DIR / file_name
            else:
                logger.warning(f"âš ï¸ æ— æ³•ç¡®å®šæ–‡ä»¶è·¯å¾„ï¼Œè·³è¿‡: {file_info}")
                continue
            
            # ğŸ”’ å®‰å…¨ï¼šéªŒè¯è·¯å¾„åœ¨å…è®¸çš„ç›®å½•å†…
            try:
                file_path = validate_file_path(file_path, UPLOAD_DIR)
            except HTTPException:
                logger.warning(f"âš ï¸ ä¸å®‰å…¨çš„æ–‡ä»¶è·¯å¾„ï¼Œè·³è¿‡: {file_path}")
                continue
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file_path.exists():
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}ï¼Œå°è¯•æŸ¥æ‰¾...")
                # å°è¯•åœ¨ UPLOAD_DIR ä¸­æŸ¥æ‰¾åŒåæ–‡ä»¶
                if file_name:
                    alt_path = UPLOAD_DIR / file_name
                    if alt_path.exists():
                        file_path = alt_path
                        logger.info(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file_path}")
                    else:
                        logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
                        continue
                else:
                    continue
            
            uploaded_files.append({
                "name": file_name or os.path.basename(str(file_path)),
                "path": str(file_path)
            })
        
        logger.info(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {len(uploaded_files)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
        logger.info(f"ğŸ“‚ æ–‡ä»¶è·¯å¾„åˆ—è¡¨: {[f['path'] for f in uploaded_files]}")
        
        # å¤„ç†æŸ¥è¯¢
        # #region debug log
        try:
            debug_log_path = Path("/app/debug.log")
            debug_log_path.parent.mkdir(parents=True, exist_ok=True)
            with open(debug_log_path, 'a') as f:
                f.write(json.dumps({"location":"server.py:1161","message":"Before process_query","data":{"query":req.message,"uploaded_files_count":len(uploaded_files),"test_dataset_id":req.test_dataset_id},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"A"})+"\n")
        except Exception as log_err:
            pass  # æ—¥å¿—å†™å…¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        # #endregion
        try:
            result = await agent.process_query(
                query=req.message,
                history=req.history,
                uploaded_files=uploaded_files,
                test_dataset_id=req.test_dataset_id
            )
        except Exception as process_err:
            # #region debug log
            try:
                debug_log_path = Path("/app/debug.log")
                debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                with open(debug_log_path, 'a') as f:
                    f.write(json.dumps({"location":"server.py:1156","message":"process_query exception","data":{"error_type":type(process_err).__name__,"error_message":str(process_err),"traceback":traceback.format_exc()},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"PROCESS_QUERY"})+"\n")
            except:
                pass
            # #endregion
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚å¼‚å¸¸å¤„ç†æ•è·
        
        # #region debug log
        try:
            debug_log_path = Path("/app/debug.log")
            debug_log_path.parent.mkdir(parents=True, exist_ok=True)
            with open(debug_log_path, 'a') as f:
                f.write(json.dumps({"location":"server.py:1168","message":"After process_query","data":{"result_type":type(result).__name__,"result_keys":list(result.keys()) if isinstance(result,dict) else None,"result_type_value":result.get('type') if isinstance(result,dict) else None},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"A"})+"\n")
        except:
            pass
        # #endregion
        
        logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œè¿”å›ç±»å‹: {result.get('type', 'unknown')}")
        
        # å¦‚æœæ˜¯å·¥ä½œæµé…ç½®ï¼Œè¿”å› JSON
        if result.get("type") == "workflow_config":
            # ä¼˜å…ˆä½¿ç”¨ result ä¸­çš„ file_pathsï¼ˆå¯èƒ½æ¥è‡ªæµ‹è¯•æ•°æ®é›†ï¼‰
            # å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨ uploaded_files
            result_file_paths = result.get("file_paths", [])
            if not result_file_paths:
                result_file_paths = [f["path"] for f in uploaded_files]
            
            response_content = {
                "type": "workflow_config",
                "workflow_data": result.get("workflow_data"),
                "file_paths": result_file_paths
            }
            
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœåŒ…å«è¯Šæ–­æŠ¥å‘Šï¼Œä¹Ÿè¿”å›ç»™å‰ç«¯
            if "diagnosis_report" in result:
                response_content["diagnosis_report"] = result["diagnosis_report"]
            
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœåŒ…å«æ¨èä¿¡æ¯ï¼Œä¹Ÿè¿”å›ç»™å‰ç«¯ï¼ˆä»£è°¢ç»„å­¦ï¼‰
            if "recommendation" in result:
                response_content["recommendation"] = result["recommendation"]
            
            logger.info(f"ğŸ“¤ è¿”å›å·¥ä½œæµé…ç½®: åŒ…å«æ¨è={('recommendation' in response_content)}, åŒ…å«è¯Šæ–­={('diagnosis_report' in response_content)}")
            
            return JSONResponse(content=response_content)
        
        # å¦‚æœæ˜¯æµ‹è¯•æ•°æ®é€‰æ‹©è¯·æ±‚ï¼Œæ ¼å¼åŒ–ä¸ºç”¨æˆ·å‹å¥½çš„æ–‡æœ¬
        if result.get("type") == "test_data_selection":
            # #region debug log
            import json
            try:
                debug_log_path = Path("/app/debug.log")
                debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                with open(debug_log_path, 'a') as f:
                    f.write(json.dumps({"location":"server.py:1178","message":"Entering test_data_selection handler","data":{"has_message":"message" in result,"has_options":"options" in result,"has_datasets_display":"datasets_display" in result,"has_datasets_json":"datasets_json" in result},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"B"})+"\n")
            except:
                pass  # æ—¥å¿—å†™å…¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            # #endregion
            async def generate():
                try:
                    # #region debug log
                    try:
                        debug_log_path = Path("/app/debug.log")
                        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(debug_log_path, 'a') as f:
                            f.write(json.dumps({"location":"server.py:1181","message":"Inside generate()","data":{},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"C"})+"\n")
                    except:
                        pass
                    # #endregion
                    # æ„å»ºç”¨æˆ·å‹å¥½çš„æ¶ˆæ¯
                    message = result.get("message", "æ£€æµ‹åˆ°æ‚¨æ²¡æœ‰ä¸Šä¼ ç›¸å…³æ•°æ®ã€‚è¯·é€‰æ‹©ï¼š")
                    options = result.get("options", [])
                    datasets_display = result.get("datasets_display", "")
                    # #region debug log
                    try:
                        debug_log_path = Path("/app/debug.log")
                        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(debug_log_path, 'a') as f:
                            f.write(json.dumps({"location":"server.py:1187","message":"Before datasets_json processing","data":{"message_type":type(message).__name__,"options_type":type(options).__name__,"datasets_display_type":type(datasets_display).__name__,"datasets_display_len":len(str(datasets_display)) if datasets_display else 0},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"D"})+"\n")
                    except:
                        pass
                    # #endregion
                    
                    response_text = f"{message}\n\n"
                    for option in options:
                        response_text += f"  {option}\n"
                    
                    if datasets_display:
                        response_text += f"\n{datasets_display}\n"
                    
                    response_text += "\nğŸ’¡ æç¤ºï¼šå›å¤æ•°æ®é›†IDï¼ˆå¦‚ï¼špbmc_1k_v3ï¼‰æˆ–ä¸Šä¼ æ‚¨è‡ªå·±çš„æ•°æ®æ–‡ä»¶ã€‚\n"
                    
                    # åŒæ—¶ä¿å­˜æ•°æ®é›†ä¿¡æ¯åˆ°å“åº”ä¸­ï¼ˆç”¨äºå‰ç«¯å¤„ç†ï¼‰
                    # è¿™é‡Œæˆ‘ä»¬é€šè¿‡ç‰¹æ®Šæ ‡è®°æ¥ä¼ é€’ JSON æ•°æ®
                    # å°† JSON ä¸­çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œé¿å…ç ´å HTML æ³¨é‡Š
                    datasets_json_raw = result.get('datasets_json', '[]')
                    # #region debug log
                    try:
                        debug_log_path = Path("/app/debug.log")
                        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(debug_log_path, 'a') as f:
                            f.write(json.dumps({"location":"server.py:1200","message":"Before datasets_json replace","data":{"datasets_json_type":type(datasets_json_raw).__name__,"datasets_json_is_none":datasets_json_raw is None,"datasets_json_len":len(str(datasets_json_raw)) if datasets_json_raw else 0},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"B"})+"\n")
                    except:
                        pass
                    # #endregion
                    datasets_json = str(datasets_json_raw).replace('\n', ' ').replace('\r', '') if datasets_json_raw else '[]'
                    # #region debug log
                    try:
                        debug_log_path = Path("/app/debug.log")
                        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(debug_log_path, 'a') as f:
                            f.write(json.dumps({"location":"server.py:1203","message":"After datasets_json replace","data":{"datasets_json_len":len(datasets_json),"response_text_len":len(response_text)},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"B"})+"\n")
                    except:
                        pass
                    # #endregion
                    response_text += f"\n<!-- DATASETS_JSON: {datasets_json} -->\n"
                    
                    # #region debug log
                    try:
                        debug_log_path = Path("/app/debug.log")
                        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(debug_log_path, 'a') as f:
                            f.write(json.dumps({"location":"server.py:1207","message":"Before yield","data":{"final_response_text_len":len(response_text)},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"C"})+"\n")
                    except:
                        pass
                    # #endregion
                    yield response_text
                    # #region debug log
                    try:
                        debug_log_path = Path("/app/debug.log")
                        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(debug_log_path, 'a') as f:
                            f.write(json.dumps({"location":"server.py:1209","message":"After yield","data":{},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"C"})+"\n")
                    except:
                        pass
                    # #endregion
                except Exception as e:
                    # #region debug log
                    import traceback
                    try:
                        debug_log_path = Path("/app/debug.log")
                        debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(debug_log_path, 'a') as f:
                            f.write(json.dumps({"location":"server.py:1212","message":"Exception in generate()","data":{"error_type":type(e).__name__,"error_message":str(e),"traceback":traceback.format_exc()},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"C"})+"\n")
                    except:
                        pass
                    # #endregion
                    logger.error(f"âŒ æ ¼å¼åŒ–æµ‹è¯•æ•°æ®é€‰æ‹©å“åº”é”™è¯¯: {e}", exc_info=True)
                    yield f"\n\nâŒ é”™è¯¯: {str(e)}"
            
            # #region debug log
            try:
                debug_log_path = Path("/app/debug.log")
                debug_log_path.parent.mkdir(parents=True, exist_ok=True)
                with open(debug_log_path, 'a') as f:
                    f.write(json.dumps({"location":"server.py:1218","message":"Before StreamingResponse","data":{},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"E"})+"\n")
            except:
                pass
            # #endregion
            return StreamingResponse(generate(), media_type="text/plain")
        
        # å¦‚æœæ˜¯èŠå¤©å“åº”ï¼Œè¿”å›æµå¼
        if result.get("type") == "chat":
            async def generate():
                try:
                    response_iter = result.get("response")
                    if response_iter:
                        # ç¡®ä¿ response_iter æ˜¯å¼‚æ­¥è¿­ä»£å™¨
                        async for chunk in response_iter:
                            if chunk:
                                yield chunk
                    else:
                        logger.warning("âš ï¸ èŠå¤©å“åº”ä¸­æ²¡æœ‰ response è¿­ä»£å™¨")
                        yield "âŒ é”™è¯¯: æ— æ³•è·å–å“åº”"
                except Exception as e:
                    logger.error(f"âŒ æµå¼å“åº”é”™è¯¯: {e}", exc_info=True)
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    yield f"\n\nâŒ é”™è¯¯: {str(e)}"
            
            return StreamingResponse(
                generate(), 
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"
                }
            )
        
        # å…¶ä»–æƒ…å†µè¿”å› JSON
        return JSONResponse(content=result)
        
    except Exception as e:
        # #region debug log
        import traceback
        try:
            debug_log_path = Path("/app/debug.log")
            debug_log_path.parent.mkdir(parents=True, exist_ok=True)
            with open(debug_log_path, 'a') as f:
                f.write(json.dumps({"location":"server.py:1210","message":"Exception in chat_endpoint","data":{"error_type":type(e).__name__,"error_message":str(e),"traceback":traceback.format_exc()},"timestamp":int(__import__('time').time()*1000),"sessionId":"debug-session","runId":"run1","hypothesisId":"ALL"})+"\n")
        except:
            pass  # æ—¥å¿—å†™å…¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        # #endregion
        error_detail = f"{type(e).__name__}: {str(e)}"
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {error_detail}", exc_info=True)
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/api/execute")
async def execute_workflow(request: dict):
    """æ‰§è¡Œå·¥ä½œæµæ¥å£"""
    if not agent:
        raise HTTPException(status_code=500, detail="æ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
    
    try:
        workflow_data = request.get("workflow_data")
        file_paths = request.get("file_paths", [])
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆæ£€æŸ¥ workflow_name ä¸­æ˜¯å¦åŒ…å«ä»£è°¢ç»„å…³é”®è¯
        workflow_name = workflow_data.get("workflow_name", "")
        routing = None
        target_agent = None
        route_query = None
        
        # æ–¹æ³•1: å¦‚æœæœ‰ workflow_nameï¼Œä¼˜å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«ä»£è°¢ç»„å…³é”®è¯
        if workflow_name:
            workflow_name_lower = workflow_name.lower()
            # å¦‚æœ workflow_name åŒ…å«ä»£è°¢ç»„å…³é”®è¯ï¼Œç›´æ¥è·¯ç”±åˆ° metabolomics_agent
            if any(kw in workflow_name_lower for kw in ["metabolomics", "ä»£è°¢ç»„", "ä»£è°¢"]):
                logger.info(f"âœ… æ ¹æ® workflow_name ç›´æ¥è·¯ç”±åˆ° metabolomics_agent: {workflow_name}")
                routing = "metabolomics_agent"
                target_agent = agent.agents.get(routing)
                if not target_agent:
                    logger.warning(f"âš ï¸ metabolomics_agent ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ rna_agent")
                    target_agent = agent.agents.get("rna_agent")
                    routing = "rna_agent"
            else:
                route_query = workflow_name
        # æ–¹æ³•2: æ ¹æ®æ–‡ä»¶ç±»å‹æ„å»ºæŸ¥è¯¢
        elif file_paths:
            file_path = file_paths[0]
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext == ".csv":
                route_query = "metabolomics analysis"
            elif file_ext in [".h5ad", ".h5"]:
                route_query = "single cell transcriptomics analysis"
            elif "fastq" in file_path.lower():
                route_query = "single cell RNA-seq analysis"
            else:
                route_query = "bioinformatics analysis"
        else:
            route_query = "bioinformatics analysis"
        
        # å‡†å¤‡ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äº RouterAgentï¼‰
        uploaded_files_for_router = []
        for file_path in file_paths:
            uploaded_files_for_router.append({
                "name": os.path.basename(file_path),
                "path": file_path
            })
        
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœè¿˜æ²¡æœ‰è·¯ç”±ï¼Œä½¿ç”¨ RouterAgent è¿›è¡Œè·¯ç”±å†³ç­–
        if not routing or not target_agent:
            try:
                route_result = await agent.router.process_query(
                    query=route_query,
                    history=[],
                    uploaded_files=uploaded_files_for_router
                )
                
                routing = route_result.get("routing", "rna_agent")
                target_agent = agent.agents.get(routing)
                
                # å¦‚æœè·¯ç”±çš„æ™ºèƒ½ä½“ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤çš„ RNA Agent
                if not target_agent:
                    logger.warning(f"âš ï¸ è·¯ç”±çš„æ™ºèƒ½ä½“ä¸å­˜åœ¨: {routing}ï¼Œä½¿ç”¨é»˜è®¤ rna_agent")
                    target_agent = agent.agents.get("rna_agent")
                    routing = "rna_agent"
                
                if not target_agent:
                    raise HTTPException(status_code=500, detail="RNA Agent æœªæ‰¾åˆ°")
                
                logger.info(f"âœ… RouterAgent è·¯ç”±ç»“æœ: {routing} (confidence: {route_result.get('confidence', 0):.2f}, modality: {route_result.get('modality', 'unknown')})")
                
            except Exception as e:
                logger.error(f"âŒ RouterAgent è·¯ç”±å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ rna_agent", exc_info=True)
                # é™çº§åˆ°é»˜è®¤ Agent
                target_agent = agent.agents.get("rna_agent")
                routing = "rna_agent"
                if not target_agent:
                    raise HTTPException(status_code=500, detail="RNA Agent æœªæ‰¾åˆ°")
        
        # ğŸ”¥ Step 4: ä½¿ç”¨é€šç”¨æ‰§è¡Œå™¨ï¼ˆåŠ¨æ€æ‰§è¡Œï¼Œä¸ä¾èµ–ç¡¬ç¼–ç é€»è¾‘ï¼‰
        try:
            from gibh_agent.core.executor import WorkflowExecutor
            
            logger.info("ğŸ”§ ä½¿ç”¨é€šç”¨æ‰§è¡Œå™¨æ‰§è¡Œå·¥ä½œæµ...")
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            output_dir = str(RESULTS_DIR / f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            
            # åˆ›å»ºæ‰§è¡Œå™¨å¹¶æ‰§è¡Œ
            executor = WorkflowExecutor(output_dir=output_dir)
            report_data = executor.execute_workflow(
                workflow_data=workflow_data,
                file_paths=file_paths,
                output_dir=output_dir
            )
            
            logger.info("âœ… é€šç”¨æ‰§è¡Œå™¨æ‰§è¡Œå®Œæˆ")
            
            # æ„å»ºè¿”å›ç»“æœï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
            return JSONResponse(content={
                "type": "analysis_report",
                "status": "success",
                "report_data": report_data,
                "reply": "âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼ˆä½¿ç”¨åŠ¨æ€æ‰§è¡Œå¼•æ“ï¼‰",
                "thought": "[THOUGHT] ä½¿ç”¨ ToolRegistry åŠ¨æ€æ‰§è¡Œï¼Œå·¥å…·æ— å…³"
            })
        
        except ImportError:
            logger.warning("âš ï¸ é€šç”¨æ‰§è¡Œå™¨æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
            # å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼
            output_dir = str(RESULTS_DIR / f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            os.makedirs(output_dir, exist_ok=True)
            
            report = await target_agent.execute_workflow(
                workflow_config=workflow_data,
                file_paths=file_paths,
                output_dir=output_dir
            )
            
            logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {report.get('status')}")
            
            # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
            if report.get("final_plot"):
                plot_path = report["final_plot"]
                if not plot_path.startswith("/results/"):
                    if plot_path.startswith("results/"):
                        plot_path = "/" + plot_path
                    elif "/" in plot_path:
                        plot_path = f"/results/{plot_path}"
                    else:
                        run_name = os.path.basename(output_dir)
                        plot_path = f"/results/{run_name}/{plot_path}"
                report["final_plot"] = plot_path
            
            # å¤„ç†æ­¥éª¤ä¸­çš„å›¾ç‰‡è·¯å¾„
            run_name = os.path.basename(output_dir)
            if report.get("steps_details"):
                for step in report["steps_details"]:
                    if step.get("plot"):
                        plot_path = step["plot"]
                        if not plot_path.startswith("/results/"):
                            if plot_path.startswith("results/"):
                                plot_path = "/" + plot_path
                            elif "/" in plot_path:
                                plot_path = f"/results/{plot_path}"
                            else:
                                plot_path = f"/results/{run_name}/{plot_path}"
                        step["plot"] = plot_path
            
            # è¿”å›ä¼ ç»Ÿæ ¼å¼çš„ç»“æœ
            return JSONResponse(content={
                "type": "analysis_report",
                "status": report.get("status", "success"),
                "report_data": report
            })
        
        # å¤„ç†é€šç”¨æ‰§è¡Œå™¨è¿”å›çš„å›¾ç‰‡è·¯å¾„
        logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {report_data.get('status')}")
        
        # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼Œè½¬æ¢ä¸ºå¯è®¿é—®çš„ URLï¼ˆåœ¨è¿”å›ä¹‹å‰ï¼‰
        # å›¾ç‰‡ä¿å­˜åœ¨ results/run_xxx/ ç›®å½•ï¼Œéœ€è¦è½¬æ¢ä¸º /results/run_xxx/filename
        if report.get("final_plot"):
            plot_path = report["final_plot"]
            # ç¡®ä¿è·¯å¾„ä»¥ /results/ å¼€å¤´
            if not plot_path.startswith("/results/"):
                if plot_path.startswith("results/"):
                    plot_path = "/" + plot_path
                elif "/" in plot_path:
                    # å¦‚æœåŒ…å« run_xxx/filename æ ¼å¼ï¼Œæ·»åŠ  results å‰ç¼€
                    plot_path = f"/results/{plot_path}"
                else:
                    # å¦‚æœåªæ˜¯æ–‡ä»¶åï¼Œéœ€è¦æ‰¾åˆ°å¯¹åº”çš„ run ç›®å½•
                    # ä» output_dir ä¸­æå– run_xxx
                    run_name = os.path.basename(output_dir)
                    plot_path = f"/results/{run_name}/{plot_path}"
            report_data["final_plot"] = plot_path
        
        # å¤„ç†æ­¥éª¤ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼ˆsteps_detailsï¼‰
        run_name = os.path.basename(output_dir)
        if report_data.get("steps_details"):
            for step in report_data["steps_details"]:
                if step.get("plot"):
                    plot_path = step["plot"]
                    # ç¡®ä¿è·¯å¾„ä»¥ /results/ å¼€å¤´
                    if not plot_path.startswith("/results/"):
                        if plot_path.startswith("results/"):
                            plot_path = "/" + plot_path
                        elif "/" in plot_path:
                            plot_path = f"/results/{plot_path}"
                        else:
                            plot_path = f"/results/{run_name}/{plot_path}"
                    step["plot"] = plot_path
                
                # å¤„ç† step_result ä¸­çš„å›¾ç‰‡è·¯å¾„
                if step.get("step_result") and step["step_result"].get("data", {}).get("images"):
                    images = step["step_result"]["data"]["images"]
                    fixed_images = []
                    for img_path in images:
                        if not img_path.startswith("/results/"):
                            if img_path.startswith("results/"):
                                img_path = "/" + img_path
                            elif "/" in img_path:
                                img_path = f"/results/{img_path}"
                            else:
                                img_path = f"/results/{run_name}/{img_path}"
                        fixed_images.append(img_path)
                    step["step_result"]["data"]["images"] = fixed_images
        
        # ç¡®ä¿ steps_results å­˜åœ¨ï¼ˆå‰ç«¯å¯ç›´æ¥ä½¿ç”¨ï¼‰
        if "steps_results" not in report and "steps_details" in report:
            steps_results = []
            for step_detail in report.get("steps_details", []):
                if "step_result" in step_detail:
                    step_result = step_detail["step_result"].copy()
                    # ç¡®ä¿å›¾ç‰‡è·¯å¾„æ­£ç¡®
                    if step_result.get("data", {}).get("images"):
                        images = step_result["data"]["images"]
                        fixed_images = []
                        for img_path in images:
                            if not img_path.startswith("/results/"):
                                if img_path.startswith("results/"):
                                    img_path = "/" + img_path
                                elif "/" in img_path:
                                    img_path = f"/results/{img_path}"
                                else:
                                    img_path = f"/results/{run_name}/{img_path}"
                                fixed_images.append(img_path)
                            else:
                                fixed_images.append(img_path)
                        step_result["data"]["images"] = fixed_images
                    steps_results.append(step_result)
                else:
                    # å…¼å®¹æ—§æ ¼å¼
                    step_result = {
                        "step_name": step_detail.get("name", "Unknown"),
                        "status": step_detail.get("status", "success"),
                        "logs": step_detail.get("summary", ""),
                        "data": {}
                    }
                    # å¦‚æœæœ‰ plotï¼Œæ·»åŠ åˆ° data.images
                    if step_detail.get("plot"):
                        plot_path = step_detail["plot"]
                        if not plot_path.startswith("/results/"):
                            if plot_path.startswith("results/"):
                                plot_path = "/" + plot_path
                            elif "/" in plot_path:
                                plot_path = f"/results/{plot_path}"
                            else:
                                plot_path = f"/results/{run_name}/{plot_path}"
                        step_result["data"]["images"] = [plot_path]
                    steps_results.append(step_result)
            report["steps_results"] = steps_results
        
        # å¤„ç† steps_results ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if report.get("steps_results"):
            for step_result in report["steps_results"]:
                if step_result.get("data", {}).get("images"):
                    images = step_result["data"]["images"]
                    fixed_images = []
                    for img_path in images:
                        if not img_path.startswith("/results/"):
                            if img_path.startswith("results/"):
                                img_path = "/" + img_path
                            elif "/" in img_path:
                                img_path = f"/results/{img_path}"
                            else:
                                img_path = f"/results/{run_name}/{img_path}"
                            fixed_images.append(img_path)
                        else:
                            fixed_images.append(img_path)
                    step_result["data"]["images"] = fixed_images
        
        # ğŸ”§ ä¿®å¤ï¼šè¿”å›æ­£ç¡®çš„å·¥ä½œæµæ‰§è¡Œç»“æœæ ¼å¼
        return JSONResponse(content={
            "type": "analysis_report",
            "status": "success",
            "report_data": report
        })
        
    except Exception as e:
        import traceback
        error_detail = f"{type(e).__name__}: {str(e)}"
        error_traceback = traceback.format_exc()
        logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error_detail}", exc_info=True)
        logger.error(f"è¯¦ç»†é”™è¯¯: {error_traceback}")
        # è¿”å›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "error": error_detail,
                "error_detail": error_traceback,
                "message": f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error_detail}"
            }
        )


@app.get("/api/logs/stream")
async def stream_logs():
    """å®æ—¶æ—¥å¿—æµæ¥å£ï¼ˆServer-Sent Eventsï¼‰"""
    logger.info("ğŸ“¡ æ–°çš„æ—¥å¿—æµè¿æ¥")
    
    async def event_generator():
        q = asyncio.Queue(maxsize=100)
        log_listeners.add(q)
        
        try:
            # å…ˆå‘é€å†å²æ—¥å¿—
            history_logs = list(log_buffer)[-100:]  # æœ€è¿‘100æ¡
            logger.info(f"ğŸ“¤ å‘é€å†å²æ—¥å¿—: {len(history_logs)} æ¡")
            for entry in history_logs:
                yield f"data: {json.dumps(entry, ensure_ascii=False)}\\n\\n"
            
            # å®æ—¶å‘é€æ–°æ—¥å¿—
            while True:
                try:
                    entry = await asyncio.wait_for(q.get(), timeout=1.0)
                    yield f"data: {json.dumps(entry, ensure_ascii=False)}\\n\\n"
                except asyncio.TimeoutError:
                    # å‘é€å¿ƒè·³ä¿æŒè¿æ¥
                    yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': datetime.now().isoformat()})}\\n\\n"
        except asyncio.CancelledError:
            logger.info("ğŸ“¡ æ—¥å¿—æµè¿æ¥å·²å–æ¶ˆ")
        except Exception as e:
            logger.error(f"âŒ æ—¥å¿—æµé”™è¯¯: {e}", exc_info=True)
        finally:
            log_listeners.discard(q)
            logger.info("ğŸ“¡ æ—¥å¿—æµè¿æ¥å·²å…³é—­")
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.get("/api/logs")
async def get_logs(limit: int = 100):
    """è·å–å†å²æ—¥å¿—"""
    return JSONResponse(content={
        "logs": list(log_buffer)[-limit:],
        "total": len(log_buffer)
    })


# ğŸ”¥ Step 2: Tool-RAG API - å·¥å…·æ£€ç´¢ç«¯ç‚¹
@app.get("/api/tools/search")
async def search_tools(
    query: str,
    top_k: int = 5,
    category: Optional[str] = None
):
    """
    è¯­ä¹‰æœç´¢å·¥å…·
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
        top_k: è¿”å›å‰ k ä¸ªæœ€ç›¸å…³çš„å·¥å…·ï¼ˆé»˜è®¤ 5ï¼‰
        category: å¯é€‰çš„ç±»åˆ«è¿‡æ»¤å™¨
    
    Returns:
        ç›¸å…³å·¥å…·çš„ JSON Schema åˆ—è¡¨
    """
    if tool_retriever is None:
        raise HTTPException(
            status_code=503,
            detail="å·¥å…·æ£€ç´¢å™¨æœªåˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥ Ollama æœåŠ¡å’Œä¾èµ–æ˜¯å¦å·²å®‰è£…ã€‚"
        )
    
    try:
        tools = tool_retriever.retrieve(query=query, top_k=top_k, category_filter=category)
        return {
            "status": "success",
            "query": query,
            "count": len(tools),
            "tools": tools
        }
    except Exception as e:
        logger.error(f"âŒ å·¥å…·æœç´¢å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å·¥å…·æœç´¢å¤±è´¥: {str(e)}")


@app.get("/api/tools/list")
async def list_tools():
    """
    åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„å·¥å…·
    
    Returns:
        å·¥å…·åç§°åˆ—è¡¨
    """
    if tool_retriever is None:
        raise HTTPException(
            status_code=503,
            detail="å·¥å…·æ£€ç´¢å™¨æœªåˆå§‹åŒ–"
        )
    
    try:
        tools = tool_retriever.list_all_tools()
        return {
            "status": "success",
            "count": len(tools),
            "tools": tools
        }
    except Exception as e:
        logger.error(f"âŒ åˆ—å‡ºå·¥å…·å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºå·¥å…·å¤±è´¥: {str(e)}")


@app.get("/api/tools/{tool_name}")
async def get_tool_schema(tool_name: str):
    """
    è·å–ç‰¹å®šå·¥å…·çš„å®Œæ•´ Schema
    
    Args:
        tool_name: å·¥å…·åç§°
    
    Returns:
        å·¥å…·çš„å®Œæ•´ JSON Schema
    """
    if tool_retriever is None:
        raise HTTPException(
            status_code=503,
            detail="å·¥å…·æ£€ç´¢å™¨æœªåˆå§‹åŒ–"
        )
    
    try:
        tool_schema = tool_retriever.get_tool_by_name(tool_name)
        if tool_schema is None:
            raise HTTPException(status_code=404, detail=f"å·¥å…· '{tool_name}' ä¸å­˜åœ¨")
        
        return {
            "status": "success",
            "tool": tool_schema
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–å·¥å…· Schema å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å·¥å…· Schema å¤±è´¥: {str(e)}")


@app.get("/api/workflow/status/{run_id}")
async def get_workflow_status(run_id: str):
    """
    æŸ¥è¯¢å·¥ä½œæµçŠ¶æ€ï¼ˆå…¼å®¹æ—§æ¶æ„ï¼‰
    å¦‚æœä½¿ç”¨ Celeryï¼ŒæŸ¥è¯¢ Celery ä»»åŠ¡çŠ¶æ€
    å¦‚æœä½¿ç”¨åŒæ­¥æ‰§è¡Œï¼Œè¿”å› not_foundï¼ˆå› ä¸ºåŒæ­¥æ‰§è¡Œæ²¡æœ‰ä»»åŠ¡IDï¼‰
    """
    try:
        # å°è¯•ä» Celery æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
        from celery.result import AsyncResult
        from gibh_agent.core.celery_app import celery_app
        
        task_result = AsyncResult(run_id, app=celery_app)
        
        response = {
            "status": "running",
            "completed": False,
            "steps_status": [],
            "error": None
        }
        
        if task_result.state == 'PENDING':
            response["status"] = "running"
        elif task_result.state == 'SUCCESS':
            response["status"] = "success"
            response["completed"] = True
            result_data = task_result.result
            if result_data:
                response["report_data"] = result_data
                if "steps_details" in result_data:
                    response["steps_status"] = result_data["steps_details"]
                elif "steps" in result_data:
                    response["steps_status"] = result_data["steps"]
        elif task_result.state == 'FAILURE':
            response["status"] = "failed"
            response["completed"] = True
            response["error"] = str(task_result.result)
        elif task_result.state == 'PROGRESS':
            info = task_result.info
            if isinstance(info, dict):
                response["steps_status"] = info.get("steps", [])
        
        return JSONResponse(content=response)
        
    except ImportError:
        # Celery æœªå®‰è£…æˆ–æœªé…ç½®ï¼Œè¿”å› not_found
        return JSONResponse(
            status_code=404,
            content={
                "status": "not_found",
                "message": "å·¥ä½œæµçŠ¶æ€æŸ¥è¯¢éœ€è¦ Celery æ”¯æŒï¼Œå½“å‰ä½¿ç”¨åŒæ­¥æ‰§è¡Œæ¨¡å¼"
            }
        )
    except Exception as e:
        logger.error(f"âŒ æŸ¥è¯¢å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "error": str(e)
            }
        )


if __name__ == "__main__":
    import uvicorn
    import json
    
    port = int(os.getenv("PORT", 8018))
    logger.info(f"ğŸš€ å¯åŠ¨æœåŠ¡å™¨ï¼Œç«¯å£: {port}")
    logger.info(f"ğŸ“ ä¸Šä¼ ç›®å½•: {UPLOAD_DIR.absolute()}")
    logger.info(f"ğŸ“ ç»“æœç›®å½•: {RESULTS_DIR.absolute()}")
    
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=port,
        log_level="info",
        reload=True
    )

