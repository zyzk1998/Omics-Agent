"""
åŸºç¡€æ™ºèƒ½ä½“æŠ½è±¡ç±»
æ‰€æœ‰é¢†åŸŸæ™ºèƒ½ä½“éƒ½ç»§æ‰¿æ­¤ç±»
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, AsyncIterator
import logging
from openai import AuthenticationError, APIError
from ..core.llm_client import LLMClient
from ..core.prompt_manager import PromptManager, DATA_DIAGNOSIS_PROMPT
from ..core.data_diagnostician import DataDiagnostician
from ..core.stream_utils import strip_suggestions_from_text

logger = logging.getLogger(__name__)


class BaseAgent(ABC):
    """
    åŸºç¡€æ™ºèƒ½ä½“æŠ½è±¡ç±»
    
    æ‰€æœ‰é¢†åŸŸæ™ºèƒ½ä½“éƒ½åº”è¯¥ç»§æ‰¿æ­¤ç±»å¹¶å®ç°ï¼š
    - process_query: å¤„ç†ç”¨æˆ·æŸ¥è¯¢
    - generate_workflow: ç”Ÿæˆå·¥ä½œæµï¼ˆå¦‚æœéœ€è¦ï¼‰
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        prompt_manager: PromptManager,
        expert_role: str
    ):
        """
        åˆå§‹åŒ–åŸºç¡€æ™ºèƒ½ä½“
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            prompt_manager: æç¤ºç®¡ç†å™¨
            expert_role: ä¸“å®¶è§’è‰²åç§°ï¼ˆå¦‚ "rna_expert"ï¼‰
        """
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.expert_role = expert_role
        self.diagnostician = DataDiagnostician()
        # ğŸ”¥ æ¶æ„é‡æ„ï¼šä¼šè¯çº§æ–‡ä»¶æ³¨å†Œè¡¨
        self.context: Dict[str, Any] = {
            "file_registry": {},  # Key: filename, Value: {path, metadata, timestamp}
            "active_file": None   # å½“å‰æ´»åŠ¨çš„æ–‡ä»¶å
        }
    
    def register_file(
        self,
        filename: str,
        file_path: str,
        file_metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        æ³¨å†Œæ–‡ä»¶åˆ°ä¼šè¯æ³¨å†Œè¡¨
        
        ğŸ”¥ æ¶æ„é‡æ„ï¼šç»´æŠ¤æ–‡ä»¶å†å²ï¼Œè€Œä¸æ˜¯æ¸…é™¤
        
        Args:
            filename: æ–‡ä»¶åï¼ˆç”¨ä½œæ³¨å†Œè¡¨çš„ keyï¼‰
            file_path: æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼Œç¨åå¯ä»¥æ›´æ–°ï¼‰
        """
        import time
        if "file_registry" not in self.context:
            self.context["file_registry"] = {}
        
        self.context["file_registry"][filename] = {
            "path": file_path,
            "metadata": file_metadata,
            "timestamp": time.time(),
            "registered_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        logger.info(f"ğŸ“ [FileRegistry] Registered file: {filename} (Total: {len(self.context['file_registry'])} files)")
    
    def set_active_file(self, filename: str) -> None:
        """
        è®¾ç½®å½“å‰æ´»åŠ¨çš„æ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶åï¼ˆå¿…é¡»åœ¨æ³¨å†Œè¡¨ä¸­å­˜åœ¨ï¼‰
        """
        if filename not in self.context.get("file_registry", {}):
            logger.warning(f"âš ï¸ [FileRegistry] File {filename} not in registry. Registering...")
            # å¦‚æœæ–‡ä»¶ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œå°è¯•æ³¨å†Œï¼ˆä½¿ç”¨è·¯å¾„ä½œä¸ºæ–‡ä»¶åï¼‰
            self.register_file(filename, filename)
        
        old_active = self.context.get("active_file")
        self.context["active_file"] = filename
        
        if old_active != filename:
            logger.info(f"ğŸ”„ [FileRegistry] Active file changed: {old_active} -> {filename}")
        else:
            logger.debug(f"âœ… [FileRegistry] Active file unchanged: {filename}")
    
    def get_active_file_info(self) -> Optional[Dict[str, Any]]:
        """
        è·å–å½“å‰æ´»åŠ¨æ–‡ä»¶çš„ä¿¡æ¯
        
        ğŸ”¥ æ¶æ„é‡æ„ï¼šç»Ÿä¸€æ¥å£è·å–æ´»åŠ¨æ–‡ä»¶ä¿¡æ¯
        
        Returns:
            åŒ…å« path å’Œ metadata çš„å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æ´»åŠ¨æ–‡ä»¶è¿”å› None
        """
        active_file = self.context.get("active_file")
        if not active_file:
            logger.debug("âš ï¸ [FileRegistry] No active file set")
            return None
        
        registry = self.context.get("file_registry", {})
        if active_file not in registry:
            logger.warning(f"âš ï¸ [FileRegistry] Active file '{active_file}' not found in registry")
            return None
        
        file_info = registry[active_file]
        logger.debug(f"âœ… [FileRegistry] Retrieved active file info: {active_file}")
        return {
            "filename": active_file,
            "path": file_info.get("path"),
            "metadata": file_info.get("metadata"),
            "timestamp": file_info.get("timestamp")
        }
    
    def _refresh_context_for_new_files(self, uploaded_files: List[Dict[str, str]]) -> None:
        """
        åˆ·æ–°ä¸Šä¸‹æ–‡ä»¥å¤„ç†æ–°æ–‡ä»¶
        
        ğŸ”¥ ä¿®å¤ï¼šå½“æ–°æ–‡ä»¶ä¸Šä¼ æ—¶ï¼Œæ¸…é™¤æ—§çš„ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿ä½¿ç”¨æ–°æ–‡ä»¶ä½œä¸ºå•ä¸€æ•°æ®æº
        
        Args:
            uploaded_files: å½“å‰è¯·æ±‚ä¸­çš„æ–‡ä»¶åˆ—è¡¨
        """
        if uploaded_files and len(uploaded_files) > 0:
            # æå–æ–‡ä»¶åç”¨äºæ—¥å¿—
            file_names = []
            for file_info in uploaded_files:
                if isinstance(file_info, dict):
                    name = file_info.get("name") or file_info.get("path") or file_info.get("file_id", "unknown")
                else:
                    name = getattr(file_info, "name", None) or getattr(file_info, "path", None) or "unknown"
                file_names.append(name)
            
            # æ¸…é™¤æ—§çš„ä¸Šä¸‹æ–‡
            old_file_paths = self.context.get("file_paths", [])
            old_file_metadata = self.context.get("file_metadata")
            
            if old_file_paths or old_file_metadata:
                logger.info(f"ğŸ”„ [System] Context refreshed. Clearing old context:")
                logger.info(f"   Old files: {old_file_paths}")
                logger.info(f"   New active files: {file_names}")
            
            # æ¸…é™¤æ–‡ä»¶ç›¸å…³çš„ä¸Šä¸‹æ–‡
            self.context.pop("file_paths", None)
            self.context.pop("file_metadata", None)
            self.context.pop("diagnosis_report", None)
            self.context.pop("diagnosis_stats", None)
            
            logger.info(f"âœ… [System] Context refreshed. New active file: {file_names[0] if file_names else 'None'}")
    
    @abstractmethod
    async def process_query(
        self,
        query: str,
        history: List[Dict[str, str]] = None,
        uploaded_files: List[Dict[str, str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            history: å¯¹è¯å†å²
            uploaded_files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        pass
    
    async def chat(
        self,
        query: str,
        context: Dict[str, Any] = None,
        stream: bool = False
    ) -> AsyncIterator[str]:
        """
        é€šç”¨èŠå¤©æ–¹æ³•
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Yields:
            å“åº”æ–‡æœ¬å—
        """
        context = context or {}
        
        # è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.prompt_manager.get_system_prompt(
            self.expert_role,
            context
        )
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ]
        
        try:
            if stream:
                # æµå¼è¾“å‡ºï¼šç›´æ¥ä¼ é€’å†…å®¹ï¼Œè®©å‰ç«¯å¤„ç† think æ ‡ç­¾
                # DeepSeek-R1 çš„ think è¿‡ç¨‹ä¼šä»¥ <think>...</think> æ ‡ç­¾å½¢å¼è¿”å›
                # ä¹Ÿæ”¯æŒæ—§åè®®çš„ <think>...</think> æ ‡ç­¾
                has_yielded = False
                try:
                    async for chunk in self.llm_client.astream(messages):
                        if chunk.choices and chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            if content:
                                # ç›´æ¥ä¼ é€’å†…å®¹ï¼Œå‰ç«¯ä¼šæ£€æµ‹å’Œå¤„ç† think æ ‡ç­¾ï¼ˆ<think> æˆ– <think>ï¼‰
                                yield content
                                has_yielded = True
                except Exception as stream_error:
                    logger.error(f"âŒ æµå¼å“åº”é”™è¯¯: {stream_error}", exc_info=True)
                    if not has_yielded:
                        yield f"\n\nâŒ é”™è¯¯: {str(stream_error)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
                    else:
                        # å¦‚æœå·²ç»æœ‰ä¸€äº›è¾“å‡ºï¼Œåªè®°å½•é”™è¯¯ï¼Œä¸é‡å¤è¾“å‡ºé”™è¯¯ä¿¡æ¯
                        logger.warning(f"âš ï¸ æµå¼å“åº”ä¸­æ–­ï¼Œä½†å·²æœ‰éƒ¨åˆ†è¾“å‡º")
            else:
                completion = await self.llm_client.achat(messages)
                # æå– think è¿‡ç¨‹å’Œå®é™…å†…å®¹
                think_content, actual_content = self.llm_client.extract_think_and_content(completion)
                
                # å¦‚æœæœ‰ think å†…å®¹ï¼ŒåŒ…è£…åœ¨æ ‡ç­¾ä¸­
                if think_content:
                    yield f"<think>{think_content}</think>\n\n{actual_content}"
                else:
                    yield actual_content
        except AuthenticationError as e:
            error_msg = (
                f"\n\nâŒ è®¤è¯é”™è¯¯ (Error code: 401 - Invalid token)\n"
                f"è¯·æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚\n"
                f"è®¾ç½®æ–¹æ³•: export SILICONFLOW_API_KEY='your_api_key_here'\n"
                f"è¯¦ç»†é”™è¯¯: {str(e)}"
            )
            logger.error(f"API è®¤è¯å¤±è´¥: {e}")
            yield error_msg
        except APIError as e:
            error_msg = (
                f"\n\nâŒ API é”™è¯¯ (Error code: {getattr(e, 'status_code', 'unknown')})\n"
                f"è¯¦ç»†é”™è¯¯: {str(e)}"
            )
            logger.error(f"API è°ƒç”¨å¤±è´¥: {e}")
            yield error_msg
        except Exception as e:
            error_msg = f"\n\nâŒ é”™è¯¯: {str(e)}"
            logger.error(f"èŠå¤©å¤„ç†å¤±è´¥: {e}", exc_info=True)
            yield error_msg
    
    def get_file_paths(self, uploaded_files: List[Dict[str, str]]) -> List[str]:
        """
        ä»ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ä¸­æå–æ–‡ä»¶è·¯å¾„ï¼Œå¹¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        
        æ ¸å¿ƒåŸåˆ™ï¼šæ™ºèƒ½ä½“åªå¤„ç†æ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œä¸å¤„ç†äºŒè¿›åˆ¶æ•°æ®
        **å…³é”®ä¿®å¤**ï¼šç¡®ä¿è¿”å›ç»å¯¹è·¯å¾„ï¼Œé¿å… "File Not Found" é”™è¯¯
        
        Args:
            uploaded_files: æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«ç›¸å¯¹è·¯å¾„æˆ– file_idï¼‰
        
        Returns:
            ç»å¯¹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        import os
        from pathlib import Path
        
        # è·å–ä¸Šä¼ ç›®å½•ï¼ˆä¸ server.py ä¿æŒä¸€è‡´ï¼‰
        upload_dir = Path(os.getenv("UPLOAD_DIR", "/app/uploads"))
        
        paths = []
        for file_info in uploaded_files:
            if isinstance(file_info, dict):
                path = file_info.get("path") or file_info.get("name") or file_info.get("file_path") or file_info.get("file_id")
            else:
                path = getattr(file_info, "path", None) or getattr(file_info, "name", None) or getattr(file_info, "file_path", None) or getattr(file_info, "file_id", None)
            
            if not path:
                continue
            
            # ğŸ”¥ ä¿®å¤ï¼šè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            path_obj = Path(path)
            
            # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
            if path_obj.is_absolute():
                absolute_path = str(path_obj.resolve())
            else:
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œæ‹¼æ¥ UPLOAD_DIR
                absolute_path = str((upload_dir / path_obj).resolve())
            
            # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­å¤„ç†ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†é”™è¯¯ï¼‰
            if not os.path.exists(absolute_path):
                logger.warning(f"âš ï¸ æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {absolute_path} (åŸå§‹è·¯å¾„: {path})")
                # ä»ç„¶æ·»åŠ åˆ°åˆ—è¡¨ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†ï¼ˆå¯èƒ½æ–‡ä»¶ç¨åä¼šè¢«åˆ›å»ºï¼‰
            
            paths.append(absolute_path)
        
        return paths
    
    def detect_file_type(self, file_path: str) -> str:
        """
        æ£€æµ‹æ–‡ä»¶ç±»å‹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
        
        Returns:
            æ–‡ä»¶ç±»å‹ï¼ˆå¦‚ "fastq", "bam", "h5ad", "10x_mtx"ï¼‰
        """
        import os
        
        # å¦‚æœæ˜¯ç›®å½•ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ FASTQ ç›®å½•æˆ– 10x MTX ç›®å½•
        if os.path.isdir(file_path):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ FASTQ ç›®å½•ï¼ˆåŒ…å« .fastq æˆ– .fq æ–‡ä»¶ï¼‰
            fastq_files = [f for f in os.listdir(file_path) if f.endswith(('.fastq', '.fq', '.fastq.gz', '.fq.gz'))]
            if fastq_files:
                return "fastq"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ 10x MTX ç›®å½•ï¼ˆåŒ…å« matrix.mtx æˆ– matrix.mtx.gzï¼‰
            mtx_files = [f for f in os.listdir(file_path) if 'matrix.mtx' in f.lower()]
            if mtx_files:
                return "10x_mtx"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Cell Ranger è¾“å‡ºç›®å½•ï¼ˆåŒ…å« filtered_feature_bc_matrixï¼‰
            if 'filtered_feature_bc_matrix' in os.listdir(file_path) or \
               any('filtered_feature_bc_matrix' in subdir for subdir in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, subdir))):
                return "10x_mtx"
            
            return "directory"
        
        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œæ£€æŸ¥æ‰©å±•å
        ext = file_path.split('.')[-1].lower()
        
        type_mapping = {
            "fastq": ["fastq", "fq"],
            "bam": ["bam"],
            "h5ad": ["h5ad"],
            "mtx": ["mtx"],
            "vcf": ["vcf"],
            "bed": ["bed"],
            "bw": ["bw", "bigwig"],
            "sam": ["sam"],
            "csv": ["csv"]  # ä»£è°¢ç»„å­¦æ•°æ®é€šå¸¸ä½¿ç”¨ CSV æ ¼å¼
        }
        
        for file_type, extensions in type_mapping.items():
            if ext in extensions:
                return file_type
        
        return "unknown"
    
    async def _perform_data_diagnosis(
        self,
        file_metadata: Dict[str, Any],
        omics_type: str,
        dataframe: Optional[Any] = None,
        system_instruction: Optional[str] = None
    ) -> Optional[str]:
        """
        æ‰§è¡Œæ•°æ®è¯Šæ–­å¹¶ç”Ÿæˆ Markdown æŠ¥å‘Š
        
        ğŸ”¥ æ¶æ„é‡æ„ï¼šä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œæ¥å— domain-specific system_instruction
        
        è¿™æ˜¯ç»Ÿä¸€çš„æ•°æ®è¯Šæ–­å…¥å£ï¼Œæ‰€æœ‰ Agent éƒ½åº”è¯¥è°ƒç”¨æ­¤æ–¹æ³•ã€‚
        
        ğŸ”¥ TASK 3 & 4: é›†æˆè¯Šæ–­ç¼“å­˜æœºåˆ¶
        - é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„è¯Šæ–­ç»“æœ
        - å¦‚æœæœ‰ï¼Œç›´æ¥è¿”å›ç¼“å­˜çš„è¯Šæ–­æŠ¥å‘Š
        - å¦‚æœæ²¡æœ‰ï¼Œæ‰§è¡Œè¯Šæ–­å¹¶ä¿å­˜ç»“æœ
        
        Args:
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®
            omics_type: ç»„å­¦ç±»å‹ï¼ˆ"scRNA", "Metabolomics", "BulkRNA", "default"ï¼‰
            dataframe: å¯é€‰çš„æ•°æ®é¢„è§ˆï¼ˆDataFrame æˆ– AnnDataï¼‰
            system_instruction: é¢†åŸŸç‰¹å®šçš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆç”±å„ä¸ª Agent æä¾›ï¼‰
        
        Returns:
            Markdown æ ¼å¼çš„è¯Šæ–­æŠ¥å‘Šï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        try:
            # ğŸ”¥ TASK 4: æ£€æŸ¥è¯Šæ–­ç¼“å­˜
            from ..core.diagnosis_cache import get_diagnosis_cache
            cache = get_diagnosis_cache()
            
            file_path = file_metadata.get("file_path", "")
            if file_path:
                cached_diagnosis = cache.load_diagnosis(file_path)
                if cached_diagnosis:
                    logger.info(f"âœ… [DataDiagnostician] ä½¿ç”¨ç¼“å­˜çš„è¯Šæ–­ç»“æœ: {file_path}")
                    # è¿”å›ç¼“å­˜çš„è¯Šæ–­æŠ¥å‘Š
                    return cached_diagnosis.get("diagnosis_report")
            
            logger.info(f"ğŸ” [DataDiagnostician] å¼€å§‹æ•°æ®è¯Šæ–­ - ç»„å­¦ç±»å‹: {omics_type}")
            
            # Step 1: ä½¿ç”¨ DataDiagnostician è®¡ç®—ç»Ÿè®¡äº‹å®
            diagnosis_result = self.diagnostician.analyze(
                file_metadata=file_metadata,
                omics_type=omics_type,
                dataframe=dataframe
            )
            
            if diagnosis_result.get("status") != "success":
                logger.warning(f"âš ï¸ æ•°æ®è¯Šæ–­å¤±è´¥: {diagnosis_result.get('error')}")
                return None
            
            stats = diagnosis_result.get("stats", {})
            omics_type = diagnosis_result.get("omics_type") or ""
            logger.info(f"âœ… [DataDiagnostician] ç»Ÿè®¡è®¡ç®—å®Œæˆ: {len(stats)} ä¸ªæŒ‡æ ‡, omics_type={omics_type}")

            # Step 2: æ„å»º LLM Promptï¼ˆæŒ‰é¢†åŸŸé€‰æ‹©æ¨¡æ¿ï¼ŒRadiomics ä½¿ç”¨å½±åƒä¸“ç”¨æ¨¡æ¿ï¼‰
            # å°†ç»Ÿè®¡äº‹å®æ ¼å¼åŒ–ä¸º JSON å­—ç¬¦ä¸²
            import json
            try:
                stats_json = json.dumps(stats, ensure_ascii=False, indent=2)
                logger.debug(f"ğŸ“ [DEBUG] Stats JSON length: {len(stats_json)}")
            except Exception as json_err:
                logger.error(f"âŒ [DataDiagnostician] JSON åºåˆ—åŒ–å¤±è´¥: {json_err}")
                stats_json = json.dumps({"error": "æ— æ³•åºåˆ—åŒ–ç»Ÿè®¡ä¿¡æ¯"}, ensure_ascii=False)
            
            # ğŸ”¥ ä¿®å¤ï¼šå®‰å…¨åœ°æˆªæ–­ JSON å­—ç¬¦ä¸²ï¼ˆè€Œä¸æ˜¯å­—å…¸ï¼‰
            # å¦‚æœ JSON å¤ªé•¿ï¼Œæˆªæ–­å®ƒï¼ˆä½†ä¿ç•™å®Œæ•´çš„ç»“æ„ï¼‰
            max_json_length = 2000  # é™åˆ¶ JSON é•¿åº¦
            if len(stats_json) > max_json_length:
                logger.warning(f"âš ï¸ Stats JSON å¤ªé•¿ ({len(stats_json)} å­—ç¬¦)ï¼Œæˆªæ–­åˆ° {max_json_length} å­—ç¬¦")
                # æˆªæ–­å­—ç¬¦ä¸²ï¼Œä½†ç¡®ä¿ JSON ç»“æ„å®Œæ•´
                truncated_json = stats_json[:max_json_length]
                # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡/æ•°ç»„è¾¹ç•Œ
                last_brace = truncated_json.rfind('}')
                last_bracket = truncated_json.rfind(']')
                last_comma = max(truncated_json.rfind(','), truncated_json.rfind('\n'))
                # é€‰æ‹©æœ€æ¥è¿‘æœ«å°¾çš„è¾¹ç•Œ
                cut_point = max(last_brace, last_bracket, last_comma)
                if cut_point > max_json_length * 0.8:  # å¦‚æœæˆªæ–­ç‚¹ä¸å¤ªæ—©
                    stats_json = truncated_json[:cut_point + 1] + "\n  ... (truncated)"
                else:
                    stats_json = truncated_json + "\n  ... (truncated)"
            
            # ğŸ”¥ å®‰å…¨åœ°æå–æ–‡ä»¶é¢„è§ˆä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # æ³¨æ„ï¼šfile_metadata æ˜¯å­—å…¸ï¼Œä¸èƒ½ç›´æ¥åˆ‡ç‰‡
            head_preview = ""
            try:
                head_data = file_metadata.get("head", {})
                if isinstance(head_data, dict):
                    # head_data æ˜¯å­—å…¸ï¼ŒåŒ…å« "markdown" æˆ– "json" é”®
                    if "markdown" in head_data:
                        head_preview = head_data["markdown"]
                    elif "json" in head_data:
                        # å¦‚æœæ˜¯ JSON æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        head_preview = json.dumps(head_data["json"], ensure_ascii=False, indent=2)
                    else:
                        head_preview = str(head_data)
                elif isinstance(head_data, str):
                    # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    head_preview = head_data
                else:
                    head_preview = str(head_data)
                
                # ğŸ”¥ å®‰å…¨åœ°æˆªæ–­å­—ç¬¦ä¸²é¢„è§ˆï¼ˆä¸æ˜¯å­—å…¸ï¼‰
                if len(head_preview) > 1000:
                    head_preview = head_preview[:1000] + "\n... (truncated)"
            except Exception as head_err:
                logger.warning(f"âš ï¸ æå–æ–‡ä»¶é¢„è§ˆå¤±è´¥: {head_err}")
                head_preview = "æ— æ³•æå–æ•°æ®é¢„è§ˆ"
            
            # ä½¿ç”¨ PromptManager è·å–è¯Šæ–­æ¨¡æ¿ï¼ˆæŒ‰ omics_type é€‰æ‹©é¢†åŸŸä¸“ç”¨æ¨¡æ¿ï¼‰
            try:
                if (omics_type or "").lower() in ("radiomics", "medical_image", "imaging") or stats.get("_imaging_only"):
                    from ..core.prompts.radiomics_prompts import RADIOMICS_DIAGNOSIS_TEMPLATE
                    dimensions = stats.get("dimensions_str", "N/A")
                    spacing = stats.get("spacing_str", "N/A")
                    mask_status = "å·²æä¾›" if stats.get("mask_present") else "æœªæä¾›"
                    origin = stats.get("origin")
                    origin_str = str(origin) if origin is not None else "N/A"
                    prompt = self.prompt_manager.get_prompt(
                        "data_diagnosis_radiomics",
                        {
                            "dimensions": dimensions,
                            "spacing": spacing,
                            "mask_status": mask_status,
                            "origin": origin_str,
                            "inspection_data": stats_json,
                        },
                        fallback=RADIOMICS_DIAGNOSIS_TEMPLATE
                    )
                else:
                    prompt = self.prompt_manager.get_prompt(
                        "data_diagnosis",
                        {
                            "inspection_data": stats_json,
                            "head_preview": head_preview[:500] if head_preview else "",
                        },
                        fallback=DATA_DIAGNOSIS_PROMPT.format(inspection_data=stats_json)
                    )
                logger.debug(f"ğŸ“ [DEBUG] Prompt length: {len(prompt)}")
            except Exception as prompt_err:
                logger.warning(f"âš ï¸ è·å–è¯Šæ–­æ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {prompt_err}")
                try:
                    if not isinstance(stats_json, str):
                        stats_json = json.dumps(stats_json, ensure_ascii=False)
                    if (omics_type or "").lower() in ("radiomics", "medical_image", "imaging") or stats.get("_imaging_only"):
                        from ..core.prompts.radiomics_prompts import RADIOMICS_DIAGNOSIS_TEMPLATE
                        dimensions = stats.get("dimensions_str", "N/A")
                        spacing = stats.get("spacing_str", "N/A")
                        mask_status = "å·²æä¾›" if stats.get("mask_present") else "æœªæä¾›"
                        origin_str = str(stats.get("origin", "N/A"))
                        prompt = RADIOMICS_DIAGNOSIS_TEMPLATE.replace("{{ dimensions }}", dimensions).replace("{{ spacing }}", spacing).replace("{{ mask_status }}", mask_status).replace("{{ origin }}", origin_str).replace("{{ inspection_data }}", stats_json)
                    else:
                        prompt = DATA_DIAGNOSIS_PROMPT.format(inspection_data=stats_json)
                except Exception as format_err:
                    logger.error(f"âŒ [DataDiagnostician] Prompt æ ¼å¼åŒ–å¤±è´¥: {format_err}")
                    if not isinstance(stats_json, str):
                        stats_json = json.dumps(stats_json, ensure_ascii=False)
                    prompt = f"""You are a Senior Bioinformatician specializing in {omics_type}.

Based on the following data statistics:
{stats_json}

Please generate a data diagnosis and parameter recommendation report in Simplified Chinese (ç®€ä½“ä¸­æ–‡).

Format:
### ğŸ” æ•°æ®ä½“æ£€æŠ¥å‘Š
- **æ•°æ®è§„æ¨¡**: [æ ·æœ¬æ•°ã€ä»£è°¢ç‰©æ•°]
- **æ•°æ®ç‰¹å¾**: [ç¼ºå¤±å€¼ç‡ã€æ•°æ®èŒƒå›´ç­‰]
- **æ•°æ®è´¨é‡**: [è´¨é‡è¯„ä¼°]

### ğŸ’¡ å‚æ•°æ¨è
Create a Markdown table with parameter recommendations.

Use Simplified Chinese for all content."""
            
            # Step 3: è°ƒç”¨ LLM ç”Ÿæˆ Markdown æŠ¥å‘Š
            # ğŸ”¥ CRITICAL FIX: å¼ºåˆ¶æ³¨å…¥ç»Ÿè®¡æ•°æ®åˆ°ç³»ç»Ÿæç¤ºï¼Œé˜²æ­¢ LLM äº§ç”Ÿå¹»è§‰ï¼ˆæŒ‰é¢†åŸŸåŒºåˆ†ï¼‰
            stats_facts = []
            if (omics_type or "").lower() in ("radiomics", "medical_image", "imaging") or stats.get("_imaging_only"):
                stats_facts.append(f"å½±åƒå°ºå¯¸: {stats.get('dimensions_str', 'N/A')}ï¼›å±‚åš/é—´è·: {stats.get('spacing_str', 'N/A')}ï¼›æ©è†œ: {'å·²æä¾›' if stats.get('mask_present') else 'æœªæä¾›'}ã€‚")
            elif omics_type.lower() in ["metabolomics", "metabolomic", "metabonomics"]:
                n_samples = stats.get("n_samples", 0)
                n_metabolites = stats.get("n_metabolites", 0)
                missing_rate = stats.get("missing_rate", 0)
                stats_facts.append(f"æ•°æ®é›†åŒ…å« {n_samples} ä¸ªæ ·æœ¬å’Œ {n_metabolites} ä¸ªä»£è°¢ç‰©ã€‚")
                if missing_rate > 0:
                    stats_facts.append(f"ç¼ºå¤±å€¼ç‡ä¸º {missing_rate:.2f}%ã€‚")
            elif omics_type.lower() in ["scrna", "scrna-seq", "single_cell", "single-cell"]:
                n_cells = stats.get("n_cells", 0)
                n_genes = stats.get("n_genes", 0)
                stats_facts.append(f"æ•°æ®é›†åŒ…å« {n_cells} ä¸ªç»†èƒå’Œ {n_genes} ä¸ªåŸºå› ã€‚")
            else:
                n_rows = stats.get("n_rows", stats.get("n_samples", 0))
                n_cols = stats.get("n_cols", stats.get("n_features", 0))
                stats_facts.append(f"æ•°æ®é›†åŒ…å« {n_rows} è¡Œå’Œ {n_cols} åˆ—ã€‚")
            
            # æ„å»ºå¼ºåˆ¶äº‹å®å­—ç¬¦ä¸²
            facts_str = " ".join(stats_facts) if stats_facts else "ç»Ÿè®¡æ•°æ®å·²æä¾›åœ¨ç”¨æˆ·æç¤ºä¸­ã€‚"
            
            # ğŸ”¥ CRITICAL DEBUGGING: æ£€æŸ¥æ•°æ®æ˜¯å¦ç¼ºå¤±ï¼Œå¦‚æœç¼ºå¤±åˆ™æ³¨å…¥è°ƒè¯•è·Ÿè¸ª
            n_samples_value = stats.get("n_samples", stats.get("n_cells", stats.get("n_rows", 0)))
            debug_trace = file_metadata.get("debug_trace")
            
            # å¦‚æœæ•°æ®ç¼ºå¤±ï¼ˆ0 samplesï¼‰ï¼Œå¼ºåˆ¶æ³¨å…¥è°ƒè¯•è·Ÿè¸ªåˆ°ç³»ç»Ÿæç¤º
            debug_section = ""
            if n_samples_value == 0 and debug_trace:
                debug_section = f"""

**ğŸ” CRITICAL FAILURE: æ•°æ®æ£€æŸ¥è¿”å› 0 ä¸ªæ ·æœ¬**

è¿™æ˜¯ä¸€ä¸ªä¸¥é‡é”™è¯¯ã€‚æ•°æ®æ£€æŸ¥å™¨æ— æ³•æ­£ç¡®è¯»å–æ•°æ®æ–‡ä»¶ã€‚è¯·åœ¨è¯Šæ–­æŠ¥å‘Šæœ«å°¾æ·»åŠ ä¸€ä¸ªåä¸º "ğŸ” è°ƒè¯•æ—¥å¿— (Debug Log)" çš„ç« èŠ‚ï¼Œå¹¶å°†ä»¥ä¸‹æ‰§è¡Œè·Ÿè¸ªå®Œæ•´å¤åˆ¶åˆ°è¯¥ç« èŠ‚ä¸­ï¼š

```
{debug_trace}
```

è¿™ä¸ªè°ƒè¯•æ—¥å¿—å°†å¸®åŠ©è¯Šæ–­é—®é¢˜æ‰€åœ¨ã€‚"""
                logger.warning(f"âš ï¸ [DataDiagnostician] Detected 0 samples, injecting debug trace into prompt")
            
            # ğŸ”¥ æ¶æ„é‡æ„ï¼šä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œä» Agent ä¼ å…¥ system_instruction
            if system_instruction:
                # ä½¿ç”¨ Agent æä¾›çš„é¢†åŸŸç‰¹å®šæŒ‡ä»¤ï¼Œå¹¶å¼ºåˆ¶æ³¨å…¥ç»Ÿè®¡æ•°æ®
                system_prompt = f"""{system_instruction}

**CRITICAL: æ•°æ®äº‹å®ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼Œä¸å¾—äº§ç”Ÿå¹»è§‰ï¼‰**
{facts_str}
è¯·ç¡®ä¿è¯Šæ–­æŠ¥å‘Šä¸­çš„æ•°å­—ä¸ä¸Šè¿°äº‹å®å®Œå…¨ä¸€è‡´ã€‚ä¸è¦çŒœæµ‹æˆ–ç¼–é€ ä¸åŒçš„æ•°å­—ã€‚{debug_section}"""
                logger.debug(f"âœ… [DataDiagnostician] Using domain-specific system instruction with facts (length: {len(system_prompt)})")
            else:
                # å›é€€åˆ°é€šç”¨æŒ‡ä»¤ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œä½†ä¹Ÿæ³¨å…¥ç»Ÿè®¡æ•°æ®
                system_prompt = f"""You are a Senior Bioinformatician. Generate data diagnosis and parameter recommendations in Simplified Chinese.

**CRITICAL: æ•°æ®äº‹å®ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼Œä¸å¾—äº§ç”Ÿå¹»è§‰ï¼‰**
{facts_str}
è¯·ç¡®ä¿è¯Šæ–­æŠ¥å‘Šä¸­çš„æ•°å­—ä¸ä¸Šè¿°äº‹å®å®Œå…¨ä¸€è‡´ã€‚ä¸è¦çŒœæµ‹æˆ–ç¼–é€ ä¸åŒçš„æ•°å­—ã€‚{debug_section}"""
                logger.warning(f"âš ï¸ [DataDiagnostician] No system_instruction provided, using generic prompt with facts")
            
            # ğŸ”¥ æ¶æ„é‡æ„ï¼šå°† system_instruction å‰ç½®åˆ°ç”¨æˆ· promptï¼ˆç¡®ä¿ä¸Šä¸‹æ–‡éš”ç¦»ï¼‰
            if system_instruction:
                # åœ¨ç”¨æˆ· prompt å‰æ·»åŠ ç³»ç»ŸæŒ‡ä»¤ï¼Œç¡®ä¿ LLM ç†è§£é¢†åŸŸçº¦æŸ
                prompt = f"""{system_instruction}

**æ•°æ®äº‹å®ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰ï¼š**
{facts_str}

{prompt}"""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            # ğŸ”¥ Step 3: è°ƒç”¨ LLM ç”Ÿæˆ Markdown æŠ¥å‘Š
            # ğŸ”¥ CRITICAL DEBUGGING: åŒ…è£…åœ¨è¯¦ç»†çš„ try-except ä¸­
            # ğŸ”¥ TASK 2: ç»Ÿä¸€LLMå®¢æˆ·ç«¯è·å–é€»è¾‘ï¼ˆä¸è§„åˆ’é˜¶æ®µä¸€è‡´ï¼‰
            try:
                # ğŸ”¥ TASK 2: å¦‚æœ self.llm_client ä¸å¯ç”¨ï¼Œä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»ºï¼ˆä¸è§„åˆ’é˜¶æ®µä¸€è‡´ï¼‰
                llm_client_to_use = self.llm_client
                if not llm_client_to_use:
                    logger.warning("âš ï¸ [DataDiagnostician] self.llm_client ä¸å¯ç”¨ï¼Œä½¿ç”¨ LLMClientFactory.create_default()")
                    from gibh_agent.core.llm_client import LLMClientFactory
                    llm_client_to_use = LLMClientFactory.create_default()
                    logger.info(f"âœ… [DataDiagnostician] å·²åˆ›å»ºé»˜è®¤LLMå®¢æˆ·ç«¯: {llm_client_to_use.base_url}")
                
                logger.info(f"ğŸ“ [DataDiagnostician] è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š...")
                logger.info(f"ğŸ“Š [DataDiagnostician] ç»Ÿè®¡æ•°æ®æ‘˜è¦: n_samples={stats.get('n_samples', stats.get('n_cells', stats.get('n_rows', 0)))}, n_features={stats.get('n_features', stats.get('n_genes', stats.get('n_metabolites', stats.get('n_cols', 0))))}")
                logger.info(f"ğŸ“Š [DataDiagnostician] Stats JSON é•¿åº¦: {len(stats_json)} å­—ç¬¦")
                if len(stats_json) < 100:
                    logger.warning(f"âš ï¸ [DataDiagnostician] è­¦å‘Šï¼šStats JSON è¿‡çŸ­ï¼Œå¯èƒ½ç¼ºå°‘å…³é”®æ•°æ®")
                logger.debug(f"ğŸ“ [DEBUG] LLM Client type: {type(llm_client_to_use)}")
                logger.debug(f"ğŸ“ [DEBUG] LLM Client methods: {dir(llm_client_to_use)}")
                
                completion = await llm_client_to_use.achat(messages, temperature=0.3, max_tokens=1500)
                logger.info(f"âœ… [DataDiagnostician] LLMè°ƒç”¨å®Œæˆï¼Œå¼€å§‹è§£æå“åº”...")
                
                logger.debug(f"ğŸ“ [DEBUG] LLM completion type: {type(completion)}")
                logger.debug(f"ğŸ“ [DEBUG] LLM completion: {completion}")
                
                think_content, response = llm_client_to_use.extract_think_and_content(completion)
                # ğŸ”¥ DRY: Strip <<<SUGGESTIONS>>> block so it never reaches the frontend
                if response:
                    response, suggestions = strip_suggestions_from_text(response)
                    self.context["diagnosis_suggestions"] = suggestions
                # ğŸ”¥ DEBUG: æ‰“å°è¯Šæ–­æŠ¥å‘Šä¿¡æ¯
                if response:
                    logger.info(f"âœ… [DataDiagnostician] è¯Šæ–­æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(response)}")
                    logger.debug(f"ğŸ“ [DEBUG] Diagnosis report preview: {response[:200]}...")
                else:
                    logger.warning(f"âš ï¸ [DataDiagnostician] è¯Šæ–­æŠ¥å‘Šä¸ºç©º")
                    logger.warning(f"âš ï¸ [DEBUG] Think content: {think_content[:200] if think_content else 'None'}")
                
                # Step 4: ä»è¯Šæ–­æŠ¥å‘Šä¸­æå–å‚æ•°æ¨è
                # ğŸ”¥ TASK 5: è§£æè¯Šæ–­æŠ¥å‘Šä¸­çš„å‚æ•°æ¨èè¡¨æ ¼
                recommendation = self._extract_parameter_recommendations(response, omics_type, stats)
                
                # Step 5: ä¿å­˜åˆ°ä¸Šä¸‹æ–‡ï¼ˆä¾› UI å’Œåç»­æ­¥éª¤ä½¿ç”¨ï¼‰
                self.context["diagnosis_report"] = response
                self.context["diagnosis_stats"] = stats
                if recommendation:
                    self.context["parameter_recommendation"] = recommendation
                
                # ğŸ”¥ TASK 4: ä¿å­˜è¯Šæ–­ç»“æœåˆ°ç¼“å­˜
                if file_path and response:
                    cache_data = {
                        "diagnosis_report": response,
                        "stats": stats,
                        "recommendation": recommendation,
                        "omics_type": omics_type
                    }
                    cache.save_diagnosis(file_path, cache_data, file_metadata)
                    logger.info(f"âœ… [DataDiagnostician] è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜")
                
                return response
                
            except AttributeError as attr_err:
                # LLM å®¢æˆ·ç«¯æ–¹æ³•ä¸å­˜åœ¨
                import traceback
                error_msg = (
                    f"âŒ [DataDiagnostician] LLM å®¢æˆ·ç«¯æ–¹æ³•è°ƒç”¨å¤±è´¥\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"é”™è¯¯ç±»å‹: AttributeError\n"
                    f"é”™è¯¯ä¿¡æ¯: {str(attr_err)}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"è°ƒç”¨ä¸Šä¸‹æ–‡:\n"
                    f"  - LLM Client type: {type(llm_client_to_use)}\n"
                    f"  - LLM Client base_url: {llm_client_to_use.base_url if hasattr(llm_client_to_use, 'base_url') else 'N/A'}\n"
                    f"  - LLM Client model: {llm_client_to_use.model if hasattr(llm_client_to_use, 'model') else 'N/A'}\n"
                    f"  - Available methods: {[m for m in dir(llm_client_to_use) if not m.startswith('_')]}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"è°ƒç”¨å‚æ•°:\n"
                    f"  - messagesæ•°é‡: {len(messages)}\n"
                    f"  - temperature: 0.3\n"
                    f"  - max_tokens: 1500\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
                )
                logger.error(error_msg)
                return f"âš ï¸ **è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥**\n\nLLM å®¢æˆ·ç«¯é”™è¯¯: {str(attr_err)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
                
            except Exception as llm_err:
                # LLM è°ƒç”¨å¤±è´¥ - ğŸ”¥ TASK 2: å¢å¼ºé”™è¯¯æ—¥å¿—è¾“å‡º
                import traceback
                error_type = type(llm_err).__name__
                error_msg = (
                    f"âŒ [DataDiagnostician] LLM è°ƒç”¨å¤±è´¥\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"é”™è¯¯ç±»å‹: {error_type}\n"
                    f"é”™è¯¯ä¿¡æ¯: {str(llm_err)}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"è°ƒç”¨ä¸Šä¸‹æ–‡:\n"
                    f"  - LLM Client base_url: {llm_client_to_use.base_url if hasattr(llm_client_to_use, 'base_url') else 'N/A'}\n"
                    f"  - LLM Client model: {llm_client_to_use.model if hasattr(llm_client_to_use, 'model') else 'N/A'}\n"
                    f"  - API Key: {'å·²è®¾ç½®' if hasattr(llm_client_to_use, 'api_key') and llm_client_to_use.api_key else 'æœªè®¾ç½®'}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"è°ƒç”¨å‚æ•°:\n"
                    f"  - messagesæ•°é‡: {len(messages)}\n"
                    f"  - system messageé•¿åº¦: {len(messages[0]['content']) if messages else 0} å­—ç¬¦\n"
                    f"  - user messageé•¿åº¦: {len(messages[1]['content']) if len(messages) > 1 else 0} å­—ç¬¦\n"
                    f"  - temperature: 0.3\n"
                    f"  - max_tokens: 1500\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"å¯èƒ½åŸå› :\n"
                    f"  - APIå¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ\n"
                    f"  - ç½‘ç»œè¿æ¥é—®é¢˜\n"
                    f"  - APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨\n"
                    f"  - è¯·æ±‚è¶…æ—¶\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
                )
                logger.error(error_msg)
                
                # ğŸ”¥ TASK: å°†è¯¦ç»†é”™è¯¯ä¿¡æ¯å­˜å‚¨åˆ°contextï¼Œä¾›orchestratoré€šè¿‡SSEå‘é€åˆ°å‰ç«¯
                self.context["last_llm_error"] = {
                    "error_type": error_type,
                    "error_message": str(llm_err),
                    "error_details": error_msg,
                    "context": {
                        "llm_client_base_url": llm_client_to_use.base_url if hasattr(llm_client_to_use, 'base_url') else 'N/A',
                        "llm_client_model": llm_client_to_use.model if hasattr(llm_client_to_use, 'model') else 'N/A',
                        "api_key_set": bool(hasattr(llm_client_to_use, 'api_key') and llm_client_to_use.api_key),
                        "messages_count": len(messages),
                        "temperature": 0.3,
                        "max_tokens": 1500
                    },
                    "possible_causes": [
                        "APIå¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ",
                        "ç½‘ç»œè¿æ¥é—®é¢˜",
                        "APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
                        "è¯·æ±‚è¶…æ—¶"
                    ]
                }
                
                return f"âš ï¸ **è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥**\n\né”™è¯¯: {str(llm_err)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
            
        except Exception as e:
            # æ•´ä½“å¼‚å¸¸å¤„ç†
            import traceback
            error_msg = (
                f"æ•°æ®è¯Šæ–­è¿‡ç¨‹å¤±è´¥: {str(e)}\n"
                f"Error type: {type(e).__name__}\n"
                f"Stack trace:\n{traceback.format_exc()}"
            )
            logger.error(f"âŒ [DataDiagnostician] {error_msg}")
            # ğŸ”¥ è¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ Noneï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥åœ¨ UI ä¸­çœ‹åˆ°
            return f"âš ï¸ **è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥**\n\né”™è¯¯: {str(e)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
    
    def _read_execution_results(self, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        ğŸ”¥ TASK 3: Read actual execution results from generated files
        
        Args:
            output_dir: Output directory path (from executor)
        
        Returns:
            Dictionary containing:
            - csv_files: List of CSV files with head rows and describe()
            - image_files: List of generated PNG images
            - summary: Text summary of findings
        """
        import pandas as pd
        from pathlib import Path
        import os
        
        results_context = {
            "csv_files": [],
            "image_files": [],
            "summary": ""
        }
        
        if not output_dir or not os.path.exists(output_dir):
            logger.warning(f"âš ï¸ [ResultReader] è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
            return results_context
        
        try:
            output_path = Path(output_dir)
            
            # ğŸ”¥ TASK 3: Read CSV files (e.g., deg_results.csv, pathway_results.csv)
            csv_files = list(output_path.glob("*.csv"))
            for csv_file in csv_files[:5]:  # Limit to first 5 CSV files
                try:
                    df = pd.read_csv(csv_file, nrows=100)  # Read first 100 rows
                    head_rows = df.head(10).to_dict(orient='records')
                    
                    # Get describe() statistics
                    numeric_cols = df.select_dtypes(include=['number']).columns
                    describe_stats = {}
                    if len(numeric_cols) > 0:
                        describe_stats = df[numeric_cols].describe().to_dict()
                    
                    results_context["csv_files"].append({
                        "filename": csv_file.name,
                        "path": str(csv_file),
                        "shape": f"{len(df)} rows Ã— {len(df.columns)} columns",
                        "columns": list(df.columns),
                        "head_rows": head_rows[:5],  # First 5 rows
                        "statistics": describe_stats,
                        "summary": f"Found {len(df)} rows with {len(df.columns)} columns. Top columns: {', '.join(df.columns[:5])}"
                    })
                    logger.info(f"âœ… [ResultReader] è¯»å–CSVæ–‡ä»¶: {csv_file.name} ({len(df)} rows)")
                except Exception as e:
                    logger.warning(f"âš ï¸ [ResultReader] æ— æ³•è¯»å–CSVæ–‡ä»¶ {csv_file.name}: {e}")
            
            # ğŸ”¥ TASK 3: List image files (PNG, JPG, etc.)
            image_extensions = ['.png', '.jpg', '.jpeg', '.pdf', '.svg']
            for ext in image_extensions:
                image_files = list(output_path.glob(f"*{ext}"))
                for img_file in image_files[:10]:  # Limit to first 10 images per type
                    results_context["image_files"].append({
                        "filename": img_file.name,
                        "path": str(img_file),
                        "type": ext[1:]  # Remove dot
                    })
            
            # Build summary text
            if results_context["csv_files"]:
                csv_summary = f"å‘ç° {len(results_context['csv_files'])} ä¸ªCSVç»“æœæ–‡ä»¶:\n"
                for csv_info in results_context["csv_files"]:
                    csv_summary += f"- {csv_info['filename']}: {csv_info['shape']}, åˆ—: {', '.join(csv_info['columns'][:5])}\n"
                results_context["summary"] += csv_summary
            
            if results_context["image_files"]:
                img_summary = f"\nå‘ç° {len(results_context['image_files'])} ä¸ªå›¾ç‰‡æ–‡ä»¶:\n"
                for img_info in results_context["image_files"][:5]:
                    img_summary += f"- {img_info['filename']}\n"
                results_context["summary"] += img_summary
            
            logger.info(f"âœ… [ResultReader] è¯»å–æ‰§è¡Œç»“æœå®Œæˆ: {len(results_context['csv_files'])} CSV, {len(results_context['image_files'])} å›¾ç‰‡")
            
        except Exception as e:
            logger.error(f"âŒ [ResultReader] è¯»å–æ‰§è¡Œç»“æœå¤±è´¥: {e}", exc_info=True)
        
        return results_context
    
    async def _generate_analysis_summary(
        self,
        steps_results: List[Dict[str, Any]],
        omics_type: str = "Metabolomics",
        workflow_name: str = "Analysis Pipeline",
        summary_context: Optional[Dict[str, Any]] = None,
        output_dir: Optional[str] = None  # ğŸ”¥ TASK 3: Add output_dir parameter
    ) -> Optional[str]:
        """
        åŸºäºå·¥ä½œæµæ‰§è¡Œç»“æœç”Ÿæˆåˆ†ææ‘˜è¦ï¼ˆAI Expert Diagnosisï¼‰
        
        Args:
            steps_results: æ­¥éª¤æ‰§è¡Œç»“æœåˆ—è¡¨ï¼ˆæ¥è‡ª ExecutionLayerï¼‰
            omics_type: ç»„å­¦ç±»å‹ï¼ˆ"Metabolomics", "scRNA", ç­‰ï¼‰
            workflow_name: å·¥ä½œæµåç§°
            summary_context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å«å¤±è´¥æ­¥éª¤ç­‰ï¼‰
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆç”¨äºè¯»å–ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
        
        Returns:
            Markdown æ ¼å¼çš„åˆ†ææ‘˜è¦ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        import json
        
        try:
            logger.info(f"ğŸ“ [AnalysisSummary] å¼€å§‹ç”Ÿæˆåˆ†ææ‘˜è¦ - ç»„å­¦ç±»å‹: {omics_type}")
            
            # ğŸ”¥ TASK 3: Read actual execution results from files
            execution_results = self._read_execution_results(output_dir)
            
            # ğŸ”¥ CRITICAL FIX: Extract failure information from context
            has_failures = False
            failed_steps_info = []
            if summary_context:
                has_failures = summary_context.get("has_failures", False)
                failed_steps_info = summary_context.get("failed_steps", [])
            
            # æå–å…³é”®ç»“æœ
            results_summary = {
                "workflow_name": workflow_name,
                "steps_completed": len(steps_results),
                "steps": [],
                "has_failures": has_failures,
                "failed_steps": failed_steps_info
            }
            
            # ğŸ”¥ CRITICAL FIX: Process both successful and failed steps
            successful_steps = []
            failed_steps = []
            
            for step_result in steps_results:
                step_data = step_result.get("data", {})
                step_name = step_result.get("step_name", "Unknown Step")
                step_status = step_result.get("status", "unknown")
                
                if step_status == "success":
                    successful_steps.append(step_result)
                else:
                    failed_steps.append({
                        "name": step_name,
                        "status": step_status,
                        "error": step_result.get("error") or step_result.get("message", "æœªçŸ¥é”™è¯¯")
                    })
                    logger.debug(f"âš ï¸ [AnalysisSummary] è®°å½•å¤±è´¥çš„æ­¥éª¤: {step_name} (status: {step_status})")
            
            # ğŸ”¥ ä¿®å¤ï¼šåªæå–æ ¸å¿ƒå­—æ®µï¼Œä¸åŒ…å«å®Œæ•´step_dataï¼Œé¿å…JSONè¿‡é•¿
            # è§£ææˆåŠŸçš„æ­¥éª¤ç»“æœ
            for step_result in successful_steps:
                step_data = step_result.get("data", {})
                step_name = step_result.get("step_name", "Unknown Step")
                step_status = step_result.get("status", "unknown")
                
                # ğŸ”¥ ä¿®å¤ï¼šåªåˆ›å»ºæœ€å°åŒ–çš„step_infoï¼Œä¸åŒ…å«ä»»ä½•å¤§å‹æ•°æ®
                step_info = {
                    "name": step_name,
                    "status": step_status
                }
                
                # ğŸ”¥ ä¿®å¤ï¼šåªä»summaryä¸­æå–å…³é”®æŒ‡æ ‡ï¼Œä¸åŒ…å«dataä¸­çš„å…¶ä»–å­—æ®µï¼ˆå¦‚file_pathã€previewç­‰ï¼‰
                # ğŸ”¥ æ³¨æ„ï¼šsummaryå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆRNAåˆ†æï¼‰æˆ–å­—å…¸ï¼ˆä»£è°¢ç»„å­¦åˆ†æï¼‰
                summary_raw = step_data.get("summary", {})
                if isinstance(summary_raw, str):
                    # å¦‚æœsummaryæ˜¯å­—ç¬¦ä¸²ï¼ˆRNAåˆ†æï¼‰ï¼Œå°†å…¶è½¬æ¢ä¸ºç©ºå­—å…¸ï¼Œä»step_dataä¸­ç›´æ¥æå–æŒ‡æ ‡
                    summary = {}
                else:
                    # å¦‚æœsummaryæ˜¯å­—å…¸ï¼ˆä»£è°¢ç»„å­¦åˆ†æï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                    summary = summary_raw if isinstance(summary_raw, dict) else {}
                
                # æ ¹æ®ä¸åŒçš„å·¥å…·ç±»å‹æå–å…³é”®æŒ‡æ ‡
                if "inspect_data" in step_name.lower() or "inspection" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šä»step_dataä¸­æå–å…³é”®æŒ‡æ ‡ï¼ˆå› ä¸ºsummaryå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
                    # ä¼˜å…ˆä»step_dataä¸­æå–ï¼Œå¦‚æœsummaryæ˜¯å­—å…¸ï¼Œä¹Ÿå¯ä»¥ä»summaryä¸­æå–
                    step_info["n_samples"] = step_data.get("n_samples", step_data.get("n_cells", summary.get("n_samples", summary.get("n_cells", "N/A"))))
                    step_info["n_features"] = step_data.get("n_features", step_data.get("n_genes", summary.get("n_features", summary.get("n_genes", "N/A"))))
                    step_info["missing_rate"] = step_data.get("missing_rate", summary.get("missing_rate", "N/A"))
                    # ğŸ”¥ RNAåˆ†æç‰¹å®šæŒ‡æ ‡
                    if "n_cells" in step_data or "n_cells" in summary:
                        step_info["n_cells"] = step_data.get("n_cells", summary.get("n_cells", "N/A"))
                    if "n_genes" in step_data or "n_genes" in summary:
                        step_info["n_genes"] = step_data.get("n_genes", summary.get("n_genes", "N/A"))
                    if "mitochondrial_percentage" in step_data or "mitochondrial_percentage" in summary:
                        step_info["mitochondrial_percentage"] = step_data.get("mitochondrial_percentage", summary.get("mitochondrial_percentage", "N/A"))
                    # ğŸ”¥ ä¸åŒ…å«file_pathã€previewç­‰å¤§å‹æ•°æ®
                
                elif "differential" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šåªä»summaryæå–å…³é”®æŒ‡æ ‡ï¼Œä¸åŒ…å«å®Œæ•´çš„resultsåˆ—è¡¨
                    if summary:
                        # Use summary dict if available (from Phase 2 enhancement)
                        step_info["significant_count"] = summary.get("significant_count", summary.get("sig_count", "N/A"))
                        step_info["total_count"] = summary.get("total_metabolites", "N/A")
                        step_info["method"] = summary.get("method", "N/A")
                        step_info["case_group"] = summary.get("case_group", "N/A")
                        step_info["control_group"] = summary.get("control_group", "N/A")
                        # ğŸ”¥ ä¿®å¤ï¼šåªä¿ç•™topæ ‡è®°ç‰©åç§°ï¼Œä¸åŒ…å«å®Œæ•´æ•°æ®
                        top_markers = summary.get("top_markers", [])
                        if top_markers:
                            step_info["top_markers"] = [
                                {
                                    "name": m.get("name", m.get("metabolite", "Unknown")),
                                    "log2fc": round(m.get("log2fc", 0), 3),  # åªä¿ç•™3ä½å°æ•°
                                    "fdr": round(m.get("fdr", m.get("fdr_corrected_pvalue", 1.0)), 4)  # åªä¿ç•™4ä½å°æ•°
                                }
                                for m in top_markers[:3]  # ğŸ”¥ åªä¿ç•™top 3
                            ]
                        else:
                            # Fallback: ä»top_up/top_downæå–åç§°ï¼ˆåªä¿ç•™åç§°ï¼Œä¸åŒ…å«å…¶ä»–æ•°æ®ï¼‰
                            top_up = summary.get("top_up", [])[:3]
                            top_down = summary.get("top_down", [])[:3]
                            step_info["top_up_names"] = [str(m) for m in top_up] if top_up else []
                            step_info["top_down_names"] = [str(m) for m in top_down] if top_down else []
                    else:
                        # Fallback: åªæå–å…³é”®è®¡æ•°ï¼Œä¸åŒ…å«å®Œæ•´resultsåˆ—è¡¨
                        step_info["significant_count"] = "N/A"
                        step_info["total_count"] = "N/A"
                        step_info["method"] = "N/A"
                        step_info["case_group"] = "N/A"
                        step_info["control_group"] = "N/A"
                    
                    # ğŸ”¥ ä¿®å¤ï¼šä¸æå–å®Œæ•´çš„results_listï¼Œåªä»summaryä¸­è·å–topæ ‡è®°ç‰©
                    # å¦‚æœsummaryä¸­æ²¡æœ‰top_markersï¼Œåˆ™è·³è¿‡ï¼ˆé¿å…å¤„ç†å¤§å‹resultsåˆ—è¡¨ï¼‰
                
                elif "plsda" in step_name.lower() or "pls-da" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šåªä»summaryæå–å…³é”®æŒ‡æ ‡ï¼Œä¸åŒ…å«å®Œæ•´çš„vip_scoresåˆ—è¡¨
                    if summary and isinstance(summary, dict):
                        # Use summary dict if available (from Phase 2 enhancement)
                        top_vip_markers = summary.get("top_vip_markers", [])
                        if top_vip_markers:
                            # ğŸ”¥ åªä¿ç•™top 3ï¼ŒåªåŒ…å«åç§°å’ŒVIPåˆ†æ•°ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
                            step_info["top_vip_markers"] = [
                                {
                                    "name": v.get("name", "Unknown"),
                                    "vip": round(v.get("vip", v.get("vip_score", 0)), 2)
                                }
                                for v in top_vip_markers[:3]  # ğŸ”¥ åªä¿ç•™top 3
                            ]
                        else:
                            step_info["top_vip_markers"] = []
                        step_info["n_components"] = summary.get("n_components", "N/A")
                        step_info["comp1_variance"] = f"{summary.get('comp1_variance', 0):.1f}%"
                        step_info["comp2_variance"] = f"{summary.get('comp2_variance', 0):.1f}%"
                    else:
                        # ğŸ”¥ ä¿®å¤ï¼šä¸å¤„ç†å®Œæ•´çš„vip_scoresåˆ—è¡¨ï¼Œåªè®¾ç½®é»˜è®¤å€¼
                        step_info["top_vip_markers"] = []
                        step_info["n_components"] = "N/A"
                        step_info["comp1_variance"] = "N/A"
                        step_info["comp2_variance"] = "N/A"
                
                elif "pathway" in step_name.lower() or "enrichment" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šåªä»summaryæå–å…³é”®æŒ‡æ ‡ï¼Œä¸åŒ…å«å®Œæ•´çš„enriched_pathwaysåˆ—è¡¨
                    if summary and isinstance(summary, dict):
                        # Use summary dict if available (from Phase 2 enhancement)
                        step_info["enriched_pathway_count"] = summary.get("n_significant", 0)
                        top_pathways_list = summary.get("top_pathways", [])
                        if top_pathways_list:
                            # ğŸ”¥ åªä¿ç•™top 3é€šè·¯åç§°ï¼Œä¸åŒ…å«å®Œæ•´æ•°æ®
                            step_info["top_pathways"] = [
                                {"name": p if isinstance(p, str) else p.get("name", "Unknown")}
                                for p in top_pathways_list[:3]  # ğŸ”¥ åªä¿ç•™top 3
                            ]
                        else:
                            step_info["top_pathways"] = []
                    else:
                        # ğŸ”¥ ä¿®å¤ï¼šä¸å¤„ç†å®Œæ•´çš„enriched_pathwaysåˆ—è¡¨ï¼Œåªè®¾ç½®é»˜è®¤å€¼
                        step_info["enriched_pathway_count"] = 0
                        step_info["top_pathways"] = []
                
                elif "pca" in step_name.lower() and "visualize" not in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šä»dataä¸­æå–å…³é”®æŒ‡æ ‡ï¼ˆexplained_varianceåœ¨dataä¸­ï¼Œä¸åœ¨summaryä¸­ï¼‰
                    explained_variance = step_data.get("explained_variance", {})
                    if explained_variance and isinstance(explained_variance, dict):
                        pc1_var = explained_variance.get("PC1", 0)
                        pc2_var = explained_variance.get("PC2", 0)
                        # è®¡ç®—æ€»æ–¹å·®ï¼ˆå‰10ä¸ªPCçš„ç´¯è®¡ï¼‰
                        total_var = sum(explained_variance.get(f"PC{i+1}", 0) for i in range(min(10, len(explained_variance))))
                        step_info["pc1_variance"] = f"{pc1_var * 100:.1f}%"
                        step_info["pc2_variance"] = f"{pc2_var * 100:.1f}%"
                        step_info["total_variance"] = f"{total_var * 100:.1f}%"
                        # RNAåˆ†æçš„PCAé€šå¸¸ä¸è¯„ä¼°åˆ†ç¦»è´¨é‡ï¼Œä½†å¯ä»¥åŸºäºæ–¹å·®åˆ¤æ–­
                        if total_var > 0.3:
                            step_info["separation_quality"] = "clear"
                        elif total_var > 0.15:
                            step_info["separation_quality"] = "moderate"
                        else:
                            step_info["separation_quality"] = "weak"
                    else:
                        # ğŸ”¥ ä¿®å¤ï¼šä¸å¤„ç†å®Œæ•´çš„explained_varianceï¼Œåªè®¾ç½®é»˜è®¤å€¼
                        step_info["pc1_variance"] = "N/A"
                        step_info["pc2_variance"] = "N/A"
                        step_info["total_variance"] = "N/A"
                        step_info["separation_quality"] = "unknown"
                
                elif "preprocess" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šåªä»summaryæˆ–shapeæå–å…³é”®ä¿¡æ¯ï¼Œä¸åŒ…å«å®Œæ•´æ•°æ®
                    if summary and isinstance(summary, dict):
                        shape = summary.get("shape", {})
                    else:
                        shape = step_data.get("shape", {})
                    step_info["preprocessed_rows"] = shape.get("rows", "N/A")
                    step_info["preprocessed_cols"] = shape.get("columns", "N/A")
                    # ğŸ”¥ ä¸åŒ…å«å®Œæ•´çš„shapeæ•°æ®æˆ–å…¶ä»–å†—ä½™ä¿¡æ¯
                
                # ğŸ”¥ RNAåˆ†æç‰¹å®šæ­¥éª¤ï¼šè´¨é‡æ§åˆ¶
                elif "qc" in step_name.lower() or "quality" in step_name.lower() or "filter" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šä»dataä¸­æå–å…³é”®æŒ‡æ ‡ï¼ˆå› ä¸ºsummaryå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
                    step_info["cells_before_qc"] = step_data.get("n_obs_before", step_data.get("cells_before_qc", "N/A"))
                    step_info["cells_after_qc"] = step_data.get("n_obs_after", step_data.get("cells_after_qc", "N/A"))
                    step_info["genes_before_qc"] = step_data.get("n_vars_before", step_data.get("genes_before_qc", "N/A"))
                    step_info["genes_after_qc"] = step_data.get("n_vars_after", step_data.get("genes_after_qc", "N/A"))
                    step_info["mitochondrial_percentage"] = step_data.get("mitochondrial_percentage", "N/A")
                
                # ğŸ”¥ RNAåˆ†æç‰¹å®šæ­¥éª¤ï¼šæ ‡è®°åŸºå› è¯†åˆ«
                elif "marker" in step_name.lower() or "find_markers" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šä»dataä¸­æå–å…³é”®æŒ‡æ ‡
                    step_info["n_markers"] = step_data.get("n_genes_per_cluster", step_data.get("n_markers", "N/A"))
                    step_info["n_clusters"] = step_data.get("n_clusters", "N/A")
                    # ğŸ”¥ ä¿®å¤ï¼šä»markers_tableä¸­æå–topæ ‡è®°åŸºå› ï¼ˆåªä¿ç•™å‰3ä¸ªclusterçš„å‰3ä¸ªåŸºå› ï¼‰
                    markers_table = step_data.get("markers_table", [])
                    if markers_table and isinstance(markers_table, list) and len(markers_table) > 0:
                        # markers_tableæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å¤šä¸ªclusterçš„æ ‡è®°åŸºå› 
                        top_markers = []
                        for i, marker_row in enumerate(markers_table[:3]):  # åªå¤„ç†å‰3è¡Œ
                            if isinstance(marker_row, dict):
                                # æå–æ¯ä¸ªclusterçš„topåŸºå› ï¼ˆä»åˆ—åä¸­æå–clusterç¼–å·ï¼‰
                                for key, value in marker_row.items():
                                    if "_names" in key and value:
                                        cluster_num = key.replace("_names", "")
                                        gene_name = value if isinstance(value, str) else str(value)
                                        if gene_name and gene_name != "None":
                                            top_markers.append({
                                                "gene": gene_name,
                                                "cluster": cluster_num,
                                                "log2fc": "N/A"  # markers_tableä¸­æ²¡æœ‰log2fc
                                            })
                                            if len(top_markers) >= 5:  # åªä¿ç•™top 5
                                                break
                            if len(top_markers) >= 5:
                                break
                        if top_markers:
                            step_info["top_markers"] = top_markers[:5]
                
                # ğŸ”¥ RNAåˆ†æç‰¹å®šæ­¥éª¤ï¼šèšç±»
                elif "cluster" in step_name.lower() and "marker" not in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šä»dataä¸­æå–å…³é”®æŒ‡æ ‡
                    step_info["n_clusters"] = step_data.get("n_clusters", "N/A")
                    step_info["resolution"] = step_data.get("resolution", "N/A")
                    step_info["algorithm"] = step_data.get("algorithm", "N/A")
                
                # ğŸ”¥ RNAåˆ†æç‰¹å®šæ­¥éª¤ï¼šUMAP
                elif "umap" in step_name.lower():
                    # ğŸ”¥ ä¿®å¤ï¼šä»dataä¸­æå–å…³é”®æŒ‡æ ‡ï¼ˆUMAPé€šå¸¸æ²¡æœ‰summaryå­—æ®µï¼‰
                    step_info["n_neighbors"] = step_data.get("n_neighbors", "N/A")
                    step_info["min_dist"] = step_data.get("min_dist", "N/A")
                    # UMAPæœ¬èº«ä¸äº§ç”Ÿèšç±»ï¼Œä½†å¯èƒ½ä»ä¹‹å‰çš„èšç±»æ­¥éª¤è·å–
                    step_info["n_clusters"] = step_data.get("n_clusters", "N/A")
                
                # ğŸ”¥ RNAåˆ†æç‰¹å®šæ­¥éª¤ï¼šç»†èƒç±»å‹æ³¨é‡Š
                elif "annotation" in step_name.lower() or "cell_type" in step_name.lower():
                    if summary:
                        step_info["cell_types"] = summary.get("cell_types", summary.get("annotated_types", []))[:5]  # åªä¿ç•™å‰5ä¸ª
                        step_info["annotation_method"] = summary.get("method", "N/A")
                
                # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿step_infoä¸åŒ…å«ä»»ä½•å¤§å‹æ•°æ®ï¼ˆå¦‚file_pathã€previewã€å®Œæ•´resultsåˆ—è¡¨ç­‰ï¼‰
                # ç§»é™¤å¯èƒ½å­˜åœ¨çš„å†—ä½™å­—æ®µ
                step_info.pop("file_path", None)
                step_info.pop("output_path", None)
                step_info.pop("preview", None)
                step_info.pop("results", None)  # å®Œæ•´çš„resultsåˆ—è¡¨
                step_info.pop("vip_scores", None)  # å®Œæ•´çš„VIPåˆ†æ•°åˆ—è¡¨
                step_info.pop("enriched_pathways", None)  # å®Œæ•´çš„é€šè·¯åˆ—è¡¨
                step_info.pop("explained_variance", None)  # å®Œæ•´çš„æ–¹å·®æ•°æ®
                
                results_summary["steps"].append(step_info)
            
            # æ ¼å¼åŒ–ç»“æœæ‘˜è¦
            # ğŸ”¥ ä¿®å¤ï¼šé™åˆ¶summary_jsonçš„é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿
            # åªä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç§»é™¤å†—ä½™æ•°æ®
            compact_summary = {
                "total_steps": len(steps_results),
                "successful_steps": len(successful_steps),
                "failed_steps": len(failed_steps),
                "steps": []
            }
            
            # åªæå–æ¯ä¸ªæ­¥éª¤çš„å…³é”®ä¿¡æ¯ï¼Œé™åˆ¶æ•°æ®é‡
            for step_info in results_summary.get("steps", []):
                compact_step = {
                    "name": step_info.get("name", "Unknown"),
                    "status": step_info.get("status", "unknown")
                }
                # ğŸ”¥ ä¿®å¤ï¼šä¿ç•™RNAåˆ†æçš„å…³é”®æŒ‡æ ‡ï¼ˆç”¨äºkey_findingsæå–ï¼‰
                if "cells_after_qc" in step_info:
                    compact_step["cells_after_qc"] = step_info.get("cells_after_qc", "N/A")
                if "genes_after_qc" in step_info:
                    compact_step["genes_after_qc"] = step_info.get("genes_after_qc", "N/A")
                if "n_clusters" in step_info:
                    compact_step["n_clusters"] = step_info.get("n_clusters", "N/A")
                if "top_markers" in step_info:
                    compact_step["top_markers"] = step_info.get("top_markers", [])
                if "n_cells" in step_info:
                    compact_step["n_cells"] = step_info.get("n_cells", "N/A")
                if "n_genes" in step_info:
                    compact_step["n_genes"] = step_info.get("n_genes", "N/A")
                
                # ğŸ”¥ ä¿®å¤ï¼šåªä¿ç•™æœ€æ ¸å¿ƒçš„æŒ‡æ ‡ï¼Œç§»é™¤æ‰€æœ‰å†—ä½™æ•°æ®
                step_name_lower = step_info.get("name", "").lower()
                if "pca" in step_name_lower:
                    compact_step["pc1_variance"] = step_info.get("pc1_variance", "N/A")
                    compact_step["pc2_variance"] = step_info.get("pc2_variance", "N/A")
                    compact_step["separation_quality"] = step_info.get("separation_quality", "N/A")
                    # ğŸ”¥ ä¸åŒ…å«pc1_varã€pc2_varã€total_varianceç­‰å†—ä½™å­—æ®µ
                elif "differential" in step_name_lower:
                    compact_step["significant_count"] = step_info.get("significant_count", "N/A")
                    compact_step["total_count"] = step_info.get("total_count", "N/A")
                    # ğŸ”¥ åªä¿ç•™top 3æ ‡è®°ç‰©åç§°ï¼Œä¸åŒ…å«log2fcã€fdrç­‰è¯¦ç»†æ•°æ®
                    top_markers = step_info.get("top_markers", [])[:3]
                    if top_markers:
                        compact_step["top_marker_names"] = [m.get("name", "Unknown") for m in top_markers]
                    # ğŸ”¥ ä¸åŒ…å«top_upã€top_downã€methodã€case_groupç­‰å†—ä½™å­—æ®µ
                elif "plsda" in step_name_lower or "pls-da" in step_name_lower:
                    # ğŸ”¥ åªä¿ç•™top 3 VIPä»£è°¢ç‰©åç§°ï¼Œä¸åŒ…å«VIPåˆ†æ•°
                    top_vip = step_info.get("top_vip_markers", [])[:3]
                    if top_vip:
                        compact_step["top_vip_names"] = [v.get("name", "Unknown") for v in top_vip]
                    # ğŸ”¥ ä¸åŒ…å«n_componentsã€comp1_varianceã€comp2_varianceç­‰å†—ä½™å­—æ®µ
                elif "pathway" in step_name_lower or "enrichment" in step_name_lower:
                    compact_step["enriched_pathway_count"] = step_info.get("enriched_pathway_count", 0)
                    # ğŸ”¥ åªä¿ç•™top 3é€šè·¯åç§°ï¼Œä¸åŒ…å«p_valueã€enrichment_scoreç­‰è¯¦ç»†æ•°æ®
                    top_pathways = step_info.get("top_pathways", [])[:3]
                    if top_pathways:
                        compact_step["top_pathway_names"] = [
                            p.get("name", "Unknown") if isinstance(p, dict) else str(p) 
                            for p in top_pathways
                        ]
                elif "inspect" in step_name_lower or "inspection" in step_name_lower:
                    compact_step["n_samples"] = step_info.get("n_samples", "N/A")
                    compact_step["n_features"] = step_info.get("n_features", "N/A")
                    # ğŸ”¥ ä¸åŒ…å«missing_rateã€file_pathç­‰å†—ä½™å­—æ®µ
                elif "preprocess" in step_name_lower:
                    compact_step["preprocessed_rows"] = step_info.get("preprocessed_rows", "N/A")
                    compact_step["preprocessed_cols"] = step_info.get("preprocessed_cols", "N/A")
                
                compact_summary["steps"].append(compact_step)
            
            summary_json = json.dumps(compact_summary, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š [AnalysisSummary] summary_jsoné•¿åº¦: {len(summary_json)}å­—ç¬¦")
            
            # æ„å»ºæç¤ºè¯
            if omics_type.lower() in ["metabolomics", "metabolomic", "metabonomics"]:
                expert_role = "ä»£è°¢ç»„å­¦åˆ†æä¸“å®¶"
                domain_context = """
- ä»£è°¢ç‰©æ•°æ®é¢„å¤„ç†ï¼ˆç¼ºå¤±å€¼å¤„ç†ã€Log2è½¬æ¢ã€æ ‡å‡†åŒ–ï¼‰
- ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç”¨äºé™ç»´å’Œå¯è§†åŒ–
- å·®å¼‚ä»£è°¢ç‰©åˆ†æï¼ˆt-test/Wilcoxonï¼‰ç”¨äºå‘ç°ç»„é—´å·®å¼‚
- ç«å±±å›¾å¯è§†åŒ–å±•ç¤ºå·®å¼‚åˆ†æç»“æœ
"""
            elif omics_type.lower() in ["scrna", "scrna-seq", "single_cell", "single-cell"]:
                expert_role = "å•ç»†èƒè½¬å½•ç»„åˆ†æä¸“å®¶"
                domain_context = """
- è´¨é‡æ§åˆ¶ï¼ˆQCï¼‰è¿‡æ»¤ä½è´¨é‡ç»†èƒ
- æ•°æ®æ ‡å‡†åŒ–å’Œç‰¹å¾é€‰æ‹©
- é™ç»´åˆ†æï¼ˆPCAã€UMAPï¼‰
- ç»†èƒèšç±»å’Œæ ‡è®°åŸºå› è¯†åˆ«
"""
            elif omics_type.lower() in ["spatial", "visium", "spatial transcriptomics"]:
                expert_role = "ç©ºé—´è½¬å½•ç»„å­¦åˆ†æä¸“å®¶"
                domain_context = """
- 10x Visium æ•°æ®ï¼šSpot çº§åŸºå› è¡¨è¾¾ä¸ç©ºé—´åæ ‡ï¼ˆobsm['spatial']ï¼‰
- è´¨é‡æ§åˆ¶ä¸æ ‡å‡†åŒ–ã€PCA é™ç»´ã€Leiden èšç±»
- ç©ºé—´é‚»åŸŸå›¾ã€ç©ºé—´è‡ªç›¸å…³ï¼ˆMoran's Iï¼‰è¯†åˆ«ç©ºé—´å¯å˜åŸºå› ï¼ˆSVGsï¼‰
- ç©ºé—´å›¾æŒ‰èšç±»/åŸºå› ç€è‰²
**é‡è¦**ï¼šä½¿ç”¨ç©ºé—´ç»„å­¦æœ¯è¯­ï¼ˆSpotsã€Clustersã€Spatial Domainsã€Gene Expressionã€Moran's Iã€SVGsï¼‰ã€‚ä¸è¦ä½¿ç”¨ä»£è°¢ç»„å­¦æœ¯è¯­ï¼ˆä»£è°¢ç‰©ã€LC-MSã€å·®å¼‚ä»£è°¢ç‰©ç­‰ï¼‰ã€‚
"""
            else:
                expert_role = "ç”Ÿç‰©ä¿¡æ¯å­¦åˆ†æä¸“å®¶"
                domain_context = "é€šç”¨ç»„å­¦æ•°æ®åˆ†ææµç¨‹"
            
            # ğŸ”¥ CRITICAL FIX: Build failure information for prompt
            failure_info = ""
            if has_failures and failed_steps:
                failure_info = f"\n\n**âš ï¸ Failed Steps ({len(failed_steps)}/{len(steps_results)}):**\n"
                for failed_step in failed_steps:
                    failure_info += f"- **{failed_step.get('name', 'Unknown')}**: {failed_step.get('error', 'Unknown error')}\n"
                failure_info += "\n**IMPORTANT**: Some steps failed, but you should still summarize the successful steps. Explain what was accomplished and note the failures."
            
            # ğŸ”¥ CRITICAL FIX: Build rule 2 text separately to avoid f-string backslash issue
            rule2_text = ""
            if has_failures:
                rule2_text = "2. **Partial Success Handling**: âš ï¸ **IMPORTANT**: Some steps failed during execution. You MUST still summarize the successful steps (e.g., PCA, PLS-DA, Volcano plots) and explain what insights can be drawn from them. For failed steps, briefly note what went wrong and why it might have failed."
            else:
                rule2_text = "2. **Complete Success**: All steps completed successfully. Provide a comprehensive analysis of all results."
            
            # ğŸ”¥ CRITICAL FIX: Build rule 3 text separately to avoid f-string backslash issue
            rule3_text = ""
            if has_failures:
                rule3_text = "- âš ï¸ Some steps failed during execution. You MUST still summarize the successful steps (e.g., PCA, PLS-DA, Volcano plots) and explain what insights can be drawn from them.\n- For failed steps, briefly note what went wrong (e.g., 'Pathway enrichment failed due to missing gseapy library') but focus on interpreting the successful results."
            
            # ğŸ”¥ TASK 3: Extract Key Findings with SPECIFIC metrics (Feed the Brain)
            # ğŸ”¥ ä¿®å¤ï¼šæ ¹æ®omics_typeåˆå§‹åŒ–ä¸åŒçš„key_findingsç»“æ„
            is_rna_analysis = omics_type.lower() in ["scrna", "scrna-seq", "single_cell", "single-cell", "rna", "rna-seq"]
            is_spatial_analysis = omics_type.lower() in ["spatial", "visium", "spatial transcriptomics"]
            
            if is_spatial_analysis:
                key_findings = {
                    "n_spots": "N/A",
                    "n_genes": "N/A",
                    "pca_variance": {"PC1": "N/A", "PC2": "N/A"},
                    "n_clusters": "N/A",
                    "spatial_domains": "N/A",
                    "top_svgs": [],  # ç©ºé—´å¯å˜åŸºå› 
                    "moran_i_summary": "N/A"
                }
            elif is_rna_analysis:
                key_findings = {
                    "n_cells": "N/A",
                    "n_genes": "N/A",
                    "mitochondrial_percentage": "N/A",
                    "pca_variance": {"PC1": "N/A", "PC2": "N/A"},
                    "n_clusters": "N/A",
                    "top_marker_genes": [],  # RNAåˆ†æï¼šæ ‡è®°åŸºå› 
                    "cell_types": []  # RNAåˆ†æï¼šç»†èƒç±»å‹
                }
            else:
                key_findings = {
                    "pca_separation": "N/A",
                    "pca_variance": {"PC1": "N/A", "PC2": "N/A"},
                    "differential_count": "N/A",
                    "differential_up_down": {"up": 0, "down": 0},
                    "top_pathways": [],
                    "top_vip_metabolites": [],  # Names only for biological interpretation
                    "top_differential_metabolites": []  # Names only for biological interpretation
                }
            
            for step_info in results_summary.get("steps", []):
                step_name = step_info.get("name", "").lower()
                
                # ğŸ”¥ RNAåˆ†æç‰¹å®šæŒ‡æ ‡æå–
                if is_rna_analysis:
                    # æå–ç»†èƒå’ŒåŸºå› æ•°é‡ï¼ˆä»QCæ­¥éª¤ï¼Œå› ä¸ºQCæ­¥éª¤æœ‰æœ€å‡†ç¡®çš„æ•°æ®ï¼‰
                    if "qc" in step_name or "quality" in step_name or "filter" in step_name:
                        if "cells_after_qc" in step_info:
                            key_findings["n_cells"] = step_info.get("cells_after_qc", "N/A")
                        if "genes_after_qc" in step_info:
                            key_findings["n_genes"] = step_info.get("genes_after_qc", "N/A")
                        if "mitochondrial_percentage" in step_info:
                            key_findings["mitochondrial_percentage"] = step_info.get("mitochondrial_percentage", "N/A")
                    # å¦‚æœQCæ­¥éª¤æ²¡æœ‰ï¼Œä»inspectæ­¥éª¤æå–
                    elif ("inspect" in step_name or "inspection" in step_name) and key_findings.get("n_cells") == "N/A":
                        if "n_cells" in step_info:
                            key_findings["n_cells"] = step_info.get("n_cells", "N/A")
                        if "n_genes" in step_info:
                            key_findings["n_genes"] = step_info.get("n_genes", "N/A")
                    
                    # æå–æ ‡è®°åŸºå› 
                    if "marker" in step_name or "find_markers" in step_name:
                        top_markers = step_info.get("top_markers", [])
                        if top_markers:
                            key_findings["top_marker_genes"] = [
                                m.get("gene", m.get("name", "Unknown")) for m in top_markers[:5]
                            ]
                    
                    # æå–èšç±»ä¿¡æ¯
                    if "cluster" in step_name and "marker" not in step_name:
                        if "n_clusters" in step_info:
                            key_findings["n_clusters"] = step_info.get("n_clusters", "N/A")
                    
                    # æå–ç»†èƒç±»å‹
                    if "annotation" in step_name or "cell_type" in step_name:
                        cell_types = step_info.get("cell_types", [])
                        if cell_types:
                            key_findings["cell_types"] = cell_types[:5]  # åªä¿ç•™å‰5ä¸ª
                
                # ğŸ”¥ TASK 3: Extract PCA metrics (Explained Variance) - é€‚ç”¨äºæ‰€æœ‰åˆ†æç±»å‹
                if "pca" in step_name and "visualize" not in step_name:
                    separation = step_info.get("separation_quality", "unknown")
                    pc1_var = step_info.get("pc1_variance", step_info.get("pc1_var", "N/A"))
                    pc2_var = step_info.get("pc2_variance", step_info.get("pc2_var", "N/A"))
                    # Extract numeric values
                    if isinstance(pc1_var, str) and "%" in pc1_var:
                        pc1_val = pc1_var.replace("%", "").strip()
                    else:
                        pc1_val = str(pc1_var)
                    if isinstance(pc2_var, str) and "%" in pc2_var:
                        pc2_val = pc2_var.replace("%", "").strip()
                    else:
                        pc2_val = str(pc2_var)
                    
                    key_findings["pca_variance"] = {"PC1": pc1_val, "PC2": pc2_val}
                    # ğŸ”¥ ä¿®å¤ï¼špca_separationåªåœ¨ä»£è°¢ç»„å­¦åˆ†æä¸­å­˜åœ¨ï¼ˆé RNAã€é Spatialï¼‰
                    if not is_rna_analysis and not is_spatial_analysis:
                        if separation == "clear":
                            key_findings["pca_separation"] = f"æ¸…æ™°åˆ†ç¦» (PC1: {pc1_var}, PC2: {pc2_var})"
                        else:
                            key_findings["pca_separation"] = f"ä¸­ç­‰åˆ†ç¦» (PC1: {pc1_var}, PC2: {pc2_var})"
                
                # ğŸ”¥ ç©ºé—´ç»„å­¦ï¼šä» load_data/qc_norm æå– n_spotsã€n_genesï¼›ä» cluster æå– n_clustersï¼›ä» spatial_autocorr æå– Moran's Iã€SVGs
                if is_spatial_analysis:
                    if "load" in step_name or "qc" in step_name or "norm" in step_name:
                        if "n_obs" in step_info or "n_spots" in step_info:
                            key_findings["n_spots"] = step_info.get("n_spots", step_info.get("n_obs", "N/A"))
                        if "n_vars" in step_info or "n_genes" in step_info:
                            key_findings["n_genes"] = step_info.get("n_genes", step_info.get("n_vars", "N/A"))
                    if "cluster" in step_name and "marker" not in step_name:
                        if "n_clusters" in step_info:
                            key_findings["n_clusters"] = step_info.get("n_clusters", "N/A")
                    if "spatial_autocorr" in step_name or "moran" in step_name:
                        key_findings["moran_i_summary"] = step_info.get("summary", step_info.get("n_svgs", "N/A"))
                        svgs = step_info.get("top_svgs", step_info.get("var_names", []))
                        if isinstance(svgs, list) and svgs:
                            key_findings["top_svgs"] = list(svgs)[:5]
                
                # ğŸ”¥ ä¿®å¤ï¼šåªæå–å…³é”®è®¡æ•°ï¼Œä¸åŒ…å«å®Œæ•´çš„top_up/top_downåˆ—è¡¨ï¼ˆä»…ä»£è°¢ç»„å­¦ï¼Œé Spatialï¼‰
                if not is_rna_analysis and not is_spatial_analysis and "differential" in step_name:
                    sig_count = step_info.get("significant_count", "N/A")
                    total_count = step_info.get("total_count", "N/A")
                    # ğŸ”¥ ä¸æå–top_upå’Œtop_downåˆ—è¡¨ï¼ˆå¯èƒ½å¾ˆé•¿ï¼‰ï¼Œåªä½¿ç”¨è®¡æ•°
                    top_up_count = len(step_info.get("top_up", [])) if isinstance(step_info.get("top_up"), list) else 0
                    top_down_count = len(step_info.get("top_down", [])) if isinstance(step_info.get("top_down"), list) else 0
                    
                    key_findings["differential_count"] = f"å‘ç° {sig_count} ä¸ªæ˜¾è‘—å·®å¼‚ä»£è°¢ç‰©ï¼ˆå…± {total_count} ä¸ªï¼‰"
                    key_findings["differential_up_down"] = {
                        "up": top_up_count,
                        "down": top_down_count
                    }
                    
                    # ğŸ”¥ ä¿®å¤ï¼šåªæå–topæ ‡è®°ç‰©åç§°ï¼Œä¸åŒ…å«å®Œæ•´æ•°æ®
                    top_markers = step_info.get("top_markers", [])
                    if top_markers:
                        key_findings["top_differential_metabolites"] = [
                            m.get('name', 'Unknown') for m in top_markers[:3]  # ğŸ”¥ åªä¿ç•™top 3
                        ]
                    elif step_info.get("top_up_names"):
                        # Fallback to top_up_namesï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        key_findings["top_differential_metabolites"] = step_info.get("top_up_names", [])[:3]
                
                # ğŸ”¥ ä¿®å¤ï¼šåªæå–top VIPä»£è°¢ç‰©åç§°ï¼Œä¸åŒ…å«å®Œæ•´æ•°æ®ï¼ˆä»…ä»£è°¢ç»„å­¦ï¼Œé Spatialï¼‰
                if not is_rna_analysis and not is_spatial_analysis and ("plsda" in step_name or "pls-da" in step_name):
                    top_vip = step_info.get("top_vip_markers", [])
                    if top_vip:
                        # Extract metabolite NAMES only (for biological interpretation)
                        key_findings["top_vip_metabolites"] = [
                            v.get('name', 'Unknown') for v in top_vip[:3]  # ğŸ”¥ åªä¿ç•™top 3
                        ]
                
                # ğŸ”¥ ä¿®å¤ï¼šåªæå–topé€šè·¯åç§°ï¼Œä¸åŒ…å«å®Œæ•´æ•°æ®ï¼ˆä»…ä»£è°¢ç»„å­¦ï¼Œé Spatialï¼‰
                if not is_rna_analysis and not is_spatial_analysis and ("pathway" in step_name or "enrichment" in step_name):
                    top_pathways = step_info.get("top_pathways", [])
                    if top_pathways:
                        # Extract pathway NAMES only (for biological interpretation)
                        key_findings["top_pathways"] = [
                            p.get('name', 'Unknown') if isinstance(p, dict) else str(p)
                            for p in top_pathways[:3]  # ğŸ”¥ åªä¿ç•™top 3
                        ]
            
            key_findings_json = json.dumps(key_findings, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š [AnalysisSummary] key_findings_jsoné•¿åº¦: {len(key_findings_json)}å­—ç¬¦")
            
            # ğŸ”¥ ä¿®å¤ï¼šé™åˆ¶execution_results_textçš„é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿
            execution_results_text = ""
            if execution_results and (execution_results.get("csv_files") or execution_results.get("image_files")):
                execution_results_text = "\n**Actual Analysis Results (From Generated Files):**\n"
                
                if execution_results.get("csv_files"):
                    execution_results_text += "\n**CSV Results Files:**\n"
                    # ğŸ”¥ é™åˆ¶ï¼šåªå¤„ç†å‰3ä¸ªCSVæ–‡ä»¶ï¼Œé¿å…æ•°æ®è¿‡å¤š
                    for csv_info in execution_results["csv_files"][:3]:
                        execution_results_text += f"\n- **{csv_info['filename']}**: {csv_info['shape']}\n"
                        # åªæ˜¾ç¤ºå‰5ä¸ªåˆ—å
                        columns_preview = ', '.join(csv_info['columns'][:5])
                        if len(csv_info['columns']) > 5:
                            columns_preview += f" ... (å…±{len(csv_info['columns'])}åˆ—)"
                        execution_results_text += f"  Columns: {columns_preview}\n"
                        # ğŸ”¥ é™åˆ¶ï¼šä¸åŒ…å«å®Œæ•´çš„è¡Œæ•°æ®ï¼Œåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        if csv_info.get("statistics"):
                            # åªä¿ç•™å…³é”®ç»Ÿè®¡ä¿¡æ¯ï¼Œé™åˆ¶é•¿åº¦
                            stats = csv_info['statistics']
                            compact_stats = {}
                            for key in ['mean', 'median', 'std', 'min', 'max']:
                                if key in stats:
                                    compact_stats[key] = stats[key]
                            if compact_stats:
                                execution_results_text += f"  Statistics: {json.dumps(compact_stats, ensure_ascii=False)}\n"
                
                if execution_results.get("image_files"):
                    execution_results_text += f"\n**Generated Images ({len(execution_results['image_files'])} files):**\n"
                    # åªæ˜¾ç¤ºå‰3ä¸ªå›¾ç‰‡
                    for img_info in execution_results["image_files"][:3]:
                        execution_results_text += f"- {img_info['filename']} ({img_info['type']})\n"
            else:
                execution_results_text = "\n**Note**: No generated files found in output directory. Analysis results are based on step summaries only.\n"
            
            logger.info(f"ğŸ“Š [AnalysisSummary] execution_results_texté•¿åº¦: {len(execution_results_text)}å­—ç¬¦")
            
            if is_spatial_analysis:
                critical_instruction_text = "Based on the provided metrics above, interpret the **spatial transcriptomics** (10x Visium) results. Use terminology: Spots, Clusters, Spatial Domains, Gene Expression, Moran's I, Spatially Variable Genes (SVGs). Do NOT mention metabolites, LC-MS, or metabolomics. Generate a structured Markdown report with spatial biology interpretation."
            else:
                critical_instruction_text = "Based on the provided metrics above, interpret the biological significance. Use your internal knowledge base (PubMed/Literature) to explain **WHY** these specific metabolites/pathways might be altered in this context. Generate a structured Markdown report with deep biological interpretation."
            
            # ç©ºé—´ç»„å­¦ä½¿ç”¨ä¸“ç”¨è¾“å‡ºç»“æ„ï¼Œç¦æ­¢å‡ºç°ä»£è°¢ç‰©/LC-MS ç­‰æœ¯è¯­
            if is_spatial_analysis:
                output_structure_section = """
### 1. ç»Ÿè®¡æ¦‚è§ˆ (Statistical Overview) â€” ç©ºé—´è½¬å½•ç»„
- å®šé‡æ€»ç»“ï¼šSpot æ•°é‡ã€åŸºå› æ•°é‡ã€Leiden èšç±»æ•°ã€PCA æ–¹å·®è§£é‡Šï¼ˆPC1/PC2ï¼‰
- æ•°æ®è´¨é‡ä¸ç»„ç»‡ç»“æ„ï¼ˆH&Eï¼‰ç®€è¦è¯„ä¼°
- ç©ºé—´åŸŸï¼ˆSpatial Domainsï¼‰ä¸æ•´ä½“æ•°æ®ç‰¹å¾

### 2. ç©ºé—´èšç±»ä¸åŸºå› è¡¨è¾¾ (Spatial Clusters & Gene Expression)
- **ç©ºé—´èšç±»**ï¼šLeiden èšç±»ç»“æœä¸ç©ºé—´åˆ†å¸ƒ
- **ç©ºé—´å¯å˜åŸºå› ï¼ˆSVGsï¼‰**ï¼šåŸºäº Moran's I è¯†åˆ«çš„åŸºå› åŠå…¶ç©ºé—´æ¨¡å¼
- **åŸºå› è¡¨è¾¾**ï¼šè®¨è®ºå…³é”®åŸºå› åœ¨ç»„ç»‡ä¸­çš„ç©ºé—´è¡¨è¾¾æ¨¡å¼ï¼ˆå‹¿ä½¿ç”¨ä»£è°¢ç‰©ã€LC-MS ç­‰ä»£è°¢ç»„å­¦æœ¯è¯­ï¼‰

### 3. ç©ºé—´ç”Ÿç‰©å­¦è§£è¯» (Spatial Biology Interpretation)
- ç»„ç»‡åŒºåŸŸä¸ç©ºé—´åŸŸçš„å…³ç³»
- ç©ºé—´è‡ªç›¸å…³ï¼ˆMoran's Iï¼‰ç»“æœçš„ç”Ÿç‰©å­¦æ„ä¹‰
- ä¸å·²çŸ¥ç©ºé—´è½¬å½•ç»„å­¦æ–‡çŒ®çš„å…³è”

### 4. ç»“è®ºä¸å»ºè®® (Conclusions & Recommendations)
- ä¸»è¦å‘ç°æ€»ç»“ï¼ˆä½¿ç”¨ Spotsã€Clustersã€SVGsã€Spatial Domains ç­‰æœ¯è¯­ï¼‰
- åç»­ç©ºé—´åˆ†ææˆ–å®éªŒéªŒè¯å»ºè®®

**ç¦æ­¢**ï¼šä¸è¦æåŠä»£è°¢ç‰©ï¼ˆmetabolitesï¼‰ã€LC-MSã€ä»£è°¢ç»„å­¦æˆ–å·®å¼‚ä»£è°¢ç‰©ã€‚ä»…ä½¿ç”¨ç©ºé—´è½¬å½•ç»„å­¦æœ¯è¯­ã€‚
"""
            else:
                output_structure_section = """
### 1. ç»Ÿè®¡æ¦‚è§ˆ (Statistical Overview)
- Quantitative summary: PCA separation quality, PC1/PC2 variance explained, differential analysis counts (up/down regulated)
- Data quality assessment based on PCA results
- Overall data characteristics and key statistics

### 2. å…³é”®ç”Ÿç‰©æ ‡å¿—ç‰© (Key Biomarkers)
- **VIPä»£è°¢ç‰©**: Discuss the top VIP metabolites from PLS-DA analysis (names: """ + (', '.join(key_findings.get('top_vip_metabolites', [])[:5]) if key_findings.get('top_vip_metabolites') else 'see data') + """)
- **å·®å¼‚ä»£è°¢ç‰©**: Discuss the top differentially expressed metabolites (names: """ + (', '.join(key_findings.get('top_differential_metabolites', [])[:5]) if key_findings.get('top_differential_metabolites') else 'see data') + """)
- **ç”Ÿç‰©å­¦åŠŸèƒ½**: Use your internal knowledge base (PubMed/Literature) to explain the potential functions and biological significance of these metabolites
- **æ ‡å¿—ç‰©æ½œåŠ›**: Discuss the potential of these metabolites as biomarkers

### 3. é€šè·¯æœºåˆ¶è§£è¯» (Pathway Mechanism Interpretation)
- **å¯Œé›†é€šè·¯**: Deep dive into the enriched pathways (names: """ + (', '.join(key_findings.get('top_pathways', [])[:5]) if key_findings.get('top_pathways') else 'see data') + """)
- **é€šè·¯åŠŸèƒ½**: Explain the biological functions of these pathways and their significance in the current research context
- **æœºåˆ¶è®¨è®º**: Relate findings to potential biological mechanisms, disease processes, or physiological states
- **åŠŸèƒ½æ„ä¹‰**: Discuss what the differentially expressed metabolites mean in terms of biological function

### 4. ç»“è®ºä¸å»ºè®® (Conclusions & Recommendations)
- **ä¸»è¦å‘ç°æ€»ç»“**: Summarize key findings and their biological significance
- **éªŒè¯å®éªŒå»ºè®®**: Suggest validation experiments (e.g., targeted metabolomics, qPCR validation)
- **åç»­ç ”ç©¶**: Propose follow-up studies based on the findings
"""
            
            prompt = f"""You are a Senior Bioinformatics Scientist writing a Results & Discussion section for a top-tier journal (Nature Medicine). Your role is to interpret biological data and provide deep scientific insights, connecting findings to biological mechanisms and literature knowledge.

**User Goal:**
{workflow_name}

**Execution Results (Successful Steps):**
{summary_json}

**Key Findings Extracted (Specific Metrics):**
{key_findings_json}
{execution_results_text}
{failure_info}

**CRITICAL INSTRUCTION:**
{critical_instruction_text}

**Domain Context:**
{domain_context}

**CRITICAL RULES:**

1. **Reasoning Process (DeepSeek-R1)**: 
   - Use the `<think>` tag to show your reasoning process before generating the final report
   - Inside `<think>`, analyze the data metrics, connect metabolites to pathways, and reason about biological mechanisms
   - After reasoning, output the final report outside the `<think>` tags

2. **Scientific Persona**: You are a Senior Bioinformatics Scientist writing a publication-quality results section for Nature Medicine. Write as if you are describing results in a Methods/Results section of a high-impact research paper.

3. **NO Technical Debugging**: 
   - DO NOT mention step names, tool names, file paths, or technical errors
   - DO NOT say "Step X failed" or "Tool Y encountered an error"
   - DO NOT mention Python errors, missing libraries, or code issues
   - If a step failed, simply state the biological limitation (e.g., "Pathway enrichment analysis could not be performed due to insufficient significant features" or "Functional annotation was not available for this dataset")

4. **Deep Biological Interpretation**:
   - Connect metabolites/pathways to biological functions using your internal knowledge base (PubMed/Literature)
   - Explain the MECHANISM, not just the numbers
   - Discuss how the identified metabolites/pathways relate to biological processes, disease mechanisms, or physiological states
   - Interpret findings in the context of known metabolic pathways and their roles

5. **Professional Language**:
   - Use scientific terminology appropriate for Nature Medicine
   - Write in Simplified Chinese (ç®€ä½“ä¸­æ–‡)
   - Be precise, detailed, and academically rigorous
   - Minimum 800 words, aim for comprehensive coverage

6. **Output Structure (MUST FOLLOW):**
{output_structure_section}

**Output Format:**
- Use Simplified Chinese (ç®€ä½“ä¸­æ–‡)
- Use Markdown format with proper headings (###)
- Be professional, academic, and detailed
- Minimum 800 words, aim for comprehensive coverage
- Include specific numbers, percentages, and statistical values from the results
- Reference biological mechanisms and pathways explicitly

**Tone**: Professional, Academic, Detailed, Nature Medicine style. Focus on deep biological interpretation and scientific insights, connecting findings to mechanisms.

**CRITICAL**: You MUST provide a detailed Biological Interpretation and Mechanism Analysis. Do NOT just list steps or metrics. Explain the biological meaning, connect findings to known pathways, and discuss mechanisms.

**IMPORTANT**: Use `<think>` tags to show your reasoning process. Analyze the data deeply, then output the final report.

ç°åœ¨ç”Ÿæˆå…¨é¢çš„åˆ†ææŠ¥å‘Šï¼ˆéµå¾ªä¸Šè¿°ç»“æ„ï¼Œè¯¦ç»†ä¸”ä¸“ä¸šï¼ŒåŒ…å«æ·±åº¦ç”Ÿç‰©å­¦æœºåˆ¶è§£è¯»ï¼‰ï¼š"""
            
            messages = [
                {
                    "role": "system",
                    "content": f"""You are a Senior Bioinformatics Scientist writing a publication-quality results section for a {omics_type} research paper. You are NOT a software engineer, IT support, or debugger.

**Your Scientific Persona:**
- You interpret biological data and provide scientific insights
- You write as if describing results in a Methods/Results section of a research paper
- You focus on biological meaning, statistical significance, and scientific interpretation

**What You MUST DO:**
- Describe sample characteristics, group comparisons, and statistical findings
- Interpret clustering patterns, separation between groups, and biological significance
- Explain what the data reveals about the biological system under study
- Discuss functional implications and potential biological mechanisms
- Use scientific terminology appropriate for {omics_type} research

**What You MUST NOT DO:**
- Do NOT mention step names, tool names, file paths, or technical implementation details
- Do NOT say "Step X failed" or "Tool Y encountered an error"
- Do NOT mention Python errors, missing libraries, code issues, or debugging information
- Do NOT act like IT support or a software engineer
- If a step failed, state it as a biological limitation (e.g., "Pathway enrichment could not be performed due to insufficient significant features")

**Output Style:**
- Write in Simplified Chinese (ç®€ä½“ä¸­æ–‡)
- Use Markdown format with proper headings
- Be precise, detailed, and academically rigorous
- Focus on biological interpretation and scientific insights"""
                },
                {"role": "user", "content": prompt}
            ]
            
            # ğŸ”¥ TASK 2: Force LLM call - ALWAYS call LLM, never return simple list
            logger.info(f"ğŸ“ [AnalysisSummary] è°ƒç”¨ LLM ç”Ÿæˆæ·±åº¦ç”Ÿç‰©å­¦è§£é‡Š...")
            logger.info(f"ğŸ“Š [AnalysisSummary] æå–çš„å…³é”®æŒ‡æ ‡: {key_findings_json}")
            logger.info(f"ğŸ“Š [AnalysisSummary] æˆåŠŸæ­¥éª¤æ•°: {len(successful_steps)}/{len(steps_results)}")
            logger.info(f"ğŸ“Š [AnalysisSummary] å¤±è´¥æ­¥éª¤æ•°: {len(failed_steps)}")
            
            # ğŸ”¥ TASK 2: Debug logging - Log metrics being sent to LLM
            if not key_findings_json or key_findings_json == "{}":
                logger.warning(f"âš ï¸ [AnalysisSummary] è­¦å‘Šï¼šå…³é”®æŒ‡æ ‡ä¸ºç©ºï¼ŒLLMå¯èƒ½æ— æ³•ç”Ÿæˆæœ‰æ„ä¹‰çš„æŠ¥å‘Š")
            else:
                logger.info(f"âœ… [AnalysisSummary] å…³é”®æŒ‡æ ‡å·²æå–ï¼ŒåŒ…å«æ•°æ®ï¼Œå‡†å¤‡å‘é€ç»™LLM")
            
            try:
                # ğŸ”¥ TASK 3: ç»Ÿä¸€LLMå®¢æˆ·ç«¯è·å–é€»è¾‘ï¼ˆä¸è§„åˆ’é˜¶æ®µä¸€è‡´ï¼‰
                llm_client_to_use = self.llm_client
                if not llm_client_to_use:
                    logger.warning("âš ï¸ [AnalysisSummary] self.llm_client ä¸å¯ç”¨ï¼Œä½¿ç”¨ LLMClientFactory.create_default()")
                    from gibh_agent.core.llm_client import LLMClientFactory
                    llm_client_to_use = LLMClientFactory.create_default()
                    logger.info(f"âœ… [AnalysisSummary] å·²åˆ›å»ºé»˜è®¤LLMå®¢æˆ·ç«¯: {llm_client_to_use.base_url}")
                
                # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥promptæ€»é•¿åº¦ï¼Œé¿å…è¶…è¿‡APIé™åˆ¶
                system_message_length = len(messages[0]["content"]) if messages else 0
                user_message_length = len(messages[1]["content"]) if len(messages) > 1 else 0
                total_prompt_length = system_message_length + user_message_length
                
                logger.info(f"ğŸ“Š [AnalysisSummary] Prompté•¿åº¦æ£€æŸ¥:")
                logger.info(f"  - System message: {system_message_length}å­—ç¬¦")
                logger.info(f"  - User message: {user_message_length}å­—ç¬¦")
                logger.info(f"  - æ€»é•¿åº¦: {total_prompt_length}å­—ç¬¦")
                
                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœpromptè¿‡é•¿ï¼ˆ>100kå­—ç¬¦ï¼‰ï¼Œæˆªæ–­æˆ–ç®€åŒ–
                MAX_PROMPT_LENGTH = 100000  # 100kå­—ç¬¦é™åˆ¶
                if total_prompt_length > MAX_PROMPT_LENGTH:
                    logger.warning(f"âš ï¸ [AnalysisSummary] Promptè¿‡é•¿ï¼ˆ{total_prompt_length}å­—ç¬¦ï¼‰ï¼Œè¶…è¿‡é™åˆ¶ï¼ˆ{MAX_PROMPT_LENGTH}å­—ç¬¦ï¼‰ï¼Œè¿›è¡Œæˆªæ–­")
                    # æˆªæ–­execution_results_text
                    if len(execution_results_text) > 5000:
                        execution_results_text = execution_results_text[:5000] + "\n... (å†…å®¹å·²æˆªæ–­)"
                        logger.warning(f"âš ï¸ [AnalysisSummary] execution_results_textå·²æˆªæ–­åˆ°5000å­—ç¬¦")
                    # ç®€åŒ–summary_jsonï¼ˆå¦‚æœä»ç„¶è¿‡é•¿ï¼‰
                    if len(summary_json) > 20000:
                        # åªä¿ç•™æœ€å…³é”®çš„æ­¥éª¤ä¿¡æ¯
                        import json
                        compact_summary = {
                            "total_steps": len(steps_results),
                            "successful_steps": len(successful_steps),
                            "failed_steps": len(failed_steps),
                            "key_metrics": {
                                "pca_variance": key_findings.get("pca_variance", {}),
                                "differential_count": key_findings.get("differential_count", "N/A"),
                                "top_pathways": key_findings.get("top_pathways", [])[:3],
                                "top_vip_metabolites": key_findings.get("top_vip_metabolites", [])[:3]
                            }
                        }
                        summary_json = json.dumps(compact_summary, ensure_ascii=False, indent=2)
                        logger.warning(f"âš ï¸ [AnalysisSummary] summary_jsonå·²ç®€åŒ–ï¼Œæ–°é•¿åº¦: {len(summary_json)}å­—ç¬¦")
                    
                    # é‡æ–°æ„å»ºpromptï¼ˆä½¿ç”¨ç®€åŒ–åçš„æ•°æ®ï¼‰
                    prompt = f"""You are a Senior Bioinformatics Scientist writing a Results & Discussion section for a top-tier journal (Nature Medicine). Your role is to interpret biological data and provide deep scientific insights, connecting findings to biological mechanisms and literature knowledge.

**User Goal:**
{workflow_name}

**Execution Results Summary:**
{summary_json}

**Key Findings Extracted (Specific Metrics):**
{key_findings_json}
{execution_results_text}
{failure_info}

**CRITICAL INSTRUCTION:**
{critical_instruction_text}

**Domain Context:**
{domain_context}

**CRITICAL RULES:**

1. **Reasoning Process (DeepSeek-R1)**: 
   - Use the `<think>` tag to show your reasoning process before generating the final report
   - Inside `<think>`, analyze the data metrics and reason about biological mechanisms
   - After reasoning, output the final report outside the `<think>` tags

2. **Scientific Persona**: You are a Senior Bioinformatics Scientist writing a publication-quality results section for Nature Medicine. Write as if you are describing results in a Methods/Results section of a high-impact research paper.

3. **NO Technical Debugging**: 
   - DO NOT mention step names, tool names, file paths, or technical errors
   - DO NOT say "Step X failed" or "Tool Y encountered an error"
   - DO NOT mention Python errors, missing libraries, or code issues
   - If a step failed, simply state the biological limitation

4. **Deep Biological Interpretation**:
   - Connect findings to biological functions using your internal knowledge base (PubMed/Literature)
   - Explain the MECHANISM, not just the numbers
   - Discuss how the identified results relate to biological processes, disease mechanisms, or physiological states

5. **Professional Language**:
   - Use scientific terminology appropriate for Nature Medicine
   - Write in Simplified Chinese (ç®€ä½“ä¸­æ–‡)
   - Be precise, detailed, and academically rigorous
   - Minimum 800 words, aim for comprehensive coverage

6. **Output Structure (MUST FOLLOW):**
{output_structure_section}

**Output Format:**
- Use Simplified Chinese (ç®€ä½“ä¸­æ–‡)
- Use Markdown format with proper headings (###)
- Be professional, academic, and detailed

**Tone**: Professional, Academic, Detailed, Nature Medicine style. Focus on deep biological interpretation and scientific insights, connecting findings to mechanisms.

**CRITICAL**: You MUST provide a detailed Biological Interpretation and Mechanism Analysis. Do NOT just list steps or metrics. Explain the biological meaning and discuss mechanisms.

**IMPORTANT**: Use `<think>` tags to show your reasoning process. Analyze the data deeply, then output the final report.

ç°åœ¨ç”Ÿæˆå…¨é¢çš„åˆ†ææŠ¥å‘Šï¼ˆéµå¾ªä¸Šè¿°ç»“æ„ï¼Œè¯¦ç»†ä¸”ä¸“ä¸šï¼ŒåŒ…å«æ·±åº¦ç”Ÿç‰©å­¦æœºåˆ¶è§£è¯»ï¼‰ï¼š"""
                    
                    messages[1]["content"] = prompt
                    
                    # é‡æ–°è®¡ç®—é•¿åº¦
                    system_message_length = len(messages[0]["content"])
                    user_message_length = len(messages[1]["content"])
                    total_prompt_length = system_message_length + user_message_length
                    logger.info(f"ğŸ“Š [AnalysisSummary] æˆªæ–­åPrompté•¿åº¦: {total_prompt_length}å­—ç¬¦")
                
                def _clean_report_content(s: str):
                    """Strip <<<SUGGESTIONS>>> block and store in context; return cleaned text."""
                    if not s:
                        return s
                    cleaned, sug = strip_suggestions_from_text(s)
                    if sug:
                        self.context["report_suggestions"] = sug
                    return cleaned

                logger.info(f"ğŸ“ [AnalysisSummary] å¼€å§‹LLMè°ƒç”¨ï¼Œmax_tokens=2500...")
                completion = await llm_client_to_use.achat(messages, temperature=0.3, max_tokens=2500)  # ğŸ”¥ TASK 2: Increase tokens for comprehensive report
                logger.info(f"âœ… [AnalysisSummary] LLMè°ƒç”¨å®Œæˆï¼Œå¼€å§‹è§£æå“åº”...")
                original_content = completion.choices[0].message.content or ""
                logger.info(f"ğŸ” [AnalysisSummary] åŸå§‹å†…å®¹é•¿åº¦: {len(original_content)}")
                
                think_content, response = llm_client_to_use.extract_think_and_content(completion)
                logger.info(f"ğŸ” [AnalysisSummary] å†…å®¹æå–ç»“æœ: think_length={len(think_content) if think_content else 0}, response_length={len(response) if response else 0}")
                logger.debug(f"ğŸ” [AnalysisSummary] original_content é¢„è§ˆ: {original_content[:300]}...")
                logger.debug(f"ğŸ” [AnalysisSummary] response é¢„è§ˆ: {response[:300] if response else 'N/A'}...")
                
                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœæå–åçš„responseå¤ªçŸ­ï¼Œä½†original_contentå¾ˆé•¿ï¼Œè¯´æ˜ä¸»è¦å†…å®¹å¯èƒ½åœ¨æ ‡ç­¾å†…
                # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåº”è¯¥ä½¿ç”¨original_contentï¼ˆå‰ç«¯ä¼šè§£ææ ‡ç­¾ï¼‰
                if response and len(response.strip()) > 100:  # Ensure meaningful response
                    logger.info(f"âœ… [AnalysisSummary] æ·±åº¦ç”Ÿç‰©å­¦è§£é‡Šç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(response)}")
                    logger.debug(f"ğŸ“ [DEBUG] Summary preview: {response[:200]}...")
                    # Return original content with tags so frontend can parse and display reasoning
                    has_think_tags = any(tag in original_content for tag in ['<think>', '<think>', '<reasoning>', '<thought>', '<thinking>'])
                    out = original_content if has_think_tags else response
                    return _clean_report_content(out)
                elif original_content and len(original_content.strip()) > 100:
                    # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœresponseå¤ªçŸ­ä½†original_contentå¾ˆé•¿ï¼Œä½¿ç”¨original_content
                    logger.warning(f"âš ï¸ [AnalysisSummary] æå–åçš„å†…å®¹è¿‡çŸ­ï¼Œä½†åŸå§‹å†…å®¹è¾ƒé•¿ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ï¼ˆé•¿åº¦: {len(original_content)}ï¼‰")
                    return _clean_report_content(original_content)
                else:
                    logger.warning(f"âš ï¸ [AnalysisSummary] LLM è¿”å›å†…å®¹è¿‡çŸ­ï¼ˆresponse: {len(response) if response else 0}å­—ç¬¦, original: {len(original_content)}å­—ç¬¦ï¼‰ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ...")
                    # Retry with simpler prompt if first attempt failed
                    retry_prompt = f"""Based on these analysis metrics: {key_findings_json}

Write a comprehensive biological interpretation report in Simplified Chinese. Include:
1. ç»“æœæ‘˜è¦ (quantitative findings)
2. ç”Ÿç‰©å­¦æœºåˆ¶è§£è¯» (connect metabolites/pathways to biological functions)
3. æ½œåœ¨æ ‡å¿—ç‰© (discuss VIP molecules)
4. ä¸‹ä¸€æ­¥å»ºè®® (validation experiments)

Minimum 500 words. Be scientific and detailed."""
                    
                    retry_messages = [
                        {"role": "system", "content": "You are a Senior Bioinformatics Scientist. Write detailed biological interpretations."},
                        {"role": "user", "content": retry_prompt}
                    ]
                    
                    retry_completion = await llm_client_to_use.achat(retry_messages, temperature=0.3, max_tokens=2000)
                    retry_think, retry_response = llm_client_to_use.extract_think_and_content(retry_completion)
                    
                    # ğŸ”¥ FEATURE: Return original content with tags for frontend parsing
                    retry_original_content = retry_completion.choices[0].message.content or ""
                    logger.info(f"ğŸ” [AnalysisSummary] é‡è¯•å†…å®¹æå–ç»“æœ: think_length={len(retry_think) if retry_think else 0}, response_length={len(retry_response) if retry_response else 0}, original_length={len(retry_original_content)}")
                    
                    if retry_response and len(retry_response.strip()) > 100:
                        logger.info(f"âœ… [AnalysisSummary] é‡è¯•æˆåŠŸï¼Œç”Ÿæˆæ·±åº¦è§£é‡Šï¼Œé•¿åº¦: {len(retry_response)}")
                        # Return original content with tags so frontend can parse and display reasoning
                        has_think_tags = any(tag in retry_original_content for tag in ['<think>', '<think>', '<reasoning>', '<thought>', '<thinking>', '<think>'])
                        out = retry_original_content if has_think_tags else retry_response
                        return _clean_report_content(out)
                    elif retry_original_content and len(retry_original_content.strip()) > 100:
                        # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœé‡è¯•åçš„responseå¤ªçŸ­ä½†original_contentå¾ˆé•¿ï¼Œä½¿ç”¨original_content
                        logger.warning(f"âš ï¸ [AnalysisSummary] é‡è¯•åæå–çš„å†…å®¹è¿‡çŸ­ï¼Œä½†åŸå§‹å†…å®¹è¾ƒé•¿ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ï¼ˆé•¿åº¦: {len(retry_original_content)}ï¼‰")
                        return _clean_report_content(retry_original_content)
                    else:
                        logger.error(f"âŒ [AnalysisSummary] é‡è¯•åä»æ— æ³•ç”Ÿæˆæœ‰æ•ˆå†…å®¹ï¼ˆresponse: {len(retry_response) if retry_response else 0}å­—ç¬¦, original: {len(retry_original_content)}å­—ç¬¦ï¼‰")
                        # ğŸ”¥ TASK 3: Return user-friendly error message instead of raw traceback
                        return f"""## âš ï¸ åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥


**å·²å®Œæˆçš„æ­¥éª¤**: {len(successful_steps)}/{len(steps_results)}

**å…³é”®æŒ‡æ ‡**:
{key_findings_json if key_findings_json != "{}" else "æš‚æ— å¯ç”¨æŒ‡æ ‡"}

**å»ºè®®**: è¯·æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†å›¾è¡¨å’Œç»Ÿè®¡ç»“æœä»¥è·å–åˆ†æä¿¡æ¯ã€‚"""
            except Exception as llm_error:
                # ğŸ”¥ TASK 2: å¢å¼ºé”™è¯¯æ—¥å¿—è¾“å‡º
                import traceback
                error_type = type(llm_error).__name__
                error_msg = (
                    f"âŒ [AnalysisSummary] LLM è°ƒç”¨å¤±è´¥\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"é”™è¯¯ç±»å‹: {error_type}\n"
                    f"é”™è¯¯ä¿¡æ¯: {str(llm_error)}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"è°ƒç”¨ä¸Šä¸‹æ–‡:\n"
                    f"  - LLM Client base_url: {llm_client_to_use.base_url if hasattr(llm_client_to_use, 'base_url') else 'N/A'}\n"
                    f"  - LLM Client model: {llm_client_to_use.model if hasattr(llm_client_to_use, 'model') else 'N/A'}\n"
                    f"  - API Key: {'å·²è®¾ç½®' if hasattr(llm_client_to_use, 'api_key') and llm_client_to_use.api_key else 'æœªè®¾ç½®'}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"è°ƒç”¨å‚æ•°:\n"
                    f"  - messagesæ•°é‡: {len(messages)}\n"
                    f"  - system messageé•¿åº¦: {len(messages[0]['content']) if messages else 0} å­—ç¬¦\n"
                    f"  - user messageé•¿åº¦: {len(messages[1]['content']) if len(messages) > 1 else 0} å­—ç¬¦\n"
                    f"  - temperature: 0.3\n"
                    f"  - max_tokens: 2500\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"æ‰§è¡Œä¸Šä¸‹æ–‡:\n"
                    f"  - æˆåŠŸæ­¥éª¤æ•°: {len(successful_steps)}/{len(steps_results)}\n"
                    f"  - å¤±è´¥æ­¥éª¤æ•°: {len(failed_steps)}\n"
                    f"  - å…³é”®æŒ‡æ ‡: {key_findings_json[:200] if key_findings_json else 'N/A'}...\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"å¯èƒ½åŸå› :\n"
                    f"  - APIå¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ\n"
                    f"  - ç½‘ç»œè¿æ¥é—®é¢˜\n"
                    f"  - APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨\n"
                    f"  - è¯·æ±‚è¶…æ—¶\n"
                    f"  - è¯·æ±‚å†…å®¹è¿‡é•¿\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}\n"
                    f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
                )
                logger.error(error_msg)
                
                # ğŸ”¥ TASK: å°†è¯¦ç»†é”™è¯¯ä¿¡æ¯å­˜å‚¨åˆ°contextï¼Œä¾›orchestratoré€šè¿‡SSEå‘é€åˆ°å‰ç«¯
                self.context["last_llm_error"] = {
                    "error_type": error_type,
                    "error_message": str(llm_error),
                    "error_details": error_msg,
                    "context": {
                        "llm_client_base_url": llm_client_to_use.base_url if hasattr(llm_client_to_use, 'base_url') else 'N/A',
                        "llm_client_model": llm_client_to_use.model if hasattr(llm_client_to_use, 'model') else 'N/A',
                        "api_key_set": bool(hasattr(llm_client_to_use, 'api_key') and llm_client_to_use.api_key),
                        "messages_count": len(messages),
                        "temperature": 0.3,
                        "max_tokens": 2500,
                        "successful_steps": len(successful_steps),
                        "failed_steps": len(failed_steps),
                        "key_findings": key_findings_json[:200] if key_findings_json else 'N/A'
                    },
                    "possible_causes": [
                        "APIå¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ",
                        "ç½‘ç»œè¿æ¥é—®é¢˜",
                        "APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
                        "è¯·æ±‚è¶…æ—¶",
                        "è¯·æ±‚å†…å®¹è¿‡é•¿"
                    ]
                }
                
                # ğŸ”¥ ä¿®å¤ï¼šå»é™¤é™çº§å¤„ç†ï¼ŒLLMå¤±è´¥æ—¶è¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
                logger.error(f"âŒ [AnalysisSummary] LLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯")
                
                # è¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼ˆé¢å‘ç”¨æˆ·ï¼ŒéæŠ€æœ¯æ€§ï¼‰
                user_friendly_error = f"""## âš ï¸ AIä¸“å®¶åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥

å¾ˆæŠ±æ­‰ï¼ŒAIä¸“å®¶åˆ†ææŠ¥å‘Šç”Ÿæˆæ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚

**åˆ†ææ‰§è¡Œæƒ…å†µ**:
- å·²å®Œæˆçš„æ­¥éª¤: {len(successful_steps)}/{len(steps_results)}
- å¤±è´¥çš„æ­¥éª¤: {len(failed_steps)}

**å»ºè®®**:
- è¯·æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†å›¾è¡¨å’Œç»Ÿè®¡ç»“æœè·å–åˆ†æä¿¡æ¯
- å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ

**é”™è¯¯ä¿¡æ¯**: {error_type} - {str(llm_error)[:100]}{'...' if len(str(llm_error)) > 100 else ''}
"""
                
                logger.info(f"âœ… [AnalysisSummary] å·²è¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼Œé•¿åº¦: {len(user_friendly_error)}")
                return user_friendly_error
                
        except Exception as e:
            logger.error(f"âŒ [AnalysisSummary] ç”Ÿæˆåˆ†ææ‘˜è¦å¤±è´¥: {e}", exc_info=True)
            return None
    
    async def _evaluate_analysis_quality(
        self,
        steps_results: List[Dict[str, Any]],
        diagnosis: str,
        workflow_name: str = "Unknown"
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ PHASE 2: Evaluate analysis quality using LLM
        
        Args:
            steps_results: List of step execution results
            diagnosis: Generated diagnosis report
            workflow_name: Name of the workflow
        
        Returns:
            Dictionary with score (0-100) and critique
        """
        try:
            # Count successful/failed/warning steps
            successful_steps = [s for s in steps_results if s.get("status") == "success"]
            failed_steps = [s for s in steps_results if s.get("status") == "error"]
            warning_steps = [s for s in steps_results if s.get("status") == "warning"]
            
            # Extract key metrics
            metrics = {
                "total_steps": len(steps_results),
                "successful_steps": len(successful_steps),
                "failed_steps": len(failed_steps),
                "warning_steps": len(warning_steps),
                "completion_rate": len(successful_steps) / len(steps_results) * 100 if steps_results else 0
            }
            
            # Check for key analysis outputs
            has_pca = any("pca" in str(s.get("step_name", "")).lower() for s in successful_steps)
            has_diff = any("differential" in str(s.get("step_name", "")).lower() for s in successful_steps)
            has_visualization = any("visualize" in str(s.get("step_name", "")).lower() or "plot" in str(s.get("data", {})).lower() for s in successful_steps)
            has_pathway = any("pathway" in str(s.get("step_name", "")).lower() or "enrichment" in str(s.get("step_name", "")).lower() for s in successful_steps)
            
            # Build evaluation prompt
            evaluation_prompt = f"""You are a Senior Bioinformatics Quality Assurance Expert. Evaluate the quality of this {workflow_name} analysis.

**Execution Metrics:**
- Total Steps: {metrics['total_steps']}
- Successful: {metrics['successful_steps']}
- Failed: {metrics['failed_steps']}
- Warnings: {metrics['warning_steps']}
- Completion Rate: {metrics['completion_rate']:.1f}%

**Analysis Components:**
- PCA Analysis: {'âœ… Present' if has_pca else 'âŒ Missing'}
- Differential Analysis: {'âœ… Present' if has_diff else 'âŒ Missing'}
- Visualization: {'âœ… Present' if has_visualization else 'âŒ Missing'}
- Pathway Enrichment: {'âœ… Present' if has_pathway else 'âŒ Missing'}

**Generated Diagnosis Report:**
{diagnosis[:1000]}...

**Evaluation Criteria:**
1. **Completeness (0-30 points)**: Did all critical steps complete? Are key analyses present?
2. **Data Quality (0-25 points)**: Were data quality issues handled? Missing values? Outliers?
3. **Statistical Rigor (0-25 points)**: Were appropriate statistical methods used? Are results significant?
4. **Biological Interpretation (0-20 points)**: Is the diagnosis report scientifically sound? Does it provide biological insights?

**Output Format (JSON only, no markdown):**
{{
    "score": <integer 0-100>,
    "critique": "<brief critique in Simplified Chinese, 2-3 sentences>",
    "strengths": ["<strength 1>", "<strength 2>"],
    "weaknesses": ["<weakness 1>", "<weakness 2>"],
    "recommendations": ["<recommendation 1>", "<recommendation 2>"]
}}

Evaluate and return ONLY the JSON object:"""
            
            messages = [
                {
                    "role": "system",
                    "content": "You are a Senior Bioinformatics Quality Assurance Expert. Evaluate analysis quality and provide constructive feedback. Output ONLY valid JSON, no markdown, no explanations."
                },
                {"role": "user", "content": evaluation_prompt}
            ]
            
            logger.info(f"ğŸ“ [QualityEvaluation] è°ƒç”¨ LLM è¯„ä¼°åˆ†æè´¨é‡...")
            completion = await self.llm_client.achat(messages, temperature=0.2, max_tokens=500)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            if response:
                # Try to parse JSON from response
                import json
                import re
                
                # Extract JSON from response (handle markdown code blocks)
                json_match = re.search(r'\{[^{}]*"score"[^{}]*\}', response, re.DOTALL)
                if json_match:
                    try:
                        evaluation = json.loads(json_match.group())
                        logger.info(f"âœ… [QualityEvaluation] è´¨é‡è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {evaluation.get('score', 'N/A')}")
                        return evaluation
                    except json.JSONDecodeError:
                        logger.warning(f"âš ï¸ [QualityEvaluation] JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°")
                
                # Fallback: generate basic evaluation
                base_score = int(metrics['completion_rate'])
                if has_pca and has_diff:
                    base_score += 10
                if has_visualization:
                    base_score += 5
                if has_pathway:
                    base_score += 5
                
                return {
                    "score": min(100, base_score),
                    "critique": f"åˆ†æå®Œæˆç‡ {metrics['completion_rate']:.1f}%ï¼Œ{'åŒ…å«å…³é”®åˆ†ææ­¥éª¤' if has_pca and has_diff else 'ç¼ºå°‘éƒ¨åˆ†å…³é”®åˆ†æ'}ã€‚",
                    "strengths": ["æ‰§è¡Œäº†ä¸»è¦åˆ†ææ­¥éª¤"] if has_pca or has_diff else [],
                    "weaknesses": ["éƒ¨åˆ†æ­¥éª¤æœªå®Œæˆ"] if metrics['failed_steps'] > 0 else [],
                    "recommendations": ["å»ºè®®æ£€æŸ¥å¤±è´¥æ­¥éª¤"] if metrics['failed_steps'] > 0 else []
                }
            else:
                logger.warning(f"âš ï¸ [QualityEvaluation] LLM å“åº”ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°")
                return {
                    "score": int(metrics['completion_rate']),
                    "critique": "æ— æ³•ç”Ÿæˆè¯¦ç»†è¯„ä¼°",
                    "strengths": [],
                    "weaknesses": [],
                    "recommendations": []
                }
                
        except Exception as e:
            logger.error(f"âŒ [QualityEvaluation] è´¨é‡è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
            # Return default evaluation
            return {
                "score": 50,
                "critique": f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "strengths": [],
                "weaknesses": [],
                "recommendations": []
            }
    
    def _extract_parameter_recommendations(
        self,
        diagnosis_report: str,
        omics_type: str,
        stats: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        ä»è¯Šæ–­æŠ¥å‘Šä¸­æå–å‚æ•°æ¨è
        
        ğŸ”¥ TASK 5: è§£æ Markdown è¡¨æ ¼ä¸­çš„å‚æ•°æ¨è
        
        Args:
            diagnosis_report: è¯Šæ–­æŠ¥å‘Š Markdown æ–‡æœ¬
            omics_type: ç»„å­¦ç±»å‹
            stats: ç»Ÿè®¡æ•°æ®
        
        Returns:
            å‚æ•°æ¨èå­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                "summary": "æ¨èæ‘˜è¦",
                "params": {
                    "param_name": {
                        "value": "æ¨èå€¼",
                        "reason": "æ¨èç†ç”±"
                    }
                }
            }
        """
        if not diagnosis_report:
            return None
        
        try:
            import re
            
            # æŸ¥æ‰¾å‚æ•°æ¨èè¡¨æ ¼ï¼ˆMarkdown è¡¨æ ¼æ ¼å¼ï¼‰
            # è¡¨æ ¼æ ¼å¼ï¼š| å‚æ•°å | é»˜è®¤å€¼ | **æ¨èå€¼** | æ¨èç†ç”± |
            table_pattern = r'###\s*ğŸ’¡\s*å‚æ•°æ¨è.*?\n(.*?)(?=\n###|\n##|$)'
            table_match = re.search(table_pattern, diagnosis_report, re.DOTALL | re.IGNORECASE)
            
            if not table_match:
                logger.debug("âš ï¸ [ParameterRecommendation] æœªæ‰¾åˆ°å‚æ•°æ¨èè¡¨æ ¼")
                return None
            
            table_content = table_match.group(1)
            
            # è§£æè¡¨æ ¼è¡Œï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
            lines = table_content.strip().split('\n')
            params = {}
            
            for line in lines:
                line = line.strip()
                if not line or not line.startswith('|'):
                    continue
                
                # è·³è¿‡è¡¨å¤´åˆ†éš”è¡Œï¼ˆå¦‚ | :--- | :--- | :--- | :--- |ï¼‰
                if re.match(r'^\|[\s:---]+\|', line):
                    continue
                
                # è§£æè¡¨æ ¼è¡Œï¼š| å‚æ•°å | é»˜è®¤å€¼ | **æ¨èå€¼** | æ¨èç†ç”± |
                cells = [cell.strip() for cell in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾ç©ºå…ƒç´ 
                
                if len(cells) >= 4:
                    param_name = cells[0].strip()
                    default_value = cells[1].strip()
                    recommended_value = cells[2].strip()
                    reason = cells[3].strip()
                    
                    # æ¸…ç†æ¨èå€¼ï¼ˆç§»é™¤ Markdown åŠ ç²—æ ‡è®°ï¼‰
                    recommended_value = re.sub(r'\*\*|\*', '', recommended_value).strip()
                    
                    # å°è¯•è½¬æ¢æ¨èå€¼ä¸ºåˆé€‚çš„ç±»å‹
                    try:
                        # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                        if '.' in recommended_value:
                            recommended_value = float(recommended_value)
                        else:
                            recommended_value = int(recommended_value)
                    except ValueError:
                        # ä¿æŒå­—ç¬¦ä¸²
                        pass
                    
                    params[param_name] = {
                        "default": default_value,
                        "value": recommended_value,
                        "reason": reason
                    }
            
            if not params:
                logger.debug("âš ï¸ [ParameterRecommendation] è¡¨æ ¼è§£ææˆåŠŸä½†æœªæ‰¾åˆ°å‚æ•°")
                return None
            
            # ç”Ÿæˆæ¨èæ‘˜è¦
            summary = f"åŸºäºæ•°æ®ç‰¹å¾ï¼ŒAI å·²ä¸ºæ‚¨æ¨è {len(params)} ä¸ªå‚æ•°çš„ä¼˜åŒ–å€¼ã€‚"
            
            recommendation = {
                "summary": summary,
                "params": params
            }
            
            logger.info(f"âœ… [ParameterRecommendation] æˆåŠŸæå– {len(params)} ä¸ªå‚æ•°æ¨è")
            return recommendation
            
        except Exception as e:
            logger.warning(f"âš ï¸ [ParameterRecommendation] æå–å‚æ•°æ¨èå¤±è´¥: {e}", exc_info=True)
            return None

