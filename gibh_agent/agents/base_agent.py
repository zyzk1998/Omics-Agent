"""
åŸºç¡€æ™ºèƒ½ä½“æŠ½è±¡ç±»
æ‰€æœ‰é¢†åŸŸæ™ºèƒ½ä½“éƒ½ç»§æ‰¿æ­¤ç±»
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, AsyncIterator
import logging
from openai import AuthenticationError, APIError
from ..core.llm_client import LLMClient
from ..core.prompt_manager import PromptManager, DATA_DIAGNOSIS_PROMPT
from ..core.data_diagnostician import DataDiagnostician

logger = logging.getLogger(__name__)


class BaseAgent(ABC):
    """
    åŸºç¡€æ™ºèƒ½ä½“æŠ½è±¡ç±»
    
    æ‰€æœ‰é¢†åŸŸæ™ºèƒ½ä½“éƒ½åº”è¯¥ç»§æ‰¿æ­¤ç±»å¹¶å®ç°ï¼š
    - process_query: å¤„ç†ç”¨æˆ·æŸ¥è¯¢
    - generate_workflow: ç”Ÿæˆå·¥ä½œæµï¼ˆå¦‚æœéœ€è¦ï¼‰
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        prompt_manager: PromptManager,
        expert_role: str
    ):
        """
        åˆå§‹åŒ–åŸºç¡€æ™ºèƒ½ä½“
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            prompt_manager: æç¤ºç®¡ç†å™¨
            expert_role: ä¸“å®¶è§’è‰²åç§°ï¼ˆå¦‚ "rna_expert"ï¼‰
        """
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.expert_role = expert_role
        self.diagnostician = DataDiagnostician()
        # ğŸ”¥ æ¶æ„é‡æ„ï¼šä¼šè¯çº§æ–‡ä»¶æ³¨å†Œè¡¨
        self.context: Dict[str, Any] = {
            "file_registry": {},  # Key: filename, Value: {path, metadata, timestamp}
            "active_file": None   # å½“å‰æ´»åŠ¨çš„æ–‡ä»¶å
        }
    
    def register_file(
        self,
        filename: str,
        file_path: str,
        file_metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        æ³¨å†Œæ–‡ä»¶åˆ°ä¼šè¯æ³¨å†Œè¡¨
        
        ğŸ”¥ æ¶æ„é‡æ„ï¼šç»´æŠ¤æ–‡ä»¶å†å²ï¼Œè€Œä¸æ˜¯æ¸…é™¤
        
        Args:
            filename: æ–‡ä»¶åï¼ˆç”¨ä½œæ³¨å†Œè¡¨çš„ keyï¼‰
            file_path: æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼Œç¨åå¯ä»¥æ›´æ–°ï¼‰
        """
        import time
        if "file_registry" not in self.context:
            self.context["file_registry"] = {}
        
        self.context["file_registry"][filename] = {
            "path": file_path,
            "metadata": file_metadata,
            "timestamp": time.time(),
            "registered_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        logger.info(f"ğŸ“ [FileRegistry] Registered file: {filename} (Total: {len(self.context['file_registry'])} files)")
    
    def set_active_file(self, filename: str) -> None:
        """
        è®¾ç½®å½“å‰æ´»åŠ¨çš„æ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶åï¼ˆå¿…é¡»åœ¨æ³¨å†Œè¡¨ä¸­å­˜åœ¨ï¼‰
        """
        if filename not in self.context.get("file_registry", {}):
            logger.warning(f"âš ï¸ [FileRegistry] File {filename} not in registry. Registering...")
            # å¦‚æœæ–‡ä»¶ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œå°è¯•æ³¨å†Œï¼ˆä½¿ç”¨è·¯å¾„ä½œä¸ºæ–‡ä»¶åï¼‰
            self.register_file(filename, filename)
        
        old_active = self.context.get("active_file")
        self.context["active_file"] = filename
        
        if old_active != filename:
            logger.info(f"ğŸ”„ [FileRegistry] Active file changed: {old_active} -> {filename}")
        else:
            logger.debug(f"âœ… [FileRegistry] Active file unchanged: {filename}")
    
    def get_active_file_info(self) -> Optional[Dict[str, Any]]:
        """
        è·å–å½“å‰æ´»åŠ¨æ–‡ä»¶çš„ä¿¡æ¯
        
        ğŸ”¥ æ¶æ„é‡æ„ï¼šç»Ÿä¸€æ¥å£è·å–æ´»åŠ¨æ–‡ä»¶ä¿¡æ¯
        
        Returns:
            åŒ…å« path å’Œ metadata çš„å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æ´»åŠ¨æ–‡ä»¶è¿”å› None
        """
        active_file = self.context.get("active_file")
        if not active_file:
            logger.debug("âš ï¸ [FileRegistry] No active file set")
            return None
        
        registry = self.context.get("file_registry", {})
        if active_file not in registry:
            logger.warning(f"âš ï¸ [FileRegistry] Active file '{active_file}' not found in registry")
            return None
        
        file_info = registry[active_file]
        logger.debug(f"âœ… [FileRegistry] Retrieved active file info: {active_file}")
        return {
            "filename": active_file,
            "path": file_info.get("path"),
            "metadata": file_info.get("metadata"),
            "timestamp": file_info.get("timestamp")
        }
    
    def _refresh_context_for_new_files(self, uploaded_files: List[Dict[str, str]]) -> None:
        """
        åˆ·æ–°ä¸Šä¸‹æ–‡ä»¥å¤„ç†æ–°æ–‡ä»¶
        
        ğŸ”¥ ä¿®å¤ï¼šå½“æ–°æ–‡ä»¶ä¸Šä¼ æ—¶ï¼Œæ¸…é™¤æ—§çš„ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿ä½¿ç”¨æ–°æ–‡ä»¶ä½œä¸ºå•ä¸€æ•°æ®æº
        
        Args:
            uploaded_files: å½“å‰è¯·æ±‚ä¸­çš„æ–‡ä»¶åˆ—è¡¨
        """
        if uploaded_files and len(uploaded_files) > 0:
            # æå–æ–‡ä»¶åç”¨äºæ—¥å¿—
            file_names = []
            for file_info in uploaded_files:
                if isinstance(file_info, dict):
                    name = file_info.get("name") or file_info.get("path") or file_info.get("file_id", "unknown")
                else:
                    name = getattr(file_info, "name", None) or getattr(file_info, "path", None) or "unknown"
                file_names.append(name)
            
            # æ¸…é™¤æ—§çš„ä¸Šä¸‹æ–‡
            old_file_paths = self.context.get("file_paths", [])
            old_file_metadata = self.context.get("file_metadata")
            
            if old_file_paths or old_file_metadata:
                logger.info(f"ğŸ”„ [System] Context refreshed. Clearing old context:")
                logger.info(f"   Old files: {old_file_paths}")
                logger.info(f"   New active files: {file_names}")
            
            # æ¸…é™¤æ–‡ä»¶ç›¸å…³çš„ä¸Šä¸‹æ–‡
            self.context.pop("file_paths", None)
            self.context.pop("file_metadata", None)
            self.context.pop("diagnosis_report", None)
            self.context.pop("diagnosis_stats", None)
            
            logger.info(f"âœ… [System] Context refreshed. New active file: {file_names[0] if file_names else 'None'}")
    
    @abstractmethod
    async def process_query(
        self,
        query: str,
        history: List[Dict[str, str]] = None,
        uploaded_files: List[Dict[str, str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            history: å¯¹è¯å†å²
            uploaded_files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        pass
    
    async def chat(
        self,
        query: str,
        context: Dict[str, Any] = None,
        stream: bool = False
    ) -> AsyncIterator[str]:
        """
        é€šç”¨èŠå¤©æ–¹æ³•
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Yields:
            å“åº”æ–‡æœ¬å—
        """
        context = context or {}
        
        # è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.prompt_manager.get_system_prompt(
            self.expert_role,
            context
        )
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ]
        
        try:
            if stream:
                # æµå¼è¾“å‡ºï¼šç›´æ¥ä¼ é€’å†…å®¹ï¼Œè®©å‰ç«¯å¤„ç† think æ ‡ç­¾
                # DeepSeek-R1 çš„ think è¿‡ç¨‹ä¼šä»¥ <think>...</think> æ ‡ç­¾å½¢å¼è¿”å›
                # ä¹Ÿæ”¯æŒæ—§åè®®çš„ <think>...</think> æ ‡ç­¾
                has_yielded = False
                try:
                    async for chunk in self.llm_client.astream(messages):
                        if chunk.choices and chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            if content:
                                # ç›´æ¥ä¼ é€’å†…å®¹ï¼Œå‰ç«¯ä¼šæ£€æµ‹å’Œå¤„ç† think æ ‡ç­¾ï¼ˆ<think> æˆ– <think>ï¼‰
                                yield content
                                has_yielded = True
                except Exception as stream_error:
                    logger.error(f"âŒ æµå¼å“åº”é”™è¯¯: {stream_error}", exc_info=True)
                    if not has_yielded:
                        yield f"\n\nâŒ é”™è¯¯: {str(stream_error)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
                    else:
                        # å¦‚æœå·²ç»æœ‰ä¸€äº›è¾“å‡ºï¼Œåªè®°å½•é”™è¯¯ï¼Œä¸é‡å¤è¾“å‡ºé”™è¯¯ä¿¡æ¯
                        logger.warning(f"âš ï¸ æµå¼å“åº”ä¸­æ–­ï¼Œä½†å·²æœ‰éƒ¨åˆ†è¾“å‡º")
            else:
                completion = await self.llm_client.achat(messages)
                # æå– think è¿‡ç¨‹å’Œå®é™…å†…å®¹
                think_content, actual_content = self.llm_client.extract_think_and_content(completion)
                
                # å¦‚æœæœ‰ think å†…å®¹ï¼ŒåŒ…è£…åœ¨æ ‡ç­¾ä¸­
                if think_content:
                    yield f"<think>{think_content}</think>\n\n{actual_content}"
                else:
                    yield actual_content
        except AuthenticationError as e:
            error_msg = (
                f"\n\nâŒ è®¤è¯é”™è¯¯ (Error code: 401 - Invalid token)\n"
                f"è¯·æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚\n"
                f"è®¾ç½®æ–¹æ³•: export SILICONFLOW_API_KEY='your_api_key_here'\n"
                f"è¯¦ç»†é”™è¯¯: {str(e)}"
            )
            logger.error(f"API è®¤è¯å¤±è´¥: {e}")
            yield error_msg
        except APIError as e:
            error_msg = (
                f"\n\nâŒ API é”™è¯¯ (Error code: {getattr(e, 'status_code', 'unknown')})\n"
                f"è¯¦ç»†é”™è¯¯: {str(e)}"
            )
            logger.error(f"API è°ƒç”¨å¤±è´¥: {e}")
            yield error_msg
        except Exception as e:
            error_msg = f"\n\nâŒ é”™è¯¯: {str(e)}"
            logger.error(f"èŠå¤©å¤„ç†å¤±è´¥: {e}", exc_info=True)
            yield error_msg
    
    def get_file_paths(self, uploaded_files: List[Dict[str, str]]) -> List[str]:
        """
        ä»ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ä¸­æå–æ–‡ä»¶è·¯å¾„ï¼Œå¹¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        
        æ ¸å¿ƒåŸåˆ™ï¼šæ™ºèƒ½ä½“åªå¤„ç†æ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œä¸å¤„ç†äºŒè¿›åˆ¶æ•°æ®
        **å…³é”®ä¿®å¤**ï¼šç¡®ä¿è¿”å›ç»å¯¹è·¯å¾„ï¼Œé¿å… "File Not Found" é”™è¯¯
        
        Args:
            uploaded_files: æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«ç›¸å¯¹è·¯å¾„æˆ– file_idï¼‰
        
        Returns:
            ç»å¯¹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        import os
        from pathlib import Path
        
        # è·å–ä¸Šä¼ ç›®å½•ï¼ˆä¸ server.py ä¿æŒä¸€è‡´ï¼‰
        upload_dir = Path(os.getenv("UPLOAD_DIR", "/app/uploads"))
        
        paths = []
        for file_info in uploaded_files:
            if isinstance(file_info, dict):
                path = file_info.get("path") or file_info.get("name") or file_info.get("file_path") or file_info.get("file_id")
            else:
                path = getattr(file_info, "path", None) or getattr(file_info, "name", None) or getattr(file_info, "file_path", None) or getattr(file_info, "file_id", None)
            
            if not path:
                continue
            
            # ğŸ”¥ ä¿®å¤ï¼šè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            path_obj = Path(path)
            
            # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
            if path_obj.is_absolute():
                absolute_path = str(path_obj.resolve())
            else:
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œæ‹¼æ¥ UPLOAD_DIR
                absolute_path = str((upload_dir / path_obj).resolve())
            
            # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­å¤„ç†ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†é”™è¯¯ï¼‰
            if not os.path.exists(absolute_path):
                logger.warning(f"âš ï¸ æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {absolute_path} (åŸå§‹è·¯å¾„: {path})")
                # ä»ç„¶æ·»åŠ åˆ°åˆ—è¡¨ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†ï¼ˆå¯èƒ½æ–‡ä»¶ç¨åä¼šè¢«åˆ›å»ºï¼‰
            
            paths.append(absolute_path)
        
        return paths
    
    def detect_file_type(self, file_path: str) -> str:
        """
        æ£€æµ‹æ–‡ä»¶ç±»å‹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
        
        Returns:
            æ–‡ä»¶ç±»å‹ï¼ˆå¦‚ "fastq", "bam", "h5ad", "10x_mtx"ï¼‰
        """
        import os
        
        # å¦‚æœæ˜¯ç›®å½•ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ FASTQ ç›®å½•æˆ– 10x MTX ç›®å½•
        if os.path.isdir(file_path):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ FASTQ ç›®å½•ï¼ˆåŒ…å« .fastq æˆ– .fq æ–‡ä»¶ï¼‰
            fastq_files = [f for f in os.listdir(file_path) if f.endswith(('.fastq', '.fq', '.fastq.gz', '.fq.gz'))]
            if fastq_files:
                return "fastq"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ 10x MTX ç›®å½•ï¼ˆåŒ…å« matrix.mtx æˆ– matrix.mtx.gzï¼‰
            mtx_files = [f for f in os.listdir(file_path) if 'matrix.mtx' in f.lower()]
            if mtx_files:
                return "10x_mtx"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Cell Ranger è¾“å‡ºç›®å½•ï¼ˆåŒ…å« filtered_feature_bc_matrixï¼‰
            if 'filtered_feature_bc_matrix' in os.listdir(file_path) or \
               any('filtered_feature_bc_matrix' in subdir for subdir in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, subdir))):
                return "10x_mtx"
            
            return "directory"
        
        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œæ£€æŸ¥æ‰©å±•å
        ext = file_path.split('.')[-1].lower()
        
        type_mapping = {
            "fastq": ["fastq", "fq"],
            "bam": ["bam"],
            "h5ad": ["h5ad"],
            "mtx": ["mtx"],
            "vcf": ["vcf"],
            "bed": ["bed"],
            "bw": ["bw", "bigwig"],
            "sam": ["sam"],
            "csv": ["csv"]  # ä»£è°¢ç»„å­¦æ•°æ®é€šå¸¸ä½¿ç”¨ CSV æ ¼å¼
        }
        
        for file_type, extensions in type_mapping.items():
            if ext in extensions:
                return file_type
        
        return "unknown"
    
    async def _perform_data_diagnosis(
        self,
        file_metadata: Dict[str, Any],
        omics_type: str,
        dataframe: Optional[Any] = None,
        system_instruction: Optional[str] = None
    ) -> Optional[str]:
        """
        æ‰§è¡Œæ•°æ®è¯Šæ–­å¹¶ç”Ÿæˆ Markdown æŠ¥å‘Š
        
        ğŸ”¥ æ¶æ„é‡æ„ï¼šä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œæ¥å— domain-specific system_instruction
        
        è¿™æ˜¯ç»Ÿä¸€çš„æ•°æ®è¯Šæ–­å…¥å£ï¼Œæ‰€æœ‰ Agent éƒ½åº”è¯¥è°ƒç”¨æ­¤æ–¹æ³•ã€‚
        
        Args:
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®
            omics_type: ç»„å­¦ç±»å‹ï¼ˆ"scRNA", "Metabolomics", "BulkRNA", "default"ï¼‰
            dataframe: å¯é€‰çš„æ•°æ®é¢„è§ˆï¼ˆDataFrame æˆ– AnnDataï¼‰
            system_instruction: é¢†åŸŸç‰¹å®šçš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆç”±å„ä¸ª Agent æä¾›ï¼‰
        
        Returns:
            Markdown æ ¼å¼çš„è¯Šæ–­æŠ¥å‘Šï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        try:
            logger.info(f"ğŸ” [DataDiagnostician] å¼€å§‹æ•°æ®è¯Šæ–­ - ç»„å­¦ç±»å‹: {omics_type}")
            
            # Step 1: ä½¿ç”¨ DataDiagnostician è®¡ç®—ç»Ÿè®¡äº‹å®
            diagnosis_result = self.diagnostician.analyze(
                file_metadata=file_metadata,
                omics_type=omics_type,
                dataframe=dataframe
            )
            
            if diagnosis_result.get("status") != "success":
                logger.warning(f"âš ï¸ æ•°æ®è¯Šæ–­å¤±è´¥: {diagnosis_result.get('error')}")
                return None
            
            stats = diagnosis_result.get("stats", {})
            logger.info(f"âœ… [DataDiagnostician] ç»Ÿè®¡è®¡ç®—å®Œæˆ: {len(stats)} ä¸ªæŒ‡æ ‡")
            
            # Step 2: æ„å»º LLM Prompt
            # å°†ç»Ÿè®¡äº‹å®æ ¼å¼åŒ–ä¸º JSON å­—ç¬¦ä¸²
            import json
            try:
                stats_json = json.dumps(stats, ensure_ascii=False, indent=2)
                logger.debug(f"ğŸ“ [DEBUG] Stats JSON length: {len(stats_json)}")
            except Exception as json_err:
                logger.error(f"âŒ [DataDiagnostician] JSON åºåˆ—åŒ–å¤±è´¥: {json_err}")
                stats_json = json.dumps({"error": "æ— æ³•åºåˆ—åŒ–ç»Ÿè®¡ä¿¡æ¯"}, ensure_ascii=False)
            
            # ğŸ”¥ ä¿®å¤ï¼šå®‰å…¨åœ°æˆªæ–­ JSON å­—ç¬¦ä¸²ï¼ˆè€Œä¸æ˜¯å­—å…¸ï¼‰
            # å¦‚æœ JSON å¤ªé•¿ï¼Œæˆªæ–­å®ƒï¼ˆä½†ä¿ç•™å®Œæ•´çš„ç»“æ„ï¼‰
            max_json_length = 2000  # é™åˆ¶ JSON é•¿åº¦
            if len(stats_json) > max_json_length:
                logger.warning(f"âš ï¸ Stats JSON å¤ªé•¿ ({len(stats_json)} å­—ç¬¦)ï¼Œæˆªæ–­åˆ° {max_json_length} å­—ç¬¦")
                # æˆªæ–­å­—ç¬¦ä¸²ï¼Œä½†ç¡®ä¿ JSON ç»“æ„å®Œæ•´
                truncated_json = stats_json[:max_json_length]
                # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡/æ•°ç»„è¾¹ç•Œ
                last_brace = truncated_json.rfind('}')
                last_bracket = truncated_json.rfind(']')
                last_comma = max(truncated_json.rfind(','), truncated_json.rfind('\n'))
                # é€‰æ‹©æœ€æ¥è¿‘æœ«å°¾çš„è¾¹ç•Œ
                cut_point = max(last_brace, last_bracket, last_comma)
                if cut_point > max_json_length * 0.8:  # å¦‚æœæˆªæ–­ç‚¹ä¸å¤ªæ—©
                    stats_json = truncated_json[:cut_point + 1] + "\n  ... (truncated)"
                else:
                    stats_json = truncated_json + "\n  ... (truncated)"
            
            # ğŸ”¥ å®‰å…¨åœ°æå–æ–‡ä»¶é¢„è§ˆä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # æ³¨æ„ï¼šfile_metadata æ˜¯å­—å…¸ï¼Œä¸èƒ½ç›´æ¥åˆ‡ç‰‡
            head_preview = ""
            try:
                head_data = file_metadata.get("head", {})
                if isinstance(head_data, dict):
                    # head_data æ˜¯å­—å…¸ï¼ŒåŒ…å« "markdown" æˆ– "json" é”®
                    if "markdown" in head_data:
                        head_preview = head_data["markdown"]
                    elif "json" in head_data:
                        # å¦‚æœæ˜¯ JSON æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        head_preview = json.dumps(head_data["json"], ensure_ascii=False, indent=2)
                    else:
                        head_preview = str(head_data)
                elif isinstance(head_data, str):
                    # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    head_preview = head_data
                else:
                    head_preview = str(head_data)
                
                # ğŸ”¥ å®‰å…¨åœ°æˆªæ–­å­—ç¬¦ä¸²é¢„è§ˆï¼ˆä¸æ˜¯å­—å…¸ï¼‰
                if len(head_preview) > 1000:
                    head_preview = head_preview[:1000] + "\n... (truncated)"
            except Exception as head_err:
                logger.warning(f"âš ï¸ æå–æ–‡ä»¶é¢„è§ˆå¤±è´¥: {head_err}")
                head_preview = "æ— æ³•æå–æ•°æ®é¢„è§ˆ"
            
            # ä½¿ç”¨ PromptManager è·å–è¯Šæ–­æ¨¡æ¿
            try:
                # ğŸ”¥ ç¡®ä¿åªä¼ é€’å­—ç¬¦ä¸²ç»™æ¨¡æ¿ï¼Œä¸ä¼ é€’å­—å…¸
                prompt = self.prompt_manager.get_prompt(
                    "data_diagnosis",
                    {
                        "inspection_data": stats_json,  # å­—ç¬¦ä¸²
                        "head_preview": head_preview[:500] if head_preview else ""  # å­—ç¬¦ä¸²ï¼Œæˆªæ–­åˆ° 500 å­—ç¬¦
                    },
                    fallback=DATA_DIAGNOSIS_PROMPT.format(inspection_data=stats_json)
                )
                logger.debug(f"ğŸ“ [DEBUG] Prompt length: {len(prompt)}")
            except Exception as prompt_err:
                logger.warning(f"âš ï¸ è·å–è¯Šæ–­æ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {prompt_err}")
                try:
                    # ğŸ”¥ å®‰å…¨åœ°æ ¼å¼åŒ– promptï¼Œé¿å… format é”™è¯¯
                    # ç¡®ä¿ stats_json æ˜¯å­—ç¬¦ä¸²
                    if not isinstance(stats_json, str):
                        stats_json = json.dumps(stats_json, ensure_ascii=False)
                    prompt = DATA_DIAGNOSIS_PROMPT.format(inspection_data=stats_json)
                except Exception as format_err:
                    logger.error(f"âŒ [DataDiagnostician] Prompt æ ¼å¼åŒ–å¤±è´¥: {format_err}")
                    # ä½¿ç”¨ç®€å•çš„ prompt
                    # ğŸ”¥ ç¡®ä¿ stats_json æ˜¯å­—ç¬¦ä¸²
                    if not isinstance(stats_json, str):
                        stats_json = json.dumps(stats_json, ensure_ascii=False)
                    prompt = f"""You are a Senior Bioinformatician specializing in {omics_type}.

Based on the following data statistics:
{stats_json}

Please generate a data diagnosis and parameter recommendation report in Simplified Chinese (ç®€ä½“ä¸­æ–‡).

Format:
### ğŸ” æ•°æ®ä½“æ£€æŠ¥å‘Š
- **æ•°æ®è§„æ¨¡**: [æ ·æœ¬æ•°ã€ä»£è°¢ç‰©æ•°]
- **æ•°æ®ç‰¹å¾**: [ç¼ºå¤±å€¼ç‡ã€æ•°æ®èŒƒå›´ç­‰]
- **æ•°æ®è´¨é‡**: [è´¨é‡è¯„ä¼°]

### ğŸ’¡ å‚æ•°æ¨è
Create a Markdown table with parameter recommendations.

Use Simplified Chinese for all content."""
            
            # Step 3: è°ƒç”¨ LLM ç”Ÿæˆ Markdown æŠ¥å‘Š
            # ğŸ”¥ CRITICAL FIX: å¼ºåˆ¶æ³¨å…¥ç»Ÿè®¡æ•°æ®åˆ°ç³»ç»Ÿæç¤ºï¼Œé˜²æ­¢ LLM äº§ç”Ÿå¹»è§‰
            stats_facts = []
            if omics_type.lower() in ["metabolomics", "metabolomic", "metabonomics"]:
                n_samples = stats.get("n_samples", 0)
                n_metabolites = stats.get("n_metabolites", 0)
                missing_rate = stats.get("missing_rate", 0)
                stats_facts.append(f"æ•°æ®é›†åŒ…å« {n_samples} ä¸ªæ ·æœ¬å’Œ {n_metabolites} ä¸ªä»£è°¢ç‰©ã€‚")
                if missing_rate > 0:
                    stats_facts.append(f"ç¼ºå¤±å€¼ç‡ä¸º {missing_rate:.2f}%ã€‚")
            elif omics_type.lower() in ["scrna", "scrna-seq", "single_cell", "single-cell"]:
                n_cells = stats.get("n_cells", 0)
                n_genes = stats.get("n_genes", 0)
                stats_facts.append(f"æ•°æ®é›†åŒ…å« {n_cells} ä¸ªç»†èƒå’Œ {n_genes} ä¸ªåŸºå› ã€‚")
            else:
                n_rows = stats.get("n_rows", stats.get("n_samples", 0))
                n_cols = stats.get("n_cols", stats.get("n_features", 0))
                stats_facts.append(f"æ•°æ®é›†åŒ…å« {n_rows} è¡Œå’Œ {n_cols} åˆ—ã€‚")
            
            # æ„å»ºå¼ºåˆ¶äº‹å®å­—ç¬¦ä¸²
            facts_str = " ".join(stats_facts) if stats_facts else "ç»Ÿè®¡æ•°æ®å·²æä¾›åœ¨ç”¨æˆ·æç¤ºä¸­ã€‚"
            
            # ğŸ”¥ æ¶æ„é‡æ„ï¼šä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œä» Agent ä¼ å…¥ system_instruction
            if system_instruction:
                # ä½¿ç”¨ Agent æä¾›çš„é¢†åŸŸç‰¹å®šæŒ‡ä»¤ï¼Œå¹¶å¼ºåˆ¶æ³¨å…¥ç»Ÿè®¡æ•°æ®
                system_prompt = f"""{system_instruction}

**CRITICAL: æ•°æ®äº‹å®ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼Œä¸å¾—äº§ç”Ÿå¹»è§‰ï¼‰**
{facts_str}
è¯·ç¡®ä¿è¯Šæ–­æŠ¥å‘Šä¸­çš„æ•°å­—ä¸ä¸Šè¿°äº‹å®å®Œå…¨ä¸€è‡´ã€‚ä¸è¦çŒœæµ‹æˆ–ç¼–é€ ä¸åŒçš„æ•°å­—ã€‚"""
                logger.debug(f"âœ… [DataDiagnostician] Using domain-specific system instruction with facts (length: {len(system_prompt)})")
            else:
                # å›é€€åˆ°é€šç”¨æŒ‡ä»¤ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œä½†ä¹Ÿæ³¨å…¥ç»Ÿè®¡æ•°æ®
                system_prompt = f"""You are a Senior Bioinformatician. Generate data diagnosis and parameter recommendations in Simplified Chinese.

**CRITICAL: æ•°æ®äº‹å®ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼Œä¸å¾—äº§ç”Ÿå¹»è§‰ï¼‰**
{facts_str}
è¯·ç¡®ä¿è¯Šæ–­æŠ¥å‘Šä¸­çš„æ•°å­—ä¸ä¸Šè¿°äº‹å®å®Œå…¨ä¸€è‡´ã€‚ä¸è¦çŒœæµ‹æˆ–ç¼–é€ ä¸åŒçš„æ•°å­—ã€‚"""
                logger.warning(f"âš ï¸ [DataDiagnostician] No system_instruction provided, using generic prompt with facts")
            
            # ğŸ”¥ æ¶æ„é‡æ„ï¼šå°† system_instruction å‰ç½®åˆ°ç”¨æˆ· promptï¼ˆç¡®ä¿ä¸Šä¸‹æ–‡éš”ç¦»ï¼‰
            if system_instruction:
                # åœ¨ç”¨æˆ· prompt å‰æ·»åŠ ç³»ç»ŸæŒ‡ä»¤ï¼Œç¡®ä¿ LLM ç†è§£é¢†åŸŸçº¦æŸ
                prompt = f"""{system_instruction}

**æ•°æ®äº‹å®ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰ï¼š**
{facts_str}

{prompt}"""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            # ğŸ”¥ Step 3: è°ƒç”¨ LLM ç”Ÿæˆ Markdown æŠ¥å‘Š
            # ğŸ”¥ CRITICAL DEBUGGING: åŒ…è£…åœ¨è¯¦ç»†çš„ try-except ä¸­
            try:
                logger.info(f"ğŸ“ [DataDiagnostician] è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š...")
                logger.debug(f"ğŸ“ [DEBUG] LLM Client type: {type(self.llm_client)}")
                logger.debug(f"ğŸ“ [DEBUG] LLM Client methods: {dir(self.llm_client)}")
                
                completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=1500)
                
                logger.debug(f"ğŸ“ [DEBUG] LLM completion type: {type(completion)}")
                logger.debug(f"ğŸ“ [DEBUG] LLM completion: {completion}")
                
                think_content, response = self.llm_client.extract_think_and_content(completion)
                
                # ğŸ”¥ DEBUG: æ‰“å°è¯Šæ–­æŠ¥å‘Šä¿¡æ¯
                if response:
                    logger.info(f"âœ… [DataDiagnostician] è¯Šæ–­æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(response)}")
                    logger.debug(f"ğŸ“ [DEBUG] Diagnosis report preview: {response[:200]}...")
                else:
                    logger.warning(f"âš ï¸ [DataDiagnostician] è¯Šæ–­æŠ¥å‘Šä¸ºç©º")
                    logger.warning(f"âš ï¸ [DEBUG] Think content: {think_content[:200] if think_content else 'None'}")
                
                # Step 4: ä¿å­˜åˆ°ä¸Šä¸‹æ–‡ï¼ˆä¾› UI å’Œåç»­æ­¥éª¤ä½¿ç”¨ï¼‰
                self.context["diagnosis_report"] = response
                self.context["diagnosis_stats"] = stats
                
                return response
                
            except AttributeError as attr_err:
                # LLM å®¢æˆ·ç«¯æ–¹æ³•ä¸å­˜åœ¨
                import traceback
                error_msg = (
                    f"LLM å®¢æˆ·ç«¯æ–¹æ³•è°ƒç”¨å¤±è´¥: {str(attr_err)}\n"
                    f"LLM Client type: {type(self.llm_client)}\n"
                    f"Available methods: {[m for m in dir(self.llm_client) if not m.startswith('_')]}\n"
                    f"Stack trace:\n{traceback.format_exc()}"
                )
                logger.error(f"âŒ [DataDiagnostician] {error_msg}")
                return f"âš ï¸ **è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥**\n\nLLM å®¢æˆ·ç«¯é”™è¯¯: {str(attr_err)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
                
            except Exception as llm_err:
                # LLM è°ƒç”¨å¤±è´¥
                import traceback
                error_msg = (
                    f"LLM è°ƒç”¨å¤±è´¥: {str(llm_err)}\n"
                    f"Error type: {type(llm_err).__name__}\n"
                    f"Stack trace:\n{traceback.format_exc()}"
                )
                logger.error(f"âŒ [DataDiagnostician] {error_msg}")
                return f"âš ï¸ **è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥**\n\né”™è¯¯: {str(llm_err)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
            
        except Exception as e:
            # æ•´ä½“å¼‚å¸¸å¤„ç†
            import traceback
            error_msg = (
                f"æ•°æ®è¯Šæ–­è¿‡ç¨‹å¤±è´¥: {str(e)}\n"
                f"Error type: {type(e).__name__}\n"
                f"Stack trace:\n{traceback.format_exc()}"
            )
            logger.error(f"âŒ [DataDiagnostician] {error_msg}")
            # ğŸ”¥ è¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ Noneï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥åœ¨ UI ä¸­çœ‹åˆ°
            return f"âš ï¸ **è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥**\n\né”™è¯¯: {str(e)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
    
    async def _generate_analysis_summary(
        self,
        steps_results: List[Dict[str, Any]],
        omics_type: str = "Metabolomics",
        workflow_name: str = "Analysis Pipeline"
    ) -> Optional[str]:
        """
        åŸºäºå·¥ä½œæµæ‰§è¡Œç»“æœç”Ÿæˆåˆ†ææ‘˜è¦ï¼ˆAI Expert Diagnosisï¼‰
        
        Args:
            steps_results: æ­¥éª¤æ‰§è¡Œç»“æœåˆ—è¡¨ï¼ˆæ¥è‡ª ExecutionLayerï¼‰
            omics_type: ç»„å­¦ç±»å‹ï¼ˆ"Metabolomics", "scRNA", ç­‰ï¼‰
            workflow_name: å·¥ä½œæµåç§°
        
        Returns:
            Markdown æ ¼å¼çš„åˆ†ææ‘˜è¦ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        import json
        
        try:
            logger.info(f"ğŸ“ [AnalysisSummary] å¼€å§‹ç”Ÿæˆåˆ†ææ‘˜è¦ - ç»„å­¦ç±»å‹: {omics_type}")
            
            # æå–å…³é”®ç»“æœ
            results_summary = {
                "workflow_name": workflow_name,
                "steps_completed": len(steps_results),
                "steps": []
            }
            
            # è§£ææ¯ä¸ªæ­¥éª¤çš„ç»“æœï¼ˆåªå¤„ç†æˆåŠŸçš„æ­¥éª¤ï¼Œå¿½ç•¥å¤±è´¥çš„æ­¥éª¤ï¼‰
            for step_result in steps_results:
                step_data = step_result.get("data", {})
                step_name = step_result.get("step_name", "Unknown Step")
                step_status = step_result.get("status", "unknown")
                
                # ğŸ”¥ CRITICAL: è·³è¿‡å¤±è´¥çš„æ­¥éª¤ï¼Œåªå¤„ç†æˆåŠŸçš„æ­¥éª¤
                if step_status != "success":
                    logger.debug(f"â­ï¸ [AnalysisSummary] è·³è¿‡å¤±è´¥çš„æ­¥éª¤: {step_name} (status: {step_status})")
                    continue
                
                step_info = {
                    "name": step_name,
                    "status": step_status
                }
                
                # æ ¹æ®ä¸åŒçš„å·¥å…·ç±»å‹æå–å…³é”®æŒ‡æ ‡
                if "inspect_data" in step_name.lower() or "inspection" in step_name.lower():
                    summary = step_data.get("summary", {})
                    step_info["n_samples"] = summary.get("n_samples", "N/A")
                    step_info["n_features"] = summary.get("n_features", "N/A")
                    step_info["missing_rate"] = summary.get("missing_rate", "N/A")
                
                elif "differential" in step_name.lower():
                    summary = step_data.get("summary", {})
                    step_info["significant_count"] = summary.get("significant_count", summary.get("n_significant", "N/A"))
                    step_info["total_count"] = summary.get("total_metabolites", summary.get("n_total", "N/A"))
                    step_info["method"] = summary.get("method", "N/A")
                    step_info["case_group"] = summary.get("case_group", "N/A")
                    step_info["control_group"] = summary.get("control_group", "N/A")
                    # æå–ç»“æœåˆ—è¡¨ï¼Œç”¨äºè¯†åˆ«å…³é”®æ ‡è®°ç‰©
                    results_list = step_data.get("results", [])
                    if results_list:
                        # æŒ‰ |log2fc| æ’åºï¼Œè·å–topæ ‡è®°ç‰©
                        sorted_results = sorted(results_list, key=lambda x: abs(x.get("log2fc", 0)), reverse=True)
                        step_info["top_markers"] = [
                            {
                                "name": r.get("metabolite", "Unknown"),
                                "log2fc": r.get("log2fc", 0),
                                "fdr": r.get("fdr", r.get("fdr_corrected_pvalue", 1.0))
                            }
                            for r in sorted_results[:5]
                        ]
                
                elif "plsda" in step_name.lower() or "pls-da" in step_name.lower():
                    # PLS-DA åˆ†æç»“æœ
                    vip_scores = step_data.get("vip_scores", [])
                    if vip_scores:
                        # æå–top VIPæ ‡è®°ç‰©
                        if isinstance(vip_scores, list):
                            sorted_vip = sorted(vip_scores, key=lambda x: x.get("vip_score", 0), reverse=True)
                            step_info["top_vip_markers"] = [
                                {
                                    "name": v.get("metabolite", "Unknown"),
                                    "vip_score": v.get("vip_score", 0)
                                }
                                for v in sorted_vip[:5]
                            ]
                
                elif "pathway" in step_name.lower() or "enrichment" in step_name.lower():
                    # é€šè·¯å¯Œé›†åˆ†æç»“æœ
                    enriched_pathways = step_data.get("enriched_pathways", [])
                    if enriched_pathways:
                        step_info["enriched_pathway_count"] = len(enriched_pathways)
                        step_info["top_pathways"] = [
                            {
                                "name": p.get("pathway", p.get("name", "Unknown")),
                                "p_value": p.get("p_value", p.get("pvalue", 1.0)),
                                "enrichment_score": p.get("enrichment_score", p.get("score", 0))
                            }
                            for p in enriched_pathways[:5]
                        ]
                
                elif "pca" in step_name.lower() and "visualize" not in step_name.lower():
                    # PCA åˆ†æç»“æœ
                    explained_var = step_data.get("explained_variance", {})
                    if explained_var:
                        pc1_var = explained_var.get("PC1", 0) * 100 if isinstance(explained_var.get("PC1"), (int, float)) else 0
                        pc2_var = explained_var.get("PC2", 0) * 100 if isinstance(explained_var.get("PC2"), (int, float)) else 0
                        step_info["pc1_variance"] = f"{pc1_var:.1f}%"
                        step_info["pc2_variance"] = f"{pc2_var:.1f}%"
                
                elif "preprocess" in step_name.lower():
                    shape = step_data.get("shape", {})
                    step_info["preprocessed_rows"] = shape.get("rows", "N/A")
                    step_info["preprocessed_cols"] = shape.get("columns", "N/A")
                
                results_summary["steps"].append(step_info)
            
            # æ ¼å¼åŒ–ç»“æœæ‘˜è¦
            summary_json = json.dumps(results_summary, ensure_ascii=False, indent=2)
            
            # æ„å»ºæç¤ºè¯
            if omics_type.lower() in ["metabolomics", "metabolomic", "metabonomics"]:
                expert_role = "ä»£è°¢ç»„å­¦åˆ†æä¸“å®¶"
                domain_context = """
- ä»£è°¢ç‰©æ•°æ®é¢„å¤„ç†ï¼ˆç¼ºå¤±å€¼å¤„ç†ã€Log2è½¬æ¢ã€æ ‡å‡†åŒ–ï¼‰
- ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç”¨äºé™ç»´å’Œå¯è§†åŒ–
- å·®å¼‚ä»£è°¢ç‰©åˆ†æï¼ˆt-test/Wilcoxonï¼‰ç”¨äºå‘ç°ç»„é—´å·®å¼‚
- ç«å±±å›¾å¯è§†åŒ–å±•ç¤ºå·®å¼‚åˆ†æç»“æœ
"""
            elif omics_type.lower() in ["scrna", "scrna-seq", "single_cell", "single-cell"]:
                expert_role = "å•ç»†èƒè½¬å½•ç»„åˆ†æä¸“å®¶"
                domain_context = """
- è´¨é‡æ§åˆ¶ï¼ˆQCï¼‰è¿‡æ»¤ä½è´¨é‡ç»†èƒ
- æ•°æ®æ ‡å‡†åŒ–å’Œç‰¹å¾é€‰æ‹©
- é™ç»´åˆ†æï¼ˆPCAã€UMAPï¼‰
- ç»†èƒèšç±»å’Œæ ‡è®°åŸºå› è¯†åˆ«
"""
            else:
                expert_role = "ç”Ÿç‰©ä¿¡æ¯å­¦åˆ†æä¸“å®¶"
                domain_context = "é€šç”¨ç»„å­¦æ•°æ®åˆ†ææµç¨‹"
            
            prompt = f"""You are a Senior Bioinformatics Analyst specializing in {omics_type} data analysis. Your task is to generate a comprehensive "Omics Analysis Report" in Markdown format.

**Execution Results (Only Successful Steps):**
{summary_json}

**Domain Context:**
{domain_context}

**CRITICAL RULES:**

1. **Academic Standard**: Generate a comprehensive, detailed report following academic standards. This is NOT a brief summary - it should be thorough and professional.

2. **IGNORE Technical Issues**: 
   - DO NOT mention failed steps, errors, or technical problems
   - DO NOT suggest checking input formats, file paths, or code issues
   - DO NOT act like IT support
   - Only interpret the data from successful steps

3. **Output Structure (MUST FOLLOW):**

### 1. æ•°æ®æ¦‚å†µ (Data Overview)
- Summarize sample size, groups, and detected features
- Evaluate Data Quality (Missing values, outliers based on PCA if available)
- Describe the overall data characteristics

### 2. ç»Ÿè®¡åˆ†æç»“æœ (Statistical Findings)
- **PCA Analysis**: If PCA was performed, interpret the separation between groups (PC1/PC2 scores, explained variance). Describe clustering patterns and what they indicate about group differences.
- **Differential Analysis**: If differential analysis was performed, report:
  - Total number of features analyzed
  - Number of Up-regulated features (Log2FC > threshold)
  - Number of Down-regulated features (Log2FC < -threshold)
  - Number of significant features (FDR < threshold)
  - Statistical method used (t-test/Wilcoxon)
- **Key Markers**: If available, list top 3-5 features with highest VIP scores (from PLS-DA) or highest |Log2FC| (from differential analysis). Include their names and fold changes.

### 3. ç”Ÿç‰©å­¦æ„ä¹‰ (Biological Interpretation)
- Interpret the biological meaning of the findings
- If Pathway Enrichment data exists, interpret the enriched KEGG pathways and their biological significance
- Relate findings to potential biological mechanisms or disease processes
- Discuss the functional implications of differentially expressed features

### 4. ç»“è®ºä¸å»ºè®® (Conclusion)
- Summarize the main takeaway from the analysis
- Highlight the most important findings
- Suggest next steps (e.g., validation experiments, targeted analysis, pathway validation)

**Output Format:**
- Use Simplified Chinese (ç®€ä½“ä¸­æ–‡)
- Use Markdown format with proper headings (###)
- Be professional, academic, and detailed
- Minimum 500 words, aim for comprehensive coverage
- Include specific numbers, percentages, and statistical values from the results

**Tone**: Professional, Academic, Detailed. Focus on biological interpretation and scientific insights.

ç°åœ¨ç”Ÿæˆå…¨é¢çš„åˆ†ææŠ¥å‘Šï¼ˆéµå¾ªä¸Šè¿°ç»“æ„ï¼Œè¯¦ç»†ä¸”ä¸“ä¸šï¼‰ï¼š"""
            
            messages = [
                {
                    "role": "system",
                    "content": f"""You are a Senior Bioinformatics Scientist specializing in {omics_type} data analysis. You are NOT a software engineer or IT support.

**Your Role:**
- Interpret biological data and patterns
- Provide scientific insights about the results
- Focus on biological meaning, not technical issues

**What to DO:**
- Interpret clustering patterns, outliers, significant findings
- Explain biological implications
- Suggest next biological analysis steps

**What NOT to DO:**
- Do NOT mention technical errors or failed steps
- Do NOT suggest checking file formats or code issues
- Do NOT act like IT support

Generate concise, professional, scientifically insightful analysis summaries based on successful execution results. Use Simplified Chinese and Markdown format."""
                },
                {"role": "user", "content": prompt}
            ]
            
            # è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
            logger.info(f"ğŸ“ [AnalysisSummary] è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦...")
            completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=500)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            if response:
                logger.info(f"âœ… [AnalysisSummary] åˆ†ææ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(response)}")
                logger.debug(f"ğŸ“ [DEBUG] Summary preview: {response[:200]}...")
                return response
            else:
                logger.warning(f"âš ï¸ [AnalysisSummary] åˆ†ææ‘˜è¦ä¸ºç©º")
                return None
                
        except Exception as e:
            logger.error(f"âŒ [AnalysisSummary] ç”Ÿæˆåˆ†ææ‘˜è¦å¤±è´¥: {e}", exc_info=True)
            return None

