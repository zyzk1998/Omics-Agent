"""ä»£è°¢ç»„å­¦æ™ºèƒ½ä½“ï¼ˆMetabolomics Agentï¼‰"""
from typing import Dict, Any, List, AsyncIterator, Optional
from ..base_agent import BaseAgent
from ...core.llm_client import LLMClient
from ...core.prompt_manager import PromptManager
from ...core.utils import sanitize_for_json
from ...tools.metabolomics_tool import MetabolomicsTool
import logging

logger = logging.getLogger(__name__)


# ğŸ”¥ æ¶æ„é‡æ„ï¼šé¢†åŸŸç‰¹å®šçš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆç­–ç•¥æ¨¡å¼ï¼‰
METABO_INSTRUCTION = """You are an expert Chemist/Metabolomics Analyst specializing in Metabolomics data analysis.

**CRITICAL CONSTRAINTS:**
- The data represents **Metabolite Abundance** (Chemical Compounds), NOT Gene Expression.
- Rows = Samples (Biological Samples), Columns = Metabolites (Chemical Compounds).
- This is Mass Spectrometry or LC-MS/GC-MS data, measuring chemical concentrations.

**STRICTLY FORBIDDEN TERMS:**
- Cell, Cells, Cellular
- Gene, Genes, Gene Expression, Transcript
- Mitochondria, Mitochondrial
- scRNA, Single-Cell RNA-seq, scRNA-seq
- Transcriptomics, Transcriptome
- RNA-seq, RNA sequencing

**REQUIRED TERMINOLOGY:**
- Metabolite, Metabolites, Metabolite Abundance
- Sample, Samples, Biological Sample
- Metabolomics, Metabolomic Analysis
- Mass Spectrometry, LC-MS, GC-MS
- Chemical Compound, Compound

**CONTEXT ISOLATION:**
This is NOT single-cell data. This is NOT transcriptomics data.
This is Metabolomics data representing metabolite abundance levels measured by mass spectrometry.

Generate data diagnosis and parameter recommendations in Simplified Chinese (ç®€ä½“ä¸­æ–‡).
Focus on metabolite-specific quality metrics (missing values, abundance range, normalization needs)."""


class MetabolomicsAgent(BaseAgent):
    """ä»£è°¢ç»„å­¦æ™ºèƒ½ä½“"""
    
    # å®šä¹‰ä¸¥æ ¼çš„æ­¥éª¤é¡ºåºï¼ˆä¾èµ–é“¾ï¼‰
    STEPS_ORDER = [
        "inspect_data",      # æ­¥éª¤1: æ•°æ®æ£€æŸ¥
        "preprocess_data",   # æ­¥éª¤2: æ•°æ®é¢„å¤„ç†
        "pca_analysis",      # æ­¥éª¤3: PCA åˆ†æ
        "differential_analysis",  # æ­¥éª¤4: å·®å¼‚åˆ†æ
        "visualize_pca",     # æ­¥éª¤5: PCA å¯è§†åŒ–
        "visualize_volcano"  # æ­¥éª¤6: ç«å±±å›¾å¯è§†åŒ–
    ]
    
    # æ­¥éª¤æ˜ å°„ï¼ˆstep_id -> åœ¨ STEPS_ORDER ä¸­çš„ç´¢å¼•ï¼‰
    STEP_INDEX_MAP = {step: idx for idx, step in enumerate(STEPS_ORDER)}
    
    def __init__(
        self,
        llm_client: LLMClient,
        prompt_manager: PromptManager,
        metabolomics_config: Dict[str, Any] = None
    ):
        super().__init__(llm_client, prompt_manager, "metabolomics_expert")
        self.metabolomics_config = metabolomics_config or {}
        self.metabolomics_tool = MetabolomicsTool(self.metabolomics_config)
        
        # æ ‡å‡†å·¥ä½œæµæ­¥éª¤ï¼ˆä»£è°¢ç»„å­¦åˆ†ææµç¨‹ï¼‰
        self.workflow_steps = [
            {"name": "1. æ•°æ®æ£€æŸ¥", "step_id": "inspect_data", "tool_id": "inspect_data", "desc": "æ£€æŸ¥æ•°æ®æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ï¼ˆæ ·æœ¬æ•°ã€ä»£è°¢ç‰©æ•°ã€ç¼ºå¤±å€¼ã€åˆ†ç»„ä¿¡æ¯ç­‰ï¼‰"},
            {"name": "2. æ•°æ®é¢„å¤„ç†", "step_id": "preprocess_data", "tool_id": "preprocess_data", "desc": "æ•°æ®é¢„å¤„ç†ï¼šå¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–ã€ç¼©æ”¾"},
            {"name": "3. ä¸»æˆåˆ†åˆ†æ", "step_id": "pca_analysis", "tool_id": "pca_analysis", "desc": "æ‰§è¡Œä¸»æˆåˆ†åˆ†æ (PCA)ï¼Œé™ç»´å¹¶æå–ä¸»è¦å˜å¼‚"},
            {"name": "4. å·®å¼‚ä»£è°¢ç‰©åˆ†æ", "step_id": "differential_analysis", "tool_id": "differential_analysis", "desc": "æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æï¼ˆä¸¤ç»„æ¯”è¾ƒï¼‰ï¼Œè¯†åˆ«æ˜¾è‘—å·®å¼‚çš„ä»£è°¢ç‰©"},
            {"name": "5. PCA å¯è§†åŒ–", "step_id": "visualize_pca", "tool_id": "visualize_pca", "desc": "ç”Ÿæˆ PCA å¯è§†åŒ–å›¾ï¼Œå±•ç¤ºæ ·æœ¬åœ¨ä¸»æˆåˆ†ç©ºé—´çš„åˆ†å¸ƒ"},
            {"name": "6. ç«å±±å›¾å¯è§†åŒ–", "step_id": "visualize_volcano", "tool_id": "visualize_volcano", "desc": "ç”Ÿæˆç«å±±å›¾ (Volcano Plot)ï¼Œå±•ç¤ºå·®å¼‚ä»£è°¢ç‰©çš„ç»Ÿè®¡æ˜¾è‘—æ€§"},
        ]
    
    async def process_query(
        self,
        query: str,
        history: List[Dict[str, str]] = None,
        uploaded_files: List[Dict[str, str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼Œå¯èƒ½åŒ…å«ï¼š
            - chat: èŠå¤©å“åº”ï¼ˆæµå¼ï¼‰
            - workflow_config: å·¥ä½œæµé…ç½®
        """
        # ğŸ”¥ æ¶æ„é‡æ„ï¼šä¼šè¯çº§æ–‡ä»¶æ³¨å†Œè¡¨
        query_lower = query.lower().strip()
        file_paths = self.get_file_paths(uploaded_files or [])
        
        # Scenario A: æ–°æ–‡ä»¶ä¸Šä¼  - æ³¨å†Œåˆ°æ–‡ä»¶æ³¨å†Œè¡¨å¹¶è®¾ç½®ä¸ºæ´»åŠ¨æ–‡ä»¶
        if uploaded_files and len(uploaded_files) > 0:
            for file_info in uploaded_files:
                if isinstance(file_info, dict):
                    filename = file_info.get("name") or file_info.get("path") or file_info.get("file_id", "unknown")
                    file_path = file_info.get("path") or file_info.get("file_id", filename)
                else:
                    filename = getattr(file_info, "name", None) or getattr(file_info, "path", None) or "unknown"
                    file_path = getattr(file_info, "path", None) or filename
                
                # æ‰¾åˆ°å¯¹åº”çš„ç»å¯¹è·¯å¾„
                if file_paths:
                    # å°è¯•åŒ¹é…è·¯å¾„
                    absolute_path = None
                    for abs_path in file_paths:
                        if file_path in abs_path or abs_path.endswith(file_path.split('/')[-1]):
                            absolute_path = abs_path
                            break
                    if not absolute_path:
                        absolute_path = file_paths[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè·¯å¾„ä½œä¸ºå›é€€
                else:
                    absolute_path = file_path
                
                # æ³¨å†Œæ–‡ä»¶
                self.register_file(filename, absolute_path, file_metadata=None)
                # è®¾ç½®ä¸ºæ´»åŠ¨æ–‡ä»¶ï¼ˆæœ€åä¸€ä¸ªä¸Šä¼ çš„æ–‡ä»¶ï¼‰
                self.set_active_file(filename)
        
        # Scenario B: æ²¡æœ‰æ–°æ–‡ä»¶ - ä½¿ç”¨å½“å‰æ´»åŠ¨æ–‡ä»¶
        if not file_paths:
            active_file_info = self.get_active_file_info()
            if active_file_info:
                file_paths = [active_file_info["path"]]
                logger.info(f"ğŸ“‚ [FileRegistry] Using active file: {active_file_info['filename']}")
            else:
                logger.warning("âš ï¸ [FileRegistry] No files available (no uploads and no active file)")
        
        # ğŸ”¥ Task 1: LLM é©±åŠ¨çš„æ„å›¾æ£€æµ‹ï¼ˆåœ¨ç”Ÿæˆå·¥ä½œæµä¹‹å‰ï¼‰
        # ğŸ”’ å®‰å…¨åŒ…è£…ï¼šå¦‚æœæ„å›¾æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘
        intent = "chat"  # é»˜è®¤å€¼
        intent_result = None
        try:
            intent_result = await self._detect_intent_with_llm(query, file_paths, uploaded_files)
            intent = intent_result.get("intent", "chat")
            reasoning = intent_result.get("reasoning", "")
            logger.info(f"ğŸ¯ æ„å›¾æ£€æµ‹ç»“æœ: {intent} (æ¨ç†: {reasoning})")
        except Exception as e:
            logger.warning(f"âš ï¸ æ„å›¾æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘: {e}", exc_info=True)
            # å›é€€åˆ°åŸå§‹çš„å·¥ä½œæµæ£€æµ‹é€»è¾‘
            intent = None  # æ ‡è®°ä¸ºæœªæ£€æµ‹ï¼Œä½¿ç”¨å›é€€é€»è¾‘
        
        # å¦‚æœæ„å›¾æ£€æµ‹æˆåŠŸä¸”ä¸º explain_fileï¼Œå¤„ç†æ–‡ä»¶è§£é‡Š
        if intent == "explain_file":
            # è§£é‡Šæ–‡ä»¶ï¼šæ£€æŸ¥æ–‡ä»¶å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Š
            if not file_paths:
                return {
                    "type": "chat",
                    "response": self._stream_string_response("æ²¡æœ‰æ£€æµ‹åˆ°ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·å…ˆä¸Šä¼ æ–‡ä»¶åå†è¯¢é—®ã€‚")
                }
            
            # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆåˆ—è¡¨æœ€åä¸€ä¸ªï¼‰ï¼Œè€Œä¸æ˜¯ç¬¬ä¸€ä¸ª
            # æ£€æŸ¥æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶
            input_path = file_paths[-1] if file_paths else None
            if not input_path:
                return {
                    "type": "chat",
                    "response": self._stream_string_response("æ²¡æœ‰æ£€æµ‹åˆ°ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·å…ˆä¸Šä¼ æ–‡ä»¶åå†è¯¢é—®ã€‚")
                }
            try:
                inspection_result = self.metabolomics_tool.inspect_data(input_path)
                if "error" in inspection_result:
                    return {
                        "type": "chat",
                        "response": self._stream_string_response(f"æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {inspection_result.get('error')}")
                    }
                
                # ä½¿ç”¨ LLM ç”Ÿæˆæ–‡ä»¶è§£é‡Š
                explanation = await self._explain_file_with_llm(query, inspection_result, input_path)
                return {
                    "type": "chat",
                    "response": self._stream_string_response(explanation)
                }
            except Exception as e:
                logger.error(f"âŒ æ–‡ä»¶è§£é‡Šå¤±è´¥: {e}", exc_info=True)
                return {
                    "type": "chat",
                    "response": self._stream_string_response(f"æ–‡ä»¶æ£€æŸ¥æ—¶å‡ºé”™: {str(e)}")
                }
        
        # ğŸ”’ å›é€€é€»è¾‘ï¼šå¦‚æœæ„å›¾æ£€æµ‹å¤±è´¥æˆ–æ„å›¾ä¸æ˜ç¡®ï¼Œä½¿ç”¨åŸå§‹é€»è¾‘
        if intent is None or intent == "chat":
            # ä½¿ç”¨åŸå§‹çš„å·¥ä½œæµæ£€æµ‹é€»è¾‘ä½œä¸ºå›é€€
            is_workflow_request = self._is_workflow_request(query_lower, file_paths)
            if is_workflow_request:
                # å·¥ä½œæµè¯·æ±‚ï¼šç”Ÿæˆå·¥ä½œæµé…ç½®
                return await self._generate_workflow_config(query, file_paths)
            else:
                # æ™®é€šèŠå¤©ï¼šæµå¼å“åº”
                return {
                    "type": "chat",
                    "response": self._stream_chat_response(query, file_paths)
                }
        
        # å¦‚æœæ„å›¾æ˜ç¡®ä¸º run_workflowï¼Œç›´æ¥ç”Ÿæˆå·¥ä½œæµé…ç½®
        elif intent == "run_workflow":
            return await self._generate_workflow_config(query, file_paths)
        
        # é»˜è®¤ï¼šæ™®é€šèŠå¤©
        else:
            return {
                "type": "chat",
                "response": self._stream_chat_response(query, file_paths)
            }
    
    async def _detect_intent_with_llm(
        self,
        query: str,
        file_paths: List[str],
        uploaded_files: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æ£€æµ‹ç”¨æˆ·æ„å›¾
        
        Returns:
            {
                "intent": "explain_file" | "run_workflow" | "chat",
                "reasoning": "..."
            }
        """
        import json
        import os
        
        # æå–æ–‡ä»¶å
        file_names = []
        if uploaded_files:
            for f in uploaded_files:
                name = f.get("name") or f.get("file_name", "")
                if name:
                    file_names.append(name)
        elif file_paths:
            for path in file_paths:
                file_names.append(os.path.basename(path))
        
        file_names_str = ", ".join(file_names) if file_names else "None"
        
        prompt = f"""åˆ†æç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­ç”¨æˆ·æ„å›¾ã€‚

User Input: {query}
Uploaded Files: {file_names_str}

è¯·å°†æ„å›¾åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ä¹‹ä¸€ï¼š
1. "explain_file" - ç”¨æˆ·æƒ³è¦äº†è§£æ–‡ä»¶å†…å®¹ã€ç»“æ„æˆ–å«ä¹‰ï¼ˆä¾‹å¦‚ï¼š"è¿™æ˜¯ä»€ä¹ˆæ–‡ä»¶ï¼Ÿ"ã€"æ–‡ä»¶é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"ã€"è§£é‡Šä¸€ä¸‹è¿™ä¸ªæ•°æ®"ï¼‰
2. "run_workflow" - ç”¨æˆ·æƒ³è¦æ‰§è¡Œåˆ†æå·¥ä½œæµï¼ˆä¾‹å¦‚ï¼š"åˆ†æä¸€ä¸‹"ã€"è¿è¡Œå·¥ä½œæµ"ã€"åšä¸€ä¸‹åˆ†æ"ã€"å¤„ç†è¿™ä¸ªæ–‡ä»¶"ï¼‰
3. "chat" - æ™®é€šå¯¹è¯æˆ–è¯¢é—®ï¼ˆä¾‹å¦‚ï¼š"ä½ å¥½"ã€"å¦‚ä½•ä½¿ç”¨"ã€"ä»‹ç»åŠŸèƒ½"ï¼‰

è¿”å› JSON æ ¼å¼ï¼š
{{
    "intent": "explain_file" | "run_workflow" | "chat",
    "reasoning": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±"
}}"""
        
        messages = [
            {"role": "system", "content": "You are an intent classification assistant. Return JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=128)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            # è§£æ JSON
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0].strip()
            
            result = json.loads(json_str)
            
            # éªŒè¯æ„å›¾å€¼
            valid_intents = ["explain_file", "run_workflow", "chat"]
            if result.get("intent") not in valid_intents:
                logger.warning(f"âš ï¸ LLM è¿”å›äº†æ— æ•ˆæ„å›¾: {result.get('intent')}, ä½¿ç”¨é»˜è®¤å€¼ 'chat'")
                result["intent"] = "chat"
            
            return result
        except Exception as e:
            logger.error(f"âŒ æ„å›¾æ£€æµ‹å¤±è´¥: {e}", exc_info=True)
            # é»˜è®¤è¿”å› chat
            return {
                "intent": "chat",
                "reasoning": f"Intent detection failed: {str(e)}"
            }
    
    async def _explain_file_with_llm(
        self,
        query: str,
        inspection_result: Dict[str, Any],
        file_path: str
    ) -> str:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆæ–‡ä»¶è§£é‡Š
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            inspection_result: æ–‡ä»¶æ£€æŸ¥ç»“æœ
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            è‡ªç„¶è¯­è¨€çš„æ–‡ä»¶è§£é‡Š
        """
        import json
        
        # æ ¼å¼åŒ–æ£€æŸ¥ç»“æœ
        inspection_summary = json.dumps(inspection_result, ensure_ascii=False, indent=2)
        
        prompt = f"""ç”¨æˆ·è¯¢é—®å…³äºæ–‡ä»¶çš„é—®é¢˜ã€‚

User Query: {query}
File Path: {file_path}

æ–‡ä»¶æ£€æŸ¥ç»“æœï¼š
{inspection_summary}

è¯·ç”¨è‡ªç„¶è¯­è¨€è§£é‡Šè¿™ä¸ªæ–‡ä»¶çš„å†…å®¹ã€ç»“æ„å’Œç‰¹ç‚¹ã€‚å›ç­”åº”è¯¥ï¼š
1. ç®€æ´æ˜äº†ï¼Œæ˜“äºç†è§£
2. åŒ…å«å…³é”®ä¿¡æ¯ï¼ˆæ ·æœ¬æ•°ã€å˜é‡æ•°ã€ç¼ºå¤±å€¼ç­‰ï¼‰
3. å¦‚æœç”¨æˆ·æœ‰ç‰¹å®šé—®é¢˜ï¼Œé’ˆå¯¹æ€§åœ°å›ç­”
4. ä½¿ç”¨ä¸­æ–‡å›ç­”

å›ç­”ï¼š"""
        
        messages = [
            {"role": "system", "content": "You are a bioinformatics data expert. Explain file contents in natural language."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=800)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            return response
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è§£é‡Šç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return f"æ–‡ä»¶è§£é‡Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _stream_string_response(self, text: str) -> AsyncIterator[str]:
        """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆç”¨äºæµå¼å“åº”ï¼‰"""
        async def _generator():
            yield text
        return _generator()
    
    def _is_workflow_request(self, query: str, file_paths: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å·¥ä½œæµè¯·æ±‚ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰"""
        workflow_keywords = [
            "è§„åˆ’", "æµç¨‹", "workflow", "pipeline", "åˆ†æ", "run",
            "æ‰§è¡Œ", "plan", "åšä¸€ä¸‹", "è·‘ä¸€ä¸‹", "åˆ†æä¸€ä¸‹", "å…¨æµç¨‹",
            "ä»£è°¢ç»„", "ä»£è°¢ç»„åˆ†æ", "metabolomics"
        ]
        
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæŸ¥è¯¢åŒ…å«å·¥ä½œæµå…³é”®è¯ï¼Œè¿”å› True
        if query and any(kw in query for kw in workflow_keywords):
            return True
        
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœåªæœ‰æ–‡ä»¶æ²¡æœ‰æ–‡æœ¬ï¼ˆæˆ–æ–‡æœ¬å¾ˆçŸ­ï¼‰ï¼Œä¸”ä¸æ˜¯è‡ªæˆ‘ä»‹ç»ç­‰å¸¸è§æŸ¥è¯¢ï¼Œè¿”å› True
        if file_paths and (not query or len(query.strip()) < 5):
            # æ’é™¤ä¸€äº›å¸¸è§çš„éå·¥ä½œæµæŸ¥è¯¢
            non_workflow_queries = ["ä½ å¥½", "hello", "hi", "ä»‹ç»", "è‡ªæˆ‘ä»‹ç»", "ä½ æ˜¯è°", "who are you"]
            if not query or query.strip().lower() not in [q.lower() for q in non_workflow_queries]:
                return True
        
        return False
    
    async def _generate_workflow_config(
        self,
        query: str,
        file_paths: List[str]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµé…ç½®
        
        ğŸ”¥ ä¿®å¤ç‰ˆæœ¬ï¼šåœ¨ planning é˜¶æ®µç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼Œä¿®å¤å˜é‡æ˜ å°„é—®é¢˜
        
        æµç¨‹ï¼š
        1. ç«‹å³æ£€æŸ¥æ–‡ä»¶ï¼ˆinspect_fileï¼‰- ä¿®å¤ N/A é—®é¢˜
        2. æ˜ å°„ shape.rows/cols åˆ° n_samples/n_features - ä¿®å¤å˜é‡ä¸åŒ¹é…
        3. ç«‹å³ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š - ä¿®å¤ UI ç¼ºå¤±æŠ¥å‘Šé—®é¢˜
        4. ä½¿ç”¨ LLM æå–ç›®æ ‡æ­¥éª¤
        5. åŸºäºæ£€æŸ¥ç»“æœæå–å‚æ•°
        6. ç”Ÿæˆå·¥ä½œæµé…ç½®
        """
        logger.info("=" * 80)
        logger.info("ğŸš€ [CHECKPOINT] _generate_workflow_config START (FIXED VERSION)")
        logger.info(f"   Query: {query}")
        logger.info(f"   File paths: {file_paths}")
        logger.info("=" * 80)
        
        # ğŸ”¥ Step 1: ç«‹å³æ£€æŸ¥æ–‡ä»¶ï¼ˆä¿®å¤ N/A é—®é¢˜ï¼‰
        file_metadata = None
        stats = {"n_samples": "N/A", "n_features": "N/A"}
        diagnosis_report = None
        recommendation = None
        
        if not file_paths:
            logger.warning("âš ï¸ æ²¡æœ‰æä¾›æ–‡ä»¶è·¯å¾„")
            return {
                "type": "workflow_config",
                "workflow_data": {
                    "workflow_name": "Metabolomics Analysis Pipeline",
                    "steps": []
                },
                "file_paths": [],
                "diagnosis_report": "âš ï¸ æœªæä¾›æ•°æ®æ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šã€‚"
            }
        
        current_file = file_paths[0]
        logger.info(f"ğŸ” [CHECKPOINT] Inspecting file IMMEDIATELY: {current_file}")
        
        try:
            # ğŸ”¥ ä½¿ç”¨ FileInspector ç«‹å³æ£€æŸ¥æ–‡ä»¶
            from ...core.file_inspector import FileInspector
            import os
            upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
            inspector = FileInspector(upload_dir)
            
            file_metadata = inspector.inspect_file(current_file)
            
            if file_metadata.get("status") != "success" or not file_metadata.get("success", True):
                error_msg = file_metadata.get("error", "æœªçŸ¥é”™è¯¯")
                logger.warning(f"âš ï¸ File inspection failed: {error_msg}")
                
                # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç æ¶ˆæ¯
                # è¿™æ ·ç”¨æˆ·å¯ä»¥çœ‹åˆ°ç³»ç»Ÿåœ¨å“ªé‡ŒæŸ¥æ‰¾æ–‡ä»¶
                diagnosis_report = f"âš ï¸ **æ–‡ä»¶è¯»å–å¤±è´¥**\n\n{error_msg}"
                
                # Fallback: ä½¿ç”¨é»˜è®¤å€¼
                stats = {"n_samples": "N/A", "n_features": "N/A"}
            else:
                logger.info(f"âœ… [CHECKPOINT] File inspection successful")
                
                # ğŸ”¥ CRITICAL FIX: æ˜ å°„ shape.rows/cols åˆ° n_samples/n_features
                shape = file_metadata.get("shape", {})
                stats = {
                    "n_samples": shape.get("rows", file_metadata.get("n_samples", "N/A")),
                    "n_features": shape.get("cols", file_metadata.get("n_features", "N/A"))
                }
                
                # å¦‚æœ shape ä¸­æ²¡æœ‰ï¼Œå°è¯•ä» file_metadata ç›´æ¥è·å–
                if stats["n_samples"] == "N/A" or stats["n_samples"] == 0:
                    stats["n_samples"] = file_metadata.get("n_samples", "N/A")
                if stats["n_features"] == "N/A" or stats["n_features"] == 0:
                    stats["n_features"] = file_metadata.get("n_features", "N/A")
                
                logger.info(f"ğŸ“Š [CHECKPOINT] Stats mapped: n_samples={stats['n_samples']}, n_features={stats['n_features']}")
                
                # ğŸ”¥ Step 2: ç«‹å³ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼ˆä¿®å¤ UI ç¼ºå¤±æŠ¥å‘Šé—®é¢˜ï¼‰
                try:
                    logger.info(f"ğŸ” [CHECKPOINT] Generating diagnosis report IMMEDIATELY...")
                    
                    # å°è¯•åŠ è½½æ•°æ®é¢„è§ˆï¼ˆç”¨äºæ›´å‡†ç¡®çš„è¯Šæ–­ï¼‰
                    dataframe = None
                    try:
                        import pandas as pd
                        head_data = file_metadata.get("head", {})
                        if head_data and isinstance(head_data, dict) and "json" in head_data:
                            dataframe = pd.DataFrame(head_data["json"])
                    except Exception as e:
                        logger.debug(f"æ— æ³•æ„å»ºæ•°æ®é¢„è§ˆ: {e}")
                    
                    # è°ƒç”¨ç»Ÿä¸€çš„è¯Šæ–­æ–¹æ³•ï¼ˆåœ¨ planning é˜¶æ®µï¼‰
                    # ğŸ”¥ æ¶æ„é‡æ„ï¼šä¼ é€’é¢†åŸŸç‰¹å®šçš„ç³»ç»ŸæŒ‡ä»¤
                    diagnosis_report = await self._perform_data_diagnosis(
                        file_metadata=file_metadata,
                        omics_type="Metabolomics",
                        dataframe=dataframe,
                        system_instruction=METABO_INSTRUCTION
                    )
                    
                    if diagnosis_report:
                        logger.info(f"âœ… [CHECKPOINT] Diagnosis report generated, length: {len(diagnosis_report)}")
                    else:
                        logger.warning(f"âš ï¸ [CHECKPOINT] Diagnosis report is None")
                        diagnosis_report = "âš ï¸ è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½†å¯ä»¥ç»§ç»­è¿›è¡Œåˆ†æã€‚"
                    
                except Exception as diag_err:
                    logger.error(f"âŒ [CHECKPOINT] Diagnosis generation failed: {diag_err}", exc_info=True)
                    diagnosis_report = "âš ï¸ è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½†å¯ä»¥ç»§ç»­è¿›è¡Œåˆ†æã€‚"
                
                # ä»è¯Šæ–­ç»“æœä¸­æå–æ¨èå‚æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if diagnosis_report and hasattr(self, 'context') and self.context.get("diagnosis_stats"):
                    stats_context = self.context.get("diagnosis_stats", {})
                    recommendations = stats_context.get("recommendations", {})
                    if recommendations:
                        # è½¬æ¢ä¸º MetabolomicsAgent æœŸæœ›çš„æ ¼å¼
                        recommendation = {
                            "params": {
                                "normalization": {
                                    "value": recommendations.get("normalization", {}).get("recommended", "log2")
                                },
                                "missing_threshold": {
                                    "value": "0.5"  # é»˜è®¤å€¼
                                },
                                "scale": {
                                    "value": True
                                },
                                "n_components": {
                                    "value": "10"
                                }
                            }
                        }
                    else:
                        recommendation = None
                else:
                    recommendation = None
                    
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error inspecting file: {e}", exc_info=True)
            stats = {"n_samples": "N/A", "n_features": "N/A"}
            diagnosis_report = "âš ï¸ æ–‡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼ã€‚"
        
        # ä½¿ç”¨ file_metadata ä½œä¸º inspection_result
        inspection_result = file_metadata
        
        # ä½¿ç”¨ LLM æå–ç›®æ ‡ç»“æŸæ­¥éª¤ï¼ˆä¾‹å¦‚ï¼š"åšåˆ°PCA" -> "pca_analysis"ï¼‰
        target_end_step = None
        try:
            logger.info(f"ğŸ” [CHECKPOINT] Extracting target end step from query...")
            target_end_step = await self._extract_target_end_step(query, inspection_result)
            logger.info(f"âœ… [CHECKPOINT] Target end step extracted: {target_end_step}")
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error extracting target end step: {e}", exc_info=True)
            target_end_step = None  # ä½¿ç”¨é»˜è®¤å€¼ï¼ˆæ‰€æœ‰æ­¥éª¤ï¼‰
        
        # ğŸ”¥ Task 1: ä½¿ç”¨æ¨èå€¼æˆ– LLM æå–å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨æ¨èå€¼ï¼‰
        extracted_params = {}
        if recommendation and "params" in recommendation:
            # ä¼˜å…ˆä½¿ç”¨æ¨èå€¼
            rec_params = recommendation["params"]
            extracted_params = {
                "normalization": rec_params.get("normalization", {}).get("value", "log2"),
                "missing_threshold": rec_params.get("missing_threshold", {}).get("value", "0.5"),
                "scale": str(rec_params.get("scale", {}).get("value", True)).lower(),
                "n_components": rec_params.get("n_components", {}).get("value", "10")
            }
            logger.info(f"âœ… [CHECKPOINT] Using recommended parameters: {extracted_params}")
        else:
            # å¦‚æœæ²¡æœ‰æ¨èï¼Œä½¿ç”¨ LLM æå–
            try:
                logger.info(f"ğŸ” [CHECKPOINT] Extracting workflow parameters with LLM...")
                extracted_params = await self._extract_workflow_params(query, file_paths, inspection_result, None)
                logger.info(f"âœ… [CHECKPOINT] Workflow parameters extracted: {list(extracted_params.keys())}")
            except Exception as e:
                logger.error(f"âŒ [CHECKPOINT] Error extracting workflow params: {e}", exc_info=True)
                extracted_params = {}  # ä½¿ç”¨é»˜è®¤å€¼
        
        # ğŸ”¥ ä¿®å¤ 2: å¯å‘å¼æ£€æµ‹åˆ†ç»„åˆ—ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if not extracted_params.get("group_column"):
            detected_group_col = self._detect_group_column_heuristic(file_metadata)
            if detected_group_col:
                extracted_params["group_column"] = detected_group_col
                logger.info(f"âœ… [Heuristic] è‡ªåŠ¨æ£€æµ‹åˆ°åˆ†ç»„åˆ—: {detected_group_col}")
            else:
                # å›é€€åˆ°é»˜è®¤å€¼ï¼ˆä½†è®°å½•è­¦å‘Šï¼‰
                extracted_params["group_column"] = "Group"  # é»˜è®¤å€¼
                logger.warning(f"âš ï¸ æœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼Œä½¿ç”¨é»˜è®¤å€¼: Group")
        
        # å®šä¹‰æ‰€æœ‰å¯ç”¨æ­¥éª¤ï¼ˆåŒ…å«å‹å¥½çš„ä¸­æ–‡åç§°ï¼‰
        all_steps = [
            {
                "step_id": "inspect_data",
                "tool_id": "inspect_data",
                "name": "æ•°æ®æ£€æŸ¥",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "æ•°æ®æ£€æŸ¥",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": f"æ£€æŸ¥æ•°æ®æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ï¼ˆæ ·æœ¬æ•°: {stats.get('n_samples', 'N/A')}, ä»£è°¢ç‰©æ•°: {stats.get('n_features', 'N/A')}ï¼‰",
                "params": {"file_path": current_file if file_paths else ""}
            },
            {
                "step_id": "preprocess_data",
                "tool_id": "preprocess_data",
                "name": "æ•°æ®é¢„å¤„ç†",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "æ•°æ®é¢„å¤„ç†",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "æ•°æ®é¢„å¤„ç†ï¼šå¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–ã€ç¼©æ”¾",
                "params": {
                    "file_path": current_file if file_paths else "",
                    "missing_threshold": extracted_params.get("missing_threshold", "0.5"),
                    "normalization": extracted_params.get("normalization", "log2"),
                    "scale": extracted_params.get("scale", "true")
                }
            },
            {
                "step_id": "pca_analysis",
                "tool_id": "pca_analysis",
                "name": "ä¸»æˆåˆ†åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "ä¸»æˆåˆ†åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "æ‰§è¡Œä¸»æˆåˆ†åˆ†æ (PCA)ï¼Œé™ç»´å¹¶æå–ä¸»è¦å˜å¼‚",
                "params": {
                    "n_components": extracted_params.get("n_components", "10")
                }
            },
            {
                "step_id": "differential_analysis",
                "tool_id": "differential_analysis",
                "name": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æï¼ˆä¸¤ç»„æ¯”è¾ƒï¼‰ï¼Œè¯†åˆ«æ˜¾è‘—å·®å¼‚çš„ä»£è°¢ç‰©",
                "params": {
                    "group_column": extracted_params.get("group_column", "Group"),  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨å¯å‘å¼æ£€æµ‹çš„å€¼
                    "method": extracted_params.get("method", "t-test"),
                    "p_value_threshold": extracted_params.get("p_value_threshold", "0.05"),
                    "fold_change_threshold": extracted_params.get("fold_change_threshold", "1.5"),
                    "group1": extracted_params.get("group1"),
                    "group2": extracted_params.get("group2")
                }
            },
            {
                "step_id": "visualize_pca",
                "tool_id": "visualize_pca",
                "name": "PCA å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "PCA å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "ç”Ÿæˆ PCA å¯è§†åŒ–å›¾ï¼Œå±•ç¤ºæ ·æœ¬åœ¨ä¸»æˆåˆ†ç©ºé—´çš„åˆ†å¸ƒ",
                "params": {
                    "group_column": extracted_params.get("group_column", "Group"),  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨å¯å‘å¼æ£€æµ‹çš„å€¼
                    "pc1": "1",
                    "pc2": "2"
                }
            },
            {
                "step_id": "visualize_volcano",
                "tool_id": "visualize_volcano",
                "name": "ç«å±±å›¾å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "ç«å±±å›¾å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "ç”Ÿæˆç«å±±å›¾ (Volcano Plot)ï¼Œå±•ç¤ºå·®å¼‚ä»£è°¢ç‰©çš„ç»Ÿè®¡æ˜¾è‘—æ€§",
                "params": {
                    "p_value_threshold": extracted_params.get("p_value_threshold", "0.05"),
                    "fold_change_threshold": extracted_params.get("fold_change_threshold", "1.5")
                }
            }
        ]
        
        # æ ¹æ®ç›®æ ‡ç»“æŸæ­¥éª¤è‡ªåŠ¨åŒ…å«æ‰€æœ‰å‰ç½®æ­¥éª¤
        if target_end_step and target_end_step in self.STEP_INDEX_MAP:
            # æ‰¾åˆ°ç›®æ ‡æ­¥éª¤çš„ç´¢å¼•
            target_index = self.STEP_INDEX_MAP[target_end_step]
            # åŒ…å«ä»å¼€å§‹åˆ°ç›®æ ‡æ­¥éª¤çš„æ‰€æœ‰æ­¥éª¤ï¼ˆåŒ…æ‹¬ç›®æ ‡æ­¥éª¤ï¼‰
            required_step_ids = self.STEPS_ORDER[:target_index + 1]
            logger.info(f"ğŸ¯ ç›®æ ‡æ­¥éª¤: {target_end_step}, å°†æ‰§è¡Œ: {required_step_ids}")
            
            # æ„å»ºæ­¥éª¤æ˜ å°„å¹¶ç­›é€‰
            step_map = {step["step_id"]: step for step in all_steps}
            selected_steps = [step_map[s] for s in required_step_ids if s in step_map]
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæˆ–æ— æ•ˆï¼Œä½¿ç”¨æ‰€æœ‰æ­¥éª¤
            selected_steps = all_steps
        
        # æ„å»ºå·¥ä½œæµé…ç½®
        workflow_config = {
            "workflow_name": "Metabolomics Analysis Pipeline",
            "steps": selected_steps
        }
        
        # ğŸ”¥ Task 1: æ„å»ºè¿”å›ç»“æœï¼ŒåŒ…å«æ¨èä¿¡æ¯å’Œè¯Šæ–­æŠ¥å‘Š
        result = {
            "type": "workflow_config",
            "workflow_data": workflow_config,
            "file_paths": file_paths
        }
        
        # æ·»åŠ è¯Šæ–­æŠ¥å‘Šï¼ˆå¦‚æœç”ŸæˆæˆåŠŸï¼‰
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥ diagnosis_report æ˜¯å¦ä¸ºæœ‰æ•ˆå­—ç¬¦ä¸²ï¼ˆé None ä¸”éç©ºï¼‰
        if diagnosis_report and isinstance(diagnosis_report, str) and diagnosis_report.strip():
            result["diagnosis_report"] = diagnosis_report
            logger.info(f"ğŸ“ [DEBUG] Adding diagnosis_report to result, length: {len(diagnosis_report)}")
        else:
            logger.warning(f"âš ï¸ [DEBUG] diagnosis_report is invalid (None/empty), NOT adding to result. Type: {type(diagnosis_report)}, Value: {diagnosis_report}")
        
        # æ·»åŠ æ¨èä¿¡æ¯ï¼ˆå¦‚æœç”ŸæˆæˆåŠŸï¼‰
        if recommendation:
            result["recommendation"] = recommendation
            # è‡ªåŠ¨å¡«å……æ¨èå€¼åˆ°æ­¥éª¤å‚æ•°
            self._apply_recommendations_to_steps(workflow_config["steps"], recommendation)
        
        logger.info("=" * 80)
        logger.info("âœ… [CHECKPOINT] _generate_workflow_config SUCCESS")
        logger.info(f"   Workflow name: {workflow_config.get('workflow_name')}")
        logger.info(f"   Steps count: {len(workflow_config.get('steps', []))}")
        logger.info(f"   Has recommendation: {recommendation is not None}")
        logger.info(f"   Has diagnosis_report: {diagnosis_report is not None}")
        
        # ğŸ”¥ DEBUG: æ‰“å°æœ€ç»ˆè¿”å›ç»“æ„
        logger.info(f"ğŸ“¤ [DEBUG] MetabolomicsAgent returning result with keys: {list(result.keys())}")
        logger.info(f"ğŸ“¤ [DEBUG] MetabolomicsAgent has diagnosis_report: {'diagnosis_report' in result}")
        if 'diagnosis_report' in result:
            logger.info(f"ğŸ“¤ [DEBUG] MetabolomicsAgent diagnosis_report length: {len(result['diagnosis_report'])}")
        logger.info("=" * 80)
        
        return result
    
    def _detect_group_column_heuristic(self, file_metadata: Dict[str, Any]) -> Optional[str]:
        """
        å¯å‘å¼æ£€æµ‹åˆ†ç»„åˆ—
        
        ğŸ”¥ ä¿®å¤ 2: å·¥å…·å¥å£®æ€§ - è‡ªåŠ¨æ£€æµ‹åˆ†ç»„åˆ—ï¼Œé¿å…ç¡¬ç¼–ç  "Group"
        
        Args:
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®
        
        Returns:
            æ£€æµ‹åˆ°çš„åˆ†ç»„åˆ—åï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
        """
        # ä¼˜å…ˆçº§å…³é”®è¯åˆ—è¡¨
        priority_keywords = ['Diet', 'diet', 'Group', 'group', 'Condition', 'condition', 
                            'Treatment', 'treatment', 'Class', 'class', 'Category', 'category',
                            'Type', 'type', 'Label', 'label', 'Status', 'status']
        
        # ä» file_metadata è·å–åˆ—ä¿¡æ¯
        columns = file_metadata.get("columns", [])
        if not columns:
            logger.warning("âš ï¸ æ— æ³•è·å–åˆ—ä¿¡æ¯ï¼Œæ— æ³•æ£€æµ‹åˆ†ç»„åˆ—")
            return None
        
        # æ–¹æ³•1: æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«ä¼˜å…ˆçº§å…³é”®è¯
        for col in columns:
            if any(keyword in col for keyword in priority_keywords):
                logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰: {col}")
                return col
        
        # æ–¹æ³•2: æ£€æŸ¥ potential_groupsï¼ˆFileInspector å¯èƒ½å·²ç»æ£€æµ‹åˆ°ï¼‰
        potential_groups = file_metadata.get("potential_groups", {})
        if isinstance(potential_groups, dict) and len(potential_groups) > 0:
            # è¿”å›ç¬¬ä¸€ä¸ªæ½œåœ¨åˆ†ç»„åˆ—
            first_group_col = list(potential_groups.keys())[0]
            logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆpotential_groupsï¼‰: {first_group_col}")
            return first_group_col
        
        # æ–¹æ³•3: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªéæ•°å€¼çš„åˆ†ç±»åˆ—ï¼ˆå”¯ä¸€å€¼æ•°é‡ < æ ·æœ¬æ•°çš„50%ï¼‰
        try:
            import pandas as pd
            head_data = file_metadata.get("head", {})
            if head_data and isinstance(head_data, dict) and "json" in head_data:
                df_preview = pd.DataFrame(head_data["json"])
                n_samples = len(df_preview)
                
                for col in columns:
                    if col in df_preview.columns:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
                        if not pd.api.types.is_numeric_dtype(df_preview[col]):
                            # æ£€æŸ¥å”¯ä¸€å€¼æ•°é‡
                            unique_count = df_preview[col].nunique()
                            if 2 <= unique_count <= max(2, n_samples * 0.5):
                                logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆåˆ†ç±»åˆ—ï¼‰: {col} (å”¯ä¸€å€¼: {unique_count})")
                                return col
        except Exception as e:
            logger.warning(f"âš ï¸ å¯å‘å¼æ£€æµ‹å¤±è´¥: {e}")
        
        logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—")
        return None
    
    # ğŸ”¥ å·²ç§»é™¤ï¼š_generate_diagnosis_and_recommendation æ–¹æ³•
    # ç°åœ¨ä½¿ç”¨ BaseAgent._perform_data_diagnosis() ç»Ÿä¸€æ–¹æ³•
    
    async def _extract_target_end_step(
        self,
        query: str,
        inspection_result: Dict[str, Any] = None
    ) -> Optional[str]:
        """
        ä½¿ç”¨ LLM ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–ç›®æ ‡ç»“æŸæ­¥éª¤
        
        æ”¯æŒï¼š
        - "åšåˆ°PCA" / "up to PCA" -> "pca_analysis" (ä¼šè‡ªåŠ¨åŒ…å« inspect_data, preprocess_data)
        - "åªåšé¢„å¤„ç†" -> "preprocess_data" (ä¼šè‡ªåŠ¨åŒ…å« inspect_data)
        - "åšåˆ°å·®å¼‚åˆ†æ" -> "differential_analysis" (ä¼šè‡ªåŠ¨åŒ…å«æ‰€æœ‰å‰ç½®æ­¥éª¤)
        - é»˜è®¤è¿”å› Noneï¼ˆæ‰§è¡Œæ‰€æœ‰æ­¥éª¤ï¼‰
        
        Returns:
            ç›®æ ‡ç»“æŸæ­¥éª¤çš„ step_idï¼Œæˆ– Noneï¼ˆæ‰§è¡Œæ‰€æœ‰æ­¥éª¤ï¼‰
        """
        # å®šä¹‰æ­¥éª¤å…³é”®è¯æ˜ å°„ï¼ˆç”¨äºåŒ¹é…"åšåˆ°XXX"ï¼‰
        step_keywords = {
            "inspect_data": ["æ£€æŸ¥", "inspect", "æ£€æŸ¥æ•°æ®", "æ•°æ®æ£€æŸ¥", "æ­¥éª¤1", "step 1", "ç¬¬ä¸€æ­¥", "åˆ°æ£€æŸ¥"],
            "preprocess_data": ["é¢„å¤„ç†", "preprocess", "æ•°æ®é¢„å¤„ç†", "æ­¥éª¤2", "step 2", "ç¬¬äºŒæ­¥", "åˆ°é¢„å¤„ç†"],
            "pca_analysis": ["pca", "ä¸»æˆåˆ†", "ä¸»æˆåˆ†åˆ†æ", "æ­¥éª¤3", "step 3", "ç¬¬ä¸‰æ­¥", "åšåˆ°pca", "åˆ°pca", "up to pca"],
            "differential_analysis": ["å·®å¼‚", "differential", "å·®å¼‚åˆ†æ", "æ­¥éª¤4", "step 4", "ç¬¬å››æ­¥", "åˆ°å·®å¼‚åˆ†æ"],
            "visualize_pca": ["pcaå›¾", "pcaå¯è§†åŒ–", "pca plot", "æ­¥éª¤5", "step 5", "ç¬¬äº”æ­¥", "åˆ°pcaå›¾"],
            "visualize_volcano": ["ç«å±±å›¾", "volcano", "volcano plot", "æ­¥éª¤6", "step 6", "ç¬¬å…­æ­¥", "åˆ°ç«å±±å›¾"]
        }
        
        # å…ˆè¿›è¡Œå…³é”®è¯åŒ¹é…ï¼ˆä¼˜å…ˆåŒ¹é…"åšåˆ°"ã€"up to"ç­‰è¡¨è¾¾ï¼‰
        query_lower = query.lower()
        
        # æ£€æŸ¥"åšåˆ°"ã€"up to"ç­‰è¡¨è¾¾
        for step_id, keywords in step_keywords.items():
            for kw in keywords:
                if kw in query_lower:
                    # ç‰¹åˆ«æ£€æŸ¥"åšåˆ°"ã€"up to"ç­‰è¡¨è¾¾
                    if any(phrase in query_lower for phrase in ["åšåˆ°", "up to", "until", "åˆ°"]):
                        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ç›®æ ‡ç»“æŸæ­¥éª¤: {step_id} (å…³é”®è¯: {kw})")
                        return step_id
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°"åšåˆ°"è¡¨è¾¾ï¼Œä½¿ç”¨ LLM æå–
        prompt = f"""
Extract the TARGET END STEP from the user query. This is the LAST step the user wants to run.

User Query: {query}

Available steps (in order):
1. inspect_data - Check data file
2. preprocess_data - Preprocess data
3. pca_analysis - PCA analysis
4. differential_analysis - Differential analysis
5. visualize_pca - PCA visualization
6. visualize_volcano - Volcano plot visualization

Important: If the user says "do analysis up to PCA" or "åšåˆ°PCA", they want to run steps 1, 2, and 3 (all prerequisites + PCA).
If they say "only preprocessing", they want steps 1 and 2.

Examples:
- "åšåˆ°PCA" / "up to PCA" -> "pca_analysis"
- "åªåšé¢„å¤„ç†" -> "preprocess_data"
- "åšåˆ°å·®å¼‚åˆ†æ" -> "differential_analysis"
- "åšå®Œæ•´åˆ†æ" / "å…¨éƒ¨" -> null (run all steps)

Return JSON only (single step_id string or null):
"""
        
        messages = [
            {"role": "system", "content": "You are a step extraction assistant. Return JSON only (single string or null)."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            logger.info(f"ğŸ” [CHECKPOINT] Calling LLM to extract target end step...")
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=64)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            logger.info(f"âœ… [CHECKPOINT] LLM response received: {response[:100]}...")
            
            import json
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0].strip()
            
            result = json.loads(json_str)
            if result is None or result == "":
                logger.info(f"âœ… [CHECKPOINT] LLM returned null, will run all steps")
                return None
            # éªŒè¯ç»“æœæ˜¯å¦åœ¨æœ‰æ•ˆæ­¥éª¤åˆ—è¡¨ä¸­
            if isinstance(result, str) and result in self.STEPS_ORDER:
                logger.info(f"ğŸ¯ LLM æå–çš„ç›®æ ‡ç»“æŸæ­¥éª¤: {result}")
                return result
            logger.warning(f"âš ï¸ LLM returned invalid step: {result}, will run all steps")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [CHECKPOINT] JSON decode error: {e}, response: {response[:200]}")
            return None  # é»˜è®¤æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error extracting target end step: {e}", exc_info=True)
            return None  # é»˜è®¤æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
    
    async def _extract_workflow_params(
        self,
        query: str,
        file_paths: List[str],
        inspection_result: Dict[str, Any] = None,
        diagnosis_report: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æå–å·¥ä½œæµå‚æ•°
        
        åŸºäºæ£€æŸ¥ç»“æœå’Œè¯Šæ–­æŠ¥å‘Šæ™ºèƒ½æ¨èå‚æ•°
        """
        # æ„å»ºåŒ…å«æ£€æŸ¥ç»“æœå’Œè¯Šæ–­æŠ¥å‘Šçš„æç¤º
        inspection_info = ""
        if diagnosis_report:
            inspection_info = f"""
ã€Data Diagnosis & Recommendationsã€‘
{diagnosis_report}

"""
        elif inspection_result and "error" not in inspection_result:
            inspection_info = f"""
ã€Data Diagnosis & Recommendationsã€‘
{diagnosis_report}

"""
        elif inspection_result and "error" not in inspection_result:
            inspection_info = f"""
ã€Data Inspection Resultsã€‘
- Number of samples: {inspection_result.get('n_samples', 'N/A')}
- Number of metabolites: {inspection_result.get('n_metabolites', 'N/A')}
- Missing values: {inspection_result.get('missing_values', {}).get('percentage', 'N/A')}%
- Group column: {inspection_result.get('group_info', {}).get('column', 'N/A')}
- Groups: {inspection_result.get('group_info', {}).get('groups', {})}
"""
        
        prompt = f"""
Extract workflow parameters for metabolomics analysis from the user query.

User Query: {query}
File Paths: {file_paths}

{inspection_info}

Based on the inspection results and user query, extract the following parameters:
- missing_threshold: Threshold for removing metabolites with high missing values (default: 0.5)
- normalization: Normalization method - "log2", "zscore", or "none" (default: "log2")
- scale: Whether to apply StandardScaler (default: true)
- n_components: Number of PCA components (default: 10)
- group_column: Column name for group comparison (default: "Muscle loss" or first metadata column)
- method: Statistical method for differential analysis - "t-test" or "mann-whitney" (default: "t-test")
- p_value_threshold: P-value threshold for significance (default: 0.05)
- fold_change_threshold: Fold change threshold (default: 1.5)

Return JSON only:
{{"missing_threshold": "0.5", "normalization": "log2", "scale": "true", "n_components": "10", "group_column": "Muscle loss", "method": "t-test", "p_value_threshold": "0.05", "fold_change_threshold": "1.5"}}
"""
        
        messages = [
            {"role": "system", "content": "You are a parameter extraction assistant. Return JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            logger.info(f"ğŸ” [CHECKPOINT] Calling LLM to extract workflow parameters...")
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=256)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            logger.info(f"âœ… [CHECKPOINT] LLM response received: {response[:200]}...")
            
            # è§£æ JSON
            import json
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            
            params = json.loads(json_str)
            logger.info(f"âœ… [CHECKPOINT] Parameters extracted: {list(params.keys())}")
            return params
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [CHECKPOINT] JSON decode error: {e}, response: {response[:200]}")
            return {}  # è¿”å›ç©ºå­—å…¸ï¼Œä½¿ç”¨é»˜è®¤å€¼
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error extracting parameters: {e}", exc_info=True)
            return {}  # è¿”å›ç©ºå­—å…¸ï¼Œä½¿ç”¨é»˜è®¤å€¼
    
    async def _peek_data_lightweight(self, file_path: str) -> Dict[str, Any]:
        """
        è½»é‡çº§æ•°æ®é¢„è§ˆï¼ˆä½¿ç”¨ FileInspectorï¼‰
        
        ğŸ”§ å‡çº§ï¼šå§”æ‰˜ç»™ FileInspectorï¼Œè·å¾—å‡†ç¡®çš„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«åŸºæœ¬ä¿¡æ¯çš„å­—å…¸ï¼ˆæ ·æœ¬æ•°ã€åˆ—æ•°ã€æ•°å€¼èŒƒå›´ç­‰ï¼‰
        """
        try:
            # ğŸ”§ ä½¿ç”¨ FileInspectorï¼ˆUniversal Eyesï¼‰
            from ...core.file_inspector import FileInspector
            upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
            inspector = FileInspector(upload_dir)
            
            # ä½¿ç”¨é€šç”¨æ£€æŸ¥å™¨
            result = inspector.inspect_file(file_path)
            
            if result.get("status") == "success" and result.get("file_type") == "tabular":
                # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
                summary = result.get("data", {}).get("summary", {})
                data_range = result.get("data_range", {})
                
                # æ„å»ºå…¼å®¹æ ¼å¼
                peek_result = {
                    "n_samples": summary.get("n_samples", "N/A"),
                    "n_metabolites": summary.get("n_features", 0),
                    "n_metadata_cols": result.get("n_metadata_cols", 0),
                    "metadata_columns": result.get("metadata_columns", []),
                    "data_range": data_range,  # ğŸ”§ æ·»åŠ æ•°æ®èŒƒå›´ï¼ˆç”¨äº Log2 åˆ¤æ–­ï¼‰
                    "missing_rate": summary.get("missing_rate", 0),
                    "numeric_stats": {
                        "min": data_range.get("min", 0),
                        "max": data_range.get("max", 0),
                        "mean": data_range.get("mean", 0),
                        "median": data_range.get("median", 0),
                        "has_large_values": data_range.get("max", 0) > 1000 if isinstance(data_range.get("max"), (int, float)) else False,
                        "has_negative": data_range.get("min", 0) < 0 if isinstance(data_range.get("min"), (int, float)) else False
                    },
                    "is_sampled": summary.get("is_sampled", False),
                    "file_path": file_path
                }
                
                return peek_result
            else:
                # æ£€æŸ¥å¤±è´¥
                logger.warning(f"âš ï¸ File inspection failed: {result.get('error', 'Unknown error')}")
                return {
                    "error": result.get("error", "File inspection failed"),
                    "n_samples": "N/A",
                    "n_metabolites": 0
                }
                
        except Exception as e:
            logger.error(f"âŒ Error in _peek_data_lightweight: {e}", exc_info=True)
            return {
                "error": str(e),
                "n_samples": "N/A",
                "n_metabolites": 0
            }
    
    async def _generate_parameter_recommendations(
        self,
        peek_result: Dict[str, Any],
        query: str
    ) -> Dict[str, Any]:
        """
        åŸºäºè½»é‡çº§é¢„è§ˆç”Ÿæˆå‚æ•°æ¨è
        
        Args:
            peek_result: è½»é‡çº§é¢„è§ˆç»“æœ
            query: ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            æ¨èå­—å…¸ï¼ŒåŒ…å« summary å’Œ params
        """
        import json
        
        try:
            # æ„å»ºé¢„è§ˆæ‘˜è¦
            preview_summary = f"""
æ•°æ®é¢„è§ˆç»“æœï¼š
- æ ·æœ¬æ•°: {peek_result.get('n_samples', 'N/A')}
- ä»£è°¢ç‰©æ•°: {peek_result.get('n_metabolites', 'N/A')}
- å…ƒæ•°æ®åˆ—æ•°: {peek_result.get('n_metadata_cols', 'N/A')}
- å…ƒæ•°æ®åˆ—: {', '.join(peek_result.get('metadata_columns', []))}
- æ•°å€¼èŒƒå›´: {peek_result.get('numeric_stats', {})}
"""
            
            # ğŸ”§ å‡çº§ï¼šåªä¼ é€’ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ä¼ é€’åŸå§‹æ•°æ®è¡Œ
            summary = peek_result.get("data", {}).get("summary", {})
            data_range = peek_result.get("data_range", {})
            
            # ğŸ”¥ Step 1: ä½¿ç”¨æ–°çš„å…ƒæ•°æ®å­—æ®µï¼ˆhead, columns, separatorï¼‰
            columns = peek_result.get('columns', [])
            head_data = peek_result.get('head', {})
            separator = peek_result.get('separator', ',')
            file_path = peek_result.get('file_path', 'N/A')
            
            # æ„å»ºåˆ—ä¿¡æ¯æ‘˜è¦
            columns_summary = ""
            if columns:
                metadata_cols = peek_result.get('metadata_columns', [])
                feature_cols = [col for col in columns if col not in metadata_cols]
                columns_summary = f"""
- æ€»åˆ—æ•°: {len(columns)}
- å…ƒæ•°æ®åˆ— ({len(metadata_cols)}): {', '.join(metadata_cols[:5])}{'...' if len(metadata_cols) > 5 else ''}
- ç‰¹å¾åˆ— ({len(feature_cols)}): {', '.join(feature_cols[:10])}{'...' if len(feature_cols) > 10 else ''}
"""
            
            # æ„å»ºæ•°æ®é¢„è§ˆæ‘˜è¦ï¼ˆä½¿ç”¨ headï¼‰
            head_summary = ""
            if head_data:
                head_markdown = head_data.get('markdown', '')
                if head_markdown:
                    # åªæ˜¾ç¤ºå‰3è¡Œï¼Œé¿å… prompt è¿‡é•¿
                    head_lines = head_markdown.split('\n')[:4]  # è¡¨å¤´ + å‰3è¡Œæ•°æ®
                    head_summary = f"""
- æ•°æ®é¢„è§ˆï¼ˆå‰3è¡Œï¼‰:
{chr(10).join(head_lines)}
"""
            
            stats_summary = f"""
æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŸºäºå®Œæ•´æ–‡ä»¶æˆ–å¤§æ–‡ä»¶é‡‡æ ·ï¼‰ï¼š
- æ–‡ä»¶è·¯å¾„: {file_path}
- åˆ†éš”ç¬¦: {separator}
- æ ·æœ¬æ•°: {summary.get('n_samples', 'N/A')}
- ç‰¹å¾æ•°: {summary.get('n_features', 'N/A')}
- ç¼ºå¤±ç‡: {summary.get('missing_rate', 0):.2f}%
- æ•°æ®èŒƒå›´:
  * æœ€å°å€¼: {data_range.get('min', 'N/A')}
  * æœ€å¤§å€¼: {data_range.get('max', 'N/A')}
  * å¹³å‡å€¼: {data_range.get('mean', 'N/A'):.2f if isinstance(data_range.get('mean'), (int, float)) else 'N/A'}
  * ä¸­ä½æ•°: {data_range.get('median', 'N/A'):.2f if isinstance(data_range.get('median'), (int, float)) else 'N/A'}
- æ˜¯å¦é‡‡æ ·: {summary.get('is_sampled', False)}
{columns_summary}{head_summary}
"""
            
            prompt = f"""åŸºäºæ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼Œç”Ÿæˆå‚æ•°æ¨èã€‚

ç”¨æˆ·æŸ¥è¯¢: {query}

{stats_summary}

è¯·åˆ†ææ•°æ®ç‰¹å¾å¹¶æ¨èåˆé€‚çš„å‚æ•°ã€‚è¿”å› JSON æ ¼å¼ï¼š
{{
    "summary": "æ•°æ®ç‰¹å¾æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰",
    "params": {{
        "normalization": {{"value": "log2" | "zscore" | "none", "reason": "æ¨èç†ç”±"}},
        "missing_threshold": {{"value": "0.5", "reason": "æ¨èç†ç”±"}},
        "scale": {{"value": true | false, "reason": "æ¨èç†ç”±"}},
        "n_components": {{"value": "10", "reason": "æ¨èç†ç”±"}}
    }}
}}

é‡è¦åˆ¤æ–­è§„åˆ™ï¼š
- **Log2 å˜æ¢åˆ¤æ–­**ï¼šå¦‚æœæœ€å¤§å€¼ > 1000 ä¸”æœ€å°å€¼ >= 0ï¼Œæ¨è "log2"ï¼ˆæ•°æ®è·¨åº¦å¤§ï¼Œéœ€è¦å¯¹æ•°å˜æ¢ï¼‰
- **Z-score æ ‡å‡†åŒ–**ï¼šå¦‚æœæ•°æ®å·²æ ‡å‡†åŒ–ï¼ˆå‡å€¼æ¥è¿‘0ï¼Œæ ‡å‡†å·®æ¥è¿‘1ï¼‰æˆ–åŒ…å«è´Ÿå€¼ï¼Œæ¨è "zscore"
- **ç¼ºå¤±å€¼é˜ˆå€¼**ï¼šæ ¹æ®ç¼ºå¤±ç‡æ¨èï¼Œå¦‚æœç¼ºå¤±ç‡ > 50%ï¼Œæ¨èæ›´é«˜çš„é˜ˆå€¼ï¼ˆå¦‚ 0.7ï¼‰
- **PCA ä¸»æˆåˆ†æ•°**ï¼šæ ¹æ®æ ·æœ¬æ•°æ¨èï¼Œé€šå¸¸ä¸º min(10, æ ·æœ¬æ•°/2)
- **ç¼©æ”¾ï¼ˆScaleï¼‰**ï¼šå¦‚æœæ•°æ®èŒƒå›´å·®å¼‚å¤§ï¼Œæ¨è true

æ³¨æ„ï¼šåªåŸºäºç»Ÿè®¡ä¿¡æ¯ï¼ˆæœ€å¤§å€¼ã€æœ€å°å€¼ã€ç¼ºå¤±ç‡ç­‰ï¼‰è¿›è¡Œæ¨èï¼Œä¸æŸ¥çœ‹åŸå§‹æ•°æ®è¡Œã€‚
"""
            
            messages = [
                {"role": "system", "content": "You are a bioinformatics expert. Return JSON only."},
                {"role": "user", "content": prompt}
            ]
            
            completion = await self.llm_client.achat(messages, temperature=0.2, max_tokens=800)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            # è§£æ JSON
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0].strip()
            
            recommendation = json.loads(json_str)
            
            # éªŒè¯å’Œè¡¥å……æ¨è
            if "summary" not in recommendation:
                recommendation["summary"] = f"æ£€æµ‹åˆ°æ•°æ®åŒ…å« {peek_result.get('n_samples', 'N/A')} ä¸ªæ ·æœ¬ï¼Œ{peek_result.get('n_metabolites', 'N/A')} ä¸ªä»£è°¢ç‰©ã€‚"
            
            if "params" not in recommendation:
                recommendation["params"] = {}
            
            # ç¡®ä¿å…³é”®å‚æ•°å­˜åœ¨
            numeric_stats = peek_result.get("numeric_stats", {})
            if "normalization" not in recommendation["params"]:
                if numeric_stats.get("has_large_values", False) and not numeric_stats.get("has_negative", False):
                    recommendation["params"]["normalization"] = {"value": "log2", "reason": "æ•°å€¼è·¨åº¦å¤§ï¼Œå»ºè®® Log å˜æ¢ä»¥ç¬¦åˆæ­£æ€åˆ†å¸ƒ"}
                else:
                    recommendation["params"]["normalization"] = {"value": "zscore", "reason": "æ ‡å‡† Z-score æ ‡å‡†åŒ–"}
            
            if "missing_threshold" not in recommendation["params"]:
                recommendation["params"]["missing_threshold"] = {"value": "0.5", "reason": "æ ‡å‡†è´¨æ§é˜ˆå€¼"}
            
            if "scale" not in recommendation["params"]:
                recommendation["params"]["scale"] = {"value": True, "reason": "æ ‡å‡†åŒ–æœ‰åŠ©äºåç»­åˆ†æ"}
            
            if "n_components" not in recommendation["params"]:
                n_samples = peek_result.get("n_samples", 100)
                n_comp = min(10, max(2, n_samples // 2))
                recommendation["params"]["n_components"] = {"value": str(n_comp), "reason": f"æ ¹æ®æ ·æœ¬æ•° ({n_samples}) æ¨è"}
            
            return recommendation
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¨èå¤±è´¥: {e}", exc_info=True)
            # è¿”å›é»˜è®¤æ¨è
            return {
                "summary": f"æ£€æµ‹åˆ°æ•°æ®åŒ…å« {peek_result.get('n_samples', 'N/A')} ä¸ªæ ·æœ¬ã€‚",
                "params": {
                    "normalization": {"value": "log2", "reason": "é»˜è®¤æ¨è"},
                    "missing_threshold": {"value": "0.5", "reason": "æ ‡å‡†è´¨æ§é˜ˆå€¼"},
                    "scale": {"value": True, "reason": "æ ‡å‡†åŒ–æœ‰åŠ©äºåç»­åˆ†æ"},
                    "n_components": {"value": "10", "reason": "é»˜è®¤å€¼"}
                }
            }
    
    def _apply_recommendations_to_steps(
        self,
        steps: List[Dict[str, Any]],
        recommendation: Dict[str, Any]
    ):
        """
        å°†æ¨èå€¼è‡ªåŠ¨å¡«å……åˆ°æ­¥éª¤å‚æ•°ä¸­
        
        Args:
            steps: å·¥ä½œæµæ­¥éª¤åˆ—è¡¨
            recommendation: æ¨èå­—å…¸
        """
        if not recommendation or "params" not in recommendation:
            return
        
        rec_params = recommendation["params"]
        
        for step in steps:
            if step.get("step_id") == "preprocess_data":
                # å¡«å……é¢„å¤„ç†å‚æ•°
                if "normalization" in rec_params:
                    step["params"]["normalization"] = rec_params["normalization"]["value"]
                if "missing_threshold" in rec_params:
                    step["params"]["missing_threshold"] = rec_params["missing_threshold"]["value"]
                if "scale" in rec_params:
                    step["params"]["scale"] = str(rec_params["scale"]["value"]).lower()
            
            elif step.get("step_id") == "pca_analysis":
                # å¡«å…… PCA å‚æ•°
                if "n_components" in rec_params:
                    step["params"]["n_components"] = rec_params["n_components"]["value"]
    
    async def _generate_final_diagnosis(
        self,
        steps_details: List[Dict[str, Any]],
        workflow_config: Dict[str, Any]
    ) -> Optional[str]:
        """
        åŸºäºå·¥ä½œæµæ‰§è¡Œç»“æœç”Ÿæˆæœ€ç»ˆè¯Šæ–­æŠ¥å‘Š
        
        Args:
            steps_details: æ­¥éª¤æ‰§è¡Œè¯¦æƒ…åˆ—è¡¨
            workflow_config: å·¥ä½œæµé…ç½®
        
        Returns:
            Markdown æ ¼å¼çš„è¯Šæ–­æŠ¥å‘Šï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        import json
        
        try:
            # æå–å…³é”®ç»“æœ
            inspection_result = None
            differential_result = None
            pca_result = None
            
            for step_detail in steps_details:
                tool_id = step_detail.get("tool_id")
                step_result = step_detail.get("step_result", {})
                
                if tool_id == "inspect_data":
                    inspection_result = step_result.get("data", {})
                elif tool_id == "differential_analysis":
                    differential_result = step_result.get("data", {})
                elif tool_id == "pca_analysis":
                    pca_result = step_result.get("data", {})
            
            # ğŸ”§ ä¿®å¤ï¼šæ„å»ºç»“æœæ‘˜è¦ï¼ˆä¿®å¤å­—æ®µåä¸åŒ¹é…é—®é¢˜ï¼‰
            # å·®å¼‚åˆ†æï¼šå·¥å…·è¿”å› n_significant å’Œ n_totalï¼Œä¸æ˜¯ significant_count å’Œ total_count
            # PCAï¼šå·¥å…·è¿”å› explained_variance åœ¨é¡¶å±‚ï¼Œä¸åœ¨ data.summary ä¸­
            results_summary = {
                "workflow_name": workflow_config.get("workflow_name", "Metabolomics Analysis"),
                "steps_completed": len(steps_details),
                "inspection": inspection_result.get("summary", {}) if inspection_result else None,
                "differential_analysis": {
                    "significant_metabolites": differential_result.get("summary", {}).get("n_significant", "N/A") if differential_result else "N/A",
                    "total_metabolites": differential_result.get("summary", {}).get("n_total", "N/A") if differential_result else "N/A"
                } if differential_result else None,
                "pca": {
                    # ğŸ”§ ä¿®å¤ï¼šPCA ç»“æœåœ¨ step_result.data ä¸­ï¼Œä½† explained_variance åœ¨é¡¶å±‚çš„ result ä¸­
                    # éœ€è¦ä»æ­¥éª¤è¯¦æƒ…ä¸­è·å–å®Œæ•´çš„ result
                    "variance_explained": self._extract_pca_variance_explained(steps_details) if pca_result else "N/A"
                } if pca_result else None
            }
            
            # æ ¼å¼åŒ–ç»“æœæ‘˜è¦
            summary_json = json.dumps(results_summary, ensure_ascii=False, indent=2)
            
            # ğŸ”¥ ä¿®å¤ï¼šä¸¥æ ¼çš„æ•°æ®é©±åŠ¨è¯Šæ–­ promptï¼ˆé˜²æ­¢å¹»è§‰ï¼‰
            prompt = f"""You are a strict Data Analyst. Generate a concise diagnosis report based ONLY on the execution results below.

æ‰§è¡Œç»“æœæ‘˜è¦ï¼š
{summary_json}

**CRITICAL RULES:**

1. **Fact-Check First**: Look at `differential_analysis.significant_metabolites`.
   - If it is 0 or "N/A", state clearly: "æœ¬æ¬¡åˆ†ææœªå‘ç°æ˜¾è‘—å·®å¼‚ä»£è°¢ç‰©ã€‚"
   - DO NOT invent hypotheses or excuses (like "technical noise", "metabolic homeostasis", "biological similarity") unless there is explicit evidence in the QC metrics.
   - DO NOT write long essays about why there might be no differences.

2. **Interpret PCA**: Look at `pca.variance_explained`.
   - If PC1 is very high (>50%), mention it might indicate a strong batch effect or dominant biological factor.
   - If PC1 is low (<20%), mention the data might be highly heterogeneous.

3. **Concise Conclusion**: Keep it short (3-5 sentences max). Do not write a thesis.
   - Focus on what the data shows, not what it might mean theoretically.

4. **Actionable Advice**: If 0 differences found, suggest:
   - "å°è¯•æ”¾å®½ P å€¼é˜ˆå€¼ï¼ˆå¦‚ 0.1ï¼‰"
   - "æ£€æŸ¥åˆ†ç»„æ ‡ç­¾æ˜¯å¦æ­£ç¡®"
   - "è€ƒè™‘å¢åŠ æ ·æœ¬é‡"
   - DO NOT suggest complex biological interpretations without evidence.

**Output Format:**
- Use Simplified Chinese (ç®€ä½“ä¸­æ–‡)
- Use Markdown format
- Be direct and factual
- Maximum 200 words

**Example of Good Output (when n_significant = 0):**
"æœ¬æ¬¡åˆ†ææœªå‘ç°æ˜¾è‘—å·®å¼‚ä»£è°¢ç‰©ï¼ˆFDR < 0.05, |Log2FC| > 1ï¼‰ã€‚å»ºè®®ï¼š1) å°è¯•æ”¾å®½ P å€¼é˜ˆå€¼è‡³ 0.1ï¼›2) æ£€æŸ¥åˆ†ç»„æ ‡ç­¾æ˜¯å¦æ­£ç¡®ï¼›3) è€ƒè™‘å¢åŠ æ ·æœ¬é‡ä»¥æé«˜ç»Ÿè®¡åŠŸæ•ˆã€‚"

**Example of Bad Output (DO NOT DO THIS):**
"è™½ç„¶æœªå‘ç°æ˜¾è‘—å·®å¼‚ï¼Œä½†è¿™å¯èƒ½åæ˜ äº†ä»£è°¢ç¨³æ€çš„ç»´æŒæœºåˆ¶ï¼Œè¡¨æ˜ä¸¤ç»„æ ·æœ¬åœ¨ä»£è°¢æ°´å¹³ä¸Šä¿æŒäº†é«˜åº¦çš„ç”Ÿç‰©å­¦ç›¸ä¼¼æ€§..." (This is speculation without evidence!)

ç°åœ¨ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼š"""
            
            messages = [
                {"role": "system", "content": "You are a strict Data Analyst. You must base your diagnosis ONLY on the provided data. Do not invent hypotheses or write speculative essays. Be concise and factual. Use Simplified Chinese."},
                {"role": "user", "content": prompt}
            ]
            
            # ğŸ”¥ ä¿®å¤ï¼šé™ä½ max_tokens ä»¥åŒ¹é…ç®€æ´æ€§è¦æ±‚ï¼ˆæœ€å¤š 200 å­—ï¼‰
            completion = await self.llm_client.achat(messages, temperature=0.2, max_tokens=500)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            logger.info(f"ğŸ“ Generating diagnosis... Result length: {len(response)}")
            logger.info("ğŸ“ Diagnosis generated successfully.")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæœ€ç»ˆè¯Šæ–­å¤±è´¥: {e}", exc_info=True)
            return None
    
    def _extract_pca_variance_explained(self, steps_details: List[Dict[str, Any]]) -> str:
        """
        ä»æ­¥éª¤è¯¦æƒ…ä¸­æå– PCA è§£é‡Šæ–¹å·®
        
        Args:
            steps_details: æ­¥éª¤è¯¦æƒ…åˆ—è¡¨
        
        Returns:
            è§£é‡Šæ–¹å·®å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "PC1: 45.23%, PC2: 12.56%"
        """
        for step_detail in steps_details:
            if step_detail.get("tool_id") == "pca_analysis":
                step_result = step_detail.get("step_result", {})
                # ä¼˜å…ˆä» _full_result ä¸­è·å–
                full_result = step_result.get("_full_result", {})
                if full_result and "explained_variance" in full_result:
                    pc1_var = full_result["explained_variance"].get("PC1", 0) * 100
                    pc2_var = full_result["explained_variance"].get("PC2", 0) * 100
                    return f"PC1: {pc1_var:.2f}%, PC2: {pc2_var:.2f}%"
                # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä» data.tables.variance_table ä¸­æå–
                elif step_result.get("data", {}).get("tables", {}).get("variance_table"):
                    variance_table = step_result["data"]["tables"]["variance_table"]
                    if variance_table and len(variance_table) > 0:
                        pc1_var = variance_table[0].get("è§£é‡Šæ–¹å·®", variance_table[0].get("Explained Variance", "N/A"))
                        pc2_var = variance_table[1].get("è§£é‡Šæ–¹å·®", variance_table[1].get("Explained Variance", "N/A")) if len(variance_table) > 1 else "N/A"
                        return f"PC1: {pc1_var}, PC2: {pc2_var}"
        return "N/A"
    
    async def _stream_chat_response(
        self,
        query: str,
        file_paths: List[str]
    ) -> AsyncIterator[str]:
        """
        æµå¼èŠå¤©å“åº”ï¼ˆæ”¯æŒ ReAct å¾ªç¯å’Œå·¥å…·è°ƒç”¨ï¼‰
        """
        context = {
            "context": f"Uploaded files: {', '.join(file_paths) if file_paths else 'None'}",
            "available_tools": list(self.metabolomics_tool.tool_map.keys()),
            "tool_descriptions": {
                "inspect_data": "æ£€æŸ¥ä»£è°¢ç»„å­¦æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ‘˜è¦ï¼ˆæ ·æœ¬æ•°ã€ä»£è°¢ç‰©æ•°ã€ç¼ºå¤±å€¼ã€åˆ†ç»„ä¿¡æ¯ç­‰ï¼‰",
                "preprocess_data": "é¢„å¤„ç†æ•°æ®ï¼šå¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–ã€ç¼©æ”¾",
                "pca_analysis": "æ‰§è¡Œä¸»æˆåˆ†åˆ†æ (PCA)",
                "differential_analysis": "æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æï¼ˆä¸¤ç»„æ¯”è¾ƒï¼‰",
                "visualize_pca": "ç”Ÿæˆ PCA å¯è§†åŒ–å›¾",
                "visualize_volcano": "ç”Ÿæˆç«å±±å›¾ï¼ˆVolcano Plotï¼‰"
            }
        }
        
        # å¦‚æœæœ‰æ–‡ä»¶ï¼Œå¼ºåˆ¶å…ˆæ£€æŸ¥ï¼ˆç¬¦åˆ SOPï¼‰
        inspection_result = None
        if file_paths:
            input_path = file_paths[0]
            try:
                inspection_result = self.metabolomics_tool.inspect_data(input_path)
                if "error" not in inspection_result:
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
                    inspection_summary = f"""
ã€Data Inspection Completedã€‘
- Samples: {inspection_result.get('n_samples', 'N/A')}
- Metabolites: {inspection_result.get('n_metabolites', 'N/A')}
- Missing values: {inspection_result.get('missing_values', {}).get('percentage', 'N/A')}%
- Group column: {inspection_result.get('group_info', {}).get('column', 'N/A')}
- Groups: {inspection_result.get('group_info', {}).get('groups', {})}
"""
                    # å…ˆè¾“å‡ºæ£€æŸ¥ç»“æœ
                    yield f"ğŸ” **Data Inspection Results:**\n{inspection_summary}\n\n"
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°æŸ¥è¯¢ä¸­ï¼Œè®© LLM åŸºäºæ­¤åˆ†æ
                    query = f"""{query}

{inspection_summary}

Based on the inspection results above, please:
1. Analyze the data characteristics
2. Propose appropriate analysis parameters
3. Ask for confirmation before proceeding with analysis
"""
            except Exception as e:
                logger.error(f"Error inspecting file: {e}", exc_info=True)
                yield f"âš ï¸ Warning: Could not inspect file: {str(e)}\n\n"
        
        # æ„å»ºå¢å¼ºçš„ç”¨æˆ·æŸ¥è¯¢ï¼ŒåŒ…å«å·¥å…·è¯´æ˜
        enhanced_query = f"""{query}

ã€Available Toolsã€‘
You have access to:
- inspect_data(file_path): Check data file structure (already executed above if files were provided)
- preprocess_data(file_path, missing_threshold, normalization, scale): Preprocess metabolomics data
- pca_analysis(n_components, file_path): Perform PCA analysis
- differential_analysis(group_column, file_path, method, p_value_threshold, fold_change_threshold): Find differential metabolites
- visualize_pca(group_column, pca_file, pc1, pc2): Generate PCA plot
- visualize_volcano(diff_file, p_value_threshold, fold_change_threshold): Generate volcano plot

ã€Workflow Ruleã€‘
- If user provides CSV files: Inspect first (already done above), then analyze and propose parameters
- Before running any analysis, you MUST have inspected the data first
- Now analyze the inspection results and propose parameters.
"""
        
        # å…ˆè¾“å‡ºä¸€ä¸ªæç¤ºï¼Œè®©ç”¨æˆ·çŸ¥é“ç³»ç»Ÿæ­£åœ¨å·¥ä½œ
        yield "ğŸ’­ æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚ï¼Œè¯·ç¨å€™...\n\n"
        
        # æµå¼è¾“å‡º LLM å“åº”
        try:
            has_content = False
            async for chunk in self.chat(enhanced_query, context, stream=True):
                if chunk:
                    has_content = True
                    yield chunk
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹è¾“å‡ºï¼Œè¯´æ˜å¯èƒ½å‡ºé”™äº†
            if not has_content:
                yield "\n\nâš ï¸ æŠ±æ­‰ï¼Œå“åº”ç”Ÿæˆå‡ºç°é—®é¢˜ã€‚è¯·é‡è¯•æˆ–æ£€æŸ¥æ—¥å¿—ã€‚"
        except Exception as e:
            logger.error(f"âŒ æµå¼å“åº”é”™è¯¯: {e}", exc_info=True)
            yield f"\n\nâŒ é”™è¯¯: {str(e)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
    
    async def execute_workflow(
        self,
        workflow_config: Dict[str, Any],
        file_paths: List[str],
        output_dir: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œä»£è°¢ç»„å­¦å·¥ä½œæµ
        
        Args:
            workflow_config: å·¥ä½œæµé…ç½®
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        import os
        
        logger.info("=" * 80)
        logger.info("ğŸš€ [CHECKPOINT] execute_workflow START")
        logger.info(f"ğŸ“ Input file paths: {file_paths}")
        logger.info(f"ğŸ“‚ Output directory: {output_dir}")
        logger.info(f"âš™ï¸  Workflow config: {workflow_config.get('workflow_name', 'Unknown')}")
        logger.info("=" * 80)
        
        input_path = file_paths[0] if file_paths else None
        if not input_path:
            error_msg = "No input files provided"
            logger.error(f"âŒ [CHECKPOINT] execute_workflow FAILED: {error_msg}")
            raise ValueError(error_msg)
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        logger.info(f"ğŸ” [CHECKPOINT] Checking input file: {input_path}")
        logger.info(f"   File exists? {os.path.exists(input_path)}")
        if not os.path.exists(input_path):
            error_msg = f"Input file not found: {input_path}"
            logger.error(f"âŒ [CHECKPOINT] execute_workflow FAILED: {error_msg}")
            raise FileNotFoundError(error_msg)
        logger.info(f"   File size: {os.path.getsize(input_path)} bytes")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        logger.info(f"ğŸ“‚ [CHECKPOINT] Setting up output directory: {output_dir}")
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"   Created output directory: {output_dir}")
        else:
            logger.info(f"   Output directory already exists: {output_dir}")
        
        # æ›´æ–°ä»£è°¢ç»„å·¥å…·çš„è¾“å‡ºç›®å½•
        from pathlib import Path
        self.metabolomics_config["output_dir"] = output_dir
        self.metabolomics_tool.output_dir = Path(output_dir)
        self.metabolomics_tool.output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ… [CHECKPOINT] Metabolomics tool output_dir set to: {self.metabolomics_tool.output_dir}")
        
        # æ‰§è¡Œå·¥ä½œæµæ­¥éª¤
        steps = workflow_config.get("steps", [])
        steps_details = []
        final_plot = None
        
        logger.info(f"ğŸ“‹ [CHECKPOINT] Workflow has {len(steps)} steps to execute")
        for idx, step in enumerate(steps, 1):
            logger.info(f"   Step {idx}: {step.get('step_id')} ({step.get('tool_id')})")
        
        try:
            for step in steps:
                step_id = step.get("step_id")
                tool_id = step.get("tool_id")
                params = step.get("params", {})
                
                logger.info(f"ğŸ”§ [CHECKPOINT] Executing step {len(steps_details) + 1}/{len(steps)}: {step_id} ({tool_id})")
                logger.info(f"   Step params: {params}")
                
                if tool_id == "inspect_data":
                    # ğŸ”¥ Step 3: ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
                    file_path_to_inspect = params.get("file_path", input_path)
                    
                    # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                    if not os.path.isabs(file_path_to_inspect):
                        from pathlib import Path
                        upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
                        file_path_to_inspect = str(Path(upload_dir) / file_path_to_inspect)
                    
                    # ç¡®ä¿è·¯å¾„å­˜åœ¨
                    file_path_to_inspect = os.path.abspath(file_path_to_inspect)
                    
                    logger.info(f"ğŸ” [CHECKPOINT] inspect_data: Using absolute path: {file_path_to_inspect}")
                    logger.info(f"   File exists? {os.path.exists(file_path_to_inspect)}")
                    if os.path.exists(file_path_to_inspect):
                        logger.info(f"   File size: {os.path.getsize(file_path_to_inspect)} bytes")
                    else:
                        logger.error(f"âŒ File not found: {file_path_to_inspect}")
                        logger.error(f"   Original path: {params.get('file_path', input_path)}")
                        logger.error(f"   Upload dir: {os.getenv('UPLOAD_DIR', '/app/uploads')}")
                    
                    result = self.metabolomics_tool.inspect_data(file_path_to_inspect)
                    logger.info(f"âœ… [CHECKPOINT] inspect_data completed: {result.get('status', 'unknown')}")
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": f"æ£€æŸ¥å®Œæˆ: {result.get('n_samples', 'N/A')} ä¸ªæ ·æœ¬, {result.get('n_metabolites', 'N/A')} ä¸ªä»£è°¢ç‰©",
                        "data": result.get("data", {})  # åŒ…å« preview å’Œ summary
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "step_result": step_result  # å®Œæ•´çš„æ­¥éª¤ç»“æœ
                    })
                
                elif tool_id == "preprocess_data":
                    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æï¼Œç¡®ä¿æ–‡ä»¶èƒ½è¢«æ‰¾åˆ°
                    file_path_to_preprocess = params.get("file_path", input_path)
                    
                    # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æ™ºèƒ½è·¯å¾„è§£æ
                    if not os.path.isabs(file_path_to_preprocess) or not os.path.exists(file_path_to_preprocess):
                        from ...core.file_inspector import FileInspector
                        upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
                        inspector = FileInspector(upload_dir)
                        resolved_path, _ = inspector._resolve_actual_path(file_path_to_preprocess)
                        if resolved_path:
                            file_path_to_preprocess = resolved_path
                            logger.info(f"âœ… [CHECKPOINT] preprocess_data: Resolved path to: {file_path_to_preprocess}")
                    
                    logger.info(f"ğŸ” [CHECKPOINT] preprocess_data: Trying to read file at: {file_path_to_preprocess}")
                    logger.info(f"   File exists? {os.path.exists(file_path_to_preprocess)}")
                    logger.info(f"   Parameters: missing_threshold={params.get('missing_threshold', '0.5')}, normalization={params.get('normalization', 'log2')}, scale={params.get('scale', 'true')}")
                    result = self.metabolomics_tool.preprocess_data(
                        file_path=file_path_to_preprocess,
                        missing_threshold=float(params.get("missing_threshold", "0.5")),
                        normalization=params.get("normalization", "log2"),
                        scale=params.get("scale", "true").lower() == "true"
                    )
                    logger.info(f"âœ… [CHECKPOINT] preprocess_data completed: {result.get('status', 'unknown')}")
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": result.get("message", "é¢„å¤„ç†å®Œæˆ"),
                        "data": result.get("data", {})  # åŒ…å« preview å’Œ summary
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "step_result": step_result
                    })
                
                elif tool_id == "pca_analysis":
                    # å¦‚æœä¸Šä¸€æ­¥æ˜¯é¢„å¤„ç†ï¼Œä½¿ç”¨é¢„å¤„ç†åçš„æ–‡ä»¶
                    preprocessed_file = None
                    for prev_step in steps_details:
                        if prev_step.get("tool_id") == "preprocess_data":
                            # ä»é¢„å¤„ç†ç»“æœä¸­è·å–æ–‡ä»¶è·¯å¾„
                            preprocessed_file = os.path.join(output_dir, "preprocessed_data.csv")
                            break
                    
                    result = self.metabolomics_tool.pca_analysis(
                        n_components=int(params.get("n_components", "10")),
                        file_path=preprocessed_file or params.get("file_path", input_path)
                    )
                    # ğŸ”§ ä¿®å¤ï¼šä¿å­˜å®Œæ•´çš„ PCA ç»“æœï¼Œä»¥ä¾¿åç»­æå– variance_explained
                    if result.get("status") == "success":
                        self.metabolomics_tool._last_pca_result = result
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": result.get("message", "PCA åˆ†æå®Œæˆ"),
                        "data": result.get("data", {}),  # åŒ…å« preview å’Œ tables
                        "_full_result": result  # ğŸ”§ ä¿®å¤ï¼šä¿å­˜å®Œæ•´ç»“æœä»¥ä¾¿åç»­æå–
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "step_result": step_result
                    })
                
                elif tool_id == "differential_analysis":
                    # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡ä»¶
                    preprocessed_file = os.path.join(output_dir, "preprocessed_data.csv")
                    if not os.path.exists(preprocessed_file):
                        preprocessed_file = input_path
                    
                    result = self.metabolomics_tool.differential_analysis(
                        group_column=params.get("group_column", "Muscle loss"),
                        file_path=preprocessed_file,
                        method=params.get("method", "t-test"),
                        p_value_threshold=float(params.get("p_value_threshold", "0.05")),
                        fold_change_threshold=float(params.get("fold_change_threshold", "1.5")),
                        group1=params.get("group1"),
                        group2=params.get("group2")
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ·é€‰æ‹©åˆ†ç»„
                    if result.get("status") == "need_selection":
                        return {
                            "status": "need_selection",
                            "message": result.get("message", "éœ€è¦é€‰æ‹©æ¯”è¾ƒçš„åˆ†ç»„"),
                            "groups": result.get("groups", []),
                            "steps_details": steps_details
                        }
                    
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": result.get("message", "å·®å¼‚åˆ†æå®Œæˆ"),
                        "data": result.get("data", {})  # åŒ…å« tables å’Œ summary
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "details": result.get("summary", ""),
                        "step_result": step_result
                    })
                
                elif tool_id == "visualize_pca":
                    # ä½¿ç”¨ PCA åˆ†æç»“æœæ–‡ä»¶
                    pca_file = params.get("pca_file") or os.path.join(output_dir, "pca_results.csv")
                    if not os.path.exists(pca_file):
                        # å¦‚æœ PCA æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ­¥éª¤è¯¦æƒ…ä¸­è·å–
                        for prev_step in steps_details:
                            if prev_step.get("tool_id") == "pca_analysis":
                                pca_file = os.path.join(output_dir, "pca_results.csv")
                                break
                    
                    result = self.metabolomics_tool.visualize_pca(
                        group_column=params.get("group_column", "Muscle loss"),
                        pca_file=pca_file,
                        pc1=int(params.get("pc1", "1")),
                        pc2=int(params.get("pc2", "2"))
                    )
                    plot_path = result.get("plot_path") or result.get("plot_file")
                    relative_plot_path = None
                    if plot_path:
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº output_dirï¼‰
                        if os.path.isabs(plot_path):
                            relative_plot_path = os.path.relpath(plot_path, output_dir)
                        else:
                            relative_plot_path = plot_path
                        # ç¡®ä¿è·¯å¾„ä½¿ç”¨æ­£æ–œæ ï¼ˆWeb å…¼å®¹ï¼‰
                        relative_plot_path = relative_plot_path.replace("\\", "/")
                        final_plot = relative_plot_path
                    
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": "PCA å¯è§†åŒ–å®Œæˆ",
                        "data": result.get("data", {})  # åŒ…å« images æ•°ç»„
                    }
                    # å¦‚æœ result.data ä¸­æ²¡æœ‰ imagesï¼Œæ·»åŠ 
                    if "images" not in step_result["data"] and relative_plot_path:
                        step_result["data"]["images"] = [relative_plot_path]
                    
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": "PCA å¯è§†åŒ–å®Œæˆ",
                        "status": result.get("status", "success"),
                        "plot": relative_plot_path,
                        "step_result": step_result
                    })
                
                elif tool_id == "visualize_volcano":
                    # ä½¿ç”¨å·®å¼‚åˆ†æç»“æœæ–‡ä»¶
                    diff_file = params.get("diff_file") or os.path.join(output_dir, "differential_analysis.csv")
                    if not os.path.exists(diff_file):
                        # å¦‚æœå·®å¼‚åˆ†ææ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ­¥éª¤è¯¦æƒ…ä¸­è·å–
                        for prev_step in steps_details:
                            if prev_step.get("tool_id") == "differential_analysis":
                                diff_file = os.path.join(output_dir, "differential_analysis.csv")
                                break
                    
                    result = self.metabolomics_tool.visualize_volcano(
                        diff_file=diff_file,
                        p_value_threshold=float(params.get("p_value_threshold", "0.05")),
                        fold_change_threshold=float(params.get("fold_change_threshold", "1.5"))
                    )
                    plot_path = result.get("plot_path") or result.get("plot_file")
                    relative_plot_path = None
                    if plot_path:
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº output_dirï¼‰
                        if os.path.isabs(plot_path):
                            relative_plot_path = os.path.relpath(plot_path, output_dir)
                        else:
                            relative_plot_path = plot_path
                        # ç¡®ä¿è·¯å¾„ä½¿ç”¨æ­£æ–œæ ï¼ˆWeb å…¼å®¹ï¼‰
                        relative_plot_path = relative_plot_path.replace("\\", "/")
                        if not final_plot:
                            final_plot = relative_plot_path
                    
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": "ç«å±±å›¾å¯è§†åŒ–å®Œæˆ",
                        "data": result.get("data", {})  # åŒ…å« images æ•°ç»„
                    }
                    # å¦‚æœ result.data ä¸­æ²¡æœ‰ imagesï¼Œæ·»åŠ 
                    if "images" not in step_result["data"] and relative_plot_path:
                        step_result["data"]["images"] = [relative_plot_path]
                    
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": "ç«å±±å›¾å¯è§†åŒ–å®Œæˆ",
                        "status": result.get("status", "success"),
                        "plot": relative_plot_path,
                        "step_result": step_result
                    })
            
            # æ„å»º steps_results åˆ—è¡¨ï¼ˆå‰ç«¯å¯ç›´æ¥ä½¿ç”¨ï¼‰
            steps_results = []
            for step_detail in steps_details:
                if "step_result" in step_detail:
                    steps_results.append(step_detail["step_result"])
                else:
                    # å…¼å®¹æ—§æ ¼å¼
                    steps_results.append({
                        "step_name": step_detail.get("name", "Unknown"),
                        "status": step_detail.get("status", "success"),
                        "logs": step_detail.get("summary", ""),
                        "data": {}
                    })
            
            logger.info("=" * 80)
            logger.info("âœ… [CHECKPOINT] execute_workflow SUCCESS")
            logger.info(f"ğŸ“Š Completed {len(steps_details)} steps")
            logger.info(f"ğŸ“‚ Output directory: {output_dir}")
            if final_plot:
                logger.info(f"ğŸ–¼ï¸  Final plot: {final_plot}")
            logger.info("=" * 80)
            
            # ğŸ”¥ Task 1: ç”Ÿæˆ AI è¯Šæ–­ï¼ˆåœ¨æ‰€æœ‰æ­¥éª¤å®Œæˆåï¼‰
            diagnosis = None
            try:
                logger.info("ğŸ“ [CHECKPOINT] Generating AI diagnosis from workflow results...")
                diagnosis = await self._generate_final_diagnosis(steps_details, workflow_config)
                if diagnosis:
                    logger.info(f"ğŸ“ Diagnosis generated successfully. Result length: {len(diagnosis)}")
                    logger.info(f"ğŸ“ Diagnosis preview: {diagnosis[:200]}...")  # æ˜¾ç¤ºå‰200å­—ç¬¦
                else:
                    logger.warning("âš ï¸ [CHECKPOINT] Diagnosis generation returned None")
            except Exception as diag_err:
                logger.error(f"âŒ [CHECKPOINT] Diagnosis generation failed: {diag_err}", exc_info=True)
                diagnosis = "âš ï¸ è¯Šæ–­ç”Ÿæˆå¤±è´¥ï¼Œä½†åˆ†æå·²å®Œæˆã€‚"
            
            # ğŸ”¥ æ„å»ºè¿”å›ç»“æœ
            workflow_result = {
                "status": "success",
                "workflow_name": workflow_config.get("workflow_name", "Metabolomics Analysis"),
                "steps_details": steps_details,  # ä¿ç•™æ—§æ ¼å¼ä»¥å…¼å®¹
                "steps_results": steps_results,  # æ–°çš„æ ¼å¼ï¼Œå‰ç«¯å¯ç›´æ¥ä½¿ç”¨
                "final_plot": final_plot,
                "output_dir": output_dir,
                "diagnosis": diagnosis or "âœ… **åˆ†ææˆåŠŸå®Œæˆï¼**"  # ğŸ”¥ Task 1: æ·»åŠ è¯Šæ–­å­—æ®µ
            }
            
            # ğŸ”¥ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨ï¼ˆå¤„ç† Numpy ç±»å‹ã€NaN/Infinity ç­‰ï¼‰
            logger.info("âœ… Workflow finished. Sanitizing data for JSON serialization...")
            sanitized_result = sanitize_for_json(workflow_result)
            logger.info("âœ… Data sanitization completed. Returning result to frontend.")
            
            return sanitized_result
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error("âŒ [CHECKPOINT] execute_workflow FAILED")
            logger.error(f"âŒ Error type: {type(e).__name__}")
            logger.error(f"âŒ Error message: {str(e)}")
            logger.error(f"âŒ Completed steps: {len(steps_details)}/{len(steps)}")
            logger.error(f"ğŸ“‚ Output directory: {output_dir}")
            logger.error("âŒ Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            
            # è¿”å›é”™è¯¯ä¿¡æ¯ç»™å‰ç«¯ï¼ˆä¹Ÿéœ€è¦æ¸…ç†ï¼‰
            error_result = {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "error_traceback": error_traceback,
                "steps_details": steps_details,
                "output_dir": output_dir,
                "message": f"âŒ å·¥ä½œæµæ‰§è¡Œå‡ºé”™: {str(e)}\n\n(è¯·æŸ¥çœ‹åå°æ—¥å¿—è·å–è¯¦ç»†å †æ ˆ)"
            }
            
            # æ¸…ç†é”™è¯¯ç»“æœï¼ˆè™½ç„¶é”™è¯¯ç»“æœé€šå¸¸ä¸åŒ…å« Numpy æ•°æ®ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼‰
            return sanitize_for_json(error_result)

