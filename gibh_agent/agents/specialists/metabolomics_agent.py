"""ä»£è°¢ç»„å­¦æ™ºèƒ½ä½“ï¼ˆMetabolomics Agentï¼‰"""
from typing import Dict, Any, List, AsyncIterator, Optional
from ..base_agent import BaseAgent
from ...core.llm_client import LLMClient
from ...core.prompt_manager import PromptManager, DATA_DIAGNOSIS_PROMPT
from ...core.utils import sanitize_for_json
from ...tools.metabolomics_tool import MetabolomicsTool
import logging

logger = logging.getLogger(__name__)


class MetabolomicsAgent(BaseAgent):
    """ä»£è°¢ç»„å­¦æ™ºèƒ½ä½“"""
    
    # å®šä¹‰ä¸¥æ ¼çš„æ­¥éª¤é¡ºåºï¼ˆä¾èµ–é“¾ï¼‰
    STEPS_ORDER = [
        "inspect_data",      # æ­¥éª¤1: æ•°æ®æ£€æŸ¥
        "preprocess_data",   # æ­¥éª¤2: æ•°æ®é¢„å¤„ç†
        "pca_analysis",      # æ­¥éª¤3: PCA åˆ†æ
        "differential_analysis",  # æ­¥éª¤4: å·®å¼‚åˆ†æ
        "visualize_pca",     # æ­¥éª¤5: PCA å¯è§†åŒ–
        "visualize_volcano"  # æ­¥éª¤6: ç«å±±å›¾å¯è§†åŒ–
    ]
    
    # æ­¥éª¤æ˜ å°„ï¼ˆstep_id -> åœ¨ STEPS_ORDER ä¸­çš„ç´¢å¼•ï¼‰
    STEP_INDEX_MAP = {step: idx for idx, step in enumerate(STEPS_ORDER)}
    
    def __init__(
        self,
        llm_client: LLMClient,
        prompt_manager: PromptManager,
        metabolomics_config: Dict[str, Any] = None
    ):
        super().__init__(llm_client, prompt_manager, "metabolomics_expert")
        self.metabolomics_config = metabolomics_config or {}
        self.metabolomics_tool = MetabolomicsTool(self.metabolomics_config)
    
    async def process_query(
        self,
        query: str,
        history: List[Dict[str, str]] = None,
        uploaded_files: List[Dict[str, str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼Œå¯èƒ½åŒ…å«ï¼š
            - chat: èŠå¤©å“åº”ï¼ˆæµå¼ï¼‰
        """
        query_lower = query.lower().strip()
        file_paths = self.get_file_paths(uploaded_files or [])
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯å·¥ä½œæµè¯·æ±‚
        is_workflow_request = self._is_workflow_request(query_lower, file_paths)
        
        if is_workflow_request:
            # å·¥ä½œæµè¯·æ±‚ï¼šå…ˆæ£€æŸ¥æ•°æ®ï¼Œç„¶åç”Ÿæˆå·¥ä½œæµé…ç½®
            return await self._generate_workflow_config(query, file_paths)
        else:
            # æ™®é€šèŠå¤©ï¼šæµå¼å“åº”
            return {
                "type": "chat",
                "response": self._stream_chat_response(query, file_paths)
            }
    
    def _is_workflow_request(self, query: str, file_paths: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å·¥ä½œæµè¯·æ±‚"""
        workflow_keywords = [
            "è§„åˆ’", "æµç¨‹", "workflow", "pipeline", "åˆ†æ", "run",
            "æ‰§è¡Œ", "plan", "åšä¸€ä¸‹", "è·‘ä¸€ä¸‹", "åˆ†æä¸€ä¸‹", "å…¨æµç¨‹",
            "ä»£è°¢ç»„", "ä»£è°¢ç»„åˆ†æ", "metabolomics"
        ]
        
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæŸ¥è¯¢åŒ…å«å·¥ä½œæµå…³é”®è¯ï¼Œè¿”å› True
        if query and any(kw in query for kw in workflow_keywords):
            return True
        
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœåªæœ‰æ–‡ä»¶æ²¡æœ‰æ–‡æœ¬ï¼ˆæˆ–æ–‡æœ¬å¾ˆçŸ­ï¼‰ï¼Œä¸”ä¸æ˜¯è‡ªæˆ‘ä»‹ç»ç­‰å¸¸è§æŸ¥è¯¢ï¼Œè¿”å› True
        if file_paths and (not query or len(query.strip()) < 5):
            # æ’é™¤ä¸€äº›å¸¸è§çš„éå·¥ä½œæµæŸ¥è¯¢
            non_workflow_queries = ["ä½ å¥½", "hello", "hi", "ä»‹ç»", "è‡ªæˆ‘ä»‹ç»", "ä½ æ˜¯è°", "who are you"]
            if not query or query.strip().lower() not in [q.lower() for q in non_workflow_queries]:
                return True
        
        return False
    
    async def _generate_workflow_config(
        self,
        query: str,
        file_paths: List[str]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµé…ç½®
        
        æµç¨‹ï¼š
        1. å…ˆæ£€æŸ¥æ•°æ®ï¼ˆinspect_dataï¼‰
        2. ä½¿ç”¨ LLM æå–ç›®æ ‡æ­¥éª¤ï¼ˆæ”¯æŒç”¨æˆ·æŒ‡å®š"åªè¿è¡Œæ­¥éª¤1"ç­‰ï¼‰
        3. åŸºäºæ£€æŸ¥ç»“æœæå–å‚æ•°
        4. ç”Ÿæˆå·¥ä½œæµé…ç½®ï¼ˆåªåŒ…å«ç›®æ ‡æ­¥éª¤ï¼‰
        """
        logger.info("=" * 80)
        logger.info("ğŸš€ [CHECKPOINT] _generate_workflow_config START")
        logger.info(f"   Query: {query}")
        logger.info(f"   File paths: {file_paths}")
        logger.info("=" * 80)
        
        # å¼ºåˆ¶æ£€æŸ¥ï¼šå¦‚æœæœ‰æ–‡ä»¶ï¼Œå…ˆæ£€æŸ¥
        inspection_result = None
        diagnosis_report = None
        if file_paths:
            input_path = file_paths[0]
            logger.info(f"ğŸ” [CHECKPOINT] Inspecting file: {input_path}")
            try:
                inspection_result = self.metabolomics_tool.inspect_data(input_path)
                if "error" in inspection_result:
                    logger.warning(f"âš ï¸ File inspection failed: {inspection_result.get('error')}")
                else:
                    logger.info(f"âœ… [CHECKPOINT] File inspection successful")
                    # ğŸ”¥ ç”Ÿæˆæ•°æ®è¯Šæ–­å’Œå‚æ•°æ¨è
                    try:
                        logger.info(f"ğŸ” [CHECKPOINT] Generating diagnosis report...")
                        diagnosis_report = await self._generate_diagnosis_and_recommendation(inspection_result)
                        logger.info(f"âœ… [CHECKPOINT] Diagnosis report generated")
                    except Exception as diag_err:
                        logger.error(f"âŒ [CHECKPOINT] Diagnosis report generation failed: {diag_err}", exc_info=True)
                        diagnosis_report = None  # ç»§ç»­æ‰§è¡Œï¼Œä¸é˜»å¡
            except Exception as e:
                logger.error(f"âŒ [CHECKPOINT] Error inspecting file: {e}", exc_info=True)
        
        # ä½¿ç”¨ LLM æå–ç›®æ ‡ç»“æŸæ­¥éª¤ï¼ˆä¾‹å¦‚ï¼š"åšåˆ°PCA" -> "pca_analysis"ï¼‰
        target_end_step = None
        try:
            logger.info(f"ğŸ” [CHECKPOINT] Extracting target end step from query...")
            target_end_step = await self._extract_target_end_step(query, inspection_result)
            logger.info(f"âœ… [CHECKPOINT] Target end step extracted: {target_end_step}")
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error extracting target end step: {e}", exc_info=True)
            target_end_step = None  # ä½¿ç”¨é»˜è®¤å€¼ï¼ˆæ‰€æœ‰æ­¥éª¤ï¼‰
        
        # ä½¿ç”¨ LLM æå–å‚æ•°ï¼ˆä¼ å…¥æ£€æŸ¥ç»“æœå’Œè¯Šæ–­æŠ¥å‘Šï¼‰
        extracted_params = {}
        try:
            logger.info(f"ğŸ” [CHECKPOINT] Extracting workflow parameters...")
            extracted_params = await self._extract_workflow_params(query, file_paths, inspection_result, diagnosis_report)
            logger.info(f"âœ… [CHECKPOINT] Workflow parameters extracted: {list(extracted_params.keys())}")
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error extracting workflow params: {e}", exc_info=True)
            extracted_params = {}  # ä½¿ç”¨é»˜è®¤å€¼
        
        # å®šä¹‰æ‰€æœ‰å¯ç”¨æ­¥éª¤ï¼ˆåŒ…å«å‹å¥½çš„ä¸­æ–‡åç§°ï¼‰
        all_steps = [
            {
                "step_id": "inspect_data",
                "tool_id": "inspect_data",
                "name": "æ•°æ®æ£€æŸ¥",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "æ•°æ®æ£€æŸ¥",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "æ£€æŸ¥æ•°æ®æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ï¼ˆæ ·æœ¬æ•°ã€ä»£è°¢ç‰©æ•°ã€ç¼ºå¤±å€¼ã€åˆ†ç»„ä¿¡æ¯ç­‰ï¼‰",
                "params": {"file_path": file_paths[0] if file_paths else ""}
            },
            {
                "step_id": "preprocess_data",
                "tool_id": "preprocess_data",
                "name": "æ•°æ®é¢„å¤„ç†",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "æ•°æ®é¢„å¤„ç†",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "æ•°æ®é¢„å¤„ç†ï¼šå¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–ã€ç¼©æ”¾",
                "params": {
                    "file_path": file_paths[0] if file_paths else "",
                    "missing_threshold": extracted_params.get("missing_threshold", "0.5"),
                    "normalization": extracted_params.get("normalization", "log2"),
                    "scale": extracted_params.get("scale", "true")
                }
            },
            {
                "step_id": "pca_analysis",
                "tool_id": "pca_analysis",
                "name": "ä¸»æˆåˆ†åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "ä¸»æˆåˆ†åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "æ‰§è¡Œä¸»æˆåˆ†åˆ†æ (PCA)ï¼Œé™ç»´å¹¶æå–ä¸»è¦å˜å¼‚",
                "params": {
                    "n_components": extracted_params.get("n_components", "10")
                }
            },
            {
                "step_id": "differential_analysis",
                "tool_id": "differential_analysis",
                "name": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æï¼ˆä¸¤ç»„æ¯”è¾ƒï¼‰ï¼Œè¯†åˆ«æ˜¾è‘—å·®å¼‚çš„ä»£è°¢ç‰©",
                "params": {
                    "group_column": extracted_params.get("group_column", "Muscle loss"),
                    "method": extracted_params.get("method", "t-test"),
                    "p_value_threshold": extracted_params.get("p_value_threshold", "0.05"),
                    "fold_change_threshold": extracted_params.get("fold_change_threshold", "1.5"),
                    "group1": extracted_params.get("group1"),
                    "group2": extracted_params.get("group2")
                }
            },
            {
                "step_id": "visualize_pca",
                "tool_id": "visualize_pca",
                "name": "PCA å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "PCA å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "ç”Ÿæˆ PCA å¯è§†åŒ–å›¾ï¼Œå±•ç¤ºæ ·æœ¬åœ¨ä¸»æˆåˆ†ç©ºé—´çš„åˆ†å¸ƒ",
                "params": {
                    "group_column": extracted_params.get("group_column", "Muscle loss"),
                    "pc1": "1",
                    "pc2": "2"
                }
            },
            {
                "step_id": "visualize_volcano",
                "tool_id": "visualize_volcano",
                "name": "ç«å±±å›¾å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  name å­—æ®µ
                "step_name": "ç«å±±å›¾å¯è§†åŒ–",  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  step_name å­—æ®µï¼ˆå…¼å®¹å‰ç«¯ï¼‰
                "desc": "ç”Ÿæˆç«å±±å›¾ (Volcano Plot)ï¼Œå±•ç¤ºå·®å¼‚ä»£è°¢ç‰©çš„ç»Ÿè®¡æ˜¾è‘—æ€§",
                "params": {
                    "p_value_threshold": extracted_params.get("p_value_threshold", "0.05"),
                    "fold_change_threshold": extracted_params.get("fold_change_threshold", "1.5")
                }
            }
        ]
        
        # æ ¹æ®ç›®æ ‡ç»“æŸæ­¥éª¤è‡ªåŠ¨åŒ…å«æ‰€æœ‰å‰ç½®æ­¥éª¤
        if target_end_step and target_end_step in self.STEP_INDEX_MAP:
            # æ‰¾åˆ°ç›®æ ‡æ­¥éª¤çš„ç´¢å¼•
            target_index = self.STEP_INDEX_MAP[target_end_step]
            # åŒ…å«ä»å¼€å§‹åˆ°ç›®æ ‡æ­¥éª¤çš„æ‰€æœ‰æ­¥éª¤ï¼ˆåŒ…æ‹¬ç›®æ ‡æ­¥éª¤ï¼‰
            required_step_ids = self.STEPS_ORDER[:target_index + 1]
            logger.info(f"ğŸ¯ ç›®æ ‡æ­¥éª¤: {target_end_step}, å°†æ‰§è¡Œ: {required_step_ids}")
            
            # æ„å»ºæ­¥éª¤æ˜ å°„å¹¶ç­›é€‰
            step_map = {step["step_id"]: step for step in all_steps}
            selected_steps = [step_map[s] for s in required_step_ids if s in step_map]
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæˆ–æ— æ•ˆï¼Œä½¿ç”¨æ‰€æœ‰æ­¥éª¤
            selected_steps = all_steps
        
        # æ„å»ºå·¥ä½œæµé…ç½®
        workflow_config = {
            "workflow_name": "Metabolomics Analysis Pipeline",
            "steps": selected_steps
        }
        
        # å¦‚æœç”Ÿæˆäº†è¯Šæ–­æŠ¥å‘Šï¼Œå°†å…¶åŒ…å«åœ¨è¿”å›ç»“æœä¸­
        result = {
            "type": "workflow_config",
            "workflow_data": workflow_config,
            "file_paths": file_paths
        }
        
        if diagnosis_report:
            result["diagnosis_report"] = diagnosis_report
        
        logger.info("=" * 80)
        logger.info("âœ… [CHECKPOINT] _generate_workflow_config SUCCESS")
        logger.info(f"   Workflow name: {workflow_config.get('workflow_name')}")
        logger.info(f"   Steps count: {len(workflow_config.get('steps', []))}")
        logger.info("=" * 80)
        
        return result
    
    async def _generate_diagnosis_and_recommendation(
        self,
        inspection_result: Dict[str, Any]
    ) -> Optional[str]:
        """
        ç”Ÿæˆæ•°æ®è¯Šæ–­å’Œå‚æ•°æ¨èæŠ¥å‘Š
        
        Args:
            inspection_result: æ–‡ä»¶æ£€æŸ¥ç»“æœ
        
        Returns:
            Markdownæ ¼å¼çš„è¯Šæ–­å’Œæ¨èæŠ¥å‘Šï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        try:
            import json
            # æ ¼å¼åŒ–æ£€æŸ¥ç»“æœä¸ºJSONå­—ç¬¦ä¸²
            inspection_json = json.dumps(inspection_result, ensure_ascii=False, indent=2)
            
            # ä½¿ç”¨ PromptManager è·å–è¯Šæ–­æ¨¡æ¿
            try:
                prompt = self.prompt_manager.get_prompt(
                    "data_diagnosis",
                    {"inspection_data": inspection_json},
                    fallback=DATA_DIAGNOSIS_PROMPT.format(inspection_data=inspection_json)
                )
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–è¯Šæ–­æ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
                prompt = DATA_DIAGNOSIS_PROMPT.format(inspection_data=inspection_json)
            
            # è°ƒç”¨LLMç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
            messages = [
                {"role": "system", "content": "You are a Senior Bioinformatician specializing in Metabolomics. Generate data diagnosis and parameter recommendations in Simplified Chinese."},
                {"role": "user", "content": prompt}
            ]
            
            completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=1500)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            logger.info("âœ… æ•°æ®è¯Šæ–­å’Œå‚æ•°æ¨èå·²ç”Ÿæˆ")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return None  # è¿”å› Noneï¼Œä¸é˜»å¡å·¥ä½œæµç”Ÿæˆ
    
    async def _extract_target_end_step(
        self,
        query: str,
        inspection_result: Dict[str, Any] = None
    ) -> Optional[str]:
        """
        ä½¿ç”¨ LLM ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–ç›®æ ‡ç»“æŸæ­¥éª¤
        
        æ”¯æŒï¼š
        - "åšåˆ°PCA" / "up to PCA" -> "pca_analysis" (ä¼šè‡ªåŠ¨åŒ…å« inspect_data, preprocess_data)
        - "åªåšé¢„å¤„ç†" -> "preprocess_data" (ä¼šè‡ªåŠ¨åŒ…å« inspect_data)
        - "åšåˆ°å·®å¼‚åˆ†æ" -> "differential_analysis" (ä¼šè‡ªåŠ¨åŒ…å«æ‰€æœ‰å‰ç½®æ­¥éª¤)
        - é»˜è®¤è¿”å› Noneï¼ˆæ‰§è¡Œæ‰€æœ‰æ­¥éª¤ï¼‰
        
        Returns:
            ç›®æ ‡ç»“æŸæ­¥éª¤çš„ step_idï¼Œæˆ– Noneï¼ˆæ‰§è¡Œæ‰€æœ‰æ­¥éª¤ï¼‰
        """
        # å®šä¹‰æ­¥éª¤å…³é”®è¯æ˜ å°„ï¼ˆç”¨äºåŒ¹é…"åšåˆ°XXX"ï¼‰
        step_keywords = {
            "inspect_data": ["æ£€æŸ¥", "inspect", "æ£€æŸ¥æ•°æ®", "æ•°æ®æ£€æŸ¥", "æ­¥éª¤1", "step 1", "ç¬¬ä¸€æ­¥", "åˆ°æ£€æŸ¥"],
            "preprocess_data": ["é¢„å¤„ç†", "preprocess", "æ•°æ®é¢„å¤„ç†", "æ­¥éª¤2", "step 2", "ç¬¬äºŒæ­¥", "åˆ°é¢„å¤„ç†"],
            "pca_analysis": ["pca", "ä¸»æˆåˆ†", "ä¸»æˆåˆ†åˆ†æ", "æ­¥éª¤3", "step 3", "ç¬¬ä¸‰æ­¥", "åšåˆ°pca", "åˆ°pca", "up to pca"],
            "differential_analysis": ["å·®å¼‚", "differential", "å·®å¼‚åˆ†æ", "æ­¥éª¤4", "step 4", "ç¬¬å››æ­¥", "åˆ°å·®å¼‚åˆ†æ"],
            "visualize_pca": ["pcaå›¾", "pcaå¯è§†åŒ–", "pca plot", "æ­¥éª¤5", "step 5", "ç¬¬äº”æ­¥", "åˆ°pcaå›¾"],
            "visualize_volcano": ["ç«å±±å›¾", "volcano", "volcano plot", "æ­¥éª¤6", "step 6", "ç¬¬å…­æ­¥", "åˆ°ç«å±±å›¾"]
        }
        
        # å…ˆè¿›è¡Œå…³é”®è¯åŒ¹é…ï¼ˆä¼˜å…ˆåŒ¹é…"åšåˆ°"ã€"up to"ç­‰è¡¨è¾¾ï¼‰
        query_lower = query.lower()
        
        # æ£€æŸ¥"åšåˆ°"ã€"up to"ç­‰è¡¨è¾¾
        for step_id, keywords in step_keywords.items():
            for kw in keywords:
                if kw in query_lower:
                    # ç‰¹åˆ«æ£€æŸ¥"åšåˆ°"ã€"up to"ç­‰è¡¨è¾¾
                    if any(phrase in query_lower for phrase in ["åšåˆ°", "up to", "until", "åˆ°"]):
                        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ç›®æ ‡ç»“æŸæ­¥éª¤: {step_id} (å…³é”®è¯: {kw})")
                        return step_id
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°"åšåˆ°"è¡¨è¾¾ï¼Œä½¿ç”¨ LLM æå–
        prompt = f"""
Extract the TARGET END STEP from the user query. This is the LAST step the user wants to run.

User Query: {query}

Available steps (in order):
1. inspect_data - Check data file
2. preprocess_data - Preprocess data
3. pca_analysis - PCA analysis
4. differential_analysis - Differential analysis
5. visualize_pca - PCA visualization
6. visualize_volcano - Volcano plot visualization

Important: If the user says "do analysis up to PCA" or "åšåˆ°PCA", they want to run steps 1, 2, and 3 (all prerequisites + PCA).
If they say "only preprocessing", they want steps 1 and 2.

Examples:
- "åšåˆ°PCA" / "up to PCA" -> "pca_analysis"
- "åªåšé¢„å¤„ç†" -> "preprocess_data"
- "åšåˆ°å·®å¼‚åˆ†æ" -> "differential_analysis"
- "åšå®Œæ•´åˆ†æ" / "å…¨éƒ¨" -> null (run all steps)

Return JSON only (single step_id string or null):
"""
        
        messages = [
            {"role": "system", "content": "You are a step extraction assistant. Return JSON only (single string or null)."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            logger.info(f"ğŸ” [CHECKPOINT] Calling LLM to extract target end step...")
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=64)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            logger.info(f"âœ… [CHECKPOINT] LLM response received: {response[:100]}...")
            
            import json
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0].strip()
            
            result = json.loads(json_str)
            if result is None or result == "":
                logger.info(f"âœ… [CHECKPOINT] LLM returned null, will run all steps")
                return None
            # éªŒè¯ç»“æœæ˜¯å¦åœ¨æœ‰æ•ˆæ­¥éª¤åˆ—è¡¨ä¸­
            if isinstance(result, str) and result in self.STEPS_ORDER:
                logger.info(f"ğŸ¯ LLM æå–çš„ç›®æ ‡ç»“æŸæ­¥éª¤: {result}")
                return result
            logger.warning(f"âš ï¸ LLM returned invalid step: {result}, will run all steps")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [CHECKPOINT] JSON decode error: {e}, response: {response[:200]}")
            return None  # é»˜è®¤æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error extracting target end step: {e}", exc_info=True)
            return None  # é»˜è®¤æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
    
    async def _extract_workflow_params(
        self,
        query: str,
        file_paths: List[str],
        inspection_result: Dict[str, Any] = None,
        diagnosis_report: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æå–å·¥ä½œæµå‚æ•°
        
        åŸºäºæ£€æŸ¥ç»“æœå’Œè¯Šæ–­æŠ¥å‘Šæ™ºèƒ½æ¨èå‚æ•°
        """
        # æ„å»ºåŒ…å«æ£€æŸ¥ç»“æœå’Œè¯Šæ–­æŠ¥å‘Šçš„æç¤º
        inspection_info = ""
        if diagnosis_report:
            inspection_info = f"""
ã€Data Diagnosis & Recommendationsã€‘
{diagnosis_report}

"""
        elif inspection_result and "error" not in inspection_result:
            inspection_info = f"""
ã€Data Diagnosis & Recommendationsã€‘
{diagnosis_report}

"""
        elif inspection_result and "error" not in inspection_result:
            inspection_info = f"""
ã€Data Inspection Resultsã€‘
- Number of samples: {inspection_result.get('n_samples', 'N/A')}
- Number of metabolites: {inspection_result.get('n_metabolites', 'N/A')}
- Missing values: {inspection_result.get('missing_values', {}).get('percentage', 'N/A')}%
- Group column: {inspection_result.get('group_info', {}).get('column', 'N/A')}
- Groups: {inspection_result.get('group_info', {}).get('groups', {})}
"""
        
        prompt = f"""
Extract workflow parameters for metabolomics analysis from the user query.

User Query: {query}
File Paths: {file_paths}

{inspection_info}

Based on the inspection results and user query, extract the following parameters:
- missing_threshold: Threshold for removing metabolites with high missing values (default: 0.5)
- normalization: Normalization method - "log2", "zscore", or "none" (default: "log2")
- scale: Whether to apply StandardScaler (default: true)
- n_components: Number of PCA components (default: 10)
- group_column: Column name for group comparison (default: "Muscle loss" or first metadata column)
- method: Statistical method for differential analysis - "t-test" or "mann-whitney" (default: "t-test")
- p_value_threshold: P-value threshold for significance (default: 0.05)
- fold_change_threshold: Fold change threshold (default: 1.5)

Return JSON only:
{{"missing_threshold": "0.5", "normalization": "log2", "scale": "true", "n_components": "10", "group_column": "Muscle loss", "method": "t-test", "p_value_threshold": "0.05", "fold_change_threshold": "1.5"}}
"""
        
        messages = [
            {"role": "system", "content": "You are a parameter extraction assistant. Return JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            logger.info(f"ğŸ” [CHECKPOINT] Calling LLM to extract workflow parameters...")
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=256)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            logger.info(f"âœ… [CHECKPOINT] LLM response received: {response[:200]}...")
            
            # è§£æ JSON
            import json
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            
            params = json.loads(json_str)
            logger.info(f"âœ… [CHECKPOINT] Parameters extracted: {list(params.keys())}")
            return params
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [CHECKPOINT] JSON decode error: {e}, response: {response[:200]}")
            return {}  # è¿”å›ç©ºå­—å…¸ï¼Œä½¿ç”¨é»˜è®¤å€¼
        except Exception as e:
            logger.error(f"âŒ [CHECKPOINT] Error extracting parameters: {e}", exc_info=True)
            return {}  # è¿”å›ç©ºå­—å…¸ï¼Œä½¿ç”¨é»˜è®¤å€¼
    
    async def _stream_chat_response(
        self,
        query: str,
        file_paths: List[str]
    ) -> AsyncIterator[str]:
        """
        æµå¼èŠå¤©å“åº”ï¼ˆæ”¯æŒ ReAct å¾ªç¯å’Œå·¥å…·è°ƒç”¨ï¼‰
        """
        context = {
            "context": f"Uploaded files: {', '.join(file_paths) if file_paths else 'None'}",
            "available_tools": list(self.metabolomics_tool.tool_map.keys()),
            "tool_descriptions": {
                "inspect_data": "æ£€æŸ¥ä»£è°¢ç»„å­¦æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ‘˜è¦ï¼ˆæ ·æœ¬æ•°ã€ä»£è°¢ç‰©æ•°ã€ç¼ºå¤±å€¼ã€åˆ†ç»„ä¿¡æ¯ç­‰ï¼‰",
                "preprocess_data": "é¢„å¤„ç†æ•°æ®ï¼šå¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–ã€ç¼©æ”¾",
                "pca_analysis": "æ‰§è¡Œä¸»æˆåˆ†åˆ†æ (PCA)",
                "differential_analysis": "æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æï¼ˆä¸¤ç»„æ¯”è¾ƒï¼‰",
                "visualize_pca": "ç”Ÿæˆ PCA å¯è§†åŒ–å›¾",
                "visualize_volcano": "ç”Ÿæˆç«å±±å›¾ï¼ˆVolcano Plotï¼‰"
            }
        }
        
        # å¦‚æœæœ‰æ–‡ä»¶ï¼Œå¼ºåˆ¶å…ˆæ£€æŸ¥ï¼ˆç¬¦åˆ SOPï¼‰
        inspection_result = None
        if file_paths:
            input_path = file_paths[0]
            try:
                inspection_result = self.metabolomics_tool.inspect_data(input_path)
                if "error" not in inspection_result:
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
                    inspection_summary = f"""
ã€Data Inspection Completedã€‘
- Samples: {inspection_result.get('n_samples', 'N/A')}
- Metabolites: {inspection_result.get('n_metabolites', 'N/A')}
- Missing values: {inspection_result.get('missing_values', {}).get('percentage', 'N/A')}%
- Group column: {inspection_result.get('group_info', {}).get('column', 'N/A')}
- Groups: {inspection_result.get('group_info', {}).get('groups', {})}
"""
                    # å…ˆè¾“å‡ºæ£€æŸ¥ç»“æœ
                    yield f"ğŸ” **Data Inspection Results:**\n{inspection_summary}\n\n"
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°æŸ¥è¯¢ä¸­ï¼Œè®© LLM åŸºäºæ­¤åˆ†æ
                    query = f"""{query}

{inspection_summary}

Based on the inspection results above, please:
1. Analyze the data characteristics
2. Propose appropriate analysis parameters
3. Ask for confirmation before proceeding with analysis
"""
            except Exception as e:
                logger.error(f"Error inspecting file: {e}", exc_info=True)
                yield f"âš ï¸ Warning: Could not inspect file: {str(e)}\n\n"
        
        # æ„å»ºå¢å¼ºçš„ç”¨æˆ·æŸ¥è¯¢ï¼ŒåŒ…å«å·¥å…·è¯´æ˜
        enhanced_query = f"""{query}

ã€Available Toolsã€‘
You have access to:
- inspect_data(file_path): Check data file structure (already executed above if files were provided)
- preprocess_data(file_path, missing_threshold, normalization, scale): Preprocess metabolomics data
- pca_analysis(n_components, file_path): Perform PCA analysis
- differential_analysis(group_column, file_path, method, p_value_threshold, fold_change_threshold): Find differential metabolites
- visualize_pca(group_column, pca_file, pc1, pc2): Generate PCA plot
- visualize_volcano(diff_file, p_value_threshold, fold_change_threshold): Generate volcano plot

ã€Workflow Ruleã€‘
- If user provides CSV files: Inspect first (already done above), then analyze and propose parameters
- Before running any analysis, you MUST have inspected the data first
- Now analyze the inspection results and propose parameters.
"""
        
        # å…ˆè¾“å‡ºä¸€ä¸ªæç¤ºï¼Œè®©ç”¨æˆ·çŸ¥é“ç³»ç»Ÿæ­£åœ¨å·¥ä½œ
        yield "ğŸ’­ æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚ï¼Œè¯·ç¨å€™...\n\n"
        
        # æµå¼è¾“å‡º LLM å“åº”
        try:
            has_content = False
            async for chunk in self.chat(enhanced_query, context, stream=True):
                if chunk:
                    has_content = True
                    yield chunk
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹è¾“å‡ºï¼Œè¯´æ˜å¯èƒ½å‡ºé”™äº†
            if not has_content:
                yield "\n\nâš ï¸ æŠ±æ­‰ï¼Œå“åº”ç”Ÿæˆå‡ºç°é—®é¢˜ã€‚è¯·é‡è¯•æˆ–æ£€æŸ¥æ—¥å¿—ã€‚"
        except Exception as e:
            logger.error(f"âŒ æµå¼å“åº”é”™è¯¯: {e}", exc_info=True)
            yield f"\n\nâŒ é”™è¯¯: {str(e)}\n\nè¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
    
    async def execute_workflow(
        self,
        workflow_config: Dict[str, Any],
        file_paths: List[str],
        output_dir: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œä»£è°¢ç»„å­¦å·¥ä½œæµ
        
        Args:
            workflow_config: å·¥ä½œæµé…ç½®
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        import os
        
        logger.info("=" * 80)
        logger.info("ğŸš€ [CHECKPOINT] execute_workflow START")
        logger.info(f"ğŸ“ Input file paths: {file_paths}")
        logger.info(f"ğŸ“‚ Output directory: {output_dir}")
        logger.info(f"âš™ï¸  Workflow config: {workflow_config.get('workflow_name', 'Unknown')}")
        logger.info("=" * 80)
        
        input_path = file_paths[0] if file_paths else None
        if not input_path:
            error_msg = "No input files provided"
            logger.error(f"âŒ [CHECKPOINT] execute_workflow FAILED: {error_msg}")
            raise ValueError(error_msg)
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        logger.info(f"ğŸ” [CHECKPOINT] Checking input file: {input_path}")
        logger.info(f"   File exists? {os.path.exists(input_path)}")
        if not os.path.exists(input_path):
            error_msg = f"Input file not found: {input_path}"
            logger.error(f"âŒ [CHECKPOINT] execute_workflow FAILED: {error_msg}")
            raise FileNotFoundError(error_msg)
        logger.info(f"   File size: {os.path.getsize(input_path)} bytes")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        logger.info(f"ğŸ“‚ [CHECKPOINT] Setting up output directory: {output_dir}")
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"   Created output directory: {output_dir}")
        else:
            logger.info(f"   Output directory already exists: {output_dir}")
        
        # æ›´æ–°ä»£è°¢ç»„å·¥å…·çš„è¾“å‡ºç›®å½•
        from pathlib import Path
        self.metabolomics_config["output_dir"] = output_dir
        self.metabolomics_tool.output_dir = Path(output_dir)
        self.metabolomics_tool.output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ… [CHECKPOINT] Metabolomics tool output_dir set to: {self.metabolomics_tool.output_dir}")
        
        # æ‰§è¡Œå·¥ä½œæµæ­¥éª¤
        steps = workflow_config.get("steps", [])
        steps_details = []
        final_plot = None
        
        logger.info(f"ğŸ“‹ [CHECKPOINT] Workflow has {len(steps)} steps to execute")
        for idx, step in enumerate(steps, 1):
            logger.info(f"   Step {idx}: {step.get('step_id')} ({step.get('tool_id')})")
        
        try:
            for step in steps:
                step_id = step.get("step_id")
                tool_id = step.get("tool_id")
                params = step.get("params", {})
                
                logger.info(f"ğŸ”§ [CHECKPOINT] Executing step {len(steps_details) + 1}/{len(steps)}: {step_id} ({tool_id})")
                logger.info(f"   Step params: {params}")
                
                if tool_id == "inspect_data":
                    file_path_to_inspect = params.get("file_path", input_path)
                    logger.info(f"ğŸ” [CHECKPOINT] inspect_data: Trying to read file at: {file_path_to_inspect}")
                    logger.info(f"   File exists? {os.path.exists(file_path_to_inspect)}")
                    if os.path.exists(file_path_to_inspect):
                        logger.info(f"   File size: {os.path.getsize(file_path_to_inspect)} bytes")
                    result = self.metabolomics_tool.inspect_data(file_path_to_inspect)
                    logger.info(f"âœ… [CHECKPOINT] inspect_data completed: {result.get('status', 'unknown')}")
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": f"æ£€æŸ¥å®Œæˆ: {result.get('n_samples', 'N/A')} ä¸ªæ ·æœ¬, {result.get('n_metabolites', 'N/A')} ä¸ªä»£è°¢ç‰©",
                        "data": result.get("data", {})  # åŒ…å« preview å’Œ summary
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "step_result": step_result  # å®Œæ•´çš„æ­¥éª¤ç»“æœ
                    })
                
                elif tool_id == "preprocess_data":
                    file_path_to_preprocess = params.get("file_path", input_path)
                    logger.info(f"ğŸ” [CHECKPOINT] preprocess_data: Trying to read file at: {file_path_to_preprocess}")
                    logger.info(f"   File exists? {os.path.exists(file_path_to_preprocess)}")
                    logger.info(f"   Parameters: missing_threshold={params.get('missing_threshold', '0.5')}, normalization={params.get('normalization', 'log2')}, scale={params.get('scale', 'true')}")
                    result = self.metabolomics_tool.preprocess_data(
                        file_path=file_path_to_preprocess,
                        missing_threshold=float(params.get("missing_threshold", "0.5")),
                        normalization=params.get("normalization", "log2"),
                        scale=params.get("scale", "true").lower() == "true"
                    )
                    logger.info(f"âœ… [CHECKPOINT] preprocess_data completed: {result.get('status', 'unknown')}")
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": result.get("message", "é¢„å¤„ç†å®Œæˆ"),
                        "data": result.get("data", {})  # åŒ…å« preview å’Œ summary
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "step_result": step_result
                    })
                
                elif tool_id == "pca_analysis":
                    # å¦‚æœä¸Šä¸€æ­¥æ˜¯é¢„å¤„ç†ï¼Œä½¿ç”¨é¢„å¤„ç†åçš„æ–‡ä»¶
                    preprocessed_file = None
                    for prev_step in steps_details:
                        if prev_step.get("tool_id") == "preprocess_data":
                            # ä»é¢„å¤„ç†ç»“æœä¸­è·å–æ–‡ä»¶è·¯å¾„
                            preprocessed_file = os.path.join(output_dir, "preprocessed_data.csv")
                            break
                    
                    result = self.metabolomics_tool.pca_analysis(
                        n_components=int(params.get("n_components", "10")),
                        file_path=preprocessed_file or params.get("file_path", input_path)
                    )
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": result.get("message", "PCA åˆ†æå®Œæˆ"),
                        "data": result.get("data", {})  # åŒ…å« preview å’Œ tables
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "step_result": step_result
                    })
                
                elif tool_id == "differential_analysis":
                    # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡ä»¶
                    preprocessed_file = os.path.join(output_dir, "preprocessed_data.csv")
                    if not os.path.exists(preprocessed_file):
                        preprocessed_file = input_path
                    
                    result = self.metabolomics_tool.differential_analysis(
                        group_column=params.get("group_column", "Muscle loss"),
                        file_path=preprocessed_file,
                        method=params.get("method", "t-test"),
                        p_value_threshold=float(params.get("p_value_threshold", "0.05")),
                        fold_change_threshold=float(params.get("fold_change_threshold", "1.5")),
                        group1=params.get("group1"),
                        group2=params.get("group2")
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ·é€‰æ‹©åˆ†ç»„
                    if result.get("status") == "need_selection":
                        return {
                            "status": "need_selection",
                            "message": result.get("message", "éœ€è¦é€‰æ‹©æ¯”è¾ƒçš„åˆ†ç»„"),
                            "groups": result.get("groups", []),
                            "steps_details": steps_details
                        }
                    
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": result.get("message", "å·®å¼‚åˆ†æå®Œæˆ"),
                        "data": result.get("data", {})  # åŒ…å« tables å’Œ summary
                    }
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": step_result["logs"],
                        "status": step_result["status"],
                        "details": result.get("summary", ""),
                        "step_result": step_result
                    })
                
                elif tool_id == "visualize_pca":
                    # ä½¿ç”¨ PCA åˆ†æç»“æœæ–‡ä»¶
                    pca_file = params.get("pca_file") or os.path.join(output_dir, "pca_results.csv")
                    if not os.path.exists(pca_file):
                        # å¦‚æœ PCA æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ­¥éª¤è¯¦æƒ…ä¸­è·å–
                        for prev_step in steps_details:
                            if prev_step.get("tool_id") == "pca_analysis":
                                pca_file = os.path.join(output_dir, "pca_results.csv")
                                break
                    
                    result = self.metabolomics_tool.visualize_pca(
                        group_column=params.get("group_column", "Muscle loss"),
                        pca_file=pca_file,
                        pc1=int(params.get("pc1", "1")),
                        pc2=int(params.get("pc2", "2"))
                    )
                    plot_path = result.get("plot_path") or result.get("plot_file")
                    relative_plot_path = None
                    if plot_path:
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº output_dirï¼‰
                        if os.path.isabs(plot_path):
                            relative_plot_path = os.path.relpath(plot_path, output_dir)
                        else:
                            relative_plot_path = plot_path
                        # ç¡®ä¿è·¯å¾„ä½¿ç”¨æ­£æ–œæ ï¼ˆWeb å…¼å®¹ï¼‰
                        relative_plot_path = relative_plot_path.replace("\\", "/")
                        final_plot = relative_plot_path
                    
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": "PCA å¯è§†åŒ–å®Œæˆ",
                        "data": result.get("data", {})  # åŒ…å« images æ•°ç»„
                    }
                    # å¦‚æœ result.data ä¸­æ²¡æœ‰ imagesï¼Œæ·»åŠ 
                    if "images" not in step_result["data"] and relative_plot_path:
                        step_result["data"]["images"] = [relative_plot_path]
                    
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": "PCA å¯è§†åŒ–å®Œæˆ",
                        "status": result.get("status", "success"),
                        "plot": relative_plot_path,
                        "step_result": step_result
                    })
                
                elif tool_id == "visualize_volcano":
                    # ä½¿ç”¨å·®å¼‚åˆ†æç»“æœæ–‡ä»¶
                    diff_file = params.get("diff_file") or os.path.join(output_dir, "differential_results.csv")
                    if not os.path.exists(diff_file):
                        # å¦‚æœå·®å¼‚åˆ†ææ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ­¥éª¤è¯¦æƒ…ä¸­è·å–
                        for prev_step in steps_details:
                            if prev_step.get("tool_id") == "differential_analysis":
                                diff_file = os.path.join(output_dir, "differential_results.csv")
                                break
                    
                    result = self.metabolomics_tool.visualize_volcano(
                        diff_file=diff_file,
                        p_value_threshold=float(params.get("p_value_threshold", "0.05")),
                        fold_change_threshold=float(params.get("fold_change_threshold", "1.5"))
                    )
                    plot_path = result.get("plot_path") or result.get("plot_file")
                    relative_plot_path = None
                    if plot_path:
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº output_dirï¼‰
                        if os.path.isabs(plot_path):
                            relative_plot_path = os.path.relpath(plot_path, output_dir)
                        else:
                            relative_plot_path = plot_path
                        # ç¡®ä¿è·¯å¾„ä½¿ç”¨æ­£æ–œæ ï¼ˆWeb å…¼å®¹ï¼‰
                        relative_plot_path = relative_plot_path.replace("\\", "/")
                        if not final_plot:
                            final_plot = relative_plot_path
                    
                    step_result = {
                        "step_name": step.get("desc", step_id),
                        "status": result.get("status", "success"),
                        "logs": "ç«å±±å›¾å¯è§†åŒ–å®Œæˆ",
                        "data": result.get("data", {})  # åŒ…å« images æ•°ç»„
                    }
                    # å¦‚æœ result.data ä¸­æ²¡æœ‰ imagesï¼Œæ·»åŠ 
                    if "images" not in step_result["data"] and relative_plot_path:
                        step_result["data"]["images"] = [relative_plot_path]
                    
                    steps_details.append({
                        "step_id": step_id,
                        "tool_id": tool_id,
                        "name": step.get("desc", step_id),
                        "summary": "ç«å±±å›¾å¯è§†åŒ–å®Œæˆ",
                        "status": result.get("status", "success"),
                        "plot": relative_plot_path,
                        "step_result": step_result
                    })
            
            # æ„å»º steps_results åˆ—è¡¨ï¼ˆå‰ç«¯å¯ç›´æ¥ä½¿ç”¨ï¼‰
            steps_results = []
            for step_detail in steps_details:
                if "step_result" in step_detail:
                    steps_results.append(step_detail["step_result"])
                else:
                    # å…¼å®¹æ—§æ ¼å¼
                    steps_results.append({
                        "step_name": step_detail.get("name", "Unknown"),
                        "status": step_detail.get("status", "success"),
                        "logs": step_detail.get("summary", ""),
                        "data": {}
                    })
            
            logger.info("=" * 80)
            logger.info("âœ… [CHECKPOINT] execute_workflow SUCCESS")
            logger.info(f"ğŸ“Š Completed {len(steps_details)} steps")
            logger.info(f"ğŸ“‚ Output directory: {output_dir}")
            if final_plot:
                logger.info(f"ğŸ–¼ï¸  Final plot: {final_plot}")
            logger.info("=" * 80)
            
            # ğŸ”¥ æ„å»ºè¿”å›ç»“æœ
            workflow_result = {
                "status": "success",
                "workflow_name": workflow_config.get("workflow_name", "Metabolomics Analysis"),
                "steps_details": steps_details,  # ä¿ç•™æ—§æ ¼å¼ä»¥å…¼å®¹
                "steps_results": steps_results,  # æ–°çš„æ ¼å¼ï¼Œå‰ç«¯å¯ç›´æ¥ä½¿ç”¨
                "final_plot": final_plot,
                "output_dir": output_dir
            }
            
            # ğŸ”¥ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨ï¼ˆå¤„ç† Numpy ç±»å‹ã€NaN/Infinity ç­‰ï¼‰
            logger.info("âœ… Workflow finished. Sanitizing data for JSON serialization...")
            sanitized_result = sanitize_for_json(workflow_result)
            logger.info("âœ… Data sanitization completed. Returning result to frontend.")
            
            return sanitized_result
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error("âŒ [CHECKPOINT] execute_workflow FAILED")
            logger.error(f"âŒ Error type: {type(e).__name__}")
            logger.error(f"âŒ Error message: {str(e)}")
            logger.error(f"âŒ Completed steps: {len(steps_details)}/{len(steps)}")
            logger.error(f"ğŸ“‚ Output directory: {output_dir}")
            logger.error("âŒ Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            
            # è¿”å›é”™è¯¯ä¿¡æ¯ç»™å‰ç«¯ï¼ˆä¹Ÿéœ€è¦æ¸…ç†ï¼‰
            error_result = {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "error_traceback": error_traceback,
                "steps_details": steps_details,
                "output_dir": output_dir,
                "message": f"âŒ å·¥ä½œæµæ‰§è¡Œå‡ºé”™: {str(e)}\n\n(è¯·æŸ¥çœ‹åå°æ—¥å¿—è·å–è¯¦ç»†å †æ ˆ)"
            }
            
            # æ¸…ç†é”™è¯¯ç»“æœï¼ˆè™½ç„¶é”™è¯¯ç»“æœé€šå¸¸ä¸åŒ…å« Numpy æ•°æ®ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼‰
            return sanitize_for_json(error_result)

