"""
è½¬å½•ç»„æ™ºèƒ½ä½“ï¼ˆRNA Agentï¼‰
å¤„ç†å•ç»†èƒè½¬å½•ç»„ï¼ˆscRNA-seqï¼‰å’Œ Bulk RNA-seq åˆ†æ
é‡æ„è‡ªç°æœ‰çš„ BioBlendAgent
"""
import json
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, AsyncIterator
from ..base_agent import BaseAgent
from ...core.llm_client import LLMClient
from ...core.prompt_manager import PromptManager, RNA_REPORT_PROMPT
from ...core.utils import sanitize_for_json
from ...core.dispatcher import TaskDispatcher
from ...core.test_data_manager import TestDataManager
from ...core.tool_retriever import ToolRetriever
from ...core.planner import RNAPlanner
from ...core.tool_registry import registry
# å¯¼å…¥æ–°å·¥å…·å‡½æ•°
from ...tools.general.file_inspector import inspect_file
from ...tools.rna.upstream import run_cellranger_count, convert_cellranger_to_h5ad
from ...tools.rna.quality_control import run_qc_filter
from ...tools.rna.analysis import run_normalize, run_hvg, run_pca, run_neighbors, run_umap, run_clustering
from ...tools.rna.annotation import run_cell_annotation
from ...tools.rna.plotting import visualize_qc, visualize_clustering
import scanpy as sc
import logging

logger = logging.getLogger(__name__)


# ğŸ”¥ æ¶æ„é‡æ„ï¼šé¢†åŸŸç‰¹å®šçš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆç­–ç•¥æ¨¡å¼ï¼‰
RNA_INSTRUCTION = """You are a Senior Bioinformatician specializing in Single-Cell RNA-seq analysis.

**CRITICAL CONSTRAINTS:**
- The data represents **Gene Expression** (RNA transcripts), NOT Metabolite Abundance.
- Rows = Cells (Single Cells), Columns = Genes (Gene Expression).
- This is RNA sequencing data, measuring transcript counts per cell.

**REQUIRED TERMINOLOGY:**
- Cell, Cells, Single Cell, Cellular
- Gene, Genes, Gene Expression, Transcript
- Mitochondria, Mitochondrial (mt-genes)
- scRNA-seq, Single-Cell RNA-seq, scRNA
- Transcriptomics, Transcriptome
- UMI, Count Matrix, Expression Matrix

**CONTEXT:**
This is single-cell transcriptomics data representing gene expression levels measured by RNA sequencing.

Generate data diagnosis and parameter recommendations in Simplified Chinese (ç®€ä½“ä¸­æ–‡).
Focus on single-cell-specific quality metrics (cells, genes, mitochondrial percentage, doublet rate)."""


class RNAAgent(BaseAgent):
    """
    è½¬å½•ç»„æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    1. å¤„ç†å•ç»†èƒè½¬å½•ç»„åˆ†æï¼ˆscRNA-seqï¼‰
    2. å¤„ç† Bulk RNA-seq åˆ†æ
    3. ç”Ÿæˆå·¥ä½œæµè„šæœ¬
    4. é€šè¿‡ TaskDispatcher æäº¤ä»»åŠ¡
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        prompt_manager: PromptManager,
        dispatcher: Optional[TaskDispatcher] = None,
        cellranger_config: Optional[Dict[str, Any]] = None,
        scanpy_config: Optional[Dict[str, Any]] = None,
        test_data_dir: Optional[str] = None,
        tool_retriever: Optional[ToolRetriever] = None
    ):
        """åˆå§‹åŒ–è½¬å½•ç»„æ™ºèƒ½ä½“"""
        super().__init__(llm_client, prompt_manager, "rna_expert")
        
        self.dispatcher = dispatcher
        self.cellranger_config = cellranger_config or {}
        self.scanpy_config = scanpy_config or {}
        # ğŸ”¥ æ¶æ„å‡çº§ï¼šç§»é™¤æ—§å·¥å…·ï¼Œä½¿ç”¨æ–°æ¨¡å—åŒ–å·¥å…·ç³»ç»Ÿ
        # åˆå§‹åŒ–æµ‹è¯•æ•°æ®ç®¡ç†å™¨
        self.test_data_manager = TestDataManager(test_data_dir)
        
        # ğŸ”¥ æ¶æ„å‡çº§ï¼šåˆå§‹åŒ– SOP é©±åŠ¨çš„åŠ¨æ€è§„åˆ’å™¨
        self.sop_planner = None
        if tool_retriever:
            try:
                # åˆ›å»º scRNA-seq ç‰¹å®šçš„ SOPPlannerï¼ˆéœ€è¦è‡ªå®šä¹‰ SOP è§„åˆ™ï¼‰
                self.sop_planner = RNAPlanner(tool_retriever, llm_client)
                logger.info("âœ… [RNAAgent] RNAPlanner å·²åˆå§‹åŒ–")
            except Exception as e:
                logger.warning(f"âš ï¸ [RNAAgent] RNAPlanner åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨å›é€€é€»è¾‘: {e}")
        else:
            logger.info("â„¹ï¸ [RNAAgent] æœªæä¾› ToolRetrieverï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿå·¥ä½œæµç”Ÿæˆé€»è¾‘")
        
        # ğŸ”¥ å·¥å…·IDæ˜ å°„è¡¨ï¼šæ—§ID -> æ–°IDï¼ˆå·²æ³¨å†Œçš„å·¥å…·åç§°ï¼‰
        self.tool_id_mapping = {
            "local_qc": "rna_qc_filter",
            "local_normalize": "rna_normalize",
            "local_hvg": "rna_hvg",
            "local_scale": "rna_scale",
            "local_pca": "rna_pca",
            "local_neighbors": "rna_neighbors",
            "local_cluster": "rna_clustering",
            "local_umap": "rna_umap",
            "local_tsne": "rna_tsne",
            "local_markers": "rna_find_markers",
        }
        
        # æ ‡å‡†å·¥ä½œæµæ­¥éª¤ï¼ˆåæ­¥æµç¨‹ï¼‰- ä½¿ç”¨æ–°çš„å·¥å…·ID
        self.workflow_steps = [
            {"name": "1. Quality Control", "tool_id": "rna_qc_filter", "desc": "è¿‡æ»¤ä½è´¨é‡ç»†èƒå’ŒåŸºå› "},
            {"name": "2. Normalization", "tool_id": "rna_normalize", "desc": "æ•°æ®æ ‡å‡†åŒ–"},
            {"name": "3. Find Variable Genes", "tool_id": "rna_hvg", "desc": "ç­›é€‰é«˜å˜åŸºå› "},
            {"name": "4. Scale Data", "tool_id": "rna_scale", "desc": "æ•°æ®ç¼©æ”¾"},
            {"name": "5. PCA", "tool_id": "rna_pca", "desc": "ä¸»æˆåˆ†åˆ†æ"},
            {"name": "6. Compute Neighbors", "tool_id": "rna_neighbors", "desc": "æ„å»ºé‚»æ¥å›¾"},
            {"name": "7. Clustering", "tool_id": "rna_clustering", "desc": "Leiden èšç±»"},
            {"name": "8. UMAP Visualization", "tool_id": "rna_umap", "desc": "UMAP å¯è§†åŒ–"},
            {"name": "9. t-SNE Visualization", "tool_id": "rna_tsne", "desc": "t-SNE å¯è§†åŒ–"},
            {"name": "10. Find Markers", "tool_id": "rna_find_markers", "desc": "å¯»æ‰¾ Marker åŸºå› "},
        ]
    
    async def process_query(
        self,
        query: str,
        history: List[Dict[str, str]] = None,
        uploaded_files: List[Dict[str, str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼Œå¯èƒ½åŒ…å«ï¼š
            - workflow_config: å·¥ä½œæµé…ç½®ï¼ˆJSONï¼‰
            - chat_response: èŠå¤©å“åº”ï¼ˆæµå¼ï¼‰
            - task_submitted: ä»»åŠ¡æäº¤ä¿¡æ¯
            - test_data_selection: æµ‹è¯•æ•°æ®é€‰æ‹©è¯·æ±‚
        """
        query_lower = query.lower().strip()
        file_paths = self.get_file_paths(uploaded_files or [])
        
        # ğŸ”¥ Task 1: LLM é©±åŠ¨çš„æ„å›¾æ£€æµ‹ï¼ˆåœ¨ç”Ÿæˆå·¥ä½œæµä¹‹å‰ï¼‰
        # ğŸ”’ å®‰å…¨åŒ…è£…ï¼šå¦‚æœæ„å›¾æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘
        intent = "chat"  # é»˜è®¤å€¼
        intent_result = None
        try:
            intent_result = await self._detect_intent_with_llm(query, file_paths, uploaded_files)
            intent = intent_result.get("intent", "chat")
            reasoning = intent_result.get("reasoning", "")
            logger.info(f"ğŸ¯ æ„å›¾æ£€æµ‹ç»“æœ: {intent} (æ¨ç†: {reasoning})")
        except Exception as e:
            logger.warning(f"âš ï¸ æ„å›¾æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘: {e}", exc_info=True)
            # å›é€€åˆ°åŸå§‹çš„å·¥ä½œæµæ£€æµ‹é€»è¾‘
            intent = None  # æ ‡è®°ä¸ºæœªæ£€æµ‹ï¼Œä½¿ç”¨å›é€€é€»è¾‘
        
        # å¦‚æœæ„å›¾æ£€æµ‹æˆåŠŸä¸”ä¸º explain_fileï¼Œå¤„ç†æ–‡ä»¶è§£é‡Š
        if intent == "explain_file":
            # è§£é‡Šæ–‡ä»¶ï¼šæ£€æŸ¥æ–‡ä»¶å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Š
            if not file_paths:
                return {
                    "type": "chat",
                    "response": self._stream_string_response("æ²¡æœ‰æ£€æµ‹åˆ°ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·å…ˆä¸Šä¼ æ–‡ä»¶åå†è¯¢é—®ã€‚")
                }
            
            # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆåˆ—è¡¨æœ€åä¸€ä¸ªï¼‰ï¼Œè€Œä¸æ˜¯ç¬¬ä¸€ä¸ª
            # æ£€æŸ¥æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå¦‚æœæ˜¯ h5adï¼Œä½¿ç”¨ scanpy å·¥å…·ï¼‰
            input_path = file_paths[-1] if file_paths else None
            if not input_path:
                return {
                    "type": "chat",
                    "response": self._stream_string_response("æ²¡æœ‰æ£€æµ‹åˆ°ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·å…ˆä¸Šä¼ æ–‡ä»¶åå†è¯¢é—®ã€‚")
                }
            try:
                # ä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿ
                if input_path.endswith('.h5ad'):
                    adata = sc.read_h5ad(input_path)
                    summary = f"""
æ–‡ä»¶ç±»å‹: H5AD (AnnData)
- ç»†èƒæ•°: {adata.n_obs}
- åŸºå› æ•°: {adata.n_vars}
- è§‚å¯Ÿå˜é‡: {list(adata.obs.columns) if hasattr(adata, 'obs') else 'None'}
- å˜é‡å˜é‡: {list(adata.var.columns) if hasattr(adata, 'var') else 'None'}
"""
                    explanation = await self._explain_file_with_llm(query, summary, input_path)
                    return {
                        "type": "chat",
                        "response": self._stream_string_response(explanation)
                    }
                else:
                    # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼Œè¯»å–æ–‡ä»¶å†…å®¹å¹¶ä½¿ç”¨ LLM è§£é‡Š
                    try:
                        # ä½¿ç”¨ file_inspector è¯»å–æ–‡ä»¶å…ƒæ•°æ®å’Œå†…å®¹
                        from ..core.file_inspector import FileInspector
                        import os
                        
                        # è·å–ä¸Šä¼ ç›®å½•
                        upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
                        file_inspector = FileInspector(upload_dir)
                        
                        # è·å–æ–‡ä»¶å…ƒæ•°æ®
                        file_name = os.path.basename(input_path)
                        metadata = file_inspector.generate_metadata(file_name)
                        
                        # è¯»å–æ–‡ä»¶å‰å‡ è¡Œä½œä¸ºå†…å®¹é¢„è§ˆ
                        file_path_obj = Path(input_path)
                        if not file_path_obj.is_absolute():
                            file_path_obj = Path(upload_dir) / file_name
                        
                        file_summary = f"æ–‡ä»¶è·¯å¾„: {input_path}\næ–‡ä»¶ç±»å‹: {os.path.splitext(input_path)[1]}\n"
                        
                        if metadata:
                            file_summary += f"æ–‡ä»¶å¤§å°: {metadata.get('size_mb', 'unknown')} MB\n"
                            if metadata.get('estimated_cells'):
                                file_summary += f"ä¼°ç®—ç»†èƒæ•°: {metadata.get('estimated_cells')}\n"
                            if metadata.get('estimated_genes'):
                                file_summary += f"ä¼°ç®—åŸºå› æ•°: {metadata.get('estimated_genes')}\n"
                        
                        # è¯»å–æ–‡ä»¶å‰å‡ è¡Œ
                        try:
                            if file_path_obj.exists() and file_path_obj.is_file():
                                head_lines = file_inspector._read_head(file_path_obj, 10)
                                if head_lines:
                                    file_summary += f"\næ–‡ä»¶å†…å®¹é¢„è§ˆï¼ˆå‰10è¡Œï¼‰ï¼š\n"
                                    for i, line in enumerate(head_lines[:10], 1):
                                        file_summary += f"{i}: {line[:200]}\n"  # é™åˆ¶æ¯è¡Œé•¿åº¦
                        except Exception as e:
                            logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
                            file_summary += "\nï¼ˆæ— æ³•è¯»å–æ–‡ä»¶å†…å®¹ï¼‰\n"
                        
                        # ä½¿ç”¨ LLM ç”Ÿæˆæ–‡ä»¶è§£é‡Š
                        explanation = await self._explain_file_with_llm(query, file_summary, input_path)
                        return {
                            "type": "chat",
                            "response": self._stream_string_response(explanation)
                        }
                    except Exception as e:
                        logger.error(f"âŒ æ–‡ä»¶è§£é‡Šå¤±è´¥: {e}", exc_info=True)
                        # å›é€€åˆ°åŸºæœ¬ä¿¡æ¯
                        return {
                            "type": "chat",
                            "response": self._stream_string_response(f"æ–‡ä»¶è·¯å¾„: {input_path}\næ–‡ä»¶ç±»å‹: {os.path.splitext(input_path)[1]}\n\nï¼ˆæ–‡ä»¶å†…å®¹è¯»å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼‰")
                        }
            except Exception as e:
                logger.error(f"âŒ æ–‡ä»¶è§£é‡Šå¤±è´¥: {e}", exc_info=True)
                return {
                    "type": "chat",
                    "response": self._stream_string_response(f"æ–‡ä»¶æ£€æŸ¥æ—¶å‡ºé”™: {str(e)}")
                }
        
        # æ™ºèƒ½æ•°æ®æ£€æµ‹ï¼šå¦‚æœéœ€è¦ Cell Ranger ä½†æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œæä¾›æµ‹è¯•æ•°æ®é€‰æ‹©
        needs_cellranger = self._needs_cellranger(query_lower)
        if needs_cellranger and not file_paths:
            # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•æ•°æ®å¯ç”¨
            test_datasets = self.test_data_manager.scan_test_datasets()
            if test_datasets:
                # è¿”å›æµ‹è¯•æ•°æ®é€‰æ‹©è¯·æ±‚
                return {
                    "type": "test_data_selection",
                    "message": "æ£€æµ‹åˆ°æ‚¨æ²¡æœ‰ä¸Šä¼ ç›¸å…³æ•°æ®ã€‚è¯·é€‰æ‹©ï¼š",
                    "options": [
                        "1. ä½¿ç”¨æœ¬åœ°æµ‹è¯•æ•°æ®é›†",
                        "2. ä¸Šä¼ æ‚¨è‡ªå·±çš„æ•°æ®"
                    ],
                    "datasets": test_datasets,
                    "datasets_json": self.test_data_manager.format_datasets_for_selection(test_datasets),
                    "datasets_display": self.test_data_manager.format_datasets_for_display(test_datasets)
                }
            else:
                # æ²¡æœ‰æµ‹è¯•æ•°æ®ï¼Œæç¤ºç”¨æˆ·ä¸Šä¼ 
                return {
                    "type": "chat",
                    "response": self._stream_string_response(
                        "æ£€æµ‹åˆ°æ‚¨æ²¡æœ‰ä¸Šä¼ ç›¸å…³æ•°æ®ï¼Œä¸”æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®é›†ã€‚\n"
                        "è¯·ä¸Šä¼  FASTQ æ–‡ä»¶æˆ– .h5ad æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚"
                    )
                }
        
        # å¤„ç†æµ‹è¯•æ•°æ®é€‰æ‹©ï¼ˆç”¨æˆ·é€šè¿‡ JSON é€‰æ‹©ï¼‰
        if "test_dataset_id" in kwargs:
            dataset_id = kwargs["test_dataset_id"]
            dataset = self.test_data_manager.get_dataset_by_id(dataset_id)
            if dataset:
                # ä½¿ç”¨é€‰å®šçš„æµ‹è¯•æ•°æ®
                if dataset.get("fastq_dir") and dataset.get("reference"):
                    # æœ‰ FASTQ å’Œå‚è€ƒåŸºå› ç»„ï¼Œä½¿ç”¨å®ƒä»¬
                    file_paths = [dataset["fastq_dir"]]
                    # å°†å‚è€ƒåŸºå› ç»„è·¯å¾„æ·»åŠ åˆ°é…ç½®ä¸­
                    self.cellranger_config["reference"] = dataset["reference"]
                elif dataset.get("h5ad_file"):
                    # åªæœ‰ .h5ad æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                    file_paths = [dataset["h5ad_file"]]
                else:
                    return {
                        "type": "chat",
                        "response": self._stream_string_response(
                            f"æµ‹è¯•æ•°æ®é›† {dataset['name']} ä¸å¯ç”¨ã€‚"
                        )
                    }
        
        # ğŸ”’ å›é€€é€»è¾‘ï¼šå¦‚æœæ„å›¾æ£€æµ‹å¤±è´¥æˆ–æ„å›¾ä¸æ˜ç¡®ï¼Œä½¿ç”¨åŸå§‹é€»è¾‘
        if intent is None or intent == "chat":
            # ä½¿ç”¨åŸå§‹çš„å·¥ä½œæµæ£€æµ‹é€»è¾‘ä½œä¸ºå›é€€
            is_workflow_request = self._is_workflow_request(query_lower, file_paths)
            if is_workflow_request:
                return await self._generate_workflow_config(query, file_paths)
            else:
                # æ™®é€šèŠå¤©
                return {
                    "type": "chat",
                    "response": self._stream_chat_response(query, file_paths)
                }
        
        # å¦‚æœæ„å›¾æ˜ç¡®ä¸º run_workflowï¼Œç›´æ¥ç”Ÿæˆå·¥ä½œæµé…ç½®
        elif intent == "run_workflow":
            return await self._generate_workflow_config(query, file_paths)
        
        # é»˜è®¤ï¼šæ™®é€šèŠå¤©
        else:
            return {
                "type": "chat",
                "response": self._stream_chat_response(query, file_paths)
            }
    
    async def _detect_intent_with_llm(
        self,
        query: str,
        file_paths: List[str],
        uploaded_files: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æ£€æµ‹ç”¨æˆ·æ„å›¾
        
        Returns:
            {
                "intent": "explain_file" | "run_workflow" | "chat",
                "reasoning": "..."
            }
        """
        import json
        import os
        
        # æå–æ–‡ä»¶å
        file_names = []
        if uploaded_files:
            for f in uploaded_files:
                name = f.get("name") or f.get("file_name", "")
                if name:
                    file_names.append(name)
        elif file_paths:
            for path in file_paths:
                file_names.append(os.path.basename(path))
        
        file_names_str = ", ".join(file_names) if file_names else "None"
        
        prompt = f"""åˆ†æç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­ç”¨æˆ·æ„å›¾ã€‚

User Input: {query}
Uploaded Files: {file_names_str}

è¯·å°†æ„å›¾åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ä¹‹ä¸€ï¼š
1. "explain_file" - ç”¨æˆ·æƒ³è¦äº†è§£æ–‡ä»¶å†…å®¹ã€ç»“æ„æˆ–å«ä¹‰ï¼ˆä¾‹å¦‚ï¼š"è¿™æ˜¯ä»€ä¹ˆæ–‡ä»¶ï¼Ÿ"ã€"æ–‡ä»¶é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"ã€"è§£é‡Šä¸€ä¸‹è¿™ä¸ªæ•°æ®"ï¼‰
2. "run_workflow" - ç”¨æˆ·æƒ³è¦æ‰§è¡Œåˆ†æå·¥ä½œæµï¼ˆä¾‹å¦‚ï¼š"åˆ†æä¸€ä¸‹"ã€"è¿è¡Œå·¥ä½œæµ"ã€"åšä¸€ä¸‹åˆ†æ"ã€"å¤„ç†è¿™ä¸ªæ–‡ä»¶"ï¼‰
3. "chat" - æ™®é€šå¯¹è¯æˆ–è¯¢é—®ï¼ˆä¾‹å¦‚ï¼š"ä½ å¥½"ã€"å¦‚ä½•ä½¿ç”¨"ã€"ä»‹ç»åŠŸèƒ½"ï¼‰

è¿”å› JSON æ ¼å¼ï¼š
{{
    "intent": "explain_file" | "run_workflow" | "chat",
    "reasoning": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±"
}}"""
        
        messages = [
            {"role": "system", "content": "You are an intent classification assistant. Return JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=128)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            # è§£æ JSON
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0].strip()
            
            result = json.loads(json_str)
            
            # éªŒè¯æ„å›¾å€¼
            valid_intents = ["explain_file", "run_workflow", "chat"]
            if result.get("intent") not in valid_intents:
                logger.warning(f"âš ï¸ LLM è¿”å›äº†æ— æ•ˆæ„å›¾: {result.get('intent')}, ä½¿ç”¨é»˜è®¤å€¼ 'chat'")
                result["intent"] = "chat"
            
            return result
        except Exception as e:
            logger.error(f"âŒ æ„å›¾æ£€æµ‹å¤±è´¥: {e}", exc_info=True)
            # é»˜è®¤è¿”å› chat
            return {
                "intent": "chat",
                "reasoning": f"Intent detection failed: {str(e)}"
            }
    
    async def _explain_file_with_llm(
        self,
        query: str,
        file_summary: str,
        file_path: str
    ) -> str:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆæ–‡ä»¶è§£é‡Š
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            file_summary: æ–‡ä»¶æ‘˜è¦ä¿¡æ¯
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            è‡ªç„¶è¯­è¨€çš„æ–‡ä»¶è§£é‡Š
        """
        prompt = f"""ç”¨æˆ·è¯¢é—®å…³äºæ–‡ä»¶çš„é—®é¢˜ã€‚

User Query: {query}
File Path: {file_path}

æ–‡ä»¶æ‘˜è¦ä¿¡æ¯ï¼š
{file_summary}

è¯·ç”¨è‡ªç„¶è¯­è¨€è§£é‡Šè¿™ä¸ªæ–‡ä»¶çš„å†…å®¹ã€ç»“æ„å’Œç‰¹ç‚¹ã€‚å›ç­”åº”è¯¥ï¼š
1. ç®€æ´æ˜äº†ï¼Œæ˜“äºç†è§£
2. åŒ…å«å…³é”®ä¿¡æ¯ï¼ˆç»†èƒæ•°ã€åŸºå› æ•°ç­‰ï¼‰
3. å¦‚æœç”¨æˆ·æœ‰ç‰¹å®šé—®é¢˜ï¼Œé’ˆå¯¹æ€§åœ°å›ç­”
4. ä½¿ç”¨ä¸­æ–‡å›ç­”

å›ç­”ï¼š"""
        
        messages = [
            {"role": "system", "content": "You are a bioinformatics data expert. Explain file contents in natural language."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=800)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            return response
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è§£é‡Šç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return f"æ–‡ä»¶è§£é‡Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _stream_string_response(self, text: str) -> AsyncIterator[str]:
        """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆç”¨äºæµå¼å“åº”ï¼‰"""
        async def _generator():
            yield text
        return _generator()
    
    def _is_workflow_request(self, query: str, file_paths: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å·¥ä½œæµè¯·æ±‚"""
        workflow_keywords = [
            "è§„åˆ’", "æµç¨‹", "workflow", "pipeline", "åˆ†æ", "run",
            "æ‰§è¡Œ", "plan", "åšä¸€ä¸‹", "è·‘ä¸€ä¸‹", "åˆ†æä¸€ä¸‹"
        ]
        
        bio_keywords = [
            "pca", "umap", "tsne", "qc", "è´¨æ§", "èšç±»", "cluster"
        ]
        
        if any(kw in query for kw in workflow_keywords):
            return True
        
        if file_paths and any(kw in query for kw in bio_keywords):
            return True
        
        if file_paths and (not query or len(query) < 5):
            return True
        
        return False
    
    def _needs_cellranger(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ Cell Rangerï¼ˆåŸºäºæŸ¥è¯¢å…³é”®è¯ï¼‰"""
        cellranger_keywords = [
            "cellranger", "cell ranger", "fastq", "fq", "æµ‹åº",
            "ç¬¬ä¸€æ­¥", "å…¨æµç¨‹", "å®Œæ•´æµç¨‹", "ä»fastq", "ä»æµ‹åº"
        ]
        return any(kw in query for kw in cellranger_keywords)
    
    async def _generate_workflow_config(
        self,
        query: str,
        file_paths: List[str]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµé…ç½®
        
        ğŸ”¥ æ¶æ„å‡çº§ï¼šä¼˜å…ˆä½¿ç”¨ SOP é©±åŠ¨çš„åŠ¨æ€è§„åˆ’å™¨ï¼Œå¤±è´¥åˆ™å›é€€åˆ°ä¼ ç»Ÿé€»è¾‘
        
        æµç¨‹ï¼š
        1. å¦‚æœ RNAPlanner å¯ç”¨ï¼Œä½¿ç”¨åŠ¨æ€è§„åˆ’å™¨ç”Ÿæˆå·¥ä½œæµ
        2. å¦åˆ™ï¼Œä½¿ç”¨ä¼ ç»Ÿç¡¬ç¼–ç é€»è¾‘ï¼ˆå›é€€ï¼‰
        """
        logger.info("=" * 80)
        logger.info("ğŸš€ [RNAAgent] _generate_workflow_config START")
        logger.info(f"   Query: {query}")
        logger.info(f"   File paths: {file_paths}")
        logger.info(f"   RNAPlanner available: {self.sop_planner is not None}")
        logger.info("=" * 80)
        
        # ğŸ”¥ Phase 3: ä¼˜å…ˆä½¿ç”¨ SOP é©±åŠ¨çš„åŠ¨æ€è§„åˆ’å™¨
        if self.sop_planner:
            try:
                logger.info("ğŸ§  [RNAPlanner] å°è¯•ä½¿ç”¨åŠ¨æ€è§„åˆ’å™¨ç”Ÿæˆå·¥ä½œæµ...")
                
                # Step 1: æ£€æŸ¥æ–‡ä»¶è·å–å…ƒæ•°æ®
                file_metadata = None
                if file_paths:
                    try:
                        from ...core.file_inspector import FileInspector
                        import os
                        upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
                        inspector = FileInspector(upload_dir)
                        file_metadata = inspector.inspect_file(file_paths[0])
                        
                        if file_metadata.get("status") != "success":
                            logger.warning(f"âš ï¸ æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {file_metadata.get('error')}")
                            file_metadata = None
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ–‡ä»¶æ£€æŸ¥å¼‚å¸¸: {e}")
                
                # Step 2: ä½¿ç”¨ RNAPlanner ç”Ÿæˆè®¡åˆ’
                if file_metadata:
                    plan_result = await self.sop_planner.generate_plan(
                        user_query=query,
                        file_metadata=file_metadata,
                        category_filter="scRNA-seq"
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                    if plan_result.get("type") != "error":
                        logger.info("âœ… [RNAPlanner] åŠ¨æ€è§„åˆ’æˆåŠŸ")
                        
                        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
                        diagnosis_report = None
                        try:
                            diagnosis_report = await self._perform_data_diagnosis(
                                file_metadata=file_metadata,
                                omics_type="scRNA",
                                system_instruction=RNA_INSTRUCTION
                            )
                        except Exception as e:
                            logger.warning(f"âš ï¸ è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
                        
                        # æ·»åŠ è¯Šæ–­æŠ¥å‘Šåˆ°ç»“æœ
                        if diagnosis_report:
                            plan_result["diagnosis_report"] = diagnosis_report
                        
                        # ğŸ”¥ TASK 5: æ·»åŠ å‚æ•°æ¨èåˆ°ç»“æœ
                        if hasattr(self, 'context') and "parameter_recommendation" in self.context:
                            recommendation = self.context.get("parameter_recommendation")
                            if recommendation:
                                plan_result["recommendation"] = recommendation
                                logger.info(f"âœ… [RNAAgent] æ·»åŠ å‚æ•°æ¨èåˆ°ç»“æœ: {len(recommendation.get('params', {}))} ä¸ªå‚æ•°")
                        
                        return plan_result
                    else:
                        logger.warning(f"âš ï¸ [RNAPlanner] è§„åˆ’å¤±è´¥: {plan_result.get('error')}")
                else:
                    logger.warning("âš ï¸ [RNAPlanner] æ–‡ä»¶å…ƒæ•°æ®ä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿé€»è¾‘")
            
            except Exception as e:
                logger.error(f"âŒ [RNAPlanner] åŠ¨æ€è§„åˆ’å¼‚å¸¸: {e}", exc_info=True)
                logger.info("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿå·¥ä½œæµç”Ÿæˆé€»è¾‘...")
        
        # ğŸ”„ å›é€€ï¼šä½¿ç”¨ä¼ ç»Ÿç¡¬ç¼–ç é€»è¾‘
        logger.info("ğŸ“‹ [Fallback] ä½¿ç”¨ä¼ ç»Ÿå·¥ä½œæµç”Ÿæˆé€»è¾‘...")
        
        # ğŸ”¥ Step 1: æ–‡ä»¶æ£€æŸ¥å’Œæ•°æ®è¯Šæ–­ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ BaseAgent æ–¹æ³•ï¼‰
        inspection_result = None
        diagnosis_report = None
        if file_paths:
            input_path = file_paths[0]
            try:
                # ä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿ
                if input_path.endswith('.h5ad'):
                    # å¯¹äº H5AD æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½å¹¶æ£€æŸ¥
                    adata = sc.read_h5ad(input_path)
                    inspection_result = {
                        "status": "success",
                        "n_obs": adata.n_obs,
                        "n_vars": adata.n_vars,
                        "file_type": "h5ad"
                    }
                else:
                    # å¯¹äºå…¶ä»–æ–‡ä»¶ï¼Œä½¿ç”¨é€šç”¨æ£€æŸ¥å·¥å…·
                    inspection_result = inspect_file(input_path)
                if "error" in inspection_result:
                    logger.warning(f"File inspection failed: {inspection_result.get('error')}")
                else:
                    # ğŸ”¥ ä½¿ç”¨ BaseAgent çš„ç»Ÿä¸€è¯Šæ–­æ–¹æ³•
                    # å°è¯•åŠ è½½æ•°æ®é¢„è§ˆï¼ˆç”¨äºæ›´å‡†ç¡®çš„è¯Šæ–­ï¼‰
                    dataframe = None
                    try:
                        if input_path.endswith('.h5ad'):
                            # ä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿ
                            adata = sc.read_h5ad(input_path)
                            # æå– obs è¡¨ä½œä¸ºé¢„è§ˆï¼ˆåŒ…å« QC æŒ‡æ ‡ï¼‰
                            if hasattr(adata, 'obs') and len(adata.obs) > 0:
                                dataframe = adata.obs.head(1000)  # æœ€å¤š1000è¡Œ
                    except Exception as e:
                        logger.debug(f"æ— æ³•åŠ è½½æ•°æ®é¢„è§ˆ: {e}")
                    
                    # è°ƒç”¨ç»Ÿä¸€çš„è¯Šæ–­æ–¹æ³•
                    # ğŸ”¥ æ¶æ„é‡æ„ï¼šä¼ é€’é¢†åŸŸç‰¹å®šçš„ç³»ç»ŸæŒ‡ä»¤
                    diagnosis_report = await self._perform_data_diagnosis(
                        file_metadata=inspection_result,
                        omics_type="scRNA",
                        dataframe=dataframe,
                        system_instruction=RNA_INSTRUCTION
                    )
                    # ğŸ”¥ DEBUG: æ‰“å°è¯Šæ–­æŠ¥å‘Šä¿¡æ¯
                    if diagnosis_report:
                        logger.info(f"ğŸ“ [DEBUG] RNAAgent diagnosis report generated, length: {len(diagnosis_report)}")
                    else:
                        logger.warning(f"âš ï¸ [DEBUG] RNAAgent diagnosis report is None")
            except Exception as e:
                logger.error(f"Error inspecting file: {e}", exc_info=True)
                diagnosis_report = None  # ğŸ”¥ ç¡®ä¿åœ¨å¼‚å¸¸æ—¶ä¹Ÿè®¾ç½®ä¸º None
        
        # ä½¿ç”¨ LLM æå–å‚æ•°ï¼ˆä¼ å…¥æ£€æŸ¥ç»“æœå’Œè¯Šæ–­æŠ¥å‘Šï¼‰
        extracted_params = await self._extract_workflow_params(query, file_paths, inspection_result, diagnosis_report)
        
        # æ„å»ºå·¥ä½œæµé…ç½®
        workflow_config = {
            "workflow_name": "Standard Single-Cell Pipeline",
            "steps": []
        }
        
        for step_template in self.workflow_steps:
            step = step_template.copy()
            
            # æ³¨å…¥å‚æ•°
            tool_id = step["tool_id"]
            # ğŸ”¥ å·¥å…·IDæ˜ å°„ï¼šå¦‚æœä½¿ç”¨æ—§IDï¼Œæ˜ å°„åˆ°æ–°ID
            if tool_id in self.tool_id_mapping:
                tool_id = self.tool_id_mapping[tool_id]
                step["tool_id"] = tool_id
            
            # æ ¹æ®å·¥å…·IDè®¾ç½®å‚æ•°
            if tool_id == "rna_qc_filter":
                step["params"] = {
                    "min_genes": extracted_params.get("min_genes", 200),
                    "max_mt": extracted_params.get("max_mt", 20.0),
                    "min_cells": extracted_params.get("min_cells", 3)
                }
            elif tool_id == "rna_hvg":
                step["params"] = {
                    "n_top_genes": extracted_params.get("n_top_genes", 2000)
                }
            elif tool_id == "rna_clustering":
                step["params"] = {
                    "resolution": extracted_params.get("resolution", 0.5)
                }
            else:
                step["params"] = {}
            
            workflow_config["steps"].append(step)
        
        # å¦‚æœç”Ÿæˆäº†è¯Šæ–­æŠ¥å‘Šï¼Œå°†å…¶åŒ…å«åœ¨è¿”å›ç»“æœä¸­
        result = {
            "type": "workflow_config",
            "workflow_data": workflow_config,
            "file_paths": file_paths
        }
        
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥ diagnosis_report æ˜¯å¦ä¸ºæœ‰æ•ˆå­—ç¬¦ä¸²ï¼ˆé None ä¸”éç©ºï¼‰
        if diagnosis_report and isinstance(diagnosis_report, str) and diagnosis_report.strip():
            result["diagnosis_report"] = diagnosis_report
            logger.info(f"ğŸ“ [DEBUG] RNAAgent: Adding diagnosis_report to result, length: {len(diagnosis_report)}")
        else:
            logger.warning(f"âš ï¸ [DEBUG] RNAAgent: diagnosis_report is invalid (None/empty), NOT adding to result. Type: {type(diagnosis_report)}, Value: {diagnosis_report}")
        
        # ğŸ”¥ TASK 5: æ·»åŠ å‚æ•°æ¨èåˆ°ç»“æœ
        if hasattr(self, 'context') and "parameter_recommendation" in self.context:
            recommendation = self.context.get("parameter_recommendation")
            if recommendation:
                result["recommendation"] = recommendation
                logger.info(f"âœ… [RNAAgent] æ·»åŠ å‚æ•°æ¨èåˆ°ç»“æœ: {len(recommendation.get('params', {}))} ä¸ªå‚æ•°")
        
        # ğŸ”¥ DEBUG: æ‰“å°æœ€ç»ˆè¿”å›ç»“æ„
        logger.info(f"ğŸ“¤ [DEBUG] RNAAgent returning result with keys: {list(result.keys())}")
        logger.info(f"ğŸ“¤ [DEBUG] RNAAgent has diagnosis_report: {'diagnosis_report' in result}")
        
        return result
    
    # ğŸ”¥ å·²ç§»é™¤ï¼š_generate_diagnosis_and_recommendation æ–¹æ³•
    # ç°åœ¨ä½¿ç”¨ BaseAgent._perform_data_diagnosis() ç»Ÿä¸€æ–¹æ³•
    
    async def _extract_workflow_params(
        self,
        query: str,
        file_paths: List[str],
        inspection_result: Optional[Dict[str, Any]] = None,
        diagnosis_report: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æå–å·¥ä½œæµå‚æ•°
        
        åŸºäºæ£€æŸ¥ç»“æœæ™ºèƒ½æ¨èå‚æ•°
        """
        # æ„å»ºåŒ…å«æ£€æŸ¥ç»“æœçš„æç¤º
        inspection_info = ""
        if inspection_result and "error" not in inspection_result:
            inspection_info = f"""
ã€Data Inspection Resultsã€‘
- Number of cells (n_obs): {inspection_result.get('n_obs', 'N/A')}
- Number of genes (n_vars): {inspection_result.get('n_vars', 'N/A')}
- Max value: {inspection_result.get('max_value', 'N/A')}
- Is normalized: {inspection_result.get('is_normalized', False)}
- Has QC metrics: {inspection_result.get('has_qc_metrics', False)}
- Has clusters: {inspection_result.get('has_clusters', False)}
- Has UMAP: {inspection_result.get('has_umap', False)}

ã€Recommendations Based on Inspectionã€‘
"""
            n_obs = inspection_result.get('n_obs', 0)
            is_normalized = inspection_result.get('is_normalized', False)
            has_qc = inspection_result.get('has_qc_metrics', False)
            
            if n_obs > 10000:
                inspection_info += "- Large dataset (>10k cells): Recommend min_genes=500, max_mt=5%\n"
            elif n_obs > 5000:
                inspection_info += "- Medium dataset (5k-10k cells): Recommend min_genes=300, max_mt=5%\n"
            else:
                inspection_info += "- Small dataset (<5k cells): Recommend min_genes=200, max_mt=10%\n"
            
            if is_normalized:
                inspection_info += "- Data appears normalized: Skip normalization step\n"
            else:
                inspection_info += "- Data appears to be raw counts: Need normalization\n"
            
            if has_qc:
                inspection_info += "- QC metrics already calculated: May skip QC calculation\n"
        
        prompt = f"""Extract workflow parameters from user query and inspection results:

Query: {query}
Files: {', '.join(file_paths) if file_paths else 'None'}
{inspection_info}

Extract these parameters (if mentioned in query, otherwise use recommendations):
- min_genes (default: 200, adjust based on dataset size)
- max_mt (default: 20, adjust based on dataset size)
- resolution (default: 0.5, for clustering)
- n_top_genes (default: 2000, for HVG selection)

Return JSON only:
{{"resolution": "0.8", "min_genes": "500", "max_mt": "5"}}
"""
        
        messages = [
            {"role": "system", "content": "You are a parameter extraction assistant. Return JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=256)
            # æå– think è¿‡ç¨‹å’Œå®é™…å†…å®¹
            think_content, response = self.llm_client.extract_think_and_content(completion)
            # å¦‚æœæœ‰ think å†…å®¹ï¼Œè®°å½•æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            if think_content:
                import logging
                logger = logging.getLogger(__name__)
                logger.debug(f"RNA Agent think process: {think_content[:200]}...")
            
            # è§£æ JSON
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            
            return json.loads(json_str)
        except:
            return {}
    
    async def _stream_chat_response(
        self,
        query: str,
        file_paths: List[str]
    ) -> AsyncIterator[str]:
        """
        æµå¼èŠå¤©å“åº”ï¼ˆæ”¯æŒ ReAct å¾ªç¯å’Œå·¥å…·è°ƒç”¨ï¼‰
        
        å®ç° ReAct å¾ªç¯ï¼š
        1. Thought: LLM æ€è€ƒ
        2. Action: è°ƒç”¨å·¥å…·ï¼ˆå¦‚ inspect_fileï¼‰
        3. Observation: å·¥å…·è¿”å›ç»“æœ
        4. Final Answer: æœ€ç»ˆå›ç­”
        """
        context = {
            "context": f"Uploaded files: {', '.join(file_paths) if file_paths else 'None'}",
            "available_tools": ["inspect_file", "run_cellranger", "convert_cellranger_to_h5ad"],
            "tool_descriptions": {
                "inspect_file": "æ£€æŸ¥æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ‘˜è¦ï¼ˆn_obs, n_vars, obs_keys, var_keys, is_normalized, etc.ï¼‰",
                "run_cellranger": "è¿è¡Œ Cell Ranger count å¯¹ FASTQ æ–‡ä»¶è¿›è¡Œè®¡æ•°åˆ†æ",
                "convert_cellranger_to_h5ad": "å°† Cell Ranger è¾“å‡ºè½¬æ¢ä¸º Scanpy æ ¼å¼ (.h5ad)"
            }
        }
        
        # å¦‚æœæœ‰æ–‡ä»¶ï¼Œå¼ºåˆ¶å…ˆæ£€æŸ¥ï¼ˆç¬¦åˆ SOPï¼‰
        inspection_result = None
        if file_paths:
            input_path = file_paths[0]
            try:
                # ä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿ
                if input_path.endswith('.h5ad'):
                    adata = sc.read_h5ad(input_path)
                    inspection_result = {
                        "status": "success",
                        "n_obs": adata.n_obs,
                        "n_vars": adata.n_vars,
                        "file_type": "h5ad"
                    }
                else:
                    inspection_result = inspect_file(input_path)
                if "error" not in inspection_result:
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
                    inspection_summary = f"""
ã€Data Inspection Completedã€‘
- Cells: {inspection_result.get('n_obs', 'N/A')}
- Genes: {inspection_result.get('n_vars', 'N/A')}
- Max value: {inspection_result.get('max_value', 'N/A')}
- Normalized: {inspection_result.get('is_normalized', False)}
- Has QC metrics: {inspection_result.get('has_qc_metrics', False)}
- Has clusters: {inspection_result.get('has_clusters', False)}
"""
                    # å…ˆè¾“å‡ºæ£€æŸ¥ç»“æœ
                    yield f"ğŸ” **Data Inspection Results:**\n{inspection_summary}\n\n"
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°æŸ¥è¯¢ä¸­ï¼Œè®© LLM åŸºäºæ­¤åˆ†æ
                    query = f"""{query}

{inspection_summary}

Based on the inspection results above, please:
1. Analyze the data characteristics
2. Propose appropriate analysis parameters
3. Ask for confirmation before proceeding with analysis
"""
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"Error inspecting file: {e}", exc_info=True)
                yield f"âš ï¸ Warning: Could not inspect file: {str(e)}\n\n"
        
        # æ„å»ºå¢å¼ºçš„ç”¨æˆ·æŸ¥è¯¢ï¼ŒåŒ…å«å·¥å…·è¯´æ˜
        enhanced_query = f"""{query}

ã€Available Toolsã€‘
You have access to:
- inspect_file(file_path): Check data file structure (already executed above if files were provided)
- run_cellranger(fastq_dir, sample_id, output_dir, reference, ...): Run Cell Ranger count on FASTQ files
- convert_cellranger_to_h5ad(matrix_dir, output_path): Convert Cell Ranger output to .h5ad format

ã€Workflow Ruleã€‘
- If user provides FASTQ files: First run Cell Ranger, then convert to .h5ad, then inspect
- If user provides .h5ad or 10x MTX files: Inspect first (already done above), then analyze and propose parameters
- Before running any analysis, you MUST have inspected the data first
- Now analyze the inspection results and propose parameters.
"""
        
        # æµå¼è¾“å‡º LLM å“åº”
        async for chunk in self.chat(enhanced_query, context, stream=True):
            yield chunk
    
    async def execute_workflow(
        self,
        workflow_config: Dict[str, Any],
        file_paths: List[str],
        output_dir: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥ä½œæµ
        
        æ ¸å¿ƒï¼šç›´æ¥æ‰§è¡Œ scanpy åˆ†ææµç¨‹ï¼ˆå‚è€ƒæ—§ç‰ˆæœ¬å®ç°ï¼‰
        
        Args:
            workflow_config: å·¥ä½œæµé…ç½®
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        # æ£€æµ‹è¾“å…¥æ–‡ä»¶ç±»å‹
        input_path = file_paths[0] if file_paths else None
        if not input_path:
            raise ValueError("No input files provided")
        
        file_type = self.detect_file_type(input_path)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # ğŸ”¥ æ¶æ„å‡çº§ï¼šä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿ
        convert_result = None
        if file_type == "fastq":
            # ä» FASTQ å¼€å§‹ï¼šå…ˆè¿è¡Œ Cell Rangerï¼ˆå¼‚æ­¥ï¼‰ï¼Œç„¶åè½¬æ¢ï¼Œæœ€åæ‰§è¡Œåˆ†æ
            fastq_dir = input_path
            sample_id = os.path.basename(fastq_dir).replace("_fastqs", "").replace("fastqs", "")
            if not sample_id:
                sample_id = "sample"
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            temp_output_dir = os.path.join(output_dir, "cellranger_output")
            os.makedirs(temp_output_dir, exist_ok=True)
            
            # ä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿè¿è¡Œ Cell Rangerï¼ˆå¼‚æ­¥ï¼‰
            transcriptome_path = self.cellranger_config.get("transcriptome_path", "/opt/refdata-gex-GRCh38-2020-A")
            cellranger_result = run_cellranger_count(
                fastqs_path=fastq_dir,
                sample_id=sample_id,
                transcriptome_path=transcriptome_path,
                output_dir=temp_output_dir,
                localcores=self.cellranger_config.get("localcores", 8),
                localmem=self.cellranger_config.get("localmem", 32),
                create_bam=self.cellranger_config.get("create_bam", False),
                cellranger_path=self.cellranger_config.get("cellranger_path", "/opt/cellranger")
            )
            
            # Cell Ranger æ˜¯å¼‚æ­¥æ‰§è¡Œçš„ï¼Œè¿”å›çŠ¶æ€ä¸º async_job_started
            if cellranger_result.get("status") == "async_job_started":
                # è¿”å›å¼‚æ­¥ä»»åŠ¡çŠ¶æ€ï¼Œå‰ç«¯ä¼šæ˜¾ç¤ºæ¶ˆæ¯
                return sanitize_for_json({
                    "status": "async_job_started",
                    "message": cellranger_result.get("message"),
                    "job_id": cellranger_result.get("job_id"),
                    "log_path": cellranger_result.get("log_path"),
                    "output_dir": cellranger_result.get("output_dir")
                })
            elif cellranger_result.get("status") != "success":
                return sanitize_for_json({
                    "status": "error",
                    "error": f"Cell Ranger failed: {cellranger_result.get('error', 'Unknown error')}",
                    "cellranger_result": cellranger_result
                })
            
            # å¦‚æœåŒæ­¥æ‰§è¡ŒæˆåŠŸï¼Œç»§ç»­è½¬æ¢
            matrix_dir = os.path.join(temp_output_dir, sample_id, "outs", "filtered_feature_bc_matrix")
            if not os.path.exists(matrix_dir):
                return sanitize_for_json({
                    "status": "error",
                    "error": f"Cell Ranger output matrix directory not found: {matrix_dir}",
                    "cellranger_result": cellranger_result
                })
            
            h5ad_path = os.path.join(output_dir, f"{sample_id}_filtered.h5ad")
            convert_result = convert_cellranger_to_h5ad(
                cellranger_matrix_dir=matrix_dir,
                output_h5ad_path=h5ad_path
            )
            
            if convert_result.get("status") != "success":
                return sanitize_for_json({
                    "status": "error",
                    "error": f"Conversion failed: {convert_result.get('error', 'Unknown error')}",
                    "cellranger_result": cellranger_result,
                    "convert_result": convert_result
                })
            
            # ä½¿ç”¨è½¬æ¢åçš„ .h5ad æ–‡ä»¶ç»§ç»­æ‰§è¡Œåˆ†æ
            input_path = h5ad_path
        
        # æ‰§è¡Œåˆ†ææµç¨‹ï¼ˆä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿï¼‰
        if file_type != "fastq" or (file_type == "fastq" and convert_result and convert_result.get("status") == "success"):
            # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ä½¿ç”¨å·¥ä½œæµæ‰§è¡Œå™¨ï¼Œè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨æ—§å·¥å…·
            # ç”±äºå·¥ä½œæµæ‰§è¡Œå™¨å¯èƒ½è¿˜æœªå®Œå…¨å®ç°ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºæœ¬å®ç°
            steps = workflow_config.get("steps", [])
            
            # åŸºæœ¬å®ç°ï¼šæŒ‰æ­¥éª¤æ‰§è¡Œ
            current_adata_path = input_path
            report = {
                "status": "success",
                "steps": [],
                "final_output": current_adata_path
            }
            
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ WorkflowExecutorï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªè®°å½•æ­¥éª¤
            logger.info(f"æ‰§è¡Œå·¥ä½œæµï¼Œå…± {len(steps)} ä¸ªæ­¥éª¤")
            logger.warning("âš ï¸ å®Œæ•´çš„å·¥ä½œæµæ‰§è¡Œéœ€è¦ä½¿ç”¨ WorkflowExecutor")
            
            # å¦‚æœæ˜¯ä» FASTQ è½¬æ¢æ¥çš„ï¼Œæ·»åŠ è½¬æ¢ä¿¡æ¯åˆ°æŠ¥å‘Š
            if file_type == "fastq" and convert_result:
                report["cellranger_result"] = {
                    "status": "success",
                    "converted_file": convert_result.get("output_path"),
                    "n_obs": convert_result.get("n_obs"),
                    "n_vars": convert_result.get("n_vars")
                }
            
            # ğŸ”¥ ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Šï¼ˆå°†å·¥å…·ç»“æœåé¦ˆç»™LLMè¿›è¡Œè§£é‡Šï¼‰
            if report.get("status") == "success":
                try:
                    final_report = await self.generate_final_report(report)
                    report["final_report"] = final_report
                except Exception as e:
                    logger.warning(f"âš ï¸ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")
                    report["final_report"] = None
            
            # ğŸ”¥ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨ï¼ˆå¤„ç† Numpy ç±»å‹ã€NaN/Infinity ç­‰ï¼‰
            logger.info("âœ… Workflow finished. Sanitizing data for JSON serialization...")
            sanitized_report = sanitize_for_json(report)
            logger.info("âœ… Data sanitization completed. Returning result to frontend.")
            
            return sanitized_report
        else:
            # å¦‚æœ FASTQ å¤„ç†å¤±è´¥ï¼Œè¿”å›é”™è¯¯ï¼ˆä¹Ÿéœ€è¦æ¸…ç†ï¼‰
            error_result = {
                "status": "error",
                "error": "Failed to process FASTQ files",
                "convert_result": convert_result
            }
            return sanitize_for_json(error_result)
    
    async def generate_final_report(self, execution_results: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š
        
        å°†å·¥å…·æ‰§è¡Œç»“æœåé¦ˆç»™LLMï¼Œç”Ÿæˆç§‘å­¦è§£é‡ŠæŠ¥å‘Š
        
        Args:
            execution_results: æ‰§è¡Œç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
                - qc_metrics: è´¨é‡æŒ‡æ ‡
                - steps_details: æ­¥éª¤è¯¦æƒ…
                - final_plot: æœ€ç»ˆå›¾ç‰‡è·¯å¾„
                - marker_genes: MarkeråŸºå› ï¼ˆå¦‚æœæœ‰ï¼‰
        
        Returns:
            Markdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
        """
        try:
            # æ”¶é›†æ‰€æœ‰è¾“å‡ºæ•°æ®
            results_summary = {
                "qc_metrics": execution_results.get("qc_metrics", {}),
                "steps_completed": len(execution_results.get("steps_details", [])),
                "final_plot": execution_results.get("final_plot"),
                "output_file": execution_results.get("output_file"),
                "steps_summary": [
                    {
                        "name": step.get("name"),
                        "status": step.get("status"),
                        "summary": step.get("summary")
                    }
                    for step in execution_results.get("steps_details", [])
                ]
            }
            
            # æå–MarkeråŸºå› ï¼ˆå¦‚æœæœ‰ï¼‰
            marker_genes = []
            for step in execution_results.get("steps_details", []):
                if step.get("tool_id") == "rna_find_markers" and step.get("details"):
                    # å°è¯•ä»detailsä¸­æå–markeråŸºå› ä¿¡æ¯
                    marker_genes.append(step.get("details"))
            
            if marker_genes:
                results_summary["marker_genes"] = marker_genes
            
            # æ„å»ºæç¤ºè¯
            import json
            results_json = json.dumps(results_summary, ensure_ascii=False, indent=2)
            
            # ä½¿ç”¨ PromptManager è·å–æŠ¥å‘Šæ¨¡æ¿
            try:
                prompt = self.prompt_manager.get_prompt(
                    "rna_report",
                    {"results_summary": results_json},
                    fallback=RNA_REPORT_PROMPT.format(results_summary=results_json)
                )
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–æŠ¥å‘Šæ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
                prompt = RNA_REPORT_PROMPT.format(results_summary=results_json)
            
            # è°ƒç”¨LLMç”ŸæˆæŠ¥å‘Š
            messages = [
                {"role": "system", "content": "You are a Senior Bioinformatician. Write analysis reports in Simplified Chinese."},
                {"role": "user", "content": prompt}
            ]
            
            completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=2000)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            from ...core.stream_utils import strip_suggestions_from_text
            if response:
                response, _ = strip_suggestions_from_text(response)
            logger.info("âœ… æœ€ç»ˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

