"""
è½¬å½•ç»„æ™ºèƒ½ä½“ï¼ˆRNA Agentï¼‰
å¤„ç†å•ç»†èƒè½¬å½•ç»„ï¼ˆscRNA-seqï¼‰å’Œ Bulk RNA-seq åˆ†æ
é‡æ„è‡ªç°æœ‰çš„ BioBlendAgent
"""
import json
import os
from typing import Dict, Any, List, Optional, AsyncIterator
from ..base_agent import BaseAgent
from ...core.llm_client import LLMClient
from ...core.prompt_manager import PromptManager, RNA_REPORT_PROMPT, DATA_DIAGNOSIS_PROMPT
from ...core.utils import sanitize_for_json
from ...core.dispatcher import TaskDispatcher
from ...core.test_data_manager import TestDataManager
from ...tools.cellranger_tool import CellRangerTool
from ...tools.scanpy_tool import ScanpyTool
import logging

logger = logging.getLogger(__name__)


class RNAAgent(BaseAgent):
    """
    è½¬å½•ç»„æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    1. å¤„ç†å•ç»†èƒè½¬å½•ç»„åˆ†æï¼ˆscRNA-seqï¼‰
    2. å¤„ç† Bulk RNA-seq åˆ†æ
    3. ç”Ÿæˆå·¥ä½œæµè„šæœ¬
    4. é€šè¿‡ TaskDispatcher æäº¤ä»»åŠ¡
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        prompt_manager: PromptManager,
        dispatcher: Optional[TaskDispatcher] = None,
        cellranger_config: Optional[Dict[str, Any]] = None,
        scanpy_config: Optional[Dict[str, Any]] = None,
        test_data_dir: Optional[str] = None
    ):
        """åˆå§‹åŒ–è½¬å½•ç»„æ™ºèƒ½ä½“"""
        super().__init__(llm_client, prompt_manager, "rna_expert")
        
        self.dispatcher = dispatcher
        self.cellranger_config = cellranger_config or {}
        self.scanpy_config = scanpy_config or {}
        self.cellranger_tool = CellRangerTool(self.cellranger_config)
        # å°† cellranger_tool ä¼ é€’ç»™ scanpy_toolï¼Œä½¿å…¶å¯ä»¥ä½¿ç”¨ Cell Ranger åŠŸèƒ½
        self.scanpy_tool = ScanpyTool(self.scanpy_config, cellranger_tool=self.cellranger_tool)
        # åˆå§‹åŒ–æµ‹è¯•æ•°æ®ç®¡ç†å™¨
        self.test_data_manager = TestDataManager(test_data_dir)
        
        # æ ‡å‡†å·¥ä½œæµæ­¥éª¤ï¼ˆåæ­¥æµç¨‹ï¼‰
        self.workflow_steps = [
            {"name": "1. Quality Control", "tool_id": "local_qc", "desc": "è¿‡æ»¤ä½è´¨é‡ç»†èƒå’ŒåŸºå› "},
            {"name": "2. Normalization", "tool_id": "local_normalize", "desc": "æ•°æ®æ ‡å‡†åŒ–"},
            {"name": "3. Find Variable Genes", "tool_id": "local_hvg", "desc": "ç­›é€‰é«˜å˜åŸºå› "},
            {"name": "4. Scale Data", "tool_id": "local_scale", "desc": "æ•°æ®ç¼©æ”¾"},
            {"name": "5. PCA", "tool_id": "local_pca", "desc": "ä¸»æˆåˆ†åˆ†æ"},
            {"name": "6. Compute Neighbors", "tool_id": "local_neighbors", "desc": "æ„å»ºé‚»æ¥å›¾"},
            {"name": "7. Clustering", "tool_id": "local_cluster", "desc": "Leiden èšç±»"},
            {"name": "8. UMAP Visualization", "tool_id": "local_umap", "desc": "UMAP å¯è§†åŒ–"},
            {"name": "9. t-SNE Visualization", "tool_id": "local_tsne", "desc": "t-SNE å¯è§†åŒ–"},
            {"name": "10. Find Markers", "tool_id": "local_markers", "desc": "å¯»æ‰¾ Marker åŸºå› "},
        ]
    
    async def process_query(
        self,
        query: str,
        history: List[Dict[str, str]] = None,
        uploaded_files: List[Dict[str, str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼Œå¯èƒ½åŒ…å«ï¼š
            - workflow_config: å·¥ä½œæµé…ç½®ï¼ˆJSONï¼‰
            - chat_response: èŠå¤©å“åº”ï¼ˆæµå¼ï¼‰
            - task_submitted: ä»»åŠ¡æäº¤ä¿¡æ¯
            - test_data_selection: æµ‹è¯•æ•°æ®é€‰æ‹©è¯·æ±‚
        """
        query_lower = query.lower().strip()
        file_paths = self.get_file_paths(uploaded_files or [])
        
        # æ™ºèƒ½æ•°æ®æ£€æµ‹ï¼šå¦‚æœéœ€è¦ Cell Ranger ä½†æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œæä¾›æµ‹è¯•æ•°æ®é€‰æ‹©
        needs_cellranger = self._needs_cellranger(query_lower)
        if needs_cellranger and not file_paths:
            # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•æ•°æ®å¯ç”¨
            test_datasets = self.test_data_manager.scan_test_datasets()
            if test_datasets:
                # è¿”å›æµ‹è¯•æ•°æ®é€‰æ‹©è¯·æ±‚
                return {
                    "type": "test_data_selection",
                    "message": "æ£€æµ‹åˆ°æ‚¨æ²¡æœ‰ä¸Šä¼ ç›¸å…³æ•°æ®ã€‚è¯·é€‰æ‹©ï¼š",
                    "options": [
                        "1. ä½¿ç”¨æœ¬åœ°æµ‹è¯•æ•°æ®é›†",
                        "2. ä¸Šä¼ æ‚¨è‡ªå·±çš„æ•°æ®"
                    ],
                    "datasets": test_datasets,
                    "datasets_json": self.test_data_manager.format_datasets_for_selection(test_datasets),
                    "datasets_display": self.test_data_manager.format_datasets_for_display(test_datasets)
                }
            else:
                # æ²¡æœ‰æµ‹è¯•æ•°æ®ï¼Œæç¤ºç”¨æˆ·ä¸Šä¼ 
                return {
                    "type": "chat",
                    "response": self._stream_string_response(
                        "æ£€æµ‹åˆ°æ‚¨æ²¡æœ‰ä¸Šä¼ ç›¸å…³æ•°æ®ï¼Œä¸”æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®é›†ã€‚\n"
                        "è¯·ä¸Šä¼  FASTQ æ–‡ä»¶æˆ– .h5ad æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚"
                    )
                }
        
        # å¤„ç†æµ‹è¯•æ•°æ®é€‰æ‹©ï¼ˆç”¨æˆ·é€šè¿‡ JSON é€‰æ‹©ï¼‰
        if "test_dataset_id" in kwargs:
            dataset_id = kwargs["test_dataset_id"]
            dataset = self.test_data_manager.get_dataset_by_id(dataset_id)
            if dataset:
                # ä½¿ç”¨é€‰å®šçš„æµ‹è¯•æ•°æ®
                if dataset.get("fastq_dir") and dataset.get("reference"):
                    # æœ‰ FASTQ å’Œå‚è€ƒåŸºå› ç»„ï¼Œä½¿ç”¨å®ƒä»¬
                    file_paths = [dataset["fastq_dir"]]
                    # å°†å‚è€ƒåŸºå› ç»„è·¯å¾„æ·»åŠ åˆ°é…ç½®ä¸­
                    self.cellranger_config["reference"] = dataset["reference"]
                elif dataset.get("h5ad_file"):
                    # åªæœ‰ .h5ad æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                    file_paths = [dataset["h5ad_file"]]
                else:
                    return {
                        "type": "chat",
                        "response": self._stream_string_response(
                            f"æµ‹è¯•æ•°æ®é›† {dataset['name']} ä¸å¯ç”¨ã€‚"
                        )
                    }
        
        # æ„å›¾è¯†åˆ«
        is_workflow_request = self._is_workflow_request(query_lower, file_paths)
        
        if is_workflow_request:
            return await self._generate_workflow_config(query, file_paths)
        else:
            # æ™®é€šèŠå¤©
            return {
                "type": "chat",
                "response": self._stream_chat_response(query, file_paths)
            }
    
    def _is_workflow_request(self, query: str, file_paths: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å·¥ä½œæµè¯·æ±‚"""
        workflow_keywords = [
            "è§„åˆ’", "æµç¨‹", "workflow", "pipeline", "åˆ†æ", "run",
            "æ‰§è¡Œ", "plan", "åšä¸€ä¸‹", "è·‘ä¸€ä¸‹", "åˆ†æä¸€ä¸‹"
        ]
        
        bio_keywords = [
            "pca", "umap", "tsne", "qc", "è´¨æ§", "èšç±»", "cluster"
        ]
        
        if any(kw in query for kw in workflow_keywords):
            return True
        
        if file_paths and any(kw in query for kw in bio_keywords):
            return True
        
        if file_paths and (not query or len(query) < 5):
            return True
        
        return False
    
    def _needs_cellranger(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ Cell Rangerï¼ˆåŸºäºæŸ¥è¯¢å…³é”®è¯ï¼‰"""
        cellranger_keywords = [
            "cellranger", "cell ranger", "fastq", "fq", "æµ‹åº",
            "ç¬¬ä¸€æ­¥", "å…¨æµç¨‹", "å®Œæ•´æµç¨‹", "ä»fastq", "ä»æµ‹åº"
        ]
        return any(kw in query for kw in cellranger_keywords)
    
    async def _generate_workflow_config(
        self,
        query: str,
        file_paths: List[str]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµé…ç½®
        
        å¼ºåˆ¶æµç¨‹ï¼š
        1. å…ˆæ£€æŸ¥æ–‡ä»¶ï¼ˆinspect_fileï¼‰
        2. åŸºäºæ£€æŸ¥ç»“æœæå–å‚æ•°
        3. ç”Ÿæˆå·¥ä½œæµé…ç½®
        """
        # å¼ºåˆ¶æ£€æŸ¥ï¼šå¦‚æœæœ‰æ–‡ä»¶ï¼Œå…ˆæ£€æŸ¥
        inspection_result = None
        diagnosis_report = None
        if file_paths:
            input_path = file_paths[0]
            try:
                inspection_result = self.scanpy_tool.inspect_file(input_path)
                if "error" in inspection_result:
                    # æ£€æŸ¥å¤±è´¥ï¼Œä½†ä»ç„¶ç»§ç»­ï¼ˆå¯èƒ½æ˜¯æ–‡ä»¶è·¯å¾„é—®é¢˜ï¼‰
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.warning(f"File inspection failed: {inspection_result.get('error')}")
                else:
                    # ğŸ”¥ ç”Ÿæˆæ•°æ®è¯Šæ–­å’Œå‚æ•°æ¨è
                    diagnosis_report = await self._generate_diagnosis_and_recommendation(inspection_result)
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"Error inspecting file: {e}", exc_info=True)
        
        # ä½¿ç”¨ LLM æå–å‚æ•°ï¼ˆä¼ å…¥æ£€æŸ¥ç»“æœå’Œè¯Šæ–­æŠ¥å‘Šï¼‰
        extracted_params = await self._extract_workflow_params(query, file_paths, inspection_result, diagnosis_report)
        
        # æ„å»ºå·¥ä½œæµé…ç½®
        workflow_config = {
            "workflow_name": "Standard Single-Cell Pipeline",
            "steps": []
        }
        
        for step_template in self.workflow_steps:
            step = step_template.copy()
            
            # æ³¨å…¥å‚æ•°
            tool_id = step["tool_id"]
            if tool_id == "local_qc":
                step["params"] = {
                    "min_genes": extracted_params.get("min_genes", "200"),
                    "max_mt": extracted_params.get("max_mt", "20")
                }
            elif tool_id == "local_hvg":
                step["params"] = {
                    "n_top_genes": extracted_params.get("n_top_genes", "2000")
                }
            elif tool_id == "local_cluster":
                step["params"] = {
                    "resolution": extracted_params.get("resolution", "0.5")
                }
            else:
                step["params"] = {}
            
            workflow_config["steps"].append(step)
        
        # å¦‚æœç”Ÿæˆäº†è¯Šæ–­æŠ¥å‘Šï¼Œå°†å…¶åŒ…å«åœ¨è¿”å›ç»“æœä¸­
        result = {
            "type": "workflow_config",
            "workflow_data": workflow_config,
            "file_paths": file_paths
        }
        
        if diagnosis_report:
            result["diagnosis_report"] = diagnosis_report
        
        return result
    
    async def _generate_diagnosis_and_recommendation(
        self,
        inspection_result: Dict[str, Any]
    ) -> str:
        """
        ç”Ÿæˆæ•°æ®è¯Šæ–­å’Œå‚æ•°æ¨èæŠ¥å‘Š
        
        Args:
            inspection_result: æ–‡ä»¶æ£€æŸ¥ç»“æœ
        
        Returns:
            Markdownæ ¼å¼çš„è¯Šæ–­å’Œæ¨èæŠ¥å‘Š
        """
        try:
            import json
            # æ ¼å¼åŒ–æ£€æŸ¥ç»“æœä¸ºJSONå­—ç¬¦ä¸²
            inspection_json = json.dumps(inspection_result, ensure_ascii=False, indent=2)
            
            # ä½¿ç”¨ PromptManager è·å–è¯Šæ–­æ¨¡æ¿
            try:
                prompt = self.prompt_manager.get_prompt(
                    "data_diagnosis",
                    {"inspection_data": inspection_json},
                    fallback=DATA_DIAGNOSIS_PROMPT.format(inspection_data=inspection_json)
                )
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.warning(f"âš ï¸ è·å–è¯Šæ–­æ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
                prompt = DATA_DIAGNOSIS_PROMPT.format(inspection_data=inspection_json)
            
            # è°ƒç”¨LLMç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
            messages = [
                {"role": "system", "content": "You are a Senior Bioinformatician. Generate data diagnosis and parameter recommendations in Simplified Chinese."},
                {"role": "user", "content": prompt}
            ]
            
            completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=1500)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            import logging
            logger = logging.getLogger(__name__)
            logger.info("âœ… æ•°æ®è¯Šæ–­å’Œå‚æ•°æ¨èå·²ç”Ÿæˆ")
            return response
            
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return f"è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def _extract_workflow_params(
        self,
        query: str,
        file_paths: List[str],
        inspection_result: Optional[Dict[str, Any]] = None,
        diagnosis_report: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æå–å·¥ä½œæµå‚æ•°
        
        åŸºäºæ£€æŸ¥ç»“æœæ™ºèƒ½æ¨èå‚æ•°
        """
        # æ„å»ºåŒ…å«æ£€æŸ¥ç»“æœçš„æç¤º
        inspection_info = ""
        if inspection_result and "error" not in inspection_result:
            inspection_info = f"""
ã€Data Inspection Resultsã€‘
- Number of cells (n_obs): {inspection_result.get('n_obs', 'N/A')}
- Number of genes (n_vars): {inspection_result.get('n_vars', 'N/A')}
- Max value: {inspection_result.get('max_value', 'N/A')}
- Is normalized: {inspection_result.get('is_normalized', False)}
- Has QC metrics: {inspection_result.get('has_qc_metrics', False)}
- Has clusters: {inspection_result.get('has_clusters', False)}
- Has UMAP: {inspection_result.get('has_umap', False)}

ã€Recommendations Based on Inspectionã€‘
"""
            n_obs = inspection_result.get('n_obs', 0)
            is_normalized = inspection_result.get('is_normalized', False)
            has_qc = inspection_result.get('has_qc_metrics', False)
            
            if n_obs > 10000:
                inspection_info += "- Large dataset (>10k cells): Recommend min_genes=500, max_mt=5%\n"
            elif n_obs > 5000:
                inspection_info += "- Medium dataset (5k-10k cells): Recommend min_genes=300, max_mt=5%\n"
            else:
                inspection_info += "- Small dataset (<5k cells): Recommend min_genes=200, max_mt=10%\n"
            
            if is_normalized:
                inspection_info += "- Data appears normalized: Skip normalization step\n"
            else:
                inspection_info += "- Data appears to be raw counts: Need normalization\n"
            
            if has_qc:
                inspection_info += "- QC metrics already calculated: May skip QC calculation\n"
        
        prompt = f"""Extract workflow parameters from user query and inspection results:

Query: {query}
Files: {', '.join(file_paths) if file_paths else 'None'}
{inspection_info}

Extract these parameters (if mentioned in query, otherwise use recommendations):
- min_genes (default: 200, adjust based on dataset size)
- max_mt (default: 20, adjust based on dataset size)
- resolution (default: 0.5, for clustering)
- n_top_genes (default: 2000, for HVG selection)

Return JSON only:
{{"resolution": "0.8", "min_genes": "500", "max_mt": "5"}}
"""
        
        messages = [
            {"role": "system", "content": "You are a parameter extraction assistant. Return JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=256)
            # æå– think è¿‡ç¨‹å’Œå®é™…å†…å®¹
            think_content, response = self.llm_client.extract_think_and_content(completion)
            # å¦‚æœæœ‰ think å†…å®¹ï¼Œè®°å½•æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            if think_content:
                import logging
                logger = logging.getLogger(__name__)
                logger.debug(f"RNA Agent think process: {think_content[:200]}...")
            
            # è§£æ JSON
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            
            return json.loads(json_str)
        except:
            return {}
    
    async def _stream_chat_response(
        self,
        query: str,
        file_paths: List[str]
    ) -> AsyncIterator[str]:
        """
        æµå¼èŠå¤©å“åº”ï¼ˆæ”¯æŒ ReAct å¾ªç¯å’Œå·¥å…·è°ƒç”¨ï¼‰
        
        å®ç° ReAct å¾ªç¯ï¼š
        1. Thought: LLM æ€è€ƒ
        2. Action: è°ƒç”¨å·¥å…·ï¼ˆå¦‚ inspect_fileï¼‰
        3. Observation: å·¥å…·è¿”å›ç»“æœ
        4. Final Answer: æœ€ç»ˆå›ç­”
        """
        context = {
            "context": f"Uploaded files: {', '.join(file_paths) if file_paths else 'None'}",
            "available_tools": ["inspect_file", "run_cellranger", "convert_cellranger_to_h5ad"],
            "tool_descriptions": {
                "inspect_file": "æ£€æŸ¥æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ‘˜è¦ï¼ˆn_obs, n_vars, obs_keys, var_keys, is_normalized, etc.ï¼‰",
                "run_cellranger": "è¿è¡Œ Cell Ranger count å¯¹ FASTQ æ–‡ä»¶è¿›è¡Œè®¡æ•°åˆ†æ",
                "convert_cellranger_to_h5ad": "å°† Cell Ranger è¾“å‡ºè½¬æ¢ä¸º Scanpy æ ¼å¼ (.h5ad)"
            }
        }
        
        # å¦‚æœæœ‰æ–‡ä»¶ï¼Œå¼ºåˆ¶å…ˆæ£€æŸ¥ï¼ˆç¬¦åˆ SOPï¼‰
        inspection_result = None
        if file_paths:
            input_path = file_paths[0]
            try:
                inspection_result = self.scanpy_tool.inspect_file(input_path)
                if "error" not in inspection_result:
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
                    inspection_summary = f"""
ã€Data Inspection Completedã€‘
- Cells: {inspection_result.get('n_obs', 'N/A')}
- Genes: {inspection_result.get('n_vars', 'N/A')}
- Max value: {inspection_result.get('max_value', 'N/A')}
- Normalized: {inspection_result.get('is_normalized', False)}
- Has QC metrics: {inspection_result.get('has_qc_metrics', False)}
- Has clusters: {inspection_result.get('has_clusters', False)}
"""
                    # å…ˆè¾“å‡ºæ£€æŸ¥ç»“æœ
                    yield f"ğŸ” **Data Inspection Results:**\n{inspection_summary}\n\n"
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°æŸ¥è¯¢ä¸­ï¼Œè®© LLM åŸºäºæ­¤åˆ†æ
                    query = f"""{query}

{inspection_summary}

Based on the inspection results above, please:
1. Analyze the data characteristics
2. Propose appropriate analysis parameters
3. Ask for confirmation before proceeding with analysis
"""
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"Error inspecting file: {e}", exc_info=True)
                yield f"âš ï¸ Warning: Could not inspect file: {str(e)}\n\n"
        
        # æ„å»ºå¢å¼ºçš„ç”¨æˆ·æŸ¥è¯¢ï¼ŒåŒ…å«å·¥å…·è¯´æ˜
        enhanced_query = f"""{query}

ã€Available Toolsã€‘
You have access to:
- inspect_file(file_path): Check data file structure (already executed above if files were provided)
- run_cellranger(fastq_dir, sample_id, output_dir, reference, ...): Run Cell Ranger count on FASTQ files
- convert_cellranger_to_h5ad(matrix_dir, output_path): Convert Cell Ranger output to .h5ad format

ã€Workflow Ruleã€‘
- If user provides FASTQ files: First run Cell Ranger, then convert to .h5ad, then inspect
- If user provides .h5ad or 10x MTX files: Inspect first (already done above), then analyze and propose parameters
- Before running any analysis, you MUST have inspected the data first
- Now analyze the inspection results and propose parameters.
"""
        
        # æµå¼è¾“å‡º LLM å“åº”
        async for chunk in self.chat(enhanced_query, context, stream=True):
            yield chunk
    
    async def execute_workflow(
        self,
        workflow_config: Dict[str, Any],
        file_paths: List[str],
        output_dir: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥ä½œæµ
        
        æ ¸å¿ƒï¼šç›´æ¥æ‰§è¡Œ scanpy åˆ†ææµç¨‹ï¼ˆå‚è€ƒæ—§ç‰ˆæœ¬å®ç°ï¼‰
        
        Args:
            workflow_config: å·¥ä½œæµé…ç½®
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        # æ£€æµ‹è¾“å…¥æ–‡ä»¶ç±»å‹
        input_path = file_paths[0] if file_paths else None
        if not input_path:
            raise ValueError("No input files provided")
        
        file_type = self.detect_file_type(input_path)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # æ›´æ–° scanpy å·¥å…·çš„è¾“å‡ºç›®å½•
        self.scanpy_config["output_dir"] = output_dir
        # é‡æ–°åˆå§‹åŒ– scanpy å·¥å…·ä»¥ä½¿ç”¨æ–°çš„è¾“å‡ºç›®å½•ï¼ˆä¿ç•™ cellranger_toolï¼‰
        self.scanpy_tool = ScanpyTool(self.scanpy_config, cellranger_tool=self.cellranger_tool)
        
        # ç›´æ¥æ‰§è¡Œ Scanpy æµç¨‹
        convert_result = None
        if file_type == "fastq":
            # ä» FASTQ å¼€å§‹ï¼šå…ˆè¿è¡Œ Cell Rangerï¼Œç„¶åè½¬æ¢ï¼Œæœ€åæ‰§è¡Œ Scanpy åˆ†æ
            # æå–å‚æ•°
            fastq_dir = input_path
            sample_id = os.path.basename(fastq_dir).replace("_fastqs", "").replace("fastqs", "")
            if not sample_id:
                sample_id = "sample"
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            temp_output_dir = os.path.join(output_dir, "cellranger_output")
            os.makedirs(temp_output_dir, exist_ok=True)
            
            # è¿è¡Œ Cell Ranger
            cellranger_result = self.scanpy_tool.run_cellranger(
                fastq_dir=fastq_dir,
                sample_id=sample_id,
                output_dir=temp_output_dir,
                localcores=self.cellranger_config.get("localcores", 8),
                localmem=self.cellranger_config.get("localmem", 32),
                create_bam=self.cellranger_config.get("create_bam", False)
            )
            
            if cellranger_result.get("status") != "success":
                return sanitize_for_json({
                    "status": "error",
                    "error": f"Cell Ranger failed: {cellranger_result.get('error', 'Unknown error')}",
                    "cellranger_result": cellranger_result
                })
            
            # è½¬æ¢ Cell Ranger è¾“å‡ºä¸º .h5ad
            matrix_dir = cellranger_result.get("matrix_dir")
            if not matrix_dir:
                return sanitize_for_json({
                    "status": "error",
                    "error": "Cell Ranger output matrix directory not found",
                    "cellranger_result": cellranger_result
                })
            
            h5ad_path = os.path.join(output_dir, f"{sample_id}_filtered.h5ad")
            convert_result = self.scanpy_tool.convert_cellranger_to_h5ad(
                cellranger_matrix_dir=matrix_dir,
                output_h5ad_path=h5ad_path
            )
            
            if convert_result.get("status") != "success":
                return sanitize_for_json({
                    "status": "error",
                    "error": f"Conversion failed: {convert_result.get('error', 'Unknown error')}",
                    "cellranger_result": cellranger_result,
                    "convert_result": convert_result
                })
            
            # ä½¿ç”¨è½¬æ¢åçš„ .h5ad æ–‡ä»¶ç»§ç»­æ‰§è¡Œ Scanpy åˆ†æ
            input_path = h5ad_path
        
        # æ‰§è¡Œ Scanpy åˆ†ææµç¨‹
        if file_type != "fastq" or (file_type == "fastq" and convert_result and convert_result.get("status") == "success"):
            # ç›´æ¥è¿è¡Œ Scanpy åˆ†æ
            steps = workflow_config.get("steps", [])
            
            # æ‰§è¡Œåˆ†ææµç¨‹
            report = self.scanpy_tool.run_pipeline(
                data_input=input_path,
                steps_config=steps
            )
            
            # å¦‚æœæ˜¯ä» FASTQ è½¬æ¢æ¥çš„ï¼Œæ·»åŠ è½¬æ¢ä¿¡æ¯åˆ°æŠ¥å‘Š
            if file_type == "fastq" and convert_result:
                report["cellranger_result"] = {
                    "status": "success",
                    "converted_file": convert_result.get("output_path"),
                    "n_obs": convert_result.get("n_obs"),
                    "n_vars": convert_result.get("n_vars")
                }
            
            # ğŸ”¥ ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Šï¼ˆå°†å·¥å…·ç»“æœåé¦ˆç»™LLMè¿›è¡Œè§£é‡Šï¼‰
            if report.get("status") == "success":
                try:
                    final_report = await self.generate_final_report(report)
                    report["final_report"] = final_report
                except Exception as e:
                    logger.warning(f"âš ï¸ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")
                    report["final_report"] = None
            
            # ğŸ”¥ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨ï¼ˆå¤„ç† Numpy ç±»å‹ã€NaN/Infinity ç­‰ï¼‰
            logger.info("âœ… Workflow finished. Sanitizing data for JSON serialization...")
            sanitized_report = sanitize_for_json(report)
            logger.info("âœ… Data sanitization completed. Returning result to frontend.")
            
            return sanitized_report
        else:
            # å¦‚æœ FASTQ å¤„ç†å¤±è´¥ï¼Œè¿”å›é”™è¯¯ï¼ˆä¹Ÿéœ€è¦æ¸…ç†ï¼‰
            error_result = {
                "status": "error",
                "error": "Failed to process FASTQ files",
                "convert_result": convert_result
            }
            return sanitize_for_json(error_result)
    
    async def generate_final_report(self, execution_results: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š
        
        å°†å·¥å…·æ‰§è¡Œç»“æœåé¦ˆç»™LLMï¼Œç”Ÿæˆç§‘å­¦è§£é‡ŠæŠ¥å‘Š
        
        Args:
            execution_results: æ‰§è¡Œç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
                - qc_metrics: è´¨é‡æŒ‡æ ‡
                - steps_details: æ­¥éª¤è¯¦æƒ…
                - final_plot: æœ€ç»ˆå›¾ç‰‡è·¯å¾„
                - marker_genes: MarkeråŸºå› ï¼ˆå¦‚æœæœ‰ï¼‰
        
        Returns:
            Markdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
        """
        try:
            # æ”¶é›†æ‰€æœ‰è¾“å‡ºæ•°æ®
            results_summary = {
                "qc_metrics": execution_results.get("qc_metrics", {}),
                "steps_completed": len(execution_results.get("steps_details", [])),
                "final_plot": execution_results.get("final_plot"),
                "output_file": execution_results.get("output_file"),
                "steps_summary": [
                    {
                        "name": step.get("name"),
                        "status": step.get("status"),
                        "summary": step.get("summary")
                    }
                    for step in execution_results.get("steps_details", [])
                ]
            }
            
            # æå–MarkeråŸºå› ï¼ˆå¦‚æœæœ‰ï¼‰
            marker_genes = []
            for step in execution_results.get("steps_details", []):
                if step.get("name") == "local_markers" and step.get("details"):
                    # å°è¯•ä»detailsä¸­æå–markeråŸºå› ä¿¡æ¯
                    marker_genes.append(step.get("details"))
            
            if marker_genes:
                results_summary["marker_genes"] = marker_genes
            
            # æ„å»ºæç¤ºè¯
            import json
            results_json = json.dumps(results_summary, ensure_ascii=False, indent=2)
            
            # ä½¿ç”¨ PromptManager è·å–æŠ¥å‘Šæ¨¡æ¿
            try:
                prompt = self.prompt_manager.get_prompt(
                    "rna_report",
                    {"results_summary": results_json},
                    fallback=RNA_REPORT_PROMPT.format(results_summary=results_json)
                )
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–æŠ¥å‘Šæ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
                prompt = RNA_REPORT_PROMPT.format(results_summary=results_json)
            
            # è°ƒç”¨LLMç”ŸæˆæŠ¥å‘Š
            messages = [
                {"role": "system", "content": "You are a Senior Bioinformatician. Write analysis reports in Simplified Chinese."},
                {"role": "user", "content": prompt}
            ]
            
            completion = await self.llm_client.achat(messages, temperature=0.3, max_tokens=2000)
            think_content, response = self.llm_client.extract_think_and_content(completion)
            
            logger.info("âœ… æœ€ç»ˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

