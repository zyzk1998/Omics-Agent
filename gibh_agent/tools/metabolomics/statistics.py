"""
ä»£è°¢ç»„å­¦ç»Ÿè®¡åˆ†æå·¥å…·
"""
import logging
from typing import Dict, Any, Optional
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from scipy import stats
from statsmodels.stats.multitest import multipletests

from ...core.tool_registry import registry

logger = logging.getLogger(__name__)


@registry.register(
    name="pca_analysis",
    description="Performs Principal Component Analysis (PCA) on metabolite abundance data. Returns PCA coordinates, explained variance, and optionally a PCA plot.",
    category="Metabolomics",
    output_type="mixed"  # è¿”å› JSON + å›¾ç‰‡è·¯å¾„
)
def run_pca(
    file_path: str,
    n_components: int = 2,
    scale: bool = True,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œ PCA åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼‰
        n_components: ä¸»æˆåˆ†æ•°é‡ï¼ˆé»˜è®¤ 2ï¼‰
        scale: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - pca_coordinates: PCA åæ ‡ (DataFrame çš„ JSON è¡¨ç¤º)
        - explained_variance: è§£é‡Šæ–¹å·®æ¯”ä¾‹
        - plot_path: PCA å›¾è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # æå–æ•°å€¼åˆ—ï¼ˆæ’é™¤éæ•°å€¼åˆ—ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        data = df[numeric_cols]
        
        # ğŸ”¥ CRITICAL FIX: æ£€æŸ¥å¹¶å¤„ç† NaN å€¼
        if data.isnull().any().any():
            logger.warning(f"âš ï¸ [PCA] æ•°æ®åŒ…å« NaN å€¼ï¼Œå°è¯•å¤„ç†...")
            # æ£€æŸ¥ NaN çš„æ¯”ä¾‹
            nan_ratio = data.isnull().sum().sum() / (data.shape[0] * data.shape[1])
            if nan_ratio > 0.5:
                logger.error(f"âŒ [PCA] NaN å€¼æ¯”ä¾‹è¿‡é«˜ ({nan_ratio:.2%})ï¼Œæ— æ³•å¤„ç†")
                return {
                    "status": "error",
                    "error": f"æ•°æ®åŒ…å«è¿‡å¤š NaN å€¼ ({nan_ratio:.2%})ï¼Œè¯·å…ˆè¿›è¡Œæ•°æ®é¢„å¤„ç†"
                }
            else:
                # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
                data = data.fillna(data.median())
                logger.info(f"âœ… [PCA] ä½¿ç”¨ä¸­ä½æ•°å¡«å…… NaN å€¼")
        
        # ğŸ”¥ æ£€æŸ¥æ•°æ®ç»´åº¦
        n_samples, n_features = data.shape
        if n_features < 2:
            return {
                "status": "error",
                "error": f"PCA éœ€è¦è‡³å°‘ 2 ä¸ªç‰¹å¾ï¼Œä½†æ•°æ®åªæœ‰ {n_features} ä¸ªç‰¹å¾ã€‚è¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤æ˜¯å¦æ­£ç¡®ä¿ç•™äº†ä»£è°¢ç‰©åˆ—ã€‚",
                "data_shape": {"rows": n_samples, "columns": n_features}
            }
        
        # ğŸ”¥ è‡ªåŠ¨è°ƒæ•´ n_componentsï¼ˆä¸èƒ½è¶…è¿‡ min(n_samples, n_features)ï¼‰
        max_components = min(n_samples, n_features)
        actual_n_components = min(n_components, max_components)
        
        if actual_n_components < n_components:
            logger.warning(f"âš ï¸ è¯·æ±‚çš„ n_components={n_components} è¶…è¿‡æ•°æ®ç»´åº¦é™åˆ¶ (min({n_samples}, {n_features})={max_components})ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º {actual_n_components}")
        
        # æ•°æ®é¢„å¤„ç†
        if scale:
            scaler = StandardScaler()
            data_scaled = scaler.fit_transform(data)
        else:
            data_scaled = data.values
        
        # ğŸ”¥ CRITICAL FIX: æœ€ç»ˆæ£€æŸ¥ NaNï¼ˆé˜²æ­¢æ ‡å‡†åŒ–åå‡ºç°ï¼‰
        if np.isnan(data_scaled).any():
            logger.warning(f"âš ï¸ [PCA] æ ‡å‡†åŒ–åçš„æ•°æ®ä»åŒ…å« NaNï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……...")
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='median')
            data_scaled = imputer.fit_transform(data_scaled)
        
        # æ‰§è¡Œ PCA
        pca = PCA(n_components=actual_n_components)
        pca_coords = pca.fit_transform(data_scaled)
        
        # åˆ›å»ºç»“æœ DataFrame
        coords_df = pd.DataFrame(
            pca_coords,
            index=data.index,
            columns=[f"PC{i+1}" for i in range(actual_n_components)]
        )
        
        # ç”Ÿæˆå›¾ç‰‡ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
        plot_path = None
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            plot_path = str(output_path / "pca_plot.png")
            
            plt.figure(figsize=(10, 8))
            plt.scatter(coords_df.iloc[:, 0], coords_df.iloc[:, 1], alpha=0.6)
            plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%})")
            plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%})")
            plt.title("PCA Plot")
            plt.grid(True, alpha=0.3)
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
        
        # ğŸ”¥ Phase 2: Extract summary metrics for AI report
        pc1_var = float(pca.explained_variance_ratio_[0]) if len(pca.explained_variance_ratio_) > 0 else 0.0
        pc2_var = float(pca.explained_variance_ratio_[1]) if len(pca.explained_variance_ratio_) > 1 else 0.0
        
        # Determine separation quality (simple heuristic: if PC1 explains > 30%, consider it "observed")
        separation = "observed" if pc1_var > 0.3 else "unclear"
        
        return {
            "status": "success",
            "pca_coordinates": coords_df.to_dict(orient='index'),
            "explained_variance": {
                f"PC{i+1}": float(ratio) 
                for i, ratio in enumerate(pca.explained_variance_ratio_)
            },
            "plot_path": plot_path,
            "n_components": actual_n_components,
            "requested_n_components": n_components,
            "data_shape": {"rows": n_samples, "columns": n_features},
            "summary": {
                "pc1_var": pc1_var,
                "pc2_var": pc2_var,
                "separation": separation,
                "total_variance_explained": float(sum(pca.explained_variance_ratio_[:2])) if len(pca.explained_variance_ratio_) >= 2 else pc1_var
            }
        }
    
    except Exception as e:
        logger.error(f"âŒ PCA åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


@registry.register(
    name="differential_analysis",
    description="Performs differential analysis between two groups in metabolite data. Supports t-test and Wilcoxon rank-sum test. Returns p-values, FDR-corrected p-values, and log2 fold changes. Automatically detects groups if not specified.",
    category="Metabolomics",
    output_type="json"
)
def run_differential_analysis(
    file_path: str,
    group_column: str,
    case_group: Optional[str] = None,
    control_group: Optional[str] = None,
    group1: Optional[str] = None,  # åˆ«åï¼Œå…¼å®¹ planner å‘é€çš„å‚æ•°
    group2: Optional[str] = None,   # åˆ«åï¼Œå…¼å®¹ planner å‘é€çš„å‚æ•°
    method: str = "t-test",
    p_value_threshold: float = 0.05,
    fold_change_threshold: float = 1.5,
    fdr_method: str = "fdr_bh",
    is_logged: bool = True,  # ğŸ”¥ CRITICAL FIX: æ•°æ®æ˜¯å¦å·²Log2è½¬æ¢ï¼ˆé»˜è®¤Trueï¼Œå› ä¸ºSOPå¼ºåˆ¶Log2è½¬æ¢ï¼‰
    output_dir: Optional[str] = None,
    **kwargs  # å®‰å…¨ç½‘ï¼šæ¥å—å…¶ä»–æ„å¤–å‚æ•°
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯ï¼‰
        group_column: åˆ†ç»„åˆ—å
        case_group: å®éªŒç»„åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
        control_group: å¯¹ç…§ç»„åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
        group1: ç¬¬ä¸€ç»„åç§°ï¼ˆåˆ«åï¼Œå…¼å®¹ planner å‚æ•°ï¼‰
        group2: ç¬¬äºŒç»„åç§°ï¼ˆåˆ«åï¼Œå…¼å®¹ planner å‚æ•°ï¼‰
        method: ç»Ÿè®¡æ–¹æ³•ï¼ˆ"t-test" æˆ– "wilcoxon"ï¼Œé»˜è®¤ "t-test"ï¼‰
        p_value_threshold: P å€¼é˜ˆå€¼ï¼ˆé»˜è®¤ 0.05ï¼‰
        fold_change_threshold: å€æ•°å˜åŒ–é˜ˆå€¼ï¼ˆé»˜è®¤ 1.5ï¼‰
        fdr_method: FDR æ ¡æ­£æ–¹æ³•ï¼ˆé»˜è®¤ "fdr_bh"ï¼‰
        is_logged: æ•°æ®æ˜¯å¦å·²Log2è½¬æ¢ï¼ˆé»˜è®¤ Trueï¼Œå› ä¸ºSOPå¼ºåˆ¶Log2è½¬æ¢ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¿å­˜ç»“æœåˆ° CSVï¼‰
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆå®‰å…¨ç½‘ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - results: å·®å¼‚åˆ†æç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªä»£è°¢ç‰©ä¸€è¡Œï¼‰
        - output_path: ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¿å­˜ï¼‰
        - output_file: ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆåˆ«åï¼Œç”¨äºæ•°æ®æµä¼ é€’ï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•æ¨¡ç³ŠåŒ¹é…
        if group_column not in df.columns:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
            group_column_normalized = group_column.lower().replace(' ', '').replace('_', '').replace('-', '')
            matched_column = None
            
            for col in df.columns:
                col_normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                if col_normalized == group_column_normalized:
                    matched_column = col
                    logger.info(f"ğŸ”„ [Differential Analysis] æ¨¡ç³ŠåŒ¹é…åˆ†ç»„åˆ—: '{group_column}' -> '{col}'")
                    break
            
            if matched_column:
                group_column = matched_column
            else:
                # ğŸ”¥ æ”¹è¿›ï¼šåŒºåˆ†æ˜¾ç¤ºå…ƒæ•°æ®åˆ—ï¼ˆå¯èƒ½çš„åˆ†ç»„åˆ—ï¼‰å’Œç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ï¼ˆå¯èƒ½æ˜¯åˆ†ç»„åˆ—ï¼‰
                potential_group_cols = []
                for col in df.columns:
                    unique_count = df[col].nunique()
                    if 2 <= unique_count <= 10:
                        potential_group_cols.append(f"{col} ({unique_count}ä¸ªå”¯ä¸€å€¼)")
                
                error_msg = f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ã€‚\n\n"
                
                # æ£€æŸ¥ç´¢å¼•åˆ—æ˜¯å¦å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯
                index_info = ""
                if df.index.nunique() <= 10:
                    index_values = df.index.unique().tolist()[:10]
                    index_info = f"âš ï¸ ç´¢å¼•åˆ—ï¼ˆç¬¬ä¸€åˆ—ï¼‰æœ‰ {df.index.nunique()} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯: {index_values}\n"
                    error_msg += index_info
                    error_msg += "ğŸ’¡ æç¤ºï¼šå¦‚æœåˆ†ç»„ä¿¡æ¯åœ¨ç´¢å¼•åˆ—ä¸­ï¼Œè¯·å°†ç´¢å¼•åˆ—è½¬æ¢ä¸ºæ•°æ®åˆ—ã€‚\n\n"
                
                if metadata_cols:
                    error_msg += f"å¯èƒ½çš„å…ƒæ•°æ®åˆ—ï¼ˆéæ•°å€¼åˆ—ï¼‰: {', '.join(metadata_cols[:10])}\n"
                else:
                    error_msg += "âŒ æœªæ‰¾åˆ°éæ•°å€¼åˆ—ï¼ˆæ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹ï¼‰ã€‚\n"
                    error_msg += "ğŸ’¡ æ•°æ®æ ¼å¼è¦æ±‚ï¼šCSVæ–‡ä»¶åº”åŒ…å«ä¸€åˆ—åˆ†ç»„ä¿¡æ¯ï¼ˆå¦‚ 'Group', 'Condition', 'Treatment' ç­‰ï¼‰ã€‚\n"
                    error_msg += "   æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\n"
                    error_msg += "   SampleID,Group,Metabolite1,Metabolite2,...\n"
                    error_msg += "   Sample1,Control,1.2,3.4,...\n"
                    error_msg += "   Sample2,Treatment,2.3,4.5,...\n\n"
                
                if potential_group_cols:
                    error_msg += f"å¯èƒ½çš„åˆ†ç»„åˆ—ï¼ˆå”¯ä¸€å€¼2-10ï¼‰: {', '.join(potential_group_cols[:10])}\n"
                
                if numeric_cols:
                    error_msg += f"ç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼Œå‰5ä¸ªï¼‰: {', '.join(numeric_cols[:5])}, ..."
                
                return {
                    "status": "error",
                    "error": error_msg.strip()
                }
        
        # ğŸ”¥ è‡ªåŠ¨æ£€æµ‹åˆ†ç»„ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        # ä¼˜å…ˆä½¿ç”¨ case_group/control_groupï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ group1/group2
        if case_group is None and group1 is not None:
            case_group = group1
        if control_group is None and group2 is not None:
            control_group = group2
        
        # å¦‚æœä»ç„¶ä¸º Noneï¼Œè‡ªåŠ¨æ£€æµ‹å‰ä¸¤ä¸ªå”¯ä¸€å€¼
        if case_group is None or control_group is None:
            unique_groups = sorted(df[group_column].unique().tolist())  # æ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§
            if len(unique_groups) < 2:
                return {
                    "status": "error",
                    "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸­åªæœ‰ {len(unique_groups)} ä¸ªå”¯ä¸€å€¼ï¼Œéœ€è¦è‡³å°‘ 2 ä¸ªç»„"
                }
            # ä½¿ç”¨å‰ä¸¤ä¸ªå”¯ä¸€å€¼
            case_group = unique_groups[0] if case_group is None else case_group
            control_group = unique_groups[1] if control_group is None else control_group
            logger.info(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ†ç»„: case_group={case_group}, control_group={control_group}")
        
        # ğŸ”¥ å°†æ£€æµ‹åˆ°çš„åˆ†ç»„ä¿¡æ¯æ·»åŠ åˆ°è¿”å›ç»“æœä¸­ï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        detected_groups = {
            "case_group": case_group,
            "control_group": control_group,
            "group1": case_group,  # åˆ«å
            "group2": control_group  # åˆ«å
        }
        
        # åˆ†ç¦»åˆ†ç»„
        groups = df[group_column]
        case_mask = groups == case_group
        control_mask = groups == control_group
        
        if not case_mask.any():
            return {
                "status": "error",
                "error": f"å®éªŒç»„ '{case_group}' ä¸å­˜åœ¨äºåˆ†ç»„åˆ—ä¸­ã€‚å¯ç”¨ç»„: {list(unique_groups)}"
            }
        if not control_mask.any():
            return {
                "status": "error",
                "error": f"å¯¹ç…§ç»„ '{control_group}' ä¸å­˜åœ¨äºåˆ†ç»„åˆ—ä¸­ã€‚å¯ç”¨ç»„: {list(unique_groups)}"
            }
        
        # æå–ä»£è°¢ç‰©åˆ—ï¼ˆæ•°å€¼åˆ—ï¼Œæ’é™¤åˆ†ç»„åˆ—ï¼‰
        metabolite_cols = [
            col for col in df.columns 
            if col != group_column and pd.api.types.is_numeric_dtype(df[col])
        ]
        
        results = []
        p_values = []
        
        # ğŸ”¥ æ ¹æ® method å‚æ•°é€‰æ‹©ç»Ÿè®¡æ–¹æ³•
        use_wilcoxon = method.lower() in ["wilcoxon", "wilcox", "ranksum", "mann-whitney"]
        
        for metabolite in metabolite_cols:
            case_values = df.loc[case_mask, metabolite].dropna()
            control_values = df.loc[control_mask, metabolite].dropna()
            
            if len(case_values) < 2 or len(control_values) < 2:
                continue
            
            # ğŸ”¥ æ ¹æ®æ–¹æ³•é€‰æ‹©ç»Ÿè®¡æ£€éªŒ
            if use_wilcoxon:
                # Wilcoxon rank-sum test (Mann-Whitney U test)
                try:
                    u_stat, p_val = stats.ranksums(case_values, control_values)
                except Exception as e:
                    logger.warning(f"âš ï¸ Wilcoxon æ£€éªŒå¤±è´¥ ({metabolite}): {e}ï¼Œè·³è¿‡")
                    continue
            else:
                # T-test (é»˜è®¤)
                try:
                    t_stat, p_val = stats.ttest_ind(case_values, control_values)
                except Exception as e:
                    logger.warning(f"âš ï¸ T-test å¤±è´¥ ({metabolite}): {e}ï¼Œè·³è¿‡")
                    continue
            
            # ğŸ”¥ CRITICAL FIX: è®¡ç®— log2 fold change
            # å¯¹äºå·²Log2è½¬æ¢çš„æ•°æ®ï¼Œä½¿ç”¨å‡æ³•ï¼šLog2FC = Mean_A - Mean_B
            # å¯¹äºåŸå§‹æ•°æ®ï¼Œä½¿ç”¨é™¤æ³•ï¼šLog2FC = log2(Mean_A / Mean_B)
            case_mean = case_values.mean()
            control_mean = control_values.mean()
            
            if is_logged:
                # æ•°æ®å·²Log2è½¬æ¢ï¼Œä½¿ç”¨å‡æ³•
                log2fc = case_mean - control_mean
                logger.debug(f"âœ… [Log2FC] ä½¿ç”¨å‡æ³•ï¼ˆæ•°æ®å·²Log2è½¬æ¢ï¼‰: {case_mean} - {control_mean} = {log2fc}")
            else:
                # åŸå§‹æ•°æ®ï¼Œä½¿ç”¨é™¤æ³•
                if control_mean > 0:
                    log2fc = np.log2(case_mean / control_mean)
                else:
                    # é¿å…é™¤é›¶ï¼Œä½¿ç”¨å°çš„epsilon
                    log2fc = np.log2(case_mean / (control_mean + 1e-9))
                logger.debug(f"âœ… [Log2FC] ä½¿ç”¨é™¤æ³•ï¼ˆåŸå§‹æ•°æ®ï¼‰: log2({case_mean} / {control_mean}) = {log2fc}")
            
            results.append({
                "metabolite": metabolite,
                "p_value": float(p_val),
                "log2fc": float(log2fc),
                "log2_fold_change": float(log2fc),  # åˆ«åï¼Œå…¼å®¹ visualize_volcano
                "case_mean": float(case_mean),
                "control_mean": float(control_mean),
                "case_group": case_group,
                "control_group": control_group
            })
            p_values.append(p_val)
        
        # FDR æ ¡æ­£
        if p_values:
            _, p_adjusted, _, _ = multipletests(p_values, method=fdr_method)
            
            # æ·»åŠ  FDR æ ¡æ­£åçš„ p å€¼
            for i, result in enumerate(results):
                result["fdr"] = float(p_adjusted[i])
                result["fdr_corrected_pvalue"] = float(p_adjusted[i])  # åˆ«åï¼Œå…¼å®¹ visualize_volcano
                # ğŸ”¥ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„é˜ˆå€¼åˆ¤æ–­æ˜¾è‘—æ€§ï¼ˆç¡®ä¿è¿”å› Python åŸç”Ÿ boolï¼‰
                result["significant"] = bool(
                    p_adjusted[i] < p_value_threshold and 
                    abs(result["log2fc"]) >= np.log2(fold_change_threshold)
                )
        
        # ğŸ”¥ ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶ï¼ˆç”¨äºæ•°æ®æµä¼ é€’å’Œå¯è§†åŒ–ï¼‰
        output_path = None
        if output_dir:
            output_path_obj = Path(output_dir)
            output_path_obj.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            input_filename = Path(file_path).stem
            output_path = str(output_path_obj / f"{input_filename}_differential_results.csv")
            
            # è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜
            results_df = pd.DataFrame(results)
            results_df.to_csv(output_path, index=False)
            logger.info(f"ğŸ’¾ å·®å¼‚åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œå°è¯•ä½¿ç”¨è¾“å…¥æ–‡ä»¶æ‰€åœ¨ç›®å½•
            input_dir = Path(file_path).parent
            output_path = str(input_dir / "differential_results.csv")
            results_df = pd.DataFrame(results)
            results_df.to_csv(output_path, index=False)
            logger.info(f"ğŸ’¾ å·®å¼‚åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
        
        # ç»Ÿè®¡æ‘˜è¦
        significant_count = sum(1 for r in results if r.get("significant", False))
        significant_results = [r for r in results if r.get("significant", False)]
        
        # ğŸ”¥ Phase 2: Extract top up/down regulated metabolites
        top_up = sorted(significant_results, key=lambda x: x.get("log2fc", 0), reverse=True)[:5]
        top_down = sorted(significant_results, key=lambda x: x.get("log2fc", 0))[:5]
        
        top_up_names = [r["metabolite"] for r in top_up]
        top_down_names = [r["metabolite"] for r in top_down]
        
        return {
            "status": "success",
            "results": results,
            "output_path": output_path,
            "output_file": output_path,  # åˆ«åï¼Œç”¨äºæ•°æ®æµä¼ é€’
            "file_path": output_path,    # å¦ä¸€ä¸ªåˆ«åï¼Œç¡®ä¿å…¼å®¹æ€§
            "summary": {
                "total_metabolites": len(results),
                "significant_count": significant_count,
                "sig_count": significant_count,  # åˆ«åï¼Œç”¨äºAIæŠ¥å‘Š
                "method": method,
                "case_group": case_group,
                "control_group": control_group,
                "p_value_threshold": p_value_threshold,
                "fold_change_threshold": fold_change_threshold,
                "top_up": top_up_names,
                "top_down": top_down_names
            }
        }
    
    except Exception as e:
        logger.error(f"âŒ å·®å¼‚åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }

