"""
ä»£è°¢ç»„å­¦ç»Ÿè®¡åˆ†æå·¥å…·
"""
import logging
from typing import Dict, Any, Optional
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from scipy import stats
from statsmodels.stats.multitest import multipletests

from ...core.tool_registry import registry

logger = logging.getLogger(__name__)


@registry.register(
    name="pca_analysis",
    description="Performs Principal Component Analysis (PCA) on metabolite abundance data. Returns PCA coordinates, explained variance, and optionally a PCA plot.",
    category="Metabolomics",
    output_type="mixed"  # è¿”å› JSON + å›¾ç‰‡è·¯å¾„
)
def run_pca(
    file_path: str,
    n_components: int = 2,
    scale: bool = True,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œ PCA åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼‰
        n_components: ä¸»æˆåˆ†æ•°é‡ï¼ˆé»˜è®¤ 2ï¼‰
        scale: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - pca_coordinates: PCA åæ ‡ (DataFrame çš„ JSON è¡¨ç¤º)
        - explained_variance: è§£é‡Šæ–¹å·®æ¯”ä¾‹
        - plot_path: PCA å›¾è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # æå–æ•°å€¼åˆ—ï¼ˆæ’é™¤éæ•°å€¼åˆ—ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        data = df[numeric_cols]
        
        # æ•°æ®é¢„å¤„ç†
        if scale:
            scaler = StandardScaler()
            data_scaled = scaler.fit_transform(data)
        else:
            data_scaled = data.values
        
        # æ‰§è¡Œ PCA
        pca = PCA(n_components=n_components)
        pca_coords = pca.fit_transform(data_scaled)
        
        # åˆ›å»ºç»“æœ DataFrame
        coords_df = pd.DataFrame(
            pca_coords,
            index=data.index,
            columns=[f"PC{i+1}" for i in range(n_components)]
        )
        
        # ç”Ÿæˆå›¾ç‰‡ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
        plot_path = None
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            plot_path = str(output_path / "pca_plot.png")
            
            plt.figure(figsize=(10, 8))
            plt.scatter(coords_df.iloc[:, 0], coords_df.iloc[:, 1], alpha=0.6)
            plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%})")
            plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%})")
            plt.title("PCA Plot")
            plt.grid(True, alpha=0.3)
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
        
        return {
            "status": "success",
            "pca_coordinates": coords_df.to_dict(orient='index'),
            "explained_variance": {
                f"PC{i+1}": float(ratio) 
                for i, ratio in enumerate(pca.explained_variance_ratio_)
            },
            "plot_path": plot_path
        }
    
    except Exception as e:
        logger.error(f"âŒ PCA åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


@registry.register(
    name="differential_analysis",
    description="Performs differential analysis between two groups in metabolite data. Supports t-test and Wilcoxon rank-sum test. Returns p-values, FDR-corrected p-values, and log2 fold changes. Automatically detects groups if not specified.",
    category="Metabolomics",
    output_type="json"
)
def run_differential_analysis(
    file_path: str,
    group_column: str,
    case_group: Optional[str] = None,
    control_group: Optional[str] = None,
    group1: Optional[str] = None,  # åˆ«åï¼Œå…¼å®¹ planner å‘é€çš„å‚æ•°
    group2: Optional[str] = None,   # åˆ«åï¼Œå…¼å®¹ planner å‘é€çš„å‚æ•°
    method: str = "t-test",
    p_value_threshold: float = 0.05,
    fold_change_threshold: float = 1.5,
    fdr_method: str = "fdr_bh",
    is_logged: bool = True,  # ğŸ”¥ CRITICAL FIX: æ•°æ®æ˜¯å¦å·²Log2è½¬æ¢ï¼ˆé»˜è®¤Trueï¼Œå› ä¸ºSOPå¼ºåˆ¶Log2è½¬æ¢ï¼‰
    output_dir: Optional[str] = None,
    **kwargs  # å®‰å…¨ç½‘ï¼šæ¥å—å…¶ä»–æ„å¤–å‚æ•°
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯ï¼‰
        group_column: åˆ†ç»„åˆ—å
        case_group: å®éªŒç»„åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
        control_group: å¯¹ç…§ç»„åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
        group1: ç¬¬ä¸€ç»„åç§°ï¼ˆåˆ«åï¼Œå…¼å®¹ planner å‚æ•°ï¼‰
        group2: ç¬¬äºŒç»„åç§°ï¼ˆåˆ«åï¼Œå…¼å®¹ planner å‚æ•°ï¼‰
        method: ç»Ÿè®¡æ–¹æ³•ï¼ˆ"t-test" æˆ– "wilcoxon"ï¼Œé»˜è®¤ "t-test"ï¼‰
        p_value_threshold: P å€¼é˜ˆå€¼ï¼ˆé»˜è®¤ 0.05ï¼‰
        fold_change_threshold: å€æ•°å˜åŒ–é˜ˆå€¼ï¼ˆé»˜è®¤ 1.5ï¼‰
        fdr_method: FDR æ ¡æ­£æ–¹æ³•ï¼ˆé»˜è®¤ "fdr_bh"ï¼‰
        is_logged: æ•°æ®æ˜¯å¦å·²Log2è½¬æ¢ï¼ˆé»˜è®¤ Trueï¼Œå› ä¸ºSOPå¼ºåˆ¶Log2è½¬æ¢ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¿å­˜ç»“æœåˆ° CSVï¼‰
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆå®‰å…¨ç½‘ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - results: å·®å¼‚åˆ†æç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªä»£è°¢ç‰©ä¸€è¡Œï¼‰
        - output_path: ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¿å­˜ï¼‰
        - output_file: ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆåˆ«åï¼Œç”¨äºæ•°æ®æµä¼ é€’ï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # æ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨
        if group_column not in df.columns:
            return {
                "status": "error",
                "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­"
            }
        
        # ğŸ”¥ è‡ªåŠ¨æ£€æµ‹åˆ†ç»„ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        # ä¼˜å…ˆä½¿ç”¨ case_group/control_groupï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ group1/group2
        if case_group is None and group1 is not None:
            case_group = group1
        if control_group is None and group2 is not None:
            control_group = group2
        
        # å¦‚æœä»ç„¶ä¸º Noneï¼Œè‡ªåŠ¨æ£€æµ‹å‰ä¸¤ä¸ªå”¯ä¸€å€¼
        if case_group is None or control_group is None:
            unique_groups = sorted(df[group_column].unique().tolist())  # æ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§
            if len(unique_groups) < 2:
                return {
                    "status": "error",
                    "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸­åªæœ‰ {len(unique_groups)} ä¸ªå”¯ä¸€å€¼ï¼Œéœ€è¦è‡³å°‘ 2 ä¸ªç»„"
                }
            # ä½¿ç”¨å‰ä¸¤ä¸ªå”¯ä¸€å€¼
            case_group = unique_groups[0] if case_group is None else case_group
            control_group = unique_groups[1] if control_group is None else control_group
            logger.info(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ†ç»„: case_group={case_group}, control_group={control_group}")
        
        # ğŸ”¥ å°†æ£€æµ‹åˆ°çš„åˆ†ç»„ä¿¡æ¯æ·»åŠ åˆ°è¿”å›ç»“æœä¸­ï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        detected_groups = {
            "case_group": case_group,
            "control_group": control_group,
            "group1": case_group,  # åˆ«å
            "group2": control_group  # åˆ«å
        }
        
        # åˆ†ç¦»åˆ†ç»„
        groups = df[group_column]
        case_mask = groups == case_group
        control_mask = groups == control_group
        
        if not case_mask.any():
            return {
                "status": "error",
                "error": f"å®éªŒç»„ '{case_group}' ä¸å­˜åœ¨äºåˆ†ç»„åˆ—ä¸­ã€‚å¯ç”¨ç»„: {list(unique_groups)}"
            }
        if not control_mask.any():
            return {
                "status": "error",
                "error": f"å¯¹ç…§ç»„ '{control_group}' ä¸å­˜åœ¨äºåˆ†ç»„åˆ—ä¸­ã€‚å¯ç”¨ç»„: {list(unique_groups)}"
            }
        
        # æå–ä»£è°¢ç‰©åˆ—ï¼ˆæ•°å€¼åˆ—ï¼Œæ’é™¤åˆ†ç»„åˆ—ï¼‰
        metabolite_cols = [
            col for col in df.columns 
            if col != group_column and pd.api.types.is_numeric_dtype(df[col])
        ]
        
        results = []
        p_values = []
        
        # ğŸ”¥ æ ¹æ® method å‚æ•°é€‰æ‹©ç»Ÿè®¡æ–¹æ³•
        use_wilcoxon = method.lower() in ["wilcoxon", "wilcox", "ranksum", "mann-whitney"]
        
        for metabolite in metabolite_cols:
            case_values = df.loc[case_mask, metabolite].dropna()
            control_values = df.loc[control_mask, metabolite].dropna()
            
            if len(case_values) < 2 or len(control_values) < 2:
                continue
            
            # ğŸ”¥ æ ¹æ®æ–¹æ³•é€‰æ‹©ç»Ÿè®¡æ£€éªŒ
            if use_wilcoxon:
                # Wilcoxon rank-sum test (Mann-Whitney U test)
                try:
                    u_stat, p_val = stats.ranksums(case_values, control_values)
                except Exception as e:
                    logger.warning(f"âš ï¸ Wilcoxon æ£€éªŒå¤±è´¥ ({metabolite}): {e}ï¼Œè·³è¿‡")
                    continue
            else:
                # T-test (é»˜è®¤)
                try:
                    t_stat, p_val = stats.ttest_ind(case_values, control_values)
                except Exception as e:
                    logger.warning(f"âš ï¸ T-test å¤±è´¥ ({metabolite}): {e}ï¼Œè·³è¿‡")
                    continue
            
            # ğŸ”¥ CRITICAL FIX: è®¡ç®— log2 fold change
            # å¯¹äºå·²Log2è½¬æ¢çš„æ•°æ®ï¼Œä½¿ç”¨å‡æ³•ï¼šLog2FC = Mean_A - Mean_B
            # å¯¹äºåŸå§‹æ•°æ®ï¼Œä½¿ç”¨é™¤æ³•ï¼šLog2FC = log2(Mean_A / Mean_B)
            case_mean = case_values.mean()
            control_mean = control_values.mean()
            
            if is_logged:
                # æ•°æ®å·²Log2è½¬æ¢ï¼Œä½¿ç”¨å‡æ³•
                log2fc = case_mean - control_mean
                logger.debug(f"âœ… [Log2FC] ä½¿ç”¨å‡æ³•ï¼ˆæ•°æ®å·²Log2è½¬æ¢ï¼‰: {case_mean} - {control_mean} = {log2fc}")
            else:
                # åŸå§‹æ•°æ®ï¼Œä½¿ç”¨é™¤æ³•
                if control_mean > 0:
                    log2fc = np.log2(case_mean / control_mean)
                else:
                    # é¿å…é™¤é›¶ï¼Œä½¿ç”¨å°çš„epsilon
                    log2fc = np.log2(case_mean / (control_mean + 1e-9))
                logger.debug(f"âœ… [Log2FC] ä½¿ç”¨é™¤æ³•ï¼ˆåŸå§‹æ•°æ®ï¼‰: log2({case_mean} / {control_mean}) = {log2fc}")
            
            results.append({
                "metabolite": metabolite,
                "p_value": float(p_val),
                "log2fc": float(log2fc),
                "log2_fold_change": float(log2fc),  # åˆ«åï¼Œå…¼å®¹ visualize_volcano
                "case_mean": float(case_mean),
                "control_mean": float(control_mean),
                "case_group": case_group,
                "control_group": control_group
            })
            p_values.append(p_val)
        
        # FDR æ ¡æ­£
        if p_values:
            _, p_adjusted, _, _ = multipletests(p_values, method=fdr_method)
            
            # æ·»åŠ  FDR æ ¡æ­£åçš„ p å€¼
            for i, result in enumerate(results):
                result["fdr"] = float(p_adjusted[i])
                result["fdr_corrected_pvalue"] = float(p_adjusted[i])  # åˆ«åï¼Œå…¼å®¹ visualize_volcano
                # ğŸ”¥ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„é˜ˆå€¼åˆ¤æ–­æ˜¾è‘—æ€§ï¼ˆç¡®ä¿è¿”å› Python åŸç”Ÿ boolï¼‰
                result["significant"] = bool(
                    p_adjusted[i] < p_value_threshold and 
                    abs(result["log2fc"]) >= np.log2(fold_change_threshold)
                )
        
        # ğŸ”¥ ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶ï¼ˆç”¨äºæ•°æ®æµä¼ é€’å’Œå¯è§†åŒ–ï¼‰
        output_path = None
        if output_dir:
            output_path_obj = Path(output_dir)
            output_path_obj.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            input_filename = Path(file_path).stem
            output_path = str(output_path_obj / f"{input_filename}_differential_results.csv")
            
            # è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜
            results_df = pd.DataFrame(results)
            results_df.to_csv(output_path, index=False)
            logger.info(f"ğŸ’¾ å·®å¼‚åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œå°è¯•ä½¿ç”¨è¾“å…¥æ–‡ä»¶æ‰€åœ¨ç›®å½•
            input_dir = Path(file_path).parent
            output_path = str(input_dir / "differential_results.csv")
            results_df = pd.DataFrame(results)
            results_df.to_csv(output_path, index=False)
            logger.info(f"ğŸ’¾ å·®å¼‚åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
        
        # ç»Ÿè®¡æ‘˜è¦
        significant_count = sum(1 for r in results if r.get("significant", False))
        
        return {
            "status": "success",
            "results": results,
            "output_path": output_path,
            "output_file": output_path,  # åˆ«åï¼Œç”¨äºæ•°æ®æµä¼ é€’
            "file_path": output_path,    # å¦ä¸€ä¸ªåˆ«åï¼Œç¡®ä¿å…¼å®¹æ€§
            "summary": {
                "total_metabolites": len(results),
                "significant_count": significant_count,
                "method": method,
                "case_group": case_group,
                "control_group": control_group,
                "p_value_threshold": p_value_threshold,
                "fold_change_threshold": fold_change_threshold
            }
        }
    
    except Exception as e:
        logger.error(f"âŒ å·®å¼‚åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }

