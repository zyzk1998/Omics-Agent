"""
ä»£è°¢ç»„å­¦æ•°æ®é¢„å¤„ç†å·¥å…·
"""
import logging
import os
from typing import Dict, Any, Optional
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

from ...core.tool_registry import registry

logger = logging.getLogger(__name__)


@registry.register(
    name="preprocess_data",
    description="Preprocesses metabolite data: handles missing values, applies log2 transformation, and standardizes the data. Returns preprocessed DataFrame and saves to CSV file.",
    category="Metabolomics",
    output_type="json"
)
def preprocess_metabolite_data(
    file_path: str,
    missing_imputation: str = "min",
    log_transform: bool = True,
    standardize: bool = True,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    é¢„å¤„ç†ä»£è°¢ç‰©æ•°æ®
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼‰
        missing_imputation: ç¼ºå¤±å€¼å¡«å……æ–¹æ³•ï¼ˆ"min", "median", "mean", "zero"ï¼‰
        log_transform: æ˜¯å¦è¿›è¡Œ log2 è½¬æ¢ï¼ˆé»˜è®¤ Trueï¼‰
        standardize: æ˜¯å¦æ ‡å‡†åŒ–ï¼ˆé»˜è®¤ Trueï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - preprocessed_data: é¢„å¤„ç†åçš„æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰
        - output_path: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¿å­˜ï¼‰
        - output_file: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼ˆåˆ«åï¼Œç”¨äºæ•°æ®æµä¼ é€’ï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # æå–æ•°å€¼åˆ—
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        data = df[numeric_cols].copy()
        
        # 1. ç¼ºå¤±å€¼å¡«å……
        if missing_imputation == "min":
            data = data.fillna(data.min())
        elif missing_imputation == "median":
            data = data.fillna(data.median())
        elif missing_imputation == "mean":
            data = data.fillna(data.mean())
        else:  # "zero"
            data = data.fillna(0)
        
        # 2. Log2 è½¬æ¢
        if log_transform:
            data = data.apply(lambda x: np.log2(x + 1))
        
        # 3. æ ‡å‡†åŒ–
        if standardize:
            scaler = StandardScaler()
            data_scaled = scaler.fit_transform(data)
            data = pd.DataFrame(
                data_scaled,
                index=data.index,
                columns=data.columns
            )
        
        # 4. ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°æ–‡ä»¶ï¼ˆç”¨äºæ•°æ®æµä¼ é€’ï¼‰
        output_path = None
        if output_dir:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path_obj = Path(output_dir)
            output_path_obj.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            input_filename = Path(file_path).stem
            output_path = str(output_path_obj / f"{input_filename}_preprocessed.csv")
            
            # ä¿å­˜æ•°æ®
            data.to_csv(output_path)
            logger.info(f"ğŸ’¾ é¢„å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {output_path}")
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œå°è¯•ä½¿ç”¨è¾“å…¥æ–‡ä»¶æ‰€åœ¨ç›®å½•
            input_dir = Path(file_path).parent
            output_path = str(input_dir / "preprocessed_data.csv")
            data.to_csv(output_path)
            logger.info(f"ğŸ’¾ é¢„å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {output_path}")
        
        return {
            "status": "success",
            "preprocessed_data": data.to_dict(orient='index'),
            "output_path": output_path,
            "output_file": output_path,  # åˆ«åï¼Œç”¨äºæ•°æ®æµä¼ é€’
            "file_path": output_path,  # å¦ä¸€ä¸ªåˆ«åï¼Œç¡®ä¿å…¼å®¹æ€§
            "shape": {
                "rows": len(data),
                "columns": len(data.columns)
            }
        }
    
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }

