"""
ä»£è°¢ç»„å­¦é«˜çº§åˆ†æå·¥å…· - PLS-DA å’Œé€šè·¯å¯Œé›†åˆ†æ
"""
import logging
from typing import Dict, Any, Optional, List
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from ...core.tool_registry import registry

logger = logging.getLogger(__name__)


@registry.register(
    name="metabolomics_plsda",
    description="Performs Partial Least Squares Discriminant Analysis (PLS-DA) on metabolite data to identify metabolites that best discriminate between groups. Returns VIP (Variable Importance in Projection) scores for feature selection.",
    category="Metabolomics",
    output_type="mixed"
)
def run_plsda(
    file_path: str,
    group_column: str,
    n_components: int = 2,
    scale: bool = True,
    output_dir: Optional[str] = None,
    **kwargs  # ğŸ”¥ CRITICAL FIX: å®‰å…¨ç½‘ï¼Œæ¥å—å…¶ä»–æ„å¤–å‚æ•°
) -> Dict[str, Any]:
    """
    æ‰§è¡Œ PLS-DA åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯ï¼‰
        group_column: åˆ†ç»„åˆ—å
        n_components: PLS æˆåˆ†æ•°é‡ï¼ˆé»˜è®¤ 2ï¼‰
        scale: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - vip_scores: VIP åˆ†æ•°ï¼ˆæŒ‰é™åºæ’åˆ—ï¼‰
        - pls_scores: PLS å¾—åˆ†
        - plot_path: PLS-DA å›¾è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        from sklearn.cross_decomposition import PLSRegression
        from sklearn.preprocessing import StandardScaler, LabelEncoder
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•æ¨¡ç³ŠåŒ¹é…
        if group_column not in df.columns:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
            group_column_normalized = group_column.lower().replace(' ', '').replace('_', '').replace('-', '')
            matched_column = None
            
            for col in df.columns:
                col_normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                if col_normalized == group_column_normalized:
                    matched_column = col
                    logger.info(f"ğŸ”„ [PLS-DA] æ¨¡ç³ŠåŒ¹é…åˆ†ç»„åˆ—: '{group_column}' -> '{col}'")
                    break
            
            if matched_column:
                group_column = matched_column
            else:
                # ğŸ”¥ æ”¹è¿›ï¼šåŒºåˆ†æ˜¾ç¤ºå…ƒæ•°æ®åˆ—ï¼ˆå¯èƒ½çš„åˆ†ç»„åˆ—ï¼‰å’Œç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ï¼ˆå¯èƒ½æ˜¯åˆ†ç»„åˆ—ï¼‰
                potential_group_cols = []
                for col in df.columns:
                    unique_count = df[col].nunique()
                    if 2 <= unique_count <= 10:
                        potential_group_cols.append(f"{col} ({unique_count}ä¸ªå”¯ä¸€å€¼)")
                
                error_msg = f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ã€‚\n\n"
                
                # æ£€æŸ¥ç´¢å¼•åˆ—æ˜¯å¦å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯
                if df.index.nunique() <= 10:
                    index_values = df.index.unique().tolist()[:10]
                    error_msg += f"âš ï¸ ç´¢å¼•åˆ—ï¼ˆç¬¬ä¸€åˆ—ï¼‰æœ‰ {df.index.nunique()} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯: {index_values}\n"
                    error_msg += "ğŸ’¡ æç¤ºï¼šå¦‚æœåˆ†ç»„ä¿¡æ¯åœ¨ç´¢å¼•åˆ—ä¸­ï¼Œè¯·å°†ç´¢å¼•åˆ—è½¬æ¢ä¸ºæ•°æ®åˆ—ã€‚\n\n"
                
                if metadata_cols:
                    error_msg += f"å¯èƒ½çš„å…ƒæ•°æ®åˆ—ï¼ˆéæ•°å€¼åˆ—ï¼‰: {', '.join(metadata_cols[:10])}\n"
                else:
                    error_msg += "âŒ æœªæ‰¾åˆ°éæ•°å€¼åˆ—ï¼ˆæ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹ï¼‰ã€‚\n"
                    error_msg += "ğŸ’¡ æ•°æ®æ ¼å¼è¦æ±‚ï¼šCSVæ–‡ä»¶åº”åŒ…å«ä¸€åˆ—åˆ†ç»„ä¿¡æ¯ï¼ˆå¦‚ 'Group', 'Condition', 'Treatment' ç­‰ï¼‰ã€‚\n"
                    error_msg += "   æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\n"
                    error_msg += "   SampleID,Group,Metabolite1,Metabolite2,...\n"
                    error_msg += "   Sample1,Control,1.2,3.4,...\n"
                    error_msg += "   Sample2,Treatment,2.3,4.5,...\n\n"
                
                if potential_group_cols:
                    error_msg += f"å¯èƒ½çš„åˆ†ç»„åˆ—ï¼ˆå”¯ä¸€å€¼2-10ï¼‰: {', '.join(potential_group_cols[:10])}\n"
                
                if numeric_cols:
                    error_msg += f"ç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼Œå‰5ä¸ªï¼‰: {', '.join(numeric_cols[:5])}, ..."
                
                return {
                    "status": "error",
                    "error": error_msg.strip()
                }
        
        # æå–æ•°å€¼åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰
        metabolite_cols = [
            col for col in df.columns 
            if col != group_column and pd.api.types.is_numeric_dtype(df[col])
        ]
        
        if len(metabolite_cols) == 0:
            return {
                "status": "error",
                "error": "æœªæ‰¾åˆ°æ•°å€¼å‹ä»£è°¢ç‰©åˆ—"
            }
        
        # å‡†å¤‡æ•°æ®
        X = df[metabolite_cols].values
        y = df[group_column].values
        
        # ğŸ”¥ CRITICAL FIX: å†æ¬¡æ£€æŸ¥ NaNï¼ˆé˜²æ­¢åœ¨æå–åå‡ºç°ï¼‰
        if np.isnan(X).any():
            logger.warning(f"âš ï¸ [PLS-DA] æå–åçš„æ•°æ®ä»åŒ…å« NaNï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……...")
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='median')
            X = imputer.fit_transform(X)
        
        # ç¼–ç åˆ†ç±»å˜é‡
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        
        # æ ‡å‡†åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if scale:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
        else:
            X_scaled = X
        
        # ğŸ”¥ CRITICAL FIX: æœ€ç»ˆæ£€æŸ¥ NaN
        if np.isnan(X_scaled).any():
            logger.error(f"âŒ [PLS-DA] æ ‡å‡†åŒ–åçš„æ•°æ®ä»åŒ…å« NaN")
            return {
                "status": "error",
                "error": "æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼šæ ‡å‡†åŒ–åçš„æ•°æ®åŒ…å« NaN å€¼ã€‚è¯·æ£€æŸ¥è¾“å…¥æ•°æ®è´¨é‡ã€‚"
            }
        
        # æ‰§è¡Œ PLS-DA
        pls = PLSRegression(n_components=n_components, scale=False)
        pls.fit(X_scaled, y_encoded)
        
        # è®¡ç®— PLS å¾—åˆ†
        X_scores = pls.x_scores_
        
        # è®¡ç®— VIP åˆ†æ•°
        # VIP = sqrt(p * (w^2 * SSY) / SSY_total)
        # å…¶ä¸­ p æ˜¯å˜é‡æ•°ï¼Œw æ˜¯æƒé‡ï¼ŒSSY æ˜¯ Y çš„æ–¹å·®è§£é‡Š
        T = pls.x_scores_  # å¾—åˆ†çŸ©é˜µ (n_samples, n_components)
        W = pls.x_weights_  # æƒé‡çŸ©é˜µ (n_features, n_components)
        Q = pls.y_loadings_  # Y çš„è½½è· (n_components,) for single output
        
        # ğŸ”¥ CRITICAL FIX: å¤„ç† y_loadings_ çš„å½¢çŠ¶
        # å¯¹äºå•è¾“å‡º PLSï¼ŒQ æ˜¯ 1D æ•°ç»„ (n_components,)
        # å¯¹äºå¤šè¾“å‡º PLSï¼ŒQ æ˜¯ 2D æ•°ç»„ (n_components, n_targets)
        if Q.ndim == 1:
            # å•è¾“å‡ºï¼šQ[i] æ˜¯æ ‡é‡
            Q_flat = Q
        else:
            # å¤šè¾“å‡ºï¼šå–ç¬¬ä¸€åˆ—ï¼ˆæˆ–å¹³å‡ï¼‰
            Q_flat = Q[:, 0] if Q.shape[1] > 0 else Q.flatten()
        
        # ğŸ”¥ CRITICAL FIX: ç¡®ä¿ Q_flat çš„é•¿åº¦è¶³å¤Ÿ
        # å¦‚æœ Q_flat çš„é•¿åº¦å°äº n_componentsï¼Œä½¿ç”¨å®é™…é•¿åº¦
        actual_n_components = min(n_components, len(Q_flat), T.shape[1])
        logger.debug(f"ğŸ” [PLS-DA] n_components={n_components}, Q_flat.shape={Q_flat.shape}, T.shape={T.shape}, actual_n_components={actual_n_components}")
        
        # è®¡ç®—æ¯ä¸ªæˆåˆ†çš„æ–¹å·®è§£é‡Š
        explained_variance = []
        for i in range(actual_n_components):
            # ğŸ”¥ CRITICAL FIX: Q_flat[i] æ˜¯æ ‡é‡ï¼Œä½¿ç”¨å…ƒç´ ä¹˜æ³•
            # SSY = sum((T[:, i] * Q_flat[i])^2) = Q_flat[i]^2 * sum(T[:, i]^2)
            if i < len(Q_flat) and i < T.shape[1]:
                ssy = Q_flat[i] ** 2 * np.sum(T[:, i] ** 2)
                explained_variance.append(ssy)
            else:
                # å¦‚æœç´¢å¼•è¶Šç•Œï¼Œä½¿ç”¨é»˜è®¤å€¼
                logger.warning(f"âš ï¸ [PLS-DA] ç´¢å¼• {i} è¶Šç•Œï¼Œè·³è¿‡è¯¥æˆåˆ†")
                explained_variance.append(0.0)
        
        total_ssy = sum(explained_variance)
        
        # è®¡ç®— VIP
        vip_scores = []
        for j in range(len(metabolite_cols)):
            vip = 0
            for i in range(actual_n_components):
                if total_ssy > 0 and i < len(explained_variance) and i < W.shape[1]:
                    vip += (W[j, i] ** 2) * (explained_variance[i] / total_ssy)
            vip = np.sqrt(len(metabolite_cols) * vip) if vip > 0 else 0.0
            vip_scores.append(vip)
        
        # åˆ›å»º VIP åˆ†æ•° DataFrame
        vip_df = pd.DataFrame({
            'metabolite': metabolite_cols,
            'vip_score': vip_scores
        }).sort_values('vip_score', ascending=False)
        
        # ç”Ÿæˆå›¾ç‰‡ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
        plot_path = None
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            plot_path = str(output_path / "plsda_plot.png")
            
            # PLS-DA å¾—åˆ†å›¾
            unique_groups = le.classes_
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_groups)))
            
            plt.figure(figsize=(10, 8))
            
            # ğŸ”¥ CRITICAL FIX: å¤„ç†ä¸åŒæ•°é‡çš„æˆåˆ†
            if actual_n_components >= 2 and X_scores.shape[1] >= 2:
                # 2D æ•£ç‚¹å›¾ï¼ˆComponent 1 vs Component 2ï¼‰
                for i, group in enumerate(unique_groups):
                    mask = y == group
                    plt.scatter(
                        X_scores[mask, 0], 
                        X_scores[mask, 1],
                        label=group,
                        color=colors[i],
                        alpha=0.6,
                        s=50
                    )
                
                comp1_var = explained_variance[0]/total_ssy*100 if len(explained_variance) > 0 and total_ssy > 0 else 0.0
                comp2_var = explained_variance[1]/total_ssy*100 if len(explained_variance) > 1 and total_ssy > 0 else 0.0
                plt.xlabel(f"PLS Component 1 ({comp1_var:.1f}%)")
                plt.ylabel(f"PLS Component 2 ({comp2_var:.1f}%)")
            elif actual_n_components == 1 and X_scores.shape[1] >= 1:
                # 1D æ•£ç‚¹å›¾ï¼ˆComponent 1 onlyï¼‰
                for i, group in enumerate(unique_groups):
                    mask = y == group
                    plt.scatter(
                        X_scores[mask, 0], 
                        np.zeros(np.sum(mask)),
                        label=group,
                        color=colors[i],
                        alpha=0.6,
                        s=50
                    )
                
                comp1_var = explained_variance[0]/total_ssy*100 if len(explained_variance) > 0 and total_ssy > 0 else 0.0
                plt.xlabel(f"PLS Component 1 ({comp1_var:.1f}%)")
                plt.ylabel("")
            else:
                # é™çº§ï¼šä½¿ç”¨å‰ä¸¤ä¸ªç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                logger.warning(f"âš ï¸ [PLS-DA] æ— æ³•ç»˜åˆ¶æ ‡å‡† PLS-DA å›¾ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                if X_scores.shape[1] >= 2:
                    for i, group in enumerate(unique_groups):
                        mask = y == group
                        plt.scatter(
                            X_scores[mask, 0], 
                            X_scores[mask, 1],
                            label=group,
                            color=colors[i],
                            alpha=0.6,
                            s=50
                        )
                    plt.xlabel("PLS Component 1")
                    plt.ylabel("PLS Component 2")
                elif X_scores.shape[1] >= 1:
                    for i, group in enumerate(unique_groups):
                        mask = y == group
                        plt.scatter(
                            X_scores[mask, 0], 
                            np.zeros(np.sum(mask)),
                            label=group,
                            color=colors[i],
                            alpha=0.6,
                            s=50
                        )
                    plt.xlabel("PLS Component 1")
                    plt.ylabel("")
            
            plt.title("PLS-DA Score Plot")
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
        
        # ğŸ”¥ Phase 2: Extract top VIP markers for AI report
        top_vip_markers = vip_df.head(10).to_dict(orient='records')
        top_vip_markers_formatted = [
            {
                "name": m.get("metabolite", "Unknown"),
                "vip": float(m.get("vip_score", 0.0))
            }
            for m in top_vip_markers
        ]
        
        return {
            "status": "success",
            "vip_scores": vip_df.to_dict(orient='records'),
            "top_vip_metabolites": vip_df.head(20).to_dict(orient='records'),
            "explained_variance": {
                f"Component{i+1}": float(var / total_ssy * 100) 
                for i, var in enumerate(explained_variance)
            },
            "plot_path": plot_path,
            "n_components": n_components,
            "summary": {
                "top_vip_markers": top_vip_markers_formatted,
                "n_components": n_components,
                "total_metabolites": len(metabolite_cols),
                "comp1_variance": float(explained_variance[0] / total_ssy * 100) if len(explained_variance) > 0 and total_ssy > 0 else 0.0,
                "comp2_variance": float(explained_variance[1] / total_ssy * 100) if len(explained_variance) > 1 and total_ssy > 0 else 0.0
            }
        }
    
    except ImportError as e:
        return {
            "status": "error",
            "error": f"ç¼ºå°‘å¿…è¦çš„ä¾èµ–: {str(e)}. è¯·å®‰è£…: pip install scikit-learn"
        }
    except Exception as e:
        logger.error(f"âŒ PLS-DA åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


@registry.register(
    name="metabolomics_pathway_enrichment",
    description="Performs KEGG pathway enrichment analysis on metabolite data using GSEApy. Identifies significantly enriched metabolic pathways based on metabolite abundance changes between groups.",
    category="Metabolomics",
    output_type="json"
)
def run_pathway_enrichment(
    file_path: str,
    group_column: str,
    case_group: Optional[str] = None,
    control_group: Optional[str] = None,
    organism: str = "hsa",
    p_value_threshold: float = 0.05,
    output_dir: Optional[str] = None,
    **kwargs  # ğŸ”¥ CRITICAL FIX: å®‰å…¨ç½‘ï¼Œæ¥å—å…¶ä»–æ„å¤–å‚æ•°
) -> Dict[str, Any]:
    """
    æ‰§è¡Œé€šè·¯å¯Œé›†åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯ï¼‰
        group_column: åˆ†ç»„åˆ—å
        case_group: å®éªŒç»„åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸º None æˆ–å ä½ç¬¦ï¼Œå°†è‡ªåŠ¨æ£€æµ‹ï¼‰
        control_group: å¯¹ç…§ç»„åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸º None æˆ–å ä½ç¬¦ï¼Œå°†è‡ªåŠ¨æ£€æµ‹ï¼‰
        organism: ç‰©ç§ä»£ç ï¼ˆé»˜è®¤ "hsa" äººç±»ï¼Œå¯é€‰ "mmu" å°é¼ ç­‰ï¼‰
        p_value_threshold: P å€¼é˜ˆå€¼ï¼ˆé»˜è®¤ 0.05ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - enriched_pathways: å¯Œé›†é€šè·¯åˆ—è¡¨
        - summary: å¯Œé›†åˆ†ææ‘˜è¦
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        import gseapy as gp
        
        # ğŸ”¥ CRITICAL FIX: Map organism codes to gseapy format
        # gseapy expects full names like "human", "mouse", not codes like "hsa", "mmu"
        organism_mapping = {
            "hsa": "human",
            "mmu": "mouse",
            "rno": "rat",
            "bta": "cow",  # ç‰›
            "gga": "chicken",  # é¸¡
            "dre": "zebrafish",  # æ–‘é©¬é±¼
            "cel": "worm",  # çº¿è™«
            "dme": "fly",  # æœè‡
        }
        
        # Convert organism code to full name if needed
        gseapy_organism = organism_mapping.get(organism.lower(), organism.lower())
        
        # Check if organism is supported
        supported_organisms = ["human", "mouse", "rat"]
        if gseapy_organism not in supported_organisms:
            logger.warning(f"âš ï¸ [Pathway Enrichment] Organism '{organism}' (mapped to '{gseapy_organism}') is not supported by gseapy database")
            return {
                "status": "warning",
                "message": f"Organism '{organism}' is not supported by the pathway enrichment database. Step skipped.",
                "enriched_pathways": [],
                "skipped_reason": f"Unsupported organism: {organism}"
            }
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # ğŸ”¥ CRITICAL FIX: è‡ªåŠ¨æ£€æµ‹åˆ†ç»„å€¼ï¼ˆå¦‚æœ case_group æˆ– control_group æ˜¯ None æˆ–å ä½ç¬¦ï¼‰
        if not case_group or not control_group or case_group.startswith("<") or control_group.startswith("<"):
            logger.info(f"ğŸ”„ [Pathway Enrichment] è‡ªåŠ¨æ£€æµ‹åˆ†ç»„å€¼...")
            if group_column not in df.columns:
                return {
                    "status": "error",
                    "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­"
                }
            
            unique_groups = sorted(df[group_column].unique().tolist())
            if len(unique_groups) < 2:
                return {
                    "status": "error",
                    "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸­åªæœ‰ {len(unique_groups)} ä¸ªå”¯ä¸€å€¼ï¼Œéœ€è¦è‡³å°‘ 2 ä¸ªç»„"
                }
            
            # ä½¿ç”¨å‰ä¸¤ä¸ªå”¯ä¸€å€¼
            if not case_group or case_group.startswith("<"):
                case_group = unique_groups[0]
            if not control_group or control_group.startswith("<"):
                control_group = unique_groups[1]
            
            logger.info(f"âœ… [Pathway Enrichment] è‡ªåŠ¨æ£€æµ‹åˆ†ç»„: case_group={case_group}, control_group={control_group}")
        
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•æ¨¡ç³ŠåŒ¹é…
        if group_column not in df.columns:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
            group_column_normalized = group_column.lower().replace(' ', '').replace('_', '').replace('-', '')
            matched_column = None
            
            for col in df.columns:
                col_normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                if col_normalized == group_column_normalized:
                    matched_column = col
                    logger.info(f"ğŸ”„ [Pathway Enrichment] æ¨¡ç³ŠåŒ¹é…åˆ†ç»„åˆ—: '{group_column}' -> '{col}'")
                    break
            
            if matched_column:
                group_column = matched_column
            else:
                # ğŸ”¥ æ”¹è¿›ï¼šåŒºåˆ†æ˜¾ç¤ºå…ƒæ•°æ®åˆ—ï¼ˆå¯èƒ½çš„åˆ†ç»„åˆ—ï¼‰å’Œç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ï¼ˆå¯èƒ½æ˜¯åˆ†ç»„åˆ—ï¼‰
                potential_group_cols = []
                for col in df.columns:
                    unique_count = df[col].nunique()
                    if 2 <= unique_count <= 10:
                        potential_group_cols.append(f"{col} ({unique_count}ä¸ªå”¯ä¸€å€¼)")
                
                error_msg = f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ã€‚\n\n"
                
                # æ£€æŸ¥ç´¢å¼•åˆ—æ˜¯å¦å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯
                if df.index.nunique() <= 10:
                    index_values = df.index.unique().tolist()[:10]
                    error_msg += f"âš ï¸ ç´¢å¼•åˆ—ï¼ˆç¬¬ä¸€åˆ—ï¼‰æœ‰ {df.index.nunique()} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯: {index_values}\n"
                    error_msg += "ğŸ’¡ æç¤ºï¼šå¦‚æœåˆ†ç»„ä¿¡æ¯åœ¨ç´¢å¼•åˆ—ä¸­ï¼Œè¯·å°†ç´¢å¼•åˆ—è½¬æ¢ä¸ºæ•°æ®åˆ—ã€‚\n\n"
                
                if metadata_cols:
                    error_msg += f"å¯èƒ½çš„å…ƒæ•°æ®åˆ—ï¼ˆéæ•°å€¼åˆ—ï¼‰: {', '.join(metadata_cols[:10])}\n"
                else:
                    error_msg += "âŒ æœªæ‰¾åˆ°éæ•°å€¼åˆ—ï¼ˆæ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹ï¼‰ã€‚\n"
                    error_msg += "ğŸ’¡ æ•°æ®æ ¼å¼è¦æ±‚ï¼šCSVæ–‡ä»¶åº”åŒ…å«ä¸€åˆ—åˆ†ç»„ä¿¡æ¯ï¼ˆå¦‚ 'Group', 'Condition', 'Treatment' ç­‰ï¼‰ã€‚\n"
                    error_msg += "   æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\n"
                    error_msg += "   SampleID,Group,Metabolite1,Metabolite2,...\n"
                    error_msg += "   Sample1,Control,1.2,3.4,...\n"
                    error_msg += "   Sample2,Treatment,2.3,4.5,...\n\n"
                
                if potential_group_cols:
                    error_msg += f"å¯èƒ½çš„åˆ†ç»„åˆ—ï¼ˆå”¯ä¸€å€¼2-10ï¼‰: {', '.join(potential_group_cols[:10])}\n"
                
                if numeric_cols:
                    error_msg += f"ç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼Œå‰5ä¸ªï¼‰: {', '.join(numeric_cols[:5])}, ..."
                
                return {
                    "status": "error",
                    "error": error_msg.strip()
                }
        
        # åˆ†ç¦»åˆ†ç»„
        groups = df[group_column]
        case_mask = groups == case_group
        control_mask = groups == control_group
        
        if not case_mask.any():
            return {
                "status": "error",
                "error": f"å®éªŒç»„ '{case_group}' ä¸å­˜åœ¨"
            }
        if not control_mask.any():
            return {
                "status": "error",
                "error": f"å¯¹ç…§ç»„ '{control_group}' ä¸å­˜åœ¨"
            }
        
        # æå–ä»£è°¢ç‰©åˆ—ï¼ˆæ•°å€¼åˆ—ï¼Œæ’é™¤åˆ†ç»„åˆ—ï¼‰
        metabolite_cols = [
            col for col in df.columns 
            if col != group_column and pd.api.types.is_numeric_dtype(df[col])
        ]
        
        # è®¡ç®— fold changeï¼ˆç”¨äºæ’åºï¼‰
        case_mean = df.loc[case_mask, metabolite_cols].mean()
        control_mean = df.loc[control_mask, metabolite_cols].mean()
        
        # é¿å…é™¤é›¶
        control_mean = control_mean.replace(0, np.nan)
        fold_change = case_mean / control_mean
        fold_change = fold_change.fillna(0)
        
        # åˆ›å»ºæ’åºçš„ä»£è°¢ç‰©åˆ—è¡¨ï¼ˆæŒ‰ fold change é™åºï¼‰
        metabolite_rank = fold_change.sort_values(ascending=False)
        
        # å‡†å¤‡ GSEApy è¾“å…¥ï¼ˆéœ€è¦ä»£è°¢ç‰©åç§°åˆ° KEGG ID çš„æ˜ å°„ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ä»£è°¢ç‰©åç§°ä½œä¸º IDï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ˜ å°„åˆ° KEGG IDï¼‰
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ä»£è°¢ç‰©åç§°
        metabolite_list = metabolite_rank.index.tolist()
        
        # æ‰§è¡Œé€šè·¯å¯Œé›†åˆ†æ
        # æ³¨æ„ï¼šgseapy éœ€è¦ä»£è°¢ç‰© ID æ˜ å°„åˆ° KEGGï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        try:
            # ğŸ”¥ CRITICAL FIX: Use mapped organism name and appropriate gene sets
            gene_sets_map = {
                "human": ['KEGG_2021_Human'],
                "mouse": ['KEGG_2021_Mouse'],
                "rat": ['KEGG_2021_Rat']
            }
            gene_sets = gene_sets_map.get(gseapy_organism, ['KEGG_2021_Human'])  # Default to human
            
            # å°è¯•ä½¿ç”¨ KEGG æ•°æ®åº“
            enr = gp.enrichr(
                gene_list=metabolite_list[:100],  # é™åˆ¶å‰100ä¸ª
                gene_sets=gene_sets,
                organism=gseapy_organism,  # ğŸ”¥ CRITICAL FIX: Use mapped organism name
                outdir=None,  # ä¸ä¿å­˜æ–‡ä»¶
                verbose=False
            )
            
            # æå–ç»“æœ
            if enr is not None and hasattr(enr, 'results'):
                results_df = enr.results
                
                # è¿‡æ»¤æ˜¾è‘—é€šè·¯
                significant_pathways = results_df[
                    results_df['Adjusted P-value'] < p_value_threshold
                ].copy()
                
                # ğŸ”¥ Phase 2: Extract top pathways for AI report
                top_pathways = []
                if len(significant_pathways) > 0:
                    # Sort by Adjusted P-value (ascending) and take top 5
                    top_pathways_df = significant_pathways.nsmallest(5, 'Adjusted P-value')
                    top_pathways = top_pathways_df['Term'].tolist() if 'Term' in top_pathways_df.columns else []
                    # Fallback: use 'Gene_set' or 'Pathway' column if 'Term' doesn't exist
                    if not top_pathways and 'Gene_set' in top_pathways_df.columns:
                        top_pathways = top_pathways_df['Gene_set'].tolist()[:5]
                    elif not top_pathways and 'Pathway' in top_pathways_df.columns:
                        top_pathways = top_pathways_df['Pathway'].tolist()[:5]
                
                # ä¿å­˜ç»“æœï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
                output_csv = None
                if output_dir:
                    output_path = Path(output_dir)
                    output_path.mkdir(parents=True, exist_ok=True)
                    output_csv = str(output_path / "pathway_enrichment.csv")
                    significant_pathways.to_csv(output_csv, index=False)
                
                return {
                    "status": "success",
                    "enriched_pathways": significant_pathways.to_dict(orient='records'),
                    "n_significant": len(significant_pathways),
                    "n_total": len(results_df),
                    "output_csv": output_csv,
                    "summary": {
                        "n_significant": len(significant_pathways),
                        "n_total": len(results_df),
                        "top_pathways": top_pathways,
                        "p_value_threshold": p_value_threshold
                    }
                }
            else:
                return {
                    "status": "warning",
                    "message": "é€šè·¯å¯Œé›†åˆ†æå®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ˜¾è‘—å¯Œé›†é€šè·¯",
                    "enriched_pathways": []
                }
        
        except Exception as e:
            # ğŸ”¥ CRITICAL FIX: If gseapy fails, return warning (not error) so pipeline continues
            error_msg = str(e)
            logger.warning(f"âš ï¸ [Pathway Enrichment] GSEApy æ‰§è¡Œå¤±è´¥: {error_msg}")
            
            # Check if it's an organism-related error
            if "organism" in error_msg.lower() or "not supported" in error_msg.lower():
                return {
                    "status": "warning",
                    "message": f"Organism '{organism}' is not supported by the pathway enrichment database. Step skipped.",
                    "enriched_pathways": [],
                    "skipped_reason": f"Unsupported organism: {organism}"
                }
            
            # Other errors: return warning (not error) to allow pipeline to continue
            return {
                "status": "warning",
                "message": f"Pathway enrichment could not be performed due to database limitations. Step skipped.",
                "enriched_pathways": [],
                "skipped_reason": error_msg
            }
    
    except ImportError as e:
        # ğŸ”¥ CRITICAL FIX: ImportError should return warning, not error, so pipeline continues
        logger.warning(f"âš ï¸ [Pathway Enrichment] gseapy æœªå®‰è£…: {e}")
        return {
            "status": "warning",
            "message": "Pathway enrichment could not be performed because gseapy library is not installed. Step skipped.",
            "enriched_pathways": [],
            "skipped_reason": "gseapy not installed"
        }
    except Exception as e:
        # ğŸ”¥ CRITICAL FIX: All other exceptions should return warning, not error
        logger.warning(f"âš ï¸ [Pathway Enrichment] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "warning",
            "message": f"Pathway enrichment could not be performed due to an error. Step skipped.",
            "enriched_pathways": [],
            "skipped_reason": str(e)
        }


