"""
ä»£è°¢ç»„å­¦é«˜çº§åˆ†æå·¥å…· - PLS-DA å’Œé€šè·¯å¯Œé›†åˆ†æ
"""
import logging
from typing import Dict, Any, Optional, List
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from ...core.tool_registry import registry

logger = logging.getLogger(__name__)


@registry.register(
    name="metabolomics_plsda",
    description="Performs Partial Least Squares Discriminant Analysis (PLS-DA) on metabolite data to identify metabolites that best discriminate between groups. Returns VIP (Variable Importance in Projection) scores for feature selection.",
    category="Metabolomics",
    output_type="mixed"
)
def run_plsda(
    file_path: str,
    group_column: str,
    n_components: int = 2,
    scale: bool = True,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œ PLS-DA åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯ï¼‰
        group_column: åˆ†ç»„åˆ—å
        n_components: PLS æˆåˆ†æ•°é‡ï¼ˆé»˜è®¤ 2ï¼‰
        scale: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - vip_scores: VIP åˆ†æ•°ï¼ˆæŒ‰é™åºæ’åˆ—ï¼‰
        - pls_scores: PLS å¾—åˆ†
        - plot_path: PLS-DA å›¾è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        from sklearn.cross_decomposition import PLSRegression
        from sklearn.preprocessing import StandardScaler, LabelEncoder
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•æ¨¡ç³ŠåŒ¹é…
        if group_column not in df.columns:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
            group_column_normalized = group_column.lower().replace(' ', '').replace('_', '').replace('-', '')
            matched_column = None
            
            for col in df.columns:
                col_normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                if col_normalized == group_column_normalized:
                    matched_column = col
                    logger.info(f"ğŸ”„ [PLS-DA] æ¨¡ç³ŠåŒ¹é…åˆ†ç»„åˆ—: '{group_column}' -> '{col}'")
                    break
            
            if matched_column:
                group_column = matched_column
            else:
                # ğŸ”¥ æ”¹è¿›ï¼šåŒºåˆ†æ˜¾ç¤ºå…ƒæ•°æ®åˆ—ï¼ˆå¯èƒ½çš„åˆ†ç»„åˆ—ï¼‰å’Œç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ï¼ˆå¯èƒ½æ˜¯åˆ†ç»„åˆ—ï¼‰
                potential_group_cols = []
                for col in df.columns:
                    unique_count = df[col].nunique()
                    if 2 <= unique_count <= 10:
                        potential_group_cols.append(f"{col} ({unique_count}ä¸ªå”¯ä¸€å€¼)")
                
                error_msg = f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ã€‚\n\n"
                
                # æ£€æŸ¥ç´¢å¼•åˆ—æ˜¯å¦å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯
                if df.index.nunique() <= 10:
                    index_values = df.index.unique().tolist()[:10]
                    error_msg += f"âš ï¸ ç´¢å¼•åˆ—ï¼ˆç¬¬ä¸€åˆ—ï¼‰æœ‰ {df.index.nunique()} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯: {index_values}\n"
                    error_msg += "ğŸ’¡ æç¤ºï¼šå¦‚æœåˆ†ç»„ä¿¡æ¯åœ¨ç´¢å¼•åˆ—ä¸­ï¼Œè¯·å°†ç´¢å¼•åˆ—è½¬æ¢ä¸ºæ•°æ®åˆ—ã€‚\n\n"
                
                if metadata_cols:
                    error_msg += f"å¯èƒ½çš„å…ƒæ•°æ®åˆ—ï¼ˆéæ•°å€¼åˆ—ï¼‰: {', '.join(metadata_cols[:10])}\n"
                else:
                    error_msg += "âŒ æœªæ‰¾åˆ°éæ•°å€¼åˆ—ï¼ˆæ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹ï¼‰ã€‚\n"
                    error_msg += "ğŸ’¡ æ•°æ®æ ¼å¼è¦æ±‚ï¼šCSVæ–‡ä»¶åº”åŒ…å«ä¸€åˆ—åˆ†ç»„ä¿¡æ¯ï¼ˆå¦‚ 'Group', 'Condition', 'Treatment' ç­‰ï¼‰ã€‚\n"
                    error_msg += "   æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\n"
                    error_msg += "   SampleID,Group,Metabolite1,Metabolite2,...\n"
                    error_msg += "   Sample1,Control,1.2,3.4,...\n"
                    error_msg += "   Sample2,Treatment,2.3,4.5,...\n\n"
                
                if potential_group_cols:
                    error_msg += f"å¯èƒ½çš„åˆ†ç»„åˆ—ï¼ˆå”¯ä¸€å€¼2-10ï¼‰: {', '.join(potential_group_cols[:10])}\n"
                
                if numeric_cols:
                    error_msg += f"ç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼Œå‰5ä¸ªï¼‰: {', '.join(numeric_cols[:5])}, ..."
                
                return {
                    "status": "error",
                    "error": error_msg.strip()
                }
        
        # æå–æ•°å€¼åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰
        metabolite_cols = [
            col for col in df.columns 
            if col != group_column and pd.api.types.is_numeric_dtype(df[col])
        ]
        
        if len(metabolite_cols) == 0:
            return {
                "status": "error",
                "error": "æœªæ‰¾åˆ°æ•°å€¼å‹ä»£è°¢ç‰©åˆ—"
            }
        
        # å‡†å¤‡æ•°æ®
        X = df[metabolite_cols].values
        y = df[group_column].values
        
        # ç¼–ç åˆ†ç±»å˜é‡
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        
        # æ ‡å‡†åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if scale:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
        else:
            X_scaled = X
        
        # æ‰§è¡Œ PLS-DA
        pls = PLSRegression(n_components=n_components, scale=False)
        pls.fit(X_scaled, y_encoded)
        
        # è®¡ç®— PLS å¾—åˆ†
        X_scores = pls.x_scores_
        
        # è®¡ç®— VIP åˆ†æ•°
        # VIP = sqrt(p * (w^2 * SSY) / SSY_total)
        # å…¶ä¸­ p æ˜¯å˜é‡æ•°ï¼Œw æ˜¯æƒé‡ï¼ŒSSY æ˜¯ Y çš„æ–¹å·®è§£é‡Š
        T = pls.x_scores_  # å¾—åˆ†çŸ©é˜µ
        W = pls.x_weights_  # æƒé‡çŸ©é˜µ
        Q = pls.y_loadings_  # Y çš„è½½è·
        
        # è®¡ç®—æ¯ä¸ªæˆåˆ†çš„æ–¹å·®è§£é‡Š
        explained_variance = []
        for i in range(n_components):
            ssy = np.sum((T[:, i] @ Q[i]) ** 2)
            explained_variance.append(ssy)
        
        total_ssy = sum(explained_variance)
        
        # è®¡ç®— VIP
        vip_scores = []
        for j in range(len(metabolite_cols)):
            vip = 0
            for i in range(n_components):
                if total_ssy > 0:
                    vip += (W[j, i] ** 2) * (explained_variance[i] / total_ssy)
            vip = np.sqrt(len(metabolite_cols) * vip)
            vip_scores.append(vip)
        
        # åˆ›å»º VIP åˆ†æ•° DataFrame
        vip_df = pd.DataFrame({
            'metabolite': metabolite_cols,
            'vip_score': vip_scores
        }).sort_values('vip_score', ascending=False)
        
        # ç”Ÿæˆå›¾ç‰‡ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
        plot_path = None
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            plot_path = str(output_path / "plsda_plot.png")
            
            # PLS-DA å¾—åˆ†å›¾
            unique_groups = le.classes_
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_groups)))
            
            plt.figure(figsize=(10, 8))
            for i, group in enumerate(unique_groups):
                mask = y == group
                plt.scatter(
                    X_scores[mask, 0], 
                    X_scores[mask, 1],
                    label=group,
                    color=colors[i],
                    alpha=0.6,
                    s=50
                )
            
            plt.xlabel(f"PLS Component 1 ({explained_variance[0]/total_ssy*100:.1f}%)")
            plt.ylabel(f"PLS Component 2 ({explained_variance[1]/total_ssy*100:.1f}%)")
            plt.title("PLS-DA Score Plot")
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
        
        return {
            "status": "success",
            "vip_scores": vip_df.to_dict(orient='records'),
            "top_vip_metabolites": vip_df.head(20).to_dict(orient='records'),
            "explained_variance": {
                f"Component{i+1}": float(var / total_ssy * 100) 
                for i, var in enumerate(explained_variance)
            },
            "plot_path": plot_path,
            "n_components": n_components
        }
    
    except ImportError as e:
        return {
            "status": "error",
            "error": f"ç¼ºå°‘å¿…è¦çš„ä¾èµ–: {str(e)}. è¯·å®‰è£…: pip install scikit-learn"
        }
    except Exception as e:
        logger.error(f"âŒ PLS-DA åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


@registry.register(
    name="metabolomics_pathway_enrichment",
    description="Performs KEGG pathway enrichment analysis on metabolite data using GSEApy. Identifies significantly enriched metabolic pathways based on metabolite abundance changes between groups.",
    category="Metabolomics",
    output_type="json"
)
def run_pathway_enrichment(
    file_path: str,
    group_column: str,
    case_group: str,
    control_group: str,
    organism: str = "hsa",
    p_value_threshold: float = 0.05,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œé€šè·¯å¯Œé›†åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯ï¼‰
        group_column: åˆ†ç»„åˆ—å
        case_group: å®éªŒç»„åç§°
        control_group: å¯¹ç…§ç»„åç§°
        organism: ç‰©ç§ä»£ç ï¼ˆé»˜è®¤ "hsa" äººç±»ï¼Œå¯é€‰ "mmu" å°é¼ ç­‰ï¼‰
        p_value_threshold: P å€¼é˜ˆå€¼ï¼ˆé»˜è®¤ 0.05ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - enriched_pathways: å¯Œé›†é€šè·¯åˆ—è¡¨
        - summary: å¯Œé›†åˆ†ææ‘˜è¦
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        import gseapy as gp
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•æ¨¡ç³ŠåŒ¹é…
        if group_column not in df.columns:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
            group_column_normalized = group_column.lower().replace(' ', '').replace('_', '').replace('-', '')
            matched_column = None
            
            for col in df.columns:
                col_normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                if col_normalized == group_column_normalized:
                    matched_column = col
                    logger.info(f"ğŸ”„ [PLS-DA] æ¨¡ç³ŠåŒ¹é…åˆ†ç»„åˆ—: '{group_column}' -> '{col}'")
                    break
            
            if matched_column:
                group_column = matched_column
            else:
                # ğŸ”¥ æ”¹è¿›ï¼šåŒºåˆ†æ˜¾ç¤ºå…ƒæ•°æ®åˆ—ï¼ˆå¯èƒ½çš„åˆ†ç»„åˆ—ï¼‰å’Œç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ï¼ˆå¯èƒ½æ˜¯åˆ†ç»„åˆ—ï¼‰
                potential_group_cols = []
                for col in df.columns:
                    unique_count = df[col].nunique()
                    if 2 <= unique_count <= 10:
                        potential_group_cols.append(f"{col} ({unique_count}ä¸ªå”¯ä¸€å€¼)")
                
                error_msg = f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ã€‚\n\n"
                
                # æ£€æŸ¥ç´¢å¼•åˆ—æ˜¯å¦å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯
                if df.index.nunique() <= 10:
                    index_values = df.index.unique().tolist()[:10]
                    error_msg += f"âš ï¸ ç´¢å¼•åˆ—ï¼ˆç¬¬ä¸€åˆ—ï¼‰æœ‰ {df.index.nunique()} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯: {index_values}\n"
                    error_msg += "ğŸ’¡ æç¤ºï¼šå¦‚æœåˆ†ç»„ä¿¡æ¯åœ¨ç´¢å¼•åˆ—ä¸­ï¼Œè¯·å°†ç´¢å¼•åˆ—è½¬æ¢ä¸ºæ•°æ®åˆ—ã€‚\n\n"
                
                if metadata_cols:
                    error_msg += f"å¯èƒ½çš„å…ƒæ•°æ®åˆ—ï¼ˆéæ•°å€¼åˆ—ï¼‰: {', '.join(metadata_cols[:10])}\n"
                else:
                    error_msg += "âŒ æœªæ‰¾åˆ°éæ•°å€¼åˆ—ï¼ˆæ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹ï¼‰ã€‚\n"
                    error_msg += "ğŸ’¡ æ•°æ®æ ¼å¼è¦æ±‚ï¼šCSVæ–‡ä»¶åº”åŒ…å«ä¸€åˆ—åˆ†ç»„ä¿¡æ¯ï¼ˆå¦‚ 'Group', 'Condition', 'Treatment' ç­‰ï¼‰ã€‚\n"
                    error_msg += "   æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\n"
                    error_msg += "   SampleID,Group,Metabolite1,Metabolite2,...\n"
                    error_msg += "   Sample1,Control,1.2,3.4,...\n"
                    error_msg += "   Sample2,Treatment,2.3,4.5,...\n\n"
                
                if potential_group_cols:
                    error_msg += f"å¯èƒ½çš„åˆ†ç»„åˆ—ï¼ˆå”¯ä¸€å€¼2-10ï¼‰: {', '.join(potential_group_cols[:10])}\n"
                
                if numeric_cols:
                    error_msg += f"ç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼Œå‰5ä¸ªï¼‰: {', '.join(numeric_cols[:5])}, ..."
                
                return {
                    "status": "error",
                    "error": error_msg.strip()
                }
        
        # åˆ†ç¦»åˆ†ç»„
        groups = df[group_column]
        case_mask = groups == case_group
        control_mask = groups == control_group
        
        if not case_mask.any():
            return {
                "status": "error",
                "error": f"å®éªŒç»„ '{case_group}' ä¸å­˜åœ¨"
            }
        if not control_mask.any():
            return {
                "status": "error",
                "error": f"å¯¹ç…§ç»„ '{control_group}' ä¸å­˜åœ¨"
            }
        
        # æå–ä»£è°¢ç‰©åˆ—ï¼ˆæ•°å€¼åˆ—ï¼Œæ’é™¤åˆ†ç»„åˆ—ï¼‰
        metabolite_cols = [
            col for col in df.columns 
            if col != group_column and pd.api.types.is_numeric_dtype(df[col])
        ]
        
        # è®¡ç®— fold changeï¼ˆç”¨äºæ’åºï¼‰
        case_mean = df.loc[case_mask, metabolite_cols].mean()
        control_mean = df.loc[control_mask, metabolite_cols].mean()
        
        # é¿å…é™¤é›¶
        control_mean = control_mean.replace(0, np.nan)
        fold_change = case_mean / control_mean
        fold_change = fold_change.fillna(0)
        
        # åˆ›å»ºæ’åºçš„ä»£è°¢ç‰©åˆ—è¡¨ï¼ˆæŒ‰ fold change é™åºï¼‰
        metabolite_rank = fold_change.sort_values(ascending=False)
        
        # å‡†å¤‡ GSEApy è¾“å…¥ï¼ˆéœ€è¦ä»£è°¢ç‰©åç§°åˆ° KEGG ID çš„æ˜ å°„ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ä»£è°¢ç‰©åç§°ä½œä¸º IDï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ˜ å°„åˆ° KEGG IDï¼‰
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ä»£è°¢ç‰©åç§°
        metabolite_list = metabolite_rank.index.tolist()
        
        # æ‰§è¡Œé€šè·¯å¯Œé›†åˆ†æ
        # æ³¨æ„ï¼šgseapy éœ€è¦ä»£è°¢ç‰© ID æ˜ å°„åˆ° KEGGï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        try:
            # å°è¯•ä½¿ç”¨ KEGG æ•°æ®åº“
            enr = gp.enrichr(
                gene_list=metabolite_list[:100],  # é™åˆ¶å‰100ä¸ª
                gene_sets=['KEGG_2021_Human'],  # KEGG é€šè·¯æ•°æ®åº“
                organism=organism,
                outdir=None,  # ä¸ä¿å­˜æ–‡ä»¶
                verbose=False
            )
            
            # æå–ç»“æœ
            if enr is not None and hasattr(enr, 'results'):
                results_df = enr.results
                
                # è¿‡æ»¤æ˜¾è‘—é€šè·¯
                significant_pathways = results_df[
                    results_df['Adjusted P-value'] < p_value_threshold
                ].copy()
                
                # ä¿å­˜ç»“æœï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
                output_csv = None
                if output_dir:
                    output_path = Path(output_dir)
                    output_path.mkdir(parents=True, exist_ok=True)
                    output_csv = str(output_path / "pathway_enrichment.csv")
                    significant_pathways.to_csv(output_csv, index=False)
                
                return {
                    "status": "success",
                    "enriched_pathways": significant_pathways.to_dict(orient='records'),
                    "n_significant": len(significant_pathways),
                    "n_total": len(results_df),
                    "output_csv": output_csv,
                    "summary": f"å‘ç° {len(significant_pathways)} ä¸ªæ˜¾è‘—å¯Œé›†é€šè·¯ï¼ˆp < {p_value_threshold}ï¼‰"
                }
            else:
                return {
                    "status": "warning",
                    "message": "é€šè·¯å¯Œé›†åˆ†æå®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ˜¾è‘—å¯Œé›†é€šè·¯",
                    "enriched_pathways": []
                }
        
        except Exception as e:
            # å¦‚æœ gseapy å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦ç»“æœ
            logger.warning(f"âš ï¸ GSEApy æ‰§è¡Œå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")
            return {
                "status": "warning",
                "message": f"é€šè·¯å¯Œé›†åˆ†æéƒ¨åˆ†å®Œæˆï¼ˆGSEApy é”™è¯¯: {str(e)}ï¼‰",
                "enriched_pathways": [],
                "error": str(e)
            }
    
    except ImportError:
        return {
            "status": "error",
            "error": "gseapy not installed. Please install: pip install gseapy"
        }
    except Exception as e:
        logger.error(f"âŒ é€šè·¯å¯Œé›†åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


