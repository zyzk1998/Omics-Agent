"""
ä»£è°¢ç»„å­¦åˆ†æå·¥å…·
æ”¯æŒä»£è°¢ç»„å­¦æ•°æ®çš„ä¸‹è½½ã€é¢„å¤„ç†å’Œåˆ†æ
"""
import os
# ğŸ”§ ä¿®å¤ï¼šè®¾ç½® Matplotlib é…ç½®ç›®å½•ï¼ˆé¿å…æƒé™é—®é¢˜ï¼‰
if 'MPLCONFIGDIR' not in os.environ:
    import tempfile
    os.environ['MPLCONFIGDIR'] = tempfile.mkdtemp(prefix='matplotlib_')

import requests
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import logging
import gc

logger = logging.getLogger(__name__)


class MetabolomicsTool:
    """
    ä»£è°¢ç»„å­¦åˆ†æå·¥å…·
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - ä¸‹è½½æ¼”ç¤ºæ•°æ®é›†
    - æ•°æ®æ£€æŸ¥å’Œé¢„å¤„ç†
    - ä¸»æˆåˆ†åˆ†æ (PCA)
    - å·®å¼‚ä»£è°¢ç‰©åˆ†æ
    - å¯è§†åŒ–
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ä»£è°¢ç»„å­¦å·¥å…·
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.output_dir = Path(self.config.get("output_dir", os.path.join(os.getcwd(), "results", "metabolomics"))).resolve()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å­˜å‚¨å½“å‰åŠ è½½çš„æ•°æ®
        self.data = None
        self.metadata = None
        self.metabolites = None
        
        # å·¥å…·æ˜ å°„è¡¨
        self.tool_map = {
            "download_demo_data": self.download_demo_data,
            "inspect_data": self.inspect_data,
            "preprocess_data": self.preprocess_data,
            "pca_analysis": self.pca_analysis,
            "differential_analysis": self.differential_analysis,
            "visualize_pca": self.visualize_pca,
            "visualize_volcano": self.visualize_volcano,
        }
    
    def download_demo_data(
        self,
        output_dir: Optional[str] = None,
        filename: str = "human_cachexia.csv"
    ) -> Dict[str, Any]:
        """
        ä¸‹è½½æ¼”ç¤ºæ•°æ®é›†ï¼ˆHuman Cachexiaï¼‰
        
        ä½¿ç”¨å®˜æ–¹ API ç«¯ç‚¹ä¸‹è½½æ•°æ®ï¼š
        https://rest.xialab.ca/api/download/metaboanalyst/human_cachexia.csv
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨ self.output_dirï¼‰
            filename: ä¿å­˜çš„æ–‡ä»¶åï¼ˆé»˜è®¤ä¸º human_cachexia.csvï¼‰
        
        Returns:
            åŒ…å«ä¸‹è½½çŠ¶æ€çš„å­—å…¸ï¼š
            {
                "status": "success" | "error",
                "message": "æè¿°ä¿¡æ¯",
                "file_path": "æ–‡ä»¶è·¯å¾„",
                "file_size": æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            }
        """
        try:
            # ç¡®å®šè¾“å‡ºç›®å½•
            if output_dir is None:
                output_dir = self.output_dir
            else:
                os.makedirs(output_dir, exist_ok=True)
            
            # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(output_dir, filename)
            
            # API ç«¯ç‚¹ URL
            url = "https://rest.xialab.ca/api/download/metaboanalyst/human_cachexia.csv"
            
            print(f"ğŸ“¥ æ­£åœ¨ä»å®˜æ–¹ API ä¸‹è½½ Human Cachexia æ•°æ®é›†...")
            print(f"   URL: {url}")
            print(f"   ä¿å­˜åˆ°: {file_path}")
            
            # ä½¿ç”¨ requests.get ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, timeout=30)
            response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 200ï¼ŒæŠ›å‡ºå¼‚å¸¸
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                os.remove(file_path)  # åˆ é™¤ç©ºæ–‡ä»¶
                return {
                    "status": "error",
                    "message": "ä¸‹è½½çš„æ–‡ä»¶ä¸ºç©º",
                    "file_path": file_path,
                    "file_size": 0
                }
            
            print(f"âœ… ä¸‹è½½æˆåŠŸï¼")
            print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")
            print(f"   æ–‡ä»¶è·¯å¾„: {file_path}")
            
            return {
                "status": "success",
                "message": "Human Cachexia æ•°æ®é›†ä¸‹è½½æˆåŠŸ",
                "file_path": file_path,
                "file_size": file_size
            }
            
        except requests.exceptions.RequestException as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "status": "error",
                "message": error_msg,
                "file_path": file_path if 'file_path' in locals() else None,
                "file_size": 0
            }
        except Exception as e:
            error_msg = f"å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "status": "error",
                "message": error_msg,
                "file_path": file_path if 'file_path' in locals() else None,
                "file_size": 0
            }
    
    def inspect_data(self, file_path: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥ä»£è°¢ç»„å­¦æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ‘˜è¦
        
        ğŸ”§ é‡æ„ï¼šå§”æ‰˜ç»™ FileInspectorï¼ˆUniversal Eyesï¼‰
        ä¸å†é‡å¤å®ç°æ£€æŸ¥é€»è¾‘ï¼Œç»Ÿä¸€ä½¿ç”¨æ ¸å¿ƒæ£€æŸ¥å™¨
        
        Args:
            file_path: CSV æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«æ•°æ®æ‘˜è¦çš„å­—å…¸ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
        """
        logger.info(f"ğŸ” [CHECKPOINT] inspect_data START (Delegating to FileInspector)")
        logger.info(f"   File path: {file_path}")
        
        try:
            # ğŸ”§ å§”æ‰˜ç»™ FileInspector
            from ..core.file_inspector import FileInspector
            upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
            inspector = FileInspector(upload_dir)
            
            # ä½¿ç”¨é€šç”¨æ£€æŸ¥å™¨
            result = inspector.inspect_file(file_path)
            
            if result.get("status") == "success" and result.get("file_type") == "tabular":
                # è½¬æ¢ä¸ºä»£è°¢ç»„å­¦å·¥å…·æœŸæœ›çš„æ ¼å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                summary = result.get("data", {}).get("summary", {})
                data_range = result.get("data_range", {})
                potential_groups = result.get("potential_groups", {})
                
                # æå–åˆ†ç»„ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                group_info = {}
                if potential_groups:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ½œåœ¨åˆ†ç»„åˆ—
                    first_group_col = list(potential_groups.keys())[0]
                    group_info = {
                        "column": first_group_col,
                        "groups": {str(v): 0 for v in potential_groups[first_group_col]["values"]},
                        "n_groups": potential_groups[first_group_col]["n_unique"]
                    }
                
                # æ„å»ºå…¼å®¹æ ¼å¼çš„ç»“æœ
                compatible_result = {
                    "status": "success",
                    "file_path": file_path,
                    "n_samples": summary.get("n_samples", "N/A"),
                    "n_metabolites": summary.get("n_features", 0),
                    "metadata_columns": result.get("metadata_columns", []),
                    "metabolite_columns": result.get("feature_columns", [])[:10],
                    "total_metabolite_columns": result.get("total_feature_columns", 0),
                    "missing_values": {
                        "total": 0,  # ä¸æä¾›å…·ä½“æ•°å€¼ï¼Œåªæä¾›ç™¾åˆ†æ¯”
                        "percentage": summary.get("missing_rate", 0)
                    },
                    "group_info": group_info,
                    "data_statistics": {
                        "min": data_range.get("min", 0),
                        "max": data_range.get("max", 0),
                        "mean": data_range.get("mean", 0),
                        "median": data_range.get("median", 0)
                    },
                    # å‰ç«¯å¯ç”¨çš„æ•°æ®
                    "data": {
                        "summary": {
                            "n_samples": summary.get("n_samples", "N/A"),
                            "n_metabolites": summary.get("n_features", 0),
                            "missing_percentage": summary.get("missing_rate", 0),
                            "group_info": group_info,
                            "data_range": data_range,
                            "is_sampled": summary.get("is_sampled", False)
                        }
                    }
                }
                
                logger.info(f"âœ… [CHECKPOINT] inspect_data SUCCESS (via FileInspector)")
                logger.info(f"   Samples: {summary.get('n_samples')}, Features: {summary.get('n_features')}, Missing: {summary.get('missing_rate', 0):.2f}%")
                return compatible_result
            else:
                # æ£€æŸ¥å¤±è´¥æˆ–éè¡¨æ ¼æ–‡ä»¶
                logger.error(f"âŒ [CHECKPOINT] inspect_data FAILED: {result.get('error', 'Unknown error')}")
                return result
                
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error(f"âŒ [CHECKPOINT] inspect_data FAILED")
            logger.error(f"   File path: {file_path}")
            logger.error(f"   Error type: {type(e).__name__}")
            logger.error(f"   Error message: {str(e)}")
            logger.error(f"   Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_path": file_path
            }
    
    def preprocess_data(
        self,
        file_path: str,
        missing_threshold: float = 0.5,
        normalization: str = "log2",
        scale: bool = True
    ) -> Dict[str, Any]:
        """
        é¢„å¤„ç†ä»£è°¢ç»„å­¦æ•°æ®
        
        Args:
            file_path: CSV æ–‡ä»¶è·¯å¾„
            missing_threshold: ç¼ºå¤±å€¼é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤æ¯”ä¾‹çš„ä»£è°¢ç‰©å°†è¢«ç§»é™¤ï¼‰
            normalization: æ ‡å‡†åŒ–æ–¹æ³• ("log2", "zscore", "none")
            scale: æ˜¯å¦è¿›è¡Œç¼©æ”¾ï¼ˆStandardScalerï¼‰
        
        Returns:
            é¢„å¤„ç†ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ” [CHECKPOINT] preprocess_data START")
        logger.info(f"   File path: {file_path}")
        logger.info(f"   File exists? {os.path.exists(file_path)}")
        logger.info(f"   Parameters: missing_threshold={missing_threshold}, normalization={normalization}, scale={scale}")
        
        try:
            logger.info(f"ğŸ”§ å¼€å§‹é¢„å¤„ç†æ•°æ®: {file_path}")
            
            # è¯»å–æ•°æ®
            logger.info(f"   Attempting to read CSV file...")
            df = pd.read_csv(file_path)
            logger.info(f"   âœ… CSV file read successfully: {len(df)} rows, {len(df.columns)} columns")
            
            # åˆ†ç¦»å…ƒæ•°æ®å’Œä»£è°¢ç‰©æ•°æ®
            metadata_cols = []
            metabolite_cols = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    metabolite_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            # æå–å…ƒæ•°æ®
            self.metadata = df[metadata_cols].copy()
            self.metabolites = df[metabolite_cols].copy()
            
            # 1. å¤„ç†ç¼ºå¤±å€¼ï¼šç§»é™¤ç¼ºå¤±å€¼æ¯”ä¾‹è¿‡é«˜çš„ä»£è°¢ç‰©
            missing_ratio = self.metabolites.isnull().sum() / len(self.metabolites)
            valid_metabolites = missing_ratio[missing_ratio < missing_threshold].index
            removed_count = len(metabolite_cols) - len(valid_metabolites)
            
            self.metabolites = self.metabolites[valid_metabolites]
            
            # 2. å¡«å……å‰©ä½™ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼‰
            self.metabolites = self.metabolites.fillna(self.metabolites.median())
            
            # 3. æ ‡å‡†åŒ–
            if normalization == "log2":
                # Log2 è½¬æ¢ï¼ˆå¤„ç†é›¶å€¼å’Œè´Ÿå€¼ï¼‰
                self.metabolites = self.metabolites.apply(lambda x: np.log2(x + 1))
                logger.info("âœ… å·²åº”ç”¨ Log2 è½¬æ¢")
            elif normalization == "zscore":
                from scipy.stats import zscore
                self.metabolites = self.metabolites.apply(zscore, axis=0)
                logger.info("âœ… å·²åº”ç”¨ Z-score æ ‡å‡†åŒ–")
            
            # 4. ç¼©æ”¾ï¼ˆå¯é€‰ï¼‰
            if scale:
                scaler = StandardScaler()
                metabolite_array = scaler.fit_transform(self.metabolites)
                self.metabolites = pd.DataFrame(
                    metabolite_array,
                    columns=self.metabolites.columns,
                    index=self.metabolites.index
                )
                logger.info("âœ… å·²åº”ç”¨ StandardScaler ç¼©æ”¾")
            
            # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
            preprocessed_path = self.output_dir / "preprocessed_data.csv"
            preprocessed_df = pd.concat([self.metadata, self.metabolites], axis=1)
            preprocessed_df.to_csv(preprocessed_path, index=False)
            
            # ç”Ÿæˆé¢„å¤„ç†åçš„æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰
            preprocessed_preview = preprocessed_df.head(5).to_dict('records')
            
            result = {
                "status": "success",
                "message": "æ•°æ®é¢„å¤„ç†å®Œæˆ",
                "n_samples": len(self.metabolites),
                "n_metabolites": len(self.metabolites.columns),
                "removed_metabolites": removed_count,
                "normalization": normalization,
                "scaled": scale,
                "preprocessed_file": str(preprocessed_path),
                # å‰ç«¯å¯ç”¨çš„æ•°æ®
                "data": {
                    "preview": preprocessed_preview,  # é¢„å¤„ç†åçš„å‰5è¡Œæ•°æ®
                    "summary": {
                        "n_samples": len(self.metabolites),
                        "n_metabolites": len(self.metabolites.columns),
                        "removed_metabolites": removed_count,
                        "normalization": normalization,
                        "scaled": scale
                    }
                }
            }
            
            logger.info(f"âœ… é¢„å¤„ç†å®Œæˆ: {result['n_metabolites']} ä¸ªä»£è°¢ç‰©ä¿ç•™")
            logger.info(f"âœ… [CHECKPOINT] preprocess_data SUCCESS")
            return result
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error(f"âŒ [CHECKPOINT] preprocess_data FAILED")
            logger.error(f"   File path: {file_path}")
            logger.error(f"   Error type: {type(e).__name__}")
            logger.error(f"   Error message: {str(e)}")
            logger.error(f"   Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__
            }
    
    def pca_analysis(
        self,
        n_components: int = 10,
        file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸»æˆåˆ†åˆ†æ (PCA)
        
        Args:
            n_components: ä¸»æˆåˆ†æ•°é‡
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœªé¢„å¤„ç†ï¼Œéœ€è¦æä¾›ï¼‰
        
        Returns:
            PCA åˆ†æç»“æœ
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ‘‰ [STEP 1] pca_analysis START")
            logger.info(f"   Parameters: n_components={n_components}, file_path={file_path}")
            logger.info("=" * 80)
            
            # å¦‚æœæ•°æ®æœªåŠ è½½ï¼Œä»æ–‡ä»¶è¯»å–
            logger.info("ğŸ‘‰ [STEP 2] Checking if data is loaded...")
            if self.metabolites is None:
                logger.info("   Data not loaded, need to read from file")
                if file_path is None:
                    logger.error("âŒ [STEP 2] æ•°æ®æœªåŠ è½½ä¸”æœªæä¾›æ–‡ä»¶è·¯å¾„")
                    return {
                        "status": "error",
                        "error": "æ•°æ®æœªåŠ è½½ä¸”æœªæä¾›æ–‡ä»¶è·¯å¾„"
                    }
                # è¯»å–é¢„å¤„ç†åçš„æ•°æ®æˆ–åŸå§‹æ•°æ®
                logger.info(f"   Reading CSV from: {file_path}")
                df = pd.read_csv(file_path)
                logger.info(f"   Loaded CSV. Shape: {df.shape}")
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                logger.info(f"   Found {len(metadata_cols)} metadata columns: {metadata_cols}")
                self.metabolites = df.drop(columns=metadata_cols)
                logger.info(f"   Metabolites shape: {self.metabolites.shape}")
                if len(metadata_cols) > 0:
                    self.metadata = df[metadata_cols]
                    logger.info(f"   Metadata shape: {self.metadata.shape}")
                logger.info("âœ… [STEP 2] Data loaded")
                gc.collect()
            else:
                logger.info(f"âœ… [STEP 2] Data already loaded. Metabolites shape: {self.metabolites.shape}")
            
            # æ‰§è¡Œ PCA
            logger.info("ğŸ‘‰ [STEP 3] Initializing PCA...")
            try:
                actual_n_components = min(n_components, len(self.metabolites.columns), len(self.metabolites))
                logger.info(f"   Actual n_components: {actual_n_components} (requested: {n_components}, features: {len(self.metabolites.columns)}, samples: {len(self.metabolites)})")
                pca = PCA(n_components=actual_n_components)
                logger.info("âœ… [STEP 3] PCA initialized")
            except Exception as e:
                logger.error(f"âŒ [STEP 3] Failed to initialize PCA: {e}", exc_info=True)
                raise
            
            logger.info("ğŸ‘‰ [STEP 4] Fitting and transforming PCA...")
            try:
                pca_result = pca.fit_transform(self.metabolites)
                logger.info(f"âœ… [STEP 4] PCA completed. Result shape: {pca_result.shape}")
            except Exception as e:
                logger.error(f"âŒ [STEP 4] Failed to fit/transform PCA: {e}", exc_info=True)
                raise
            
            # è®¡ç®—è§£é‡Šæ–¹å·®
            logger.info("ğŸ‘‰ [STEP 5] Calculating explained variance...")
            try:
                explained_variance = pca.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance)
                logger.info(f"âœ… [STEP 5] Explained variance calculated. PC1: {explained_variance[0]*100:.2f}%")
            except Exception as e:
                logger.error(f"âŒ [STEP 5] Failed to calculate variance: {e}", exc_info=True)
                raise
            
            # ä¿å­˜ PCA ç»“æœ
            logger.info("ğŸ‘‰ [STEP 6] Creating PCA DataFrame...")
            try:
                pca_df = pd.DataFrame(
                    pca_result,
                    columns=[f"PC{i+1}" for i in range(pca_result.shape[1])],
                    index=self.metabolites.index
                )
                logger.info(f"âœ… [STEP 6] PCA DataFrame created. Shape: {pca_df.shape}")
            except Exception as e:
                logger.error(f"âŒ [STEP 6] Failed to create DataFrame: {e}", exc_info=True)
                raise
            
            # å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œåˆå¹¶
            logger.info("ğŸ‘‰ [STEP 7] Merging with metadata...")
            try:
                if self.metadata is not None:
                    pca_df = pd.concat([self.metadata, pca_df], axis=1)
                    logger.info(f"âœ… [STEP 7] Merged with metadata. Final shape: {pca_df.shape}")
                else:
                    logger.info("âœ… [STEP 7] No metadata to merge")
            except Exception as e:
                logger.error(f"âŒ [STEP 7] Failed to merge metadata: {e}", exc_info=True)
                raise
            
            logger.info("ğŸ‘‰ [STEP 8] Saving PCA results to CSV...")
            pca_path = self.output_dir / "pca_results.csv"
            try:
                pca_df.to_csv(pca_path, index=False)
                logger.info(f"âœ… [STEP 8] Saved to {pca_path}")
                gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            except Exception as e:
                logger.error(f"âŒ [STEP 8] Failed to save CSV: {e}", exc_info=True)
                raise
            
            # PCA ç»“æœé¢„è§ˆï¼ˆå‰5è¡Œï¼ŒåŒ…å«ä¸­æ–‡åˆ—åï¼‰
            logger.info("ğŸ‘‰ [STEP 9] Generating preview table...")
            try:
                pca_preview_df = pca_df.head(5).copy()
                pca_preview = []
                for _, row in pca_preview_df.iterrows():
                    preview_row = {}
                    for col in pca_preview_df.columns:
                        preview_row[col] = row[col]
                        # æ·»åŠ ä¸­æ–‡åˆ—åæ˜ å°„ï¼ˆå¦‚æœåˆ—åæ˜¯ PC1, PC2 ç­‰ï¼‰
                        if col.startswith("PC"):
                            preview_row[f"ä¸»æˆåˆ†{col}"] = row[col]
                    pca_preview.append(preview_row)
                logger.info(f"âœ… [STEP 9] Preview table generated. Rows: {len(pca_preview)}")
            except Exception as e:
                logger.error(f"âŒ [STEP 9] Failed to generate preview: {e}", exc_info=True)
                raise
            
            # ç”Ÿæˆç»Ÿè®¡è¡¨æ ¼ï¼ˆå‰10ä¸ªä¸»æˆåˆ†çš„è§£é‡Šæ–¹å·®ï¼Œä¸­è‹±æ–‡åŒè¯­ï¼‰
            logger.info("ğŸ‘‰ [STEP 10] Generating variance table...")
            try:
                variance_table = []
                for i in range(min(10, len(explained_variance))):
                    variance_table.append({
                        "ä¸»æˆåˆ†": f"PC{i+1}",
                        "PC": f"PC{i+1}",
                        "è§£é‡Šæ–¹å·®": f"{explained_variance[i]*100:.2f}%",
                        "Explained Variance": f"{explained_variance[i]*100:.2f}%",
                        "ç´¯ç§¯æ–¹å·®": f"{cumulative_variance[i]*100:.2f}%",
                        "Cumulative Variance": f"{cumulative_variance[i]*100:.2f}%"
                    })
                logger.info(f"âœ… [STEP 10] Variance table generated. Rows: {len(variance_table)}")
            except Exception as e:
                logger.error(f"âŒ [STEP 10] Failed to generate variance table: {e}", exc_info=True)
                raise
            
            # è·å–è½½è·è¡¨æ ¼ï¼ˆä¸­è‹±æ–‡åŒè¯­ï¼‰
            logger.info("ğŸ‘‰ [STEP 11] Getting top loadings...")
            try:
                top_loadings_raw = self._get_top_loadings(pca.components_[0], self.metabolites.columns, 10)
                top_loadings = [
                    {
                        "ä»£è°¢ç‰©": item["metabolite"],
                        "Metabolite": item["metabolite"],
                        "è½½è·å€¼": round(item["loading"], 4),
                        "Loading": round(item["loading"], 4)
                    }
                    for item in top_loadings_raw
                ]
                logger.info(f"âœ… [STEP 11] Top loadings retrieved. Count: {len(top_loadings)}")
            except Exception as e:
                logger.error(f"âŒ [STEP 11] Failed to get loadings: {e}", exc_info=True)
                raise
            
            logger.info("ğŸ‘‰ [STEP 12] Building result dictionary...")
            result = {
                "status": "success",
                "message": "PCA åˆ†æå®Œæˆ",
                "n_components": pca_result.shape[1],
                "explained_variance": {
                    "PC1": float(explained_variance[0]),
                    "PC2": float(explained_variance[1]) if len(explained_variance) > 1 else 0.0,
                    "PC3": float(explained_variance[2]) if len(explained_variance) > 2 else 0.0,
                },
                "cumulative_variance_pc10": float(cumulative_variance[min(9, len(cumulative_variance)-1)]),
                "pca_file": str(pca_path),
                "loadings": {
                    "top_10_pc1": top_loadings_raw  # ä¿ç•™åŸå§‹æ ¼å¼ä»¥å…¼å®¹
                },
                # å‰ç«¯å¯ç”¨çš„æ•°æ®
                "data": {
                    "preview": pca_preview,  # PCA ç»“æœå‰5è¡Œï¼ˆåŒ…å«ä¸­æ–‡åˆ—åï¼‰
                    "tables": {
                        "variance_table": variance_table,  # è§£é‡Šæ–¹å·®è¡¨æ ¼ï¼ˆä¸­è‹±æ–‡åŒè¯­ï¼‰
                        "top_loadings": top_loadings  # è½½è·è¡¨æ ¼ï¼ˆä¸­è‹±æ–‡åŒè¯­ï¼‰
                    }
                }
            }
            
            logger.info("=" * 80)
            logger.info("âœ… [STEP 13] pca_analysis SUCCESS")
            logger.info(f"   PC1 explains {result['explained_variance']['PC1']*100:.2f}% variance")
            logger.info("=" * 80)
            gc.collect()  # æœ€ç»ˆåƒåœ¾å›æ”¶
            return result
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error(f"âŒ [STEP X] pca_analysis FAILED")
            logger.error(f"   Error type: {type(e).__name__}")
            logger.error(f"   Error message: {str(e)}")
            logger.error(f"   Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            gc.collect()
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _get_top_loadings(self, loadings: np.ndarray, metabolite_names: List[str], n: int = 10) -> List[Dict[str, Any]]:
        """è·å–è½½è·æœ€é«˜çš„ä»£è°¢ç‰©"""
        indices = np.argsort(np.abs(loadings))[-n:][::-1]
        return [
            {
                "metabolite": metabolite_names[i],
                "loading": float(loadings[i])
            }
            for i in indices
        ]
    
    def differential_analysis(
        self,
        group_column: str,
        file_path: Optional[str] = None,
        method: str = "t-test",
        p_value_threshold: float = 0.05,
        fold_change_threshold: float = 1.5,
        group1: Optional[str] = None,
        group2: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æ
        
        Args:
            group_column: åˆ†ç»„åˆ—å
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœªé¢„å¤„ç†ï¼Œéœ€è¦æä¾›ï¼‰
            method: ç»Ÿè®¡æ–¹æ³• ("t-test", "mann-whitney")
            p_value_threshold: P å€¼é˜ˆå€¼
            fold_change_threshold: å€æ•°å˜åŒ–é˜ˆå€¼
        
        Returns:
            å·®å¼‚åˆ†æç»“æœ
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ‘‰ [STEP 1] differential_analysis START")
            logger.info(f"   Parameters: group_column={group_column}, method={method}, p_value_threshold={p_value_threshold}, fold_change_threshold={fold_change_threshold}")
            logger.info("=" * 80)
            
            # å¦‚æœæ•°æ®æœªåŠ è½½ï¼Œä»æ–‡ä»¶è¯»å–
            logger.info("ğŸ‘‰ [STEP 2] Checking if data is loaded...")
            if self.metabolites is None or self.metadata is None:
                logger.info("   Data not loaded, need to read from file")
                if file_path is None:
                    logger.error("âŒ [STEP 2] æ•°æ®æœªåŠ è½½ä¸”æœªæä¾›æ–‡ä»¶è·¯å¾„")
                    return {
                        "status": "error",
                        "error": "æ•°æ®æœªåŠ è½½ä¸”æœªæä¾›æ–‡ä»¶è·¯å¾„"
                    }
                logger.info(f"   Reading CSV from: {file_path}")
                df = pd.read_csv(file_path)
                logger.info(f"   Loaded CSV. Shape: {df.shape}")
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                logger.info(f"   Found {len(metadata_cols)} metadata columns: {metadata_cols}")
                self.metabolites = df.drop(columns=metadata_cols)
                self.metadata = df[metadata_cols]
                logger.info(f"   Metabolites shape: {self.metabolites.shape}, Metadata shape: {self.metadata.shape}")
                logger.info("âœ… [STEP 2] Data loaded")
                gc.collect()
            else:
                logger.info(f"âœ… [STEP 2] Data already loaded. Metabolites: {self.metabolites.shape}, Metadata: {self.metadata.shape}")
            
            # æ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨
            logger.info("ğŸ‘‰ [STEP 3] Validating group column...")
            if group_column not in self.metadata.columns:
                logger.error(f"âŒ [STEP 3] åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(self.metadata.columns)}")
                return {
                    "status": "error",
                    "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(self.metadata.columns)}"
                }
            logger.info(f"âœ… [STEP 3] Group column '{group_column}' found")
            
            # è·å–åˆ†ç»„
            logger.info("ğŸ‘‰ [STEP 4] Getting unique groups...")
            groups = self.metadata[group_column].unique()
            logger.info(f"   Found {len(groups)} groups: {list(groups)}")
            
            if len(groups) < 2:
                logger.error(f"âŒ [STEP 4] éœ€è¦è‡³å°‘2ä¸ªåˆ†ç»„ï¼Œä½†æ‰¾åˆ° {len(groups)} ä¸ª: {groups}")
                return {
                    "status": "error",
                    "error": f"éœ€è¦è‡³å°‘2ä¸ªåˆ†ç»„ï¼Œä½†æ‰¾åˆ° {len(groups)} ä¸ª: {groups}"
                }
            elif len(groups) > 2:
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº† group1 å’Œ group2ï¼Œä½¿ç”¨å®ƒä»¬
                if group1 and group2:
                    if group1 not in groups or group2 not in groups:
                        logger.error(f"âŒ [STEP 4] æŒ‡å®šçš„åˆ†ç»„ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ†ç»„: {list(groups)}")
                        return {
                            "status": "error",
                            "error": f"æŒ‡å®šçš„åˆ†ç»„ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ†ç»„: {list(groups)}"
                        }
                    logger.info(f"âœ… [STEP 4] Using specified groups: {group1} vs {group2}")
                    # ä½¿ç”¨æŒ‡å®šçš„åˆ†ç»„
                    pass  # group1 å’Œ group2 å·²ç»è®¾ç½®
                else:
                    logger.warning(f"âš ï¸ [STEP 4] æ£€æµ‹åˆ° {len(groups)} ä¸ªåˆ†ç»„ï¼Œéœ€è¦ç”¨æˆ·é€‰æ‹©")
                    # è¿”å›éœ€è¦ç”¨æˆ·é€‰æ‹©çš„ä¿¡æ¯
                    return {
                        "status": "need_selection",
                        "message": f"æ£€æµ‹åˆ° {len(groups)} ä¸ªåˆ†ç»„: {list(groups)}",
                        "groups": list(groups),
                        "error": f"éœ€è¦é€‰æ‹©2ä¸ªåˆ†ç»„è¿›è¡Œæ¯”è¾ƒï¼Œä½†æ‰¾åˆ° {len(groups)} ä¸ªåˆ†ç»„: {groups}"
                    }
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†ç»„ï¼Œä½¿ç”¨å‰ä¸¤ä¸ª
            if not group1 or not group2:
                group1, group2 = groups[0], groups[1]
                logger.info(f"âœ… [STEP 4] Using first two groups: {group1} vs {group2}")
            
            logger.info("ğŸ‘‰ [STEP 5] Creating group masks...")
            group1_mask = self.metadata[group_column] == group1
            group2_mask = self.metadata[group_column] == group2
            logger.info(f"   Group1 ({group1}): {group1_mask.sum()} samples")
            logger.info(f"   Group2 ({group2}): {group2_mask.sum()} samples")
            logger.info("âœ… [STEP 5] Group masks created")
            
            # å¯¹æ¯ä¸ªä»£è°¢ç‰©æ‰§è¡Œç»Ÿè®¡æ£€éªŒ
            logger.info("ğŸ‘‰ [STEP 6] Running statistical tests for each metabolite...")
            logger.info(f"   Total metabolites: {len(self.metabolites.columns)}")
            results = []
            processed = 0
            for metabolite in self.metabolites.columns:
                group1_data = self.metabolites.loc[group1_mask, metabolite].values
                group2_data = self.metabolites.loc[group2_mask, metabolite].values
                
                # è®¡ç®—å‡å€¼
                mean1 = np.mean(group1_data)
                mean2 = np.mean(group2_data)
                
                # è®¡ç®—å€æ•°å˜åŒ–
                if mean2 != 0:
                    fold_change = mean1 / mean2
                    log2_fc = np.log2(fold_change) if fold_change > 0 else 0
                else:
                    fold_change = np.inf
                    log2_fc = 0
                
                # ç»Ÿè®¡æ£€éªŒ
                if method == "t-test":
                    try:
                        stat, p_value = stats.ttest_ind(group1_data, group2_data)
                    except:
                        p_value = 1.0
                        stat = 0
                elif method == "mann-whitney":
                    try:
                        stat, p_value = stats.mannwhitneyu(group1_data, group2_data, alternative='two-sided')
                    except:
                        p_value = 1.0
                        stat = 0
                else:
                    p_value = 1.0
                    stat = 0
                
                # åˆ¤æ–­æ˜¯å¦æ˜¾è‘—
                is_significant = p_value < p_value_threshold and abs(log2_fc) > np.log2(fold_change_threshold)
                
                results.append({
                    "metabolite": metabolite,
                    "group1_mean": float(mean1),
                    "group2_mean": float(mean2),
                    "fold_change": float(fold_change),
                    "log2_fold_change": float(log2_fc),
                    "p_value": float(p_value),
                    "statistic": float(stat),
                    "significant": is_significant
                })
                processed += 1
                if processed % 100 == 0:
                    logger.info(f"   Processed {processed}/{len(self.metabolites.columns)} metabolites...")
            
            logger.info(f"âœ… [STEP 6] Statistical tests completed. Processed {len(results)} metabolites")
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            # è½¬æ¢ä¸º DataFrame
            logger.info("ğŸ‘‰ [STEP 7] Converting results to DataFrame...")
            try:
                results_df = pd.DataFrame(results)
                results_df = results_df.sort_values("p_value")
                logger.info(f"âœ… [STEP 7] DataFrame created. Shape: {results_df.shape}")
            except Exception as e:
                logger.error(f"âŒ [STEP 7] Failed to create DataFrame: {e}", exc_info=True)
                raise
            
            # ä¿å­˜ç»“æœ
            logger.info("ğŸ‘‰ [STEP 8] Saving results to CSV...")
            diff_path = self.output_dir / "differential_analysis.csv"
            try:
                results_df.to_csv(diff_path, index=False)
                logger.info(f"âœ… [STEP 8] Saved to {diff_path}")
                gc.collect()
            except Exception as e:
                logger.error(f"âŒ [STEP 8] Failed to save CSV: {e}", exc_info=True)
                raise
            
            # ç»Ÿè®¡æ˜¾è‘—ä»£è°¢ç‰©
            logger.info("ğŸ‘‰ [STEP 9] Identifying significant metabolites...")
            significant = results_df[results_df["significant"] == True]
            logger.info(f"âœ… [STEP 9] Found {len(significant)} significant metabolites out of {len(results_df)} total")
            
            # ç”Ÿæˆå·®å¼‚åˆ†æç»“æœè¡¨æ ¼ï¼ˆå‰20ä¸ªæ˜¾è‘—ä»£è°¢ç‰©ï¼ŒåŒ…å«ä¸­æ–‡åˆ—åï¼‰
            top_significant_df = significant.head(20).copy()
            # æ·»åŠ ä¸­æ–‡åˆ—åæ˜ å°„
            top_significant_table = []
            for _, row in top_significant_df.iterrows():
                top_significant_table.append({
                    "ä»£è°¢ç‰©": row["metabolite"],
                    "Metabolite": row["metabolite"],  # ä¿ç•™è‹±æ–‡åˆ—åä»¥å…¼å®¹
                    "På€¼": round(row["p_value"], 6),
                    "P-value": round(row["p_value"], 6),
                    "å€æ•°å˜åŒ–": round(row["fold_change"], 3),
                    "Fold Change": round(row["fold_change"], 3),
                    "Log2å€æ•°å˜åŒ–": round(row["log2_fold_change"], 3),
                    "Log2 Fold Change": round(row["log2_fold_change"], 3),
                    "ç»„1å‡å€¼": round(row["group1_mean"], 3),
                    "Group1 Mean": round(row["group1_mean"], 3),
                    "ç»„2å‡å€¼": round(row["group2_mean"], 3),
                    "Group2 Mean": round(row["group2_mean"], 3),
                    "çŠ¶æ€": "ä¸Šè°ƒ" if row["log2_fold_change"] > 0 else "ä¸‹è°ƒ",
                    "Status": "Up-regulated" if row["log2_fold_change"] > 0 else "Down-regulated"
                })
            
            # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦è¡¨æ ¼ï¼ˆä¸­è‹±æ–‡åŒè¯­ï¼‰
            summary_table = [
                {
                    "ç±»åˆ«": "æ€»ä»£è°¢ç‰©æ•°",
                    "Category": "Total Metabolites",
                    "æ•°é‡": len(results_df),
                    "Count": len(results_df)
                },
                {
                    "ç±»åˆ«": "æ˜¾è‘—ä»£è°¢ç‰©æ•°",
                    "Category": "Significant Metabolites",
                    "æ•°é‡": len(significant),
                    "Count": len(significant)
                },
                {
                    "ç±»åˆ«": "ä¸Šè°ƒä»£è°¢ç‰©",
                    "Category": "Up-regulated",
                    "æ•°é‡": len(significant[significant["log2_fold_change"] > 0]),
                    "Count": len(significant[significant["log2_fold_change"] > 0])
                },
                {
                    "ç±»åˆ«": "ä¸‹è°ƒä»£è°¢ç‰©",
                    "Category": "Down-regulated",
                    "æ•°é‡": len(significant[significant["log2_fold_change"] < 0]),
                    "Count": len(significant[significant["log2_fold_change"] < 0])
                }
            ]
            
            result = {
                "status": "success",
                "message": "å·®å¼‚åˆ†æå®Œæˆ",
                "n_total": len(results_df),
                "n_significant": len(significant),
                "groups": {
                    "group1": str(group1),
                    "group2": str(group2)
                },
                "top_significant": top_significant_table,
                "results_file": str(diff_path),
                # å‰ç«¯å¯ç”¨çš„æ•°æ®
                "data": {
                    "tables": {
                        "top_significant": top_significant_table,  # å‰20ä¸ªæ˜¾è‘—ä»£è°¢ç‰©è¡¨æ ¼
                        "summary": summary_table  # ç»Ÿè®¡æ‘˜è¦è¡¨æ ¼
                    },
                    "summary": {
                        "n_total": len(results_df),
                        "n_significant": len(significant),
                        "groups": {
                            "group1": str(group1),
                            "group2": str(group2)
                        }
                    }
                }
            }
            
            logger.info("=" * 80)
            logger.info("âœ… [STEP 12] differential_analysis SUCCESS")
            logger.info(f"   Total metabolites: {result['n_total']}")
            logger.info(f"   Significant metabolites: {result['n_significant']}")
            logger.info("=" * 80)
            gc.collect()  # æœ€ç»ˆåƒåœ¾å›æ”¶
            return result
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error(f"âŒ [STEP X] differential_analysis FAILED")
            logger.error(f"   Error type: {type(e).__name__}")
            logger.error(f"   Error message: {str(e)}")
            logger.error(f"   Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            gc.collect()
            return {
                "status": "error",
                "error": str(e)
            }
    
    def visualize_pca(
        self,
        group_column: Optional[str] = None,
        pca_file: Optional[str] = None,
        pc1: int = 1,
        pc2: int = 2
    ) -> Dict[str, Any]:
        """
        å¯è§†åŒ– PCA ç»“æœ
        
        Args:
            group_column: ç”¨äºç€è‰²çš„åˆ†ç»„åˆ—
            pca_file: PCA ç»“æœæ–‡ä»¶è·¯å¾„
            pc1: ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼ˆ1-basedï¼‰
            pc2: ç¬¬äºŒä¸ªä¸»æˆåˆ†ï¼ˆ1-basedï¼‰
        
        Returns:
            å¯è§†åŒ–ç»“æœ
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ‘‰ [STEP 1] visualize_pca START")
            logger.info(f"   Parameters: group_column={group_column}, pca_file={pca_file}, pc1={pc1}, pc2={pc2}")
            logger.info("=" * 80)
            
            # è¯»å– PCA ç»“æœ
            logger.info("ğŸ‘‰ [STEP 2] Loading PCA results file...")
            if pca_file is None:
                pca_file = self.output_dir / "pca_results.csv"
            
            logger.info(f"   Checking file: {pca_file}")
            if not os.path.exists(pca_file):
                logger.error(f"âŒ [STEP 2] PCA ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {pca_file}")
                return {
                    "status": "error",
                    "error": f"PCA ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {pca_file}"
                }
            
            logger.info(f"   File exists. Reading CSV...")
            df = pd.read_csv(pca_file)
            logger.info(f"âœ… [STEP 2] Loaded PCA data. Shape: {df.shape}, Columns: {list(df.columns)[:5]}...")
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            # æŸ¥æ‰¾ä¸»æˆåˆ†åˆ—
            logger.info("ğŸ‘‰ [STEP 3] Validating principal components...")
            pc_cols = [col for col in df.columns if col.startswith("PC")]
            logger.info(f"   Found {len(pc_cols)} PC columns: {pc_cols[:5]}...")
            
            if len(pc_cols) < max(pc1, pc2):
                logger.error(f"âŒ [STEP 3] ä¸»æˆåˆ†æ•°é‡ä¸è¶³: éœ€è¦ PC{pc1} å’Œ PC{pc2}, ä½†åªæœ‰ {len(pc_cols)} ä¸ª")
                return {
                    "status": "error",
                    "error": f"ä¸»æˆåˆ†æ•°é‡ä¸è¶³: éœ€è¦ PC{pc1} å’Œ PC{pc2}"
                }
            
            pc1_col = f"PC{pc1}"
            pc2_col = f"PC{pc2}"
            logger.info(f"âœ… [STEP 3] Using {pc1_col} and {pc2_col}")
            
            # åˆ›å»ºå›¾å½¢
            logger.info("ğŸ‘‰ [STEP 4] Creating matplotlib figure...")
            try:
                plt.figure(figsize=(10, 8))
                logger.info("âœ… [STEP 4] Figure created")
            except Exception as e:
                logger.error(f"âŒ [STEP 4] Failed to create figure: {e}", exc_info=True)
                raise
            
            # ç»˜åˆ¶æ•£ç‚¹å›¾
            logger.info("ğŸ‘‰ [STEP 5] Plotting scatter points...")
            try:
                if group_column and group_column in df.columns:
                    # æŒ‰åˆ†ç»„ç€è‰²
                    groups = df[group_column].unique()
                    logger.info(f"   Found {len(groups)} groups: {list(groups)}")
                    colors = plt.cm.Set3(np.linspace(0, 1, len(groups)))
                    for i, group in enumerate(groups):
                        mask = df[group_column] == group
                        logger.info(f"   Plotting group {group}: {mask.sum()} points")
                        plt.scatter(
                            df.loc[mask, pc1_col],
                            df.loc[mask, pc2_col],
                            label=str(group),
                            alpha=0.7,
                            s=100,
                            c=[colors[i]]
                        )
                    plt.legend(title=group_column)
                else:
                    logger.info("   Plotting without grouping")
                    plt.scatter(df[pc1_col], df[pc2_col], alpha=0.7, s=100)
                logger.info("âœ… [STEP 5] Scatter points plotted")
            except Exception as e:
                logger.error(f"âŒ [STEP 5] Failed to plot scatter: {e}", exc_info=True)
                plt.close('all')
                gc.collect()
                raise
            
            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            logger.info("ğŸ‘‰ [STEP 6] Setting labels and title...")
            try:
                plt.xlabel(f"{pc1_col}", fontsize=12)
                plt.ylabel(f"{pc2_col}", fontsize=12)
                plt.title(f"PCA Plot: {pc1_col} vs {pc2_col}", fontsize=14)
                plt.grid(True, alpha=0.3)
                logger.info("âœ… [STEP 6] Labels and title set")
            except Exception as e:
                logger.error(f"âŒ [STEP 6] Failed to set labels: {e}", exc_info=True)
                plt.close('all')
                gc.collect()
                raise
            
            # ä¿å­˜å›¾ç‰‡
            logger.info("ğŸ‘‰ [STEP 7] Saving figure to file...")
            plot_path = self.output_dir / f"pca_plot_pc{pc1}_pc{pc2}.png"
            logger.info(f"   Output path: {plot_path}")
            try:
                plt.tight_layout()
                logger.info("   Layout adjusted, saving...")
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                logger.info(f"âœ… [STEP 7] Figure saved to {plot_path}")
            except Exception as e:
                logger.error(f"âŒ [STEP 7] Failed to save figure: {e}", exc_info=True)
                plt.close('all')
                gc.collect()
                raise
            
            # CRITICAL: å…³é—­æ‰€æœ‰å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
            logger.info("ğŸ‘‰ [STEP 8] Closing matplotlib figures and freeing memory...")
            try:
                plt.close('all')
                gc.collect()
                logger.info("âœ… [STEP 8] Figures closed and memory freed")
            except Exception as e:
                logger.warning(f"âš ï¸ [STEP 8] Error closing figures: {e}")
                gc.collect()  # å³ä½¿å…³é—­å¤±è´¥ä¹Ÿå¼ºåˆ¶ GC
            
            # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº output_dirï¼‰
            logger.info("ğŸ‘‰ [STEP 9] Converting to relative path...")
            relative_path = str(plot_path)
            if os.path.isabs(relative_path):
                relative_path = os.path.relpath(relative_path, self.output_dir)
            relative_path = relative_path.replace("\\", "/")
            logger.info(f"âœ… [STEP 9] Relative path: {relative_path}")
            
            result = {
                "status": "success",
                "message": "PCA å¯è§†åŒ–å®Œæˆ",
                "plot_path": str(plot_path),
                "plot_file": str(plot_path),  # å…¼å®¹æ—§å­—æ®µå
                # å‰ç«¯å¯ç”¨çš„æ•°æ®
                "data": {
                    "images": [relative_path]  # ç›¸å¯¹è·¯å¾„ï¼Œå‰ç«¯å¯ä»¥ç›´æ¥ä½¿ç”¨
                }
            }
            
            logger.info("=" * 80)
            logger.info("âœ… [STEP 10] visualize_pca SUCCESS")
            logger.info(f"   Plot saved: {plot_path}")
            logger.info("=" * 80)
            return result
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error(f"âŒ [STEP X] visualize_pca FAILED")
            logger.error(f"   Error type: {type(e).__name__}")
            logger.error(f"   Error message: {str(e)}")
            logger.error(f"   Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            
            # ç¡®ä¿æ¸…ç†èµ„æº
            try:
                plt.close('all')
            except:
                pass
            gc.collect()
            
            return {
                "status": "error",
                "error": str(e)
            }
    
    def visualize_volcano(
        self,
        diff_file: Optional[str] = None,
        p_value_threshold: float = 0.05,
        fold_change_threshold: float = 1.5
    ) -> Dict[str, Any]:
        """
        ç»˜åˆ¶ç«å±±å›¾ï¼ˆVolcano Plotï¼‰
        
        Args:
            diff_file: å·®å¼‚åˆ†æç»“æœæ–‡ä»¶
            p_value_threshold: P å€¼é˜ˆå€¼
            fold_change_threshold: å€æ•°å˜åŒ–é˜ˆå€¼
        
        Returns:
            å¯è§†åŒ–ç»“æœ
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ‘‰ [STEP 1] visualize_volcano START")
            logger.info(f"   Parameters: diff_file={diff_file}, p_value_threshold={p_value_threshold}, fold_change_threshold={fold_change_threshold}")
            logger.info("=" * 80)
            
            # è¯»å–å·®å¼‚åˆ†æç»“æœ
            logger.info("ğŸ‘‰ [STEP 2] Loading differential analysis results...")
            if diff_file is None:
                diff_file = self.output_dir / "differential_analysis.csv"
            
            logger.info(f"   Checking file: {diff_file}")
            if not os.path.exists(diff_file):
                logger.error(f"âŒ [STEP 2] å·®å¼‚åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {diff_file}")
                return {
                    "status": "error",
                    "error": f"å·®å¼‚åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {diff_file}"
                }
            
            logger.info(f"   File exists. Reading CSV...")
            df = pd.read_csv(diff_file)
            logger.info(f"âœ… [STEP 2] Loaded differential data. Shape: {df.shape}, Columns: {list(df.columns)[:5]}...")
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            # è®¡ç®— -log10(p_value)
            logger.info("ğŸ‘‰ [STEP 3] Calculating -log10(p_value)...")
            try:
                df["neg_log10_p"] = -np.log10(df["p_value"] + 1e-10)  # é¿å… log(0)
                logger.info(f"âœ… [STEP 3] Calculated -log10(p). Range: [{df['neg_log10_p'].min():.2f}, {df['neg_log10_p'].max():.2f}]")
            except Exception as e:
                logger.error(f"âŒ [STEP 3] Failed to calculate -log10(p): {e}", exc_info=True)
                raise
            
            # åˆ†ç±»ï¼šæ˜¾è‘—ä¸Šè°ƒã€æ˜¾è‘—ä¸‹è°ƒã€ä¸æ˜¾è‘—
            logger.info("ğŸ‘‰ [STEP 4] Categorizing metabolites...")
            try:
                df["category"] = "Not Significant"
                df.loc[
                    (df["p_value"] < p_value_threshold) & (df["log2_fold_change"] > np.log2(fold_change_threshold)),
                    "category"
                ] = "Up"
                df.loc[
                    (df["p_value"] < p_value_threshold) & (df["log2_fold_change"] < -np.log2(fold_change_threshold)),
                    "category"
                ] = "Down"
                
                category_counts = df["category"].value_counts().to_dict()
                logger.info(f"âœ… [STEP 4] Categorized. Counts: {category_counts}")
            except Exception as e:
                logger.error(f"âŒ [STEP 4] Failed to categorize: {e}", exc_info=True)
                raise
            
            # åˆ›å»ºç«å±±å›¾
            logger.info("ğŸ‘‰ [STEP 5] Creating matplotlib figure...")
            try:
                plt.figure(figsize=(12, 8))
                logger.info("âœ… [STEP 5] Figure created")
            except Exception as e:
                logger.error(f"âŒ [STEP 5] Failed to create figure: {e}", exc_info=True)
                raise
            
            # ç»˜åˆ¶æ•£ç‚¹å›¾
            logger.info("ğŸ‘‰ [STEP 6] Plotting scatter points by category...")
            try:
                colors = {"Up": "red", "Down": "blue", "Not Significant": "gray"}
                for cat in ["Not Significant", "Up", "Down"]:
                    mask = df["category"] == cat
                    count = mask.sum()
                    logger.info(f"   Plotting {cat}: {count} points")
                    if count > 0:
                        plt.scatter(
                            df.loc[mask, "log2_fold_change"],
                            df.loc[mask, "neg_log10_p"],
                            label=cat,
                            alpha=0.6,
                            s=50,
                            c=colors[cat]
                        )
                logger.info("âœ… [STEP 6] Scatter points plotted")
            except Exception as e:
                logger.error(f"âŒ [STEP 6] Failed to plot scatter: {e}", exc_info=True)
                plt.close('all')
                gc.collect()
                raise
            
            # æ·»åŠ é˜ˆå€¼çº¿
            logger.info("ğŸ‘‰ [STEP 7] Adding threshold lines...")
            try:
                plt.axhline(y=-np.log10(p_value_threshold), color='black', linestyle='--', alpha=0.5, label=f'p={p_value_threshold}')
                plt.axvline(x=np.log2(fold_change_threshold), color='black', linestyle='--', alpha=0.5)
                plt.axvline(x=-np.log2(fold_change_threshold), color='black', linestyle='--', alpha=0.5)
                logger.info("âœ… [STEP 7] Threshold lines added")
            except Exception as e:
                logger.error(f"âŒ [STEP 7] Failed to add threshold lines: {e}", exc_info=True)
                plt.close('all')
                gc.collect()
                raise
            
            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            logger.info("ğŸ‘‰ [STEP 8] Setting labels and title...")
            try:
                plt.xlabel("Log2 Fold Change", fontsize=12)
                plt.ylabel("-Log10 P-value", fontsize=12)
                plt.title("Volcano Plot: Differential Metabolites", fontsize=14)
                plt.legend()
                plt.grid(True, alpha=0.3)
                logger.info("âœ… [STEP 8] Labels and title set")
            except Exception as e:
                logger.error(f"âŒ [STEP 8] Failed to set labels: {e}", exc_info=True)
                plt.close('all')
                gc.collect()
                raise
            
            # ä¿å­˜å›¾ç‰‡
            logger.info("ğŸ‘‰ [STEP 9] Saving figure to file...")
            plot_path = self.output_dir / "volcano_plot.png"
            logger.info(f"   Output path: {plot_path}")
            try:
                plt.tight_layout()
                logger.info("   Layout adjusted, saving...")
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                logger.info(f"âœ… [STEP 9] Figure saved to {plot_path}")
            except Exception as e:
                logger.error(f"âŒ [STEP 9] Failed to save figure: {e}", exc_info=True)
                plt.close('all')
                gc.collect()
                raise
            
            # CRITICAL: å…³é—­æ‰€æœ‰å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
            logger.info("ğŸ‘‰ [STEP 10] Closing matplotlib figures and freeing memory...")
            try:
                plt.close('all')
                gc.collect()
                logger.info("âœ… [STEP 10] Figures closed and memory freed")
            except Exception as e:
                logger.warning(f"âš ï¸ [STEP 10] Error closing figures: {e}")
                gc.collect()  # å³ä½¿å…³é—­å¤±è´¥ä¹Ÿå¼ºåˆ¶ GC
            
            # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº output_dirï¼‰
            logger.info("ğŸ‘‰ [STEP 11] Converting to relative path...")
            relative_path = str(plot_path)
            if os.path.isabs(relative_path):
                relative_path = os.path.relpath(relative_path, self.output_dir)
            relative_path = relative_path.replace("\\", "/")
            logger.info(f"âœ… [STEP 11] Relative path: {relative_path}")
            
            result = {
                "status": "success",
                "message": "ç«å±±å›¾ç”Ÿæˆå®Œæˆ",
                "plot_path": str(plot_path),
                "plot_file": str(plot_path),  # å…¼å®¹æ—§å­—æ®µå
                # å‰ç«¯å¯ç”¨çš„æ•°æ®
                "data": {
                    "images": [relative_path]  # ç›¸å¯¹è·¯å¾„ï¼Œå‰ç«¯å¯ä»¥ç›´æ¥ä½¿ç”¨
                }
            }
            
            logger.info("=" * 80)
            logger.info("âœ… [STEP 12] visualize_volcano SUCCESS")
            logger.info(f"   Plot saved: {plot_path}")
            logger.info("=" * 80)
            return result
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error("=" * 80)
            logger.error(f"âŒ [STEP X] visualize_volcano FAILED")
            logger.error(f"   Error type: {type(e).__name__}")
            logger.error(f"   Error message: {str(e)}")
            logger.error(f"   Full traceback:")
            logger.error(error_traceback)
            logger.error("=" * 80)
            
            # ç¡®ä¿æ¸…ç†èµ„æº
            try:
                plt.close('all')
            except:
                pass
            gc.collect()
            
            return {
                "status": "error",
                "error": str(e)
            }

