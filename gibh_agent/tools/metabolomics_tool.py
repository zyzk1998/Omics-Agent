"""
ä»£è°¢ç»„å­¦åˆ†æå·¥å…·
æ”¯æŒä»£è°¢ç»„å­¦æ•°æ®çš„ä¸‹è½½ã€é¢„å¤„ç†å’Œåˆ†æ
"""
import os
import requests
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import logging

logger = logging.getLogger(__name__)


class MetabolomicsTool:
    """
    ä»£è°¢ç»„å­¦åˆ†æå·¥å…·
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - ä¸‹è½½æ¼”ç¤ºæ•°æ®é›†
    - æ•°æ®æ£€æŸ¥å’Œé¢„å¤„ç†
    - ä¸»æˆåˆ†åˆ†æ (PCA)
    - å·®å¼‚ä»£è°¢ç‰©åˆ†æ
    - å¯è§†åŒ–
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ä»£è°¢ç»„å­¦å·¥å…·
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.output_dir = Path(self.config.get("output_dir", os.path.join(os.getcwd(), "results", "metabolomics"))).resolve()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å­˜å‚¨å½“å‰åŠ è½½çš„æ•°æ®
        self.data = None
        self.metadata = None
        self.metabolites = None
        
        # å·¥å…·æ˜ å°„è¡¨
        self.tool_map = {
            "download_demo_data": self.download_demo_data,
            "inspect_data": self.inspect_data,
            "preprocess_data": self.preprocess_data,
            "pca_analysis": self.pca_analysis,
            "differential_analysis": self.differential_analysis,
            "visualize_pca": self.visualize_pca,
            "visualize_volcano": self.visualize_volcano,
        }
    
    def download_demo_data(
        self,
        output_dir: Optional[str] = None,
        filename: str = "human_cachexia.csv"
    ) -> Dict[str, Any]:
        """
        ä¸‹è½½æ¼”ç¤ºæ•°æ®é›†ï¼ˆHuman Cachexiaï¼‰
        
        ä½¿ç”¨å®˜æ–¹ API ç«¯ç‚¹ä¸‹è½½æ•°æ®ï¼š
        https://rest.xialab.ca/api/download/metaboanalyst/human_cachexia.csv
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨ self.output_dirï¼‰
            filename: ä¿å­˜çš„æ–‡ä»¶åï¼ˆé»˜è®¤ä¸º human_cachexia.csvï¼‰
        
        Returns:
            åŒ…å«ä¸‹è½½çŠ¶æ€çš„å­—å…¸ï¼š
            {
                "status": "success" | "error",
                "message": "æè¿°ä¿¡æ¯",
                "file_path": "æ–‡ä»¶è·¯å¾„",
                "file_size": æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            }
        """
        try:
            # ç¡®å®šè¾“å‡ºç›®å½•
            if output_dir is None:
                output_dir = self.output_dir
            else:
                os.makedirs(output_dir, exist_ok=True)
            
            # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(output_dir, filename)
            
            # API ç«¯ç‚¹ URL
            url = "https://rest.xialab.ca/api/download/metaboanalyst/human_cachexia.csv"
            
            print(f"ğŸ“¥ æ­£åœ¨ä»å®˜æ–¹ API ä¸‹è½½ Human Cachexia æ•°æ®é›†...")
            print(f"   URL: {url}")
            print(f"   ä¿å­˜åˆ°: {file_path}")
            
            # ä½¿ç”¨ requests.get ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, timeout=30)
            response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 200ï¼ŒæŠ›å‡ºå¼‚å¸¸
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                os.remove(file_path)  # åˆ é™¤ç©ºæ–‡ä»¶
                return {
                    "status": "error",
                    "message": "ä¸‹è½½çš„æ–‡ä»¶ä¸ºç©º",
                    "file_path": file_path,
                    "file_size": 0
                }
            
            print(f"âœ… ä¸‹è½½æˆåŠŸï¼")
            print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")
            print(f"   æ–‡ä»¶è·¯å¾„: {file_path}")
            
            return {
                "status": "success",
                "message": "Human Cachexia æ•°æ®é›†ä¸‹è½½æˆåŠŸ",
                "file_path": file_path,
                "file_size": file_size
            }
            
        except requests.exceptions.RequestException as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "status": "error",
                "message": error_msg,
                "file_path": file_path if 'file_path' in locals() else None,
                "file_size": 0
            }
        except Exception as e:
            error_msg = f"å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "status": "error",
                "message": error_msg,
                "file_path": file_path if 'file_path' in locals() else None,
                "file_size": 0
            }
    
    def inspect_data(self, file_path: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥ä»£è°¢ç»„å­¦æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ‘˜è¦
        
        Args:
            file_path: CSV æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«æ•°æ®æ‘˜è¦çš„å­—å…¸
        """
        try:
            logger.info(f"ğŸ“– æ­£åœ¨æ£€æŸ¥æ•°æ®æ–‡ä»¶: {file_path}")
            
            # è¯»å– CSV æ–‡ä»¶
            df = pd.read_csv(file_path)
            
            # è¯†åˆ«å…ƒæ•°æ®åˆ—ï¼ˆé€šå¸¸æ˜¯å‰å‡ åˆ—ï¼Œå¦‚ Patient ID, Group ç­‰ï¼‰
            # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ ·æœ¬IDï¼Œç¬¬äºŒåˆ—æ˜¯åˆ†ç»„ä¿¡æ¯
            metadata_cols = []
            metabolite_cols = []
            
            for col in df.columns:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å€¼åˆ—ï¼ˆä»£è°¢ç‰©æ•°æ®ï¼‰
                if pd.api.types.is_numeric_dtype(df[col]):
                    metabolite_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            # åŸºæœ¬ä¿¡æ¯
            n_samples = len(df)
            n_metabolites = len(metabolite_cols)
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_counts = df[metabolite_cols].isnull().sum()
            total_missing = missing_counts.sum()
            missing_percentage = (total_missing / (n_samples * n_metabolites)) * 100 if n_metabolites > 0 else 0
            
            # æ£€æŸ¥åˆ†ç»„ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            group_info = {}
            if len(metadata_cols) > 1:
                group_col = metadata_cols[1]  # å‡è®¾ç¬¬äºŒåˆ—æ˜¯åˆ†ç»„
                if group_col in df.columns:
                    group_counts = df[group_col].value_counts().to_dict()
                    group_info = {
                        "column": group_col,
                        "groups": group_counts,
                        "n_groups": len(group_counts)
                    }
            
            # æ•°æ®èŒƒå›´
            metabolite_data = df[metabolite_cols]
            data_stats = {
                "min": float(metabolite_data.min().min()),
                "max": float(metabolite_data.max().max()),
                "mean": float(metabolite_data.mean().mean()),
                "median": float(metabolite_data.median().median()),
            }
            
            # é¢„è§ˆå‰å‡ è¡Œ
            preview = df.head(3).to_dict('records')
            
            result = {
                "status": "success",
                "file_path": file_path,
                "n_samples": n_samples,
                "n_metabolites": n_metabolites,
                "metadata_columns": metadata_cols,
                "metabolite_columns": metabolite_cols[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
                "total_metabolite_columns": len(metabolite_cols),
                "missing_values": {
                    "total": int(total_missing),
                    "percentage": round(missing_percentage, 2)
                },
                "group_info": group_info,
                "data_statistics": data_stats,
                "preview": preview
            }
            
            logger.info(f"âœ… æ•°æ®æ£€æŸ¥å®Œæˆ: {n_samples} ä¸ªæ ·æœ¬, {n_metabolites} ä¸ªä»£è°¢ç‰©")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "file_path": file_path
            }
    
    def preprocess_data(
        self,
        file_path: str,
        missing_threshold: float = 0.5,
        normalization: str = "log2",
        scale: bool = True
    ) -> Dict[str, Any]:
        """
        é¢„å¤„ç†ä»£è°¢ç»„å­¦æ•°æ®
        
        Args:
            file_path: CSV æ–‡ä»¶è·¯å¾„
            missing_threshold: ç¼ºå¤±å€¼é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤æ¯”ä¾‹çš„ä»£è°¢ç‰©å°†è¢«ç§»é™¤ï¼‰
            normalization: æ ‡å‡†åŒ–æ–¹æ³• ("log2", "zscore", "none")
            scale: æ˜¯å¦è¿›è¡Œç¼©æ”¾ï¼ˆStandardScalerï¼‰
        
        Returns:
            é¢„å¤„ç†ç»“æœå­—å…¸
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹é¢„å¤„ç†æ•°æ®: {file_path}")
            
            # è¯»å–æ•°æ®
            df = pd.read_csv(file_path)
            
            # åˆ†ç¦»å…ƒæ•°æ®å’Œä»£è°¢ç‰©æ•°æ®
            metadata_cols = []
            metabolite_cols = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    metabolite_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            # æå–å…ƒæ•°æ®
            self.metadata = df[metadata_cols].copy()
            self.metabolites = df[metabolite_cols].copy()
            
            # 1. å¤„ç†ç¼ºå¤±å€¼ï¼šç§»é™¤ç¼ºå¤±å€¼æ¯”ä¾‹è¿‡é«˜çš„ä»£è°¢ç‰©
            missing_ratio = self.metabolites.isnull().sum() / len(self.metabolites)
            valid_metabolites = missing_ratio[missing_ratio < missing_threshold].index
            removed_count = len(metabolite_cols) - len(valid_metabolites)
            
            self.metabolites = self.metabolites[valid_metabolites]
            
            # 2. å¡«å……å‰©ä½™ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼‰
            self.metabolites = self.metabolites.fillna(self.metabolites.median())
            
            # 3. æ ‡å‡†åŒ–
            if normalization == "log2":
                # Log2 è½¬æ¢ï¼ˆå¤„ç†é›¶å€¼å’Œè´Ÿå€¼ï¼‰
                self.metabolites = self.metabolites.apply(lambda x: np.log2(x + 1))
                logger.info("âœ… å·²åº”ç”¨ Log2 è½¬æ¢")
            elif normalization == "zscore":
                from scipy.stats import zscore
                self.metabolites = self.metabolites.apply(zscore, axis=0)
                logger.info("âœ… å·²åº”ç”¨ Z-score æ ‡å‡†åŒ–")
            
            # 4. ç¼©æ”¾ï¼ˆå¯é€‰ï¼‰
            if scale:
                scaler = StandardScaler()
                metabolite_array = scaler.fit_transform(self.metabolites)
                self.metabolites = pd.DataFrame(
                    metabolite_array,
                    columns=self.metabolites.columns,
                    index=self.metabolites.index
                )
                logger.info("âœ… å·²åº”ç”¨ StandardScaler ç¼©æ”¾")
            
            # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
            preprocessed_path = self.output_dir / "preprocessed_data.csv"
            preprocessed_df = pd.concat([self.metadata, self.metabolites], axis=1)
            preprocessed_df.to_csv(preprocessed_path, index=False)
            
            result = {
                "status": "success",
                "message": "æ•°æ®é¢„å¤„ç†å®Œæˆ",
                "n_samples": len(self.metabolites),
                "n_metabolites": len(self.metabolites.columns),
                "removed_metabolites": removed_count,
                "normalization": normalization,
                "scaled": scale,
                "preprocessed_file": str(preprocessed_path)
            }
            
            logger.info(f"âœ… é¢„å¤„ç†å®Œæˆ: {result['n_metabolites']} ä¸ªä»£è°¢ç‰©ä¿ç•™")
            return result
            
        except Exception as e:
            logger.error(f"âŒ é¢„å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }
    
    def pca_analysis(
        self,
        n_components: int = 10,
        file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸»æˆåˆ†åˆ†æ (PCA)
        
        Args:
            n_components: ä¸»æˆåˆ†æ•°é‡
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœªé¢„å¤„ç†ï¼Œéœ€è¦æä¾›ï¼‰
        
        Returns:
            PCA åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ“Š å¼€å§‹ PCA åˆ†æ (n_components={n_components})")
            
            # å¦‚æœæ•°æ®æœªåŠ è½½ï¼Œä»æ–‡ä»¶è¯»å–
            if self.metabolites is None:
                if file_path is None:
                    return {
                        "status": "error",
                        "error": "æ•°æ®æœªåŠ è½½ä¸”æœªæä¾›æ–‡ä»¶è·¯å¾„"
                    }
                # è¯»å–é¢„å¤„ç†åçš„æ•°æ®æˆ–åŸå§‹æ•°æ®
                df = pd.read_csv(file_path)
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                self.metabolites = df.drop(columns=metadata_cols)
                if len(metadata_cols) > 0:
                    self.metadata = df[metadata_cols]
            
            # æ‰§è¡Œ PCA
            pca = PCA(n_components=min(n_components, len(self.metabolites.columns), len(self.metabolites)))
            pca_result = pca.fit_transform(self.metabolites)
            
            # è®¡ç®—è§£é‡Šæ–¹å·®
            explained_variance = pca.explained_variance_ratio_
            cumulative_variance = np.cumsum(explained_variance)
            
            # ä¿å­˜ PCA ç»“æœ
            pca_df = pd.DataFrame(
                pca_result,
                columns=[f"PC{i+1}" for i in range(pca_result.shape[1])],
                index=self.metabolites.index
            )
            
            # å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œåˆå¹¶
            if self.metadata is not None:
                pca_df = pd.concat([self.metadata, pca_df], axis=1)
            
            pca_path = self.output_dir / "pca_results.csv"
            pca_df.to_csv(pca_path, index=False)
            
            result = {
                "status": "success",
                "message": "PCA åˆ†æå®Œæˆ",
                "n_components": pca_result.shape[1],
                "explained_variance": {
                    "PC1": float(explained_variance[0]),
                    "PC2": float(explained_variance[1]) if len(explained_variance) > 1 else 0.0,
                    "PC3": float(explained_variance[2]) if len(explained_variance) > 2 else 0.0,
                },
                "cumulative_variance_pc10": float(cumulative_variance[min(9, len(cumulative_variance)-1)]),
                "pca_file": str(pca_path),
                "loadings": {
                    "top_10_pc1": self._get_top_loadings(pca.components_[0], self.metabolites.columns, 10)
                }
            }
            
            logger.info(f"âœ… PCA å®Œæˆ: PC1 è§£é‡Š {result['explained_variance']['PC1']*100:.2f}% æ–¹å·®")
            return result
            
        except Exception as e:
            logger.error(f"âŒ PCA åˆ†æå¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _get_top_loadings(self, loadings: np.ndarray, metabolite_names: List[str], n: int = 10) -> List[Dict[str, Any]]:
        """è·å–è½½è·æœ€é«˜çš„ä»£è°¢ç‰©"""
        indices = np.argsort(np.abs(loadings))[-n:][::-1]
        return [
            {
                "metabolite": metabolite_names[i],
                "loading": float(loadings[i])
            }
            for i in indices
        ]
    
    def differential_analysis(
        self,
        group_column: str,
        file_path: Optional[str] = None,
        method: str = "t-test",
        p_value_threshold: float = 0.05,
        fold_change_threshold: float = 1.5,
        group1: Optional[str] = None,
        group2: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æ
        
        Args:
            group_column: åˆ†ç»„åˆ—å
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœªé¢„å¤„ç†ï¼Œéœ€è¦æä¾›ï¼‰
            method: ç»Ÿè®¡æ–¹æ³• ("t-test", "mann-whitney")
            p_value_threshold: P å€¼é˜ˆå€¼
            fold_change_threshold: å€æ•°å˜åŒ–é˜ˆå€¼
        
        Returns:
            å·®å¼‚åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ”¬ å¼€å§‹å·®å¼‚ä»£è°¢ç‰©åˆ†æ (åˆ†ç»„: {group_column})")
            
            # å¦‚æœæ•°æ®æœªåŠ è½½ï¼Œä»æ–‡ä»¶è¯»å–
            if self.metabolites is None or self.metadata is None:
                if file_path is None:
                    return {
                        "status": "error",
                        "error": "æ•°æ®æœªåŠ è½½ä¸”æœªæä¾›æ–‡ä»¶è·¯å¾„"
                    }
                df = pd.read_csv(file_path)
                metadata_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
                self.metabolites = df.drop(columns=metadata_cols)
                self.metadata = df[metadata_cols]
            
            # æ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨
            if group_column not in self.metadata.columns:
                return {
                    "status": "error",
                    "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(self.metadata.columns)}"
                }
            
            # è·å–åˆ†ç»„
            groups = self.metadata[group_column].unique()
            if len(groups) < 2:
                return {
                    "status": "error",
                    "error": f"éœ€è¦è‡³å°‘2ä¸ªåˆ†ç»„ï¼Œä½†æ‰¾åˆ° {len(groups)} ä¸ª: {groups}"
                }
            elif len(groups) > 2:
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº† group1 å’Œ group2ï¼Œä½¿ç”¨å®ƒä»¬
                if group1 and group2:
                    if group1 not in groups or group2 not in groups:
                        return {
                            "status": "error",
                            "error": f"æŒ‡å®šçš„åˆ†ç»„ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ†ç»„: {list(groups)}"
                        }
                    # ä½¿ç”¨æŒ‡å®šçš„åˆ†ç»„
                    pass  # group1 å’Œ group2 å·²ç»è®¾ç½®
                else:
                    # è¿”å›éœ€è¦ç”¨æˆ·é€‰æ‹©çš„ä¿¡æ¯
                    return {
                        "status": "need_selection",
                        "message": f"æ£€æµ‹åˆ° {len(groups)} ä¸ªåˆ†ç»„: {list(groups)}",
                        "groups": list(groups),
                        "error": f"éœ€è¦é€‰æ‹©2ä¸ªåˆ†ç»„è¿›è¡Œæ¯”è¾ƒï¼Œä½†æ‰¾åˆ° {len(groups)} ä¸ªåˆ†ç»„: {groups}"
                    }
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†ç»„ï¼Œä½¿ç”¨å‰ä¸¤ä¸ª
            if not group1 or not group2:
                group1, group2 = groups[0], groups[1]
            group1_mask = self.metadata[group_column] == group1
            group2_mask = self.metadata[group_column] == group2
            
            # å¯¹æ¯ä¸ªä»£è°¢ç‰©æ‰§è¡Œç»Ÿè®¡æ£€éªŒ
            results = []
            for metabolite in self.metabolites.columns:
                group1_data = self.metabolites.loc[group1_mask, metabolite].values
                group2_data = self.metabolites.loc[group2_mask, metabolite].values
                
                # è®¡ç®—å‡å€¼
                mean1 = np.mean(group1_data)
                mean2 = np.mean(group2_data)
                
                # è®¡ç®—å€æ•°å˜åŒ–
                if mean2 != 0:
                    fold_change = mean1 / mean2
                    log2_fc = np.log2(fold_change) if fold_change > 0 else 0
                else:
                    fold_change = np.inf
                    log2_fc = 0
                
                # ç»Ÿè®¡æ£€éªŒ
                if method == "t-test":
                    try:
                        stat, p_value = stats.ttest_ind(group1_data, group2_data)
                    except:
                        p_value = 1.0
                        stat = 0
                elif method == "mann-whitney":
                    try:
                        stat, p_value = stats.mannwhitneyu(group1_data, group2_data, alternative='two-sided')
                    except:
                        p_value = 1.0
                        stat = 0
                else:
                    p_value = 1.0
                    stat = 0
                
                # åˆ¤æ–­æ˜¯å¦æ˜¾è‘—
                is_significant = p_value < p_value_threshold and abs(log2_fc) > np.log2(fold_change_threshold)
                
                results.append({
                    "metabolite": metabolite,
                    "group1_mean": float(mean1),
                    "group2_mean": float(mean2),
                    "fold_change": float(fold_change),
                    "log2_fold_change": float(log2_fc),
                    "p_value": float(p_value),
                    "statistic": float(stat),
                    "significant": is_significant
                })
            
            # è½¬æ¢ä¸º DataFrame
            results_df = pd.DataFrame(results)
            results_df = results_df.sort_values("p_value")
            
            # ä¿å­˜ç»“æœ
            diff_path = self.output_dir / "differential_analysis.csv"
            results_df.to_csv(diff_path, index=False)
            
            # ç»Ÿè®¡æ˜¾è‘—ä»£è°¢ç‰©
            significant = results_df[results_df["significant"] == True]
            
            result = {
                "status": "success",
                "message": "å·®å¼‚åˆ†æå®Œæˆ",
                "n_total": len(results_df),
                "n_significant": len(significant),
                "groups": {
                    "group1": str(group1),
                    "group2": str(group2)
                },
                "top_significant": significant.head(20).to_dict('records'),
                "results_file": str(diff_path)
            }
            
            logger.info(f"âœ… å·®å¼‚åˆ†æå®Œæˆ: {result['n_significant']} ä¸ªæ˜¾è‘—ä»£è°¢ç‰©")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å·®å¼‚åˆ†æå¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }
    
    def visualize_pca(
        self,
        group_column: Optional[str] = None,
        pca_file: Optional[str] = None,
        pc1: int = 1,
        pc2: int = 2
    ) -> Dict[str, Any]:
        """
        å¯è§†åŒ– PCA ç»“æœ
        
        Args:
            group_column: ç”¨äºç€è‰²çš„åˆ†ç»„åˆ—
            pca_file: PCA ç»“æœæ–‡ä»¶è·¯å¾„
            pc1: ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼ˆ1-basedï¼‰
            pc2: ç¬¬äºŒä¸ªä¸»æˆåˆ†ï¼ˆ1-basedï¼‰
        
        Returns:
            å¯è§†åŒ–ç»“æœ
        """
        try:
            logger.info(f"ğŸ“ˆ ç”Ÿæˆ PCA å¯è§†åŒ–å›¾")
            
            # è¯»å– PCA ç»“æœ
            if pca_file is None:
                pca_file = self.output_dir / "pca_results.csv"
            
            if not os.path.exists(pca_file):
                return {
                    "status": "error",
                    "error": f"PCA ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {pca_file}"
                }
            
            df = pd.read_csv(pca_file)
            
            # æŸ¥æ‰¾ä¸»æˆåˆ†åˆ—
            pc_cols = [col for col in df.columns if col.startswith("PC")]
            if len(pc_cols) < max(pc1, pc2):
                return {
                    "status": "error",
                    "error": f"ä¸»æˆåˆ†æ•°é‡ä¸è¶³: éœ€è¦ PC{pc1} å’Œ PC{pc2}"
                }
            
            pc1_col = f"PC{pc1}"
            pc2_col = f"PC{pc2}"
            
            # åˆ›å»ºå›¾å½¢
            plt.figure(figsize=(10, 8))
            
            if group_column and group_column in df.columns:
                # æŒ‰åˆ†ç»„ç€è‰²
                groups = df[group_column].unique()
                colors = plt.cm.Set3(np.linspace(0, 1, len(groups)))
                for i, group in enumerate(groups):
                    mask = df[group_column] == group
                    plt.scatter(
                        df.loc[mask, pc1_col],
                        df.loc[mask, pc2_col],
                        label=str(group),
                        alpha=0.7,
                        s=100,
                        c=[colors[i]]
                    )
                plt.legend(title=group_column)
            else:
                plt.scatter(df[pc1_col], df[pc2_col], alpha=0.7, s=100)
            
            plt.xlabel(f"{pc1_col}", fontsize=12)
            plt.ylabel(f"{pc2_col}", fontsize=12)
            plt.title(f"PCA Plot: {pc1_col} vs {pc2_col}", fontsize=14)
            plt.grid(True, alpha=0.3)
            
            # ä¿å­˜å›¾ç‰‡
            plot_path = self.output_dir / f"pca_plot_pc{pc1}_pc{pc2}.png"
            plt.tight_layout()
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            result = {
                "status": "success",
                "message": "PCA å¯è§†åŒ–å®Œæˆ",
                "plot_path": str(plot_path),
                "plot_file": str(plot_path)  # å…¼å®¹æ—§å­—æ®µå
            }
            
            logger.info(f"âœ… PCA å›¾å·²ä¿å­˜: {plot_path}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ PCA å¯è§†åŒ–å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }
    
    def visualize_volcano(
        self,
        diff_file: Optional[str] = None,
        p_value_threshold: float = 0.05,
        fold_change_threshold: float = 1.5
    ) -> Dict[str, Any]:
        """
        ç»˜åˆ¶ç«å±±å›¾ï¼ˆVolcano Plotï¼‰
        
        Args:
            diff_file: å·®å¼‚åˆ†æç»“æœæ–‡ä»¶
            p_value_threshold: P å€¼é˜ˆå€¼
            fold_change_threshold: å€æ•°å˜åŒ–é˜ˆå€¼
        
        Returns:
            å¯è§†åŒ–ç»“æœ
        """
        try:
            logger.info(f"ğŸ“ˆ ç”Ÿæˆç«å±±å›¾")
            
            # è¯»å–å·®å¼‚åˆ†æç»“æœ
            if diff_file is None:
                diff_file = self.output_dir / "differential_analysis.csv"
            
            if not os.path.exists(diff_file):
                return {
                    "status": "error",
                    "error": f"å·®å¼‚åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {diff_file}"
                }
            
            df = pd.read_csv(diff_file)
            
            # è®¡ç®— -log10(p_value)
            df["neg_log10_p"] = -np.log10(df["p_value"] + 1e-10)  # é¿å… log(0)
            
            # åˆ†ç±»ï¼šæ˜¾è‘—ä¸Šè°ƒã€æ˜¾è‘—ä¸‹è°ƒã€ä¸æ˜¾è‘—
            df["category"] = "Not Significant"
            df.loc[
                (df["p_value"] < p_value_threshold) & (df["log2_fold_change"] > np.log2(fold_change_threshold)),
                "category"
            ] = "Up"
            df.loc[
                (df["p_value"] < p_value_threshold) & (df["log2_fold_change"] < -np.log2(fold_change_threshold)),
                "category"
            ] = "Down"
            
            # åˆ›å»ºç«å±±å›¾
            plt.figure(figsize=(12, 8))
            
            colors = {"Up": "red", "Down": "blue", "Not Significant": "gray"}
            for cat in ["Not Significant", "Up", "Down"]:
                mask = df["category"] == cat
                plt.scatter(
                    df.loc[mask, "log2_fold_change"],
                    df.loc[mask, "neg_log10_p"],
                    label=cat,
                    alpha=0.6,
                    s=50,
                    c=colors[cat]
                )
            
            # æ·»åŠ é˜ˆå€¼çº¿
            plt.axhline(y=-np.log10(p_value_threshold), color='black', linestyle='--', alpha=0.5, label=f'p={p_value_threshold}')
            plt.axvline(x=np.log2(fold_change_threshold), color='black', linestyle='--', alpha=0.5)
            plt.axvline(x=-np.log2(fold_change_threshold), color='black', linestyle='--', alpha=0.5)
            
            plt.xlabel("Log2 Fold Change", fontsize=12)
            plt.ylabel("-Log10 P-value", fontsize=12)
            plt.title("Volcano Plot: Differential Metabolites", fontsize=14)
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ä¿å­˜å›¾ç‰‡
            plot_path = self.output_dir / "volcano_plot.png"
            plt.tight_layout()
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            result = {
                "status": "success",
                "message": "ç«å±±å›¾ç”Ÿæˆå®Œæˆ",
                "plot_path": str(plot_path),
                "plot_file": str(plot_path)  # å…¼å®¹æ—§å­—æ®µå
            }
            
            logger.info(f"âœ… ç«å±±å›¾å·²ä¿å­˜: {plot_path}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ç«å±±å›¾ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }

