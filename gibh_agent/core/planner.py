"""
åŠ¨æ€å·¥ä½œæµè§„åˆ’å™¨ - The Brain

ä½¿ç”¨ Tool-RAG æ¶æ„åŠ¨æ€ç”Ÿæˆå¯æ‰§è¡Œçš„å·¥ä½œæµè®¡åˆ’ã€‚
ç»“åˆå·¥å…·æ£€ç´¢å’Œ LLM æ¨ç†ï¼Œç”Ÿæˆç¬¦åˆå‰ç«¯æ ¼å¼çš„å·¥ä½œæµé…ç½®ã€‚

åŒ…å«ä¸¤ä¸ªè§„åˆ’å™¨ï¼š
1. WorkflowPlanner: é€šç”¨å·¥ä½œæµè§„åˆ’å™¨
2. SOPPlanner: åŸºäº SOPï¼ˆæ ‡å‡†æ“ä½œç¨‹åºï¼‰çš„é¢†åŸŸç‰¹å®šè§„åˆ’å™¨
"""
import json
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path

from .tool_retriever import ToolRetriever
from .tool_registry import registry
from .llm_client import LLMClient

logger = logging.getLogger(__name__)


class WorkflowPlanner:
    """
    å·¥ä½œæµè§„åˆ’å™¨
    
    èŒè´£ï¼š
    1. ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³å·¥å…·
    2. ä½¿ç”¨ LLM ç”Ÿæˆå·¥ä½œæµè®¡åˆ’
    3. éªŒè¯å’Œè½¬æ¢è¾“å‡ºæ ¼å¼
    """
    
    def __init__(
        self,
        tool_retriever: ToolRetriever,
        llm_client: LLMClient
    ):
        """
        åˆå§‹åŒ–å·¥ä½œæµè§„åˆ’å™¨
        
        Args:
            tool_retriever: å·¥å…·æ£€ç´¢å™¨å®ä¾‹
            llm_client: LLM å®¢æˆ·ç«¯å®ä¾‹
        """
        self.tool_retriever = tool_retriever
        self.llm_client = llm_client
    
    async def plan(
        self,
        user_query: str,
        context_files: List[str] = None,
        category_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµè®¡åˆ’
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            context_files: å¯ç”¨çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            category_filter: å¯é€‰çš„ç±»åˆ«è¿‡æ»¤å™¨ï¼ˆå¦‚ "Metabolomics"ï¼‰
        
        Returns:
            å·¥ä½œæµé…ç½®å­—å…¸ï¼Œç¬¦åˆå‰ç«¯æ ¼å¼
        """
        try:
            logger.info(f"ğŸ§  å¼€å§‹è§„åˆ’å·¥ä½œæµ: '{user_query}'")
            
            # Step 1: æ£€ç´¢ç›¸å…³å·¥å…·
            logger.info("ğŸ” Step 1: æ£€ç´¢ç›¸å…³å·¥å…·...")
            retrieved_tools = self.tool_retriever.retrieve(
                query=user_query,
                top_k=10,
                category_filter=category_filter
            )
            
            if not retrieved_tools:
                logger.warning("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³å·¥å…·")
                return {
                    "type": "error",
                    "error": "æœªæ‰¾åˆ°ç›¸å…³å·¥å…·ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢æˆ–å·¥å…·æ³¨å†Œ",
                    "message": "æ— æ³•ç”Ÿæˆå·¥ä½œæµè®¡åˆ’"
                }
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(retrieved_tools)} ä¸ªç›¸å…³å·¥å…·")
            for tool in retrieved_tools[:3]:  # åªæ‰“å°å‰3ä¸ª
                logger.info(f"   - {tool['name']} (ç›¸ä¼¼åº¦: {tool['similarity_score']:.4f})")
            
            # Step 2: æ„å»º LLM Prompt
            logger.info("ğŸ“ Step 2: æ„å»º LLM Prompt...")
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(
                user_query=user_query,
                retrieved_tools=retrieved_tools,
                context_files=context_files or []
            )
            
            # Step 3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’
            logger.info("ğŸ¤– Step 3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’...")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # å°è¯•ä½¿ç”¨ JSON modeï¼ˆå¦‚æœæ”¯æŒï¼‰
            response = await self.llm_client.achat(
                messages=messages,
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=2048
            )
            
            # Step 4: è§£æ LLM å“åº”
            logger.info("ğŸ”§ Step 4: è§£æ LLM å“åº”...")
            workflow_plan = self._parse_llm_response(response)
            
            # Step 5: éªŒè¯å·¥å…·å­˜åœ¨æ€§
            logger.info("âœ… Step 5: éªŒè¯å·¥å…·...")
            validated_plan = self._validate_and_adapt(workflow_plan, context_files or [])
            
            logger.info(f"âœ… å·¥ä½œæµè§„åˆ’å®Œæˆ: {len(validated_plan.get('steps', []))} ä¸ªæ­¥éª¤")
            return validated_plan
        
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµè§„åˆ’å¤±è´¥: {e}", exc_info=True)
            return {
                "type": "error",
                "error": str(e),
                "message": f"å·¥ä½œæµè§„åˆ’å¤±è´¥: {str(e)}"
            }
    
    def _build_system_prompt(self) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯
        
        Returns:
            ç³»ç»Ÿæç¤ºè¯æ–‡æœ¬
        """
        return """You are a Senior Bioinformatics Workflow Architect.

Your task is to generate executable workflow plans based on user queries and available tools.

**CRITICAL RULES:**
1. Output MUST be a valid JSON object (no markdown code blocks, no extra text).
2. Structure: {"workflow_name": "...", "steps": [{"id": "step1", "tool_name": "...", "params": {...}, "dependency": "..."}]}
3. **Data Flow**: The output of Step N must match the input of Step N+1 (e.g., file paths).
4. If a parameter is a file path, use placeholders like `<step1_output>` or match the uploaded filename.
5. Use tool names EXACTLY as provided in the tool schemas.
6. Only include parameters that exist in the tool's args_schema.
7. For file paths, prefer using the actual uploaded filename if available.

**Step Structure:**
- "id": Unique step identifier (e.g., "step1", "step2")
- "tool_name": The exact tool name from the retrieved tools
- "params": Dictionary of parameters matching the tool's args_schema
- "dependency": Optional, the step ID this step depends on (e.g., "step1")

**Example Output:**
{
  "workflow_name": "PCA Analysis Pipeline",
  "steps": [
    {
      "id": "step1",
      "tool_name": "metabolomics_pca",
      "params": {
        "file_path": "cow_diet.csv",
        "n_components": 2,
        "scale": true
      },
      "dependency": null
    }
  ]
}

Generate ONLY the JSON object, no additional text."""
    
    def _build_user_prompt(
        self,
        user_query: str,
        retrieved_tools: List[Dict[str, Any]],
        context_files: List[str]
    ) -> str:
        """
        æ„å»ºç”¨æˆ·æç¤ºè¯
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_tools: æ£€ç´¢åˆ°çš„å·¥å…·åˆ—è¡¨
            context_files: å¯ç”¨æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            ç”¨æˆ·æç¤ºè¯æ–‡æœ¬
        """
        # æ ¼å¼åŒ–å·¥å…·ä¿¡æ¯
        tools_text = []
        for i, tool in enumerate(retrieved_tools, 1):
            tool_info = f"""
Tool {i}: {tool['name']}
  Description: {tool['description']}
  Category: {tool['category']}
  Output Type: {tool['output_type']}
  Parameters Schema:
{json.dumps(tool['args_schema'], indent=2, ensure_ascii=False)}
"""
            tools_text.append(tool_info)
        
        # æ ¼å¼åŒ–æ–‡ä»¶ä¿¡æ¯
        files_text = ""
        if context_files:
            files_text = "\n**Available Files:**\n"
            for file_path in context_files:
                filename = Path(file_path).name
                files_text += f"- {filename} (path: {file_path})\n"
        else:
            files_text = "\n**Available Files:** None (user may upload files later)\n"
        
        prompt = f"""**User Query:**
{user_query}

**Retrieved Tools:**
{''.join(tools_text)}

{files_text}

**Task:**
Generate a workflow plan that fulfills the user's request. Use the retrieved tools and available files.

**Instructions:**
1. Select the most relevant tools from the retrieved list.
2. Arrange them in a logical order (data flow: output of step N â†’ input of step N+1).
3. Fill in parameters using:
   - Actual file names from "Available Files" if they match the query
   - Default values from the tool's args_schema
   - Placeholders like `<step1_output>` if a step depends on another step's output
4. Keep the workflow concise - only include necessary steps.

**Output Format:**
Return ONLY a valid JSON object with this structure:
{{
  "workflow_name": "Descriptive workflow name",
  "steps": [
    {{
      "id": "step1",
      "tool_name": "exact_tool_name",
      "params": {{"param1": "value1", "param2": "value2"}},
      "dependency": null
    }},
    {{
      "id": "step2",
      "tool_name": "another_tool_name",
      "params": {{"file_path": "<step1_output>", "other_param": "value"}},
      "dependency": "step1"
    }}
  ]
}}

Remember: Output ONLY the JSON object, no markdown, no code blocks, no explanations."""
        
        return prompt
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æ LLM å“åº”
        
        Args:
            response: LLM è¿”å›çš„æ–‡æœ¬
        
        Returns:
            è§£æåçš„å·¥ä½œæµè®¡åˆ’å­—å…¸
        """
        # å°è¯•æå– JSONï¼ˆå¯èƒ½è¢« markdown ä»£ç å—åŒ…è£¹ï¼‰
        response = response.strip()
        
        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        if response.startswith("```json"):
            response = response[7:]
        elif response.startswith("```"):
            response = response[3:]
        
        if response.endswith("```"):
            response = response[:-3]
        
        response = response.strip()
        
        try:
            plan = json.loads(response)
            return plan
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response[:500]}")
            
            # å°è¯•æå– JSON å¯¹è±¡ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
            import re
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                try:
                    plan = json.loads(json_match.group())
                    logger.info("âœ… ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆåŠŸæå– JSON")
                    return plan
                except:
                    pass
            
            raise ValueError(f"æ— æ³•è§£æ LLM å“åº”ä¸º JSON: {e}")
    
    def _validate_and_adapt(
        self,
        workflow_plan: Dict[str, Any],
        context_files: List[str]
    ) -> Dict[str, Any]:
        """
        éªŒè¯å·¥ä½œæµè®¡åˆ’å¹¶é€‚é…ä¸ºå‰ç«¯æ ¼å¼
        
        Args:
            workflow_plan: LLM ç”Ÿæˆçš„åŸå§‹è®¡åˆ’
            context_files: å¯ç”¨æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            éªŒè¯å’Œé€‚é…åçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        """
        # éªŒè¯åŸºæœ¬ç»“æ„
        if "workflow_name" not in workflow_plan:
            workflow_plan["workflow_name"] = "Generated Workflow"
        
        if "steps" not in workflow_plan or not isinstance(workflow_plan["steps"], list):
            raise ValueError("å·¥ä½œæµè®¡åˆ’å¿…é¡»åŒ…å« 'steps' æ•°ç»„")
        
        # é€‚é…æ­¥éª¤æ ¼å¼
        adapted_steps = []
        for i, step in enumerate(workflow_plan["steps"], 1):
            # éªŒè¯å·¥å…·å­˜åœ¨æ€§
            tool_name = step.get("tool_name") or step.get("tool_id")
            if not tool_name:
                logger.warning(f"âš ï¸ æ­¥éª¤ {i} ç¼ºå°‘ tool_nameï¼Œè·³è¿‡")
                continue
            
            # æ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨æ³¨å†Œè¡¨ä¸­
            tool_metadata = registry.get_metadata(tool_name)
            if not tool_metadata:
                logger.warning(f"âš ï¸ å·¥å…· '{tool_name}' ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œè·³è¿‡")
                continue
            
            # è·å–å·¥å…·æè¿°
            tool_desc = tool_metadata.description
            
            # å¤„ç†æ–‡ä»¶è·¯å¾„å‚æ•°
            params = step.get("params", {})
            adapted_params = {}
            
            for param_name, param_value in params.items():
                # å¦‚æœå‚æ•°å€¼æ˜¯æ–‡ä»¶è·¯å¾„å ä½ç¬¦ï¼Œå°è¯•åŒ¹é…å®é™…æ–‡ä»¶
                if isinstance(param_value, str) and param_value.startswith("<") and param_value.endswith(">"):
                    # å ä½ç¬¦ï¼Œä¿æŒåŸæ ·ï¼ˆæ‰§è¡Œæ—¶ä¼šå¤„ç†ï¼‰
                    adapted_params[param_name] = param_value
                elif param_name == "file_path" and context_files:
                    # å¦‚æœæ˜¯ file_path å‚æ•°ï¼Œå°è¯•åŒ¹é…ä¸Šä¼ çš„æ–‡ä»¶
                    if isinstance(param_value, str):
                        # å°è¯•åŒ¹é…æ–‡ä»¶å
                        matched_file = None
                        for file_path in context_files:
                            if param_value.lower() in Path(file_path).name.lower():
                                matched_file = file_path
                                break
                        
                        if matched_file:
                            adapted_params[param_name] = matched_file
                        else:
                            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºé»˜è®¤å€¼
                            adapted_params[param_name] = context_files[0]
                    else:
                        adapted_params[param_name] = param_value
                else:
                    adapted_params[param_name] = param_value
            
            # æ„å»ºé€‚é…åçš„æ­¥éª¤ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
            # ğŸ”¥ Frontend Contract: å¿…é¡»åŒ…å« step_id, tool_id, name, step_name, desc
            step_display_name = self._get_step_display_name(tool_name, tool_desc)
            step_desc = tool_desc[:100] if tool_desc else ""
            
            adapted_step = {
                "step_id": tool_name,  # ğŸ”¥ Frontend requires step_id (must match tool_id)
                "id": tool_name,  # ä¿ç•™ id ä½œä¸ºå…¼å®¹å­—æ®µ
                "tool_id": tool_name,  # Frontend requires tool_id
                "name": step_display_name,  # Frontend requires name
                "step_name": step_display_name,  # Frontend requires step_name (compatibility)
                "description": tool_desc if tool_desc else "",  # å®Œæ•´æè¿°
                "desc": step_desc,  # Frontend requires desc (truncated to 100 chars)
                "selected": True,  # Frontend may use this
                "params": adapted_params
            }
            
            adapted_steps.append(adapted_step)
        
        # æ„å»ºæœ€ç»ˆçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        workflow_config = {
            "type": "workflow_config",
            "workflow_data": {
                "workflow_name": workflow_plan["workflow_name"],
                "steps": adapted_steps
            },
            "file_paths": context_files
        }
        
        return workflow_config
    
    def _get_step_display_name(self, tool_name: str, tool_desc: str) -> str:
        """
        è·å–æ­¥éª¤çš„æ˜¾ç¤ºåç§°
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_desc: å·¥å…·æè¿°
        
        Returns:
            æ˜¾ç¤ºåç§°
        """
        # ç®€å•çš„åç§°æ˜ å°„
        name_mapping = {
            "metabolomics_pca": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "metabolomics_differential_analysis": "å·®å¼‚åˆ†æ",
            "metabolomics_preprocess": "æ•°æ®é¢„å¤„ç†",
            "file_inspect": "æ–‡ä»¶æ£€æŸ¥"
        }
        
        if tool_name in name_mapping:
            return name_mapping[tool_name]
        
        # ä»æè¿°ä¸­æå–ï¼ˆç®€å•å¯å‘å¼ï¼‰
        if "PCA" in tool_desc or "principal component" in tool_desc.lower():
            return "ä¸»æˆåˆ†åˆ†æ"
        elif "differential" in tool_desc.lower():
            return "å·®å¼‚åˆ†æ"
        elif "preprocess" in tool_desc.lower() or "é¢„å¤„ç†" in tool_desc:
            return "æ•°æ®é¢„å¤„ç†"
        elif "inspect" in tool_desc.lower() or "æ£€æŸ¥" in tool_desc:
            return "æ–‡ä»¶æ£€æŸ¥"
        
        # é»˜è®¤ï¼šä½¿ç”¨å·¥å…·åç§°ï¼ˆç¾åŒ–ï¼‰
        return tool_name.replace("_", " ").title()



class SOPPlanner:
    """
    SOP é©±åŠ¨çš„åŠ¨æ€è§„åˆ’å™¨
    
    åŸºäºæ ‡å‡†æ“ä½œç¨‹åºï¼ˆSOPï¼‰è§„åˆ™ï¼Œä½¿ç”¨ LLM ç”Ÿæˆç¬¦åˆä¸“ä¸šæµç¨‹çš„å·¥ä½œæµè®¡åˆ’ã€‚
    ä¸“é—¨ä¸ºä»£è°¢ç»„å­¦åˆ†æè®¾è®¡ï¼Œä¸¥æ ¼éµå¾ª SOP è§„åˆ™ï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆå‰ç«¯ UI æ ¼å¼ã€‚
    """
    
    def __init__(
        self,
        tool_retriever: ToolRetriever,
        llm_client: LLMClient
    ):
        """
        åˆå§‹åŒ– SOP è§„åˆ’å™¨
        
        Args:
            tool_retriever: å·¥å…·æ£€ç´¢å™¨å®ä¾‹
            llm_client: LLM å®¢æˆ·ç«¯å®ä¾‹
        """
        self.tool_retriever = tool_retriever
        self.llm_client = llm_client
    
    async def generate_plan(
        self,
        user_query: str,
        file_metadata: Dict[str, Any],
        category_filter: str = "Metabolomics"
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆåŸºäº SOP è§„åˆ™çš„å·¥ä½œæµè®¡åˆ’
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®
            category_filter: å·¥å…·ç±»åˆ«è¿‡æ»¤å™¨ï¼ˆé»˜è®¤ "Metabolomics"ï¼‰
        
        Returns:
            ç¬¦åˆå‰ç«¯æ ¼å¼çš„å·¥ä½œæµé…ç½®å­—å…¸
        """
        try:
            logger.info(f"ğŸ§  [SOPPlanner] å¼€å§‹ç”Ÿæˆè®¡åˆ’: '{user_query}'")
            
            # ğŸ”¥ CRITICAL: ä½¿ç”¨ LLM ç”Ÿæˆå·¥ä½œæµè®¡åˆ’ï¼ˆä¿æŒæ™ºèƒ½æ€§ï¼‰
            # å¯¹äºæ‰€æœ‰ç±»å‹ï¼ˆåŒ…æ‹¬ä»£è°¢ç»„å­¦ï¼‰ï¼Œéƒ½ä½¿ç”¨ LLM è§„åˆ’
            # Step 1: æ£€ç´¢ç›¸å…³å·¥å…·
            logger.info("ğŸ” [SOPPlanner] Step 1: æ£€ç´¢ç›¸å…³å·¥å…·...")
            retrieved_tools = self.tool_retriever.retrieve(
                query=user_query,
                top_k=15,  # è·å–æ›´å¤šå·¥å…·ä»¥æ”¯æŒ SOP è§„åˆ™
                category_filter=category_filter
            )
            
            if not retrieved_tools:
                logger.warning("âš ï¸ [SOPPlanner] æœªæ£€ç´¢åˆ°ç›¸å…³å·¥å…·")
                return {
                    "type": "error",
                    "error": "æœªæ‰¾åˆ°ç›¸å…³å·¥å…·ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢æˆ–å·¥å…·æ³¨å†Œ",
                    "message": "æ— æ³•ç”Ÿæˆå·¥ä½œæµè®¡åˆ’"
                }
            
            logger.info(f"âœ… [SOPPlanner] æ£€ç´¢åˆ° {len(retrieved_tools)} ä¸ªç›¸å…³å·¥å…·")
            
            # Step 2: æ„å»º SOP é©±åŠ¨çš„ç³»ç»Ÿæç¤ºè¯
            logger.info("ğŸ“ [SOPPlanner] Step 2: æ„å»º SOP æç¤ºè¯...")
            system_prompt = self._build_sop_system_prompt()
            user_prompt = self._build_sop_user_prompt(
                user_query=user_query,
                file_metadata=file_metadata,
                retrieved_tools=retrieved_tools
            )
            
            # Step 3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’
            logger.info("ğŸ¤– [SOPPlanner] Step 3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’...")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            response = await self.llm_client.achat(
                messages=messages,
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿éµå¾ª SOP è§„åˆ™
                max_tokens=2048
            )
            
            # Step 4: è§£æ LLM å“åº”
            logger.info("ğŸ”§ [SOPPlanner] Step 4: è§£æ LLM å“åº”...")
            workflow_plan = self._parse_llm_response(response)
            
            # Step 5: éªŒè¯å’Œé€‚é…ä¸ºå‰ç«¯æ ¼å¼
            logger.info("âœ… [SOPPlanner] Step 5: éªŒè¯å’Œé€‚é…...")
            validated_plan = self._validate_and_adapt_sop_plan(
                workflow_plan,
                file_metadata,
                retrieved_tools
            )
            
            logger.info(f"âœ… [SOPPlanner] å·¥ä½œæµè§„åˆ’å®Œæˆ: {len(validated_plan.get('steps', []))} ä¸ªæ­¥éª¤")
            return validated_plan
        
        except Exception as e:
            logger.error(f"âŒ [SOPPlanner] å·¥ä½œæµè§„åˆ’å¤±è´¥: {e}", exc_info=True)
            return {
                "type": "error",
                "error": str(e),
                "message": f"å·¥ä½œæµè§„åˆ’å¤±è´¥: {str(e)}"
            }
    
    def _detect_group_column_heuristic(self, file_metadata: Dict[str, Any]) -> Optional[str]:
        """
        å¯å‘å¼æ£€æµ‹åˆ†ç»„åˆ—
        
        ğŸ”¥ CRITICAL FIX: å³ä½¿åˆ—æ˜¯æ•°å€¼å‹ï¼ˆint/floatï¼‰ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œä¹Ÿå½“ä½œåˆ†ç±»å˜é‡
        
        Args:
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®
        
        Returns:
            æ£€æµ‹åˆ°çš„åˆ†ç»„åˆ—åï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
        """
        # ä¼˜å…ˆçº§å…³é”®è¯åˆ—è¡¨
        priority_keywords = ['Diet', 'diet', 'Group', 'group', 'Condition', 'condition', 
                            'Treatment', 'treatment', 'Class', 'class', 'Category', 'category',
                            'Type', 'type', 'Label', 'label', 'Status', 'status']
        
        # æ–¹æ³•1: æ£€æŸ¥ metadata_columnsï¼ˆFileInspector å¯èƒ½å·²ç»æ£€æµ‹åˆ°ï¼‰
        metadata_cols = file_metadata.get("metadata_columns", [])
        if metadata_cols:
            # ä¼˜å…ˆæ£€æŸ¥å…³é”®è¯åŒ¹é…
            for col in metadata_cols:
                if any(keyword in col for keyword in priority_keywords):
                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰: {col}")
                    return col
            # å¦‚æœæ²¡æœ‰å…³é”®è¯åŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå…ƒæ•°æ®åˆ—
            logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆmetadata_columnsï¼‰: {metadata_cols[0]}")
            return metadata_cols[0]
        
        # æ–¹æ³•2: æ£€æŸ¥ potential_groups
        potential_groups = file_metadata.get("potential_groups", {})
        if isinstance(potential_groups, dict) and len(potential_groups) > 0:
            first_group_col = list(potential_groups.keys())[0]
            logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆpotential_groupsï¼‰: {first_group_col}")
            return first_group_col
        
        # æ–¹æ³•3: ğŸ”¥ CRITICAL FIX - æ£€æŸ¥æ•°å€¼åˆ—ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œå½“ä½œåˆ†ç±»å˜é‡
        columns = file_metadata.get("columns", [])
        head_data = file_metadata.get("head", {})
        
        if columns and head_data:
            try:
                import pandas as pd
                head_json = head_data.get("json", [])
                if isinstance(head_json, list) and len(head_json) > 0:
                    df_preview = pd.DataFrame(head_json)
                    
                    # é¦–å…ˆæ£€æŸ¥å…³é”®è¯åŒ¹é…çš„åˆ—
                    for col in columns:
                        if col in df_preview.columns:
                            if any(keyword in col for keyword in priority_keywords):
                                # æ£€æŸ¥å”¯ä¸€å€¼æ•°é‡
                                unique_count = df_preview[col].nunique()
                                if 2 <= unique_count <= 5:  # ğŸ”¥ å…³é”®ï¼šå³ä½¿æ•°å€¼å‹ï¼Œå”¯ä¸€å€¼ <= 5 ä¹Ÿå½“ä½œåˆ†ç±»
                                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆæ•°å€¼å‹å…³é”®è¯åŒ¹é…ï¼‰: {col} (å”¯ä¸€å€¼: {unique_count})")
                                    return col
                    
                    # ç„¶åæ£€æŸ¥æ‰€æœ‰åˆ—ï¼ˆåŒ…æ‹¬æ•°å€¼åˆ—ï¼‰
                    for col in columns:
                        if col in df_preview.columns:
                            unique_count = df_preview[col].nunique()
                            # ğŸ”¥ CRITICAL: å³ä½¿åˆ—æ˜¯æ•°å€¼å‹ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œå½“ä½œåˆ†ç±»å˜é‡
                            if 2 <= unique_count <= 5:
                                # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«åˆ†ç»„å…³é”®è¯
                                if any(keyword in col for keyword in priority_keywords):
                                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆæ•°å€¼å‹ï¼Œå”¯ä¸€å€¼ <= 5ï¼‰: {col} (å”¯ä¸€å€¼: {unique_count})")
                                    return col
                                # æˆ–è€…å¦‚æœå”¯ä¸€å€¼æ­£å¥½æ˜¯ 2ï¼ˆå…¸å‹çš„äºŒå…ƒåˆ†ç»„ï¼‰
                                elif unique_count == 2:
                                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆäºŒå…ƒæ•°å€¼å‹ï¼‰: {col} (å”¯ä¸€å€¼: {unique_count})")
                                    return col
            except Exception as e:
                logger.warning(f"âš ï¸ [Heuristic] æ•°å€¼åˆ—æ£€æµ‹å¤±è´¥: {e}")
        
        # æ–¹æ³•4: æ£€æŸ¥æ‰€æœ‰åˆ—åï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        if columns:
            for col in columns:
                if any(keyword in col for keyword in priority_keywords):
                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆåˆ—åå…³é”®è¯åŒ¹é…ï¼‰: {col}")
                    return col
        
        logger.info("âš ï¸ [Heuristic] æœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—")
        return None
    
    def _generate_metabolomics_plan(self, file_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆç¡®å®šæ€§çš„ä»£è°¢ç»„å­¦ SOP æµç¨‹
        
        ä¸¥æ ¼æŒ‰ç…§çº¿æ€§æµç¨‹å›¾é€»è¾‘ï¼š
        1. inspect_data (Always)
        2. preprocess_data (Always)
        3. pca_analysis (Always)
        4. Decision: å¦‚æœæœ‰åˆ†ç»„åˆ—
           - metabolomics_plsda
           - differential_analysis
           - visualize_volcano
           - metabolomics_pathway_enrichment
        5. å¦‚æœæ²¡æœ‰åˆ†ç»„åˆ—ï¼šåœæ­¢
        
        Args:
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®
        
        Returns:
            ç¬¦åˆå‰ç«¯æ ¼å¼çš„å·¥ä½œæµé…ç½®å­—å…¸
        """
        logger.info("ğŸ“‹ [SOPPlanner] ç”Ÿæˆç¡®å®šæ€§ä»£è°¢ç»„å­¦ SOP æµç¨‹")
        
        # è·å–æ–‡ä»¶è·¯å¾„
        file_path = file_metadata.get("file_path", "")
        if not file_path:
            raise ValueError("æ–‡ä»¶å…ƒæ•°æ®ä¸­ç¼ºå°‘ file_path")
        
        # æ£€æµ‹åˆ†ç»„åˆ—
        group_column = self._detect_group_column_heuristic(file_metadata)
        has_groups = group_column is not None
        
        logger.info(f"ğŸ” [SOPPlanner] åˆ†ç»„æ£€æµ‹ç»“æœ: {group_column if has_groups else 'æ— åˆ†ç»„åˆ—'}")
        
        # æ„å»ºæ­¥éª¤åˆ—è¡¨
        steps = []
        
        # Step 1: Data Inspection (Always)
        steps.append({
            "id": "inspect_data",
            "name": "æ•°æ®æ£€æŸ¥",
            "description": "SOPè§„åˆ™ï¼šå¿…é¡»é¦–å…ˆè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°ï¼Œæ£€æŸ¥ç¼ºå¤±å€¼ã€æ•°æ®èŒƒå›´ç­‰",
            "selected": True,
            "params": {
                "file_path": file_path
            }
        })
        
        # Step 2: Preprocessing (Always)
        steps.append({
            "id": "preprocess_data",
            "name": "æ•°æ®é¢„å¤„ç†",
            "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒLog2è½¬æ¢å’Œæ ‡å‡†åŒ–ï¼Œç¼ºå¤±å€¼å¤„ç†",
            "selected": True,
            "params": {
                "file_path": file_path,  # å°†è‡ªåŠ¨æ›´æ–°ä¸ºé¢„å¤„ç†åçš„æ–‡ä»¶
                "log_transform": True,
                "standardize": True,
                "missing_imputation": "min"
            }
        })
        
        # Step 3: Unsupervised Analysis - PCA (Always)
        steps.append({
            "id": "pca_analysis",
            "name": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒPCAåˆ†æä»¥æ¢ç´¢æ•°æ®ç»“æ„å’Œé™ç»´",
            "selected": True,
            "params": {
                "file_path": "<preprocess_data_output>",  # å°†è‡ªåŠ¨æ›´æ–°
                "n_components": 2,
                "scale": True
            }
        })
        
        # Decision Node: Check for Group Column
        if has_groups:
            logger.info(f"âœ… [SOPPlanner] æ£€æµ‹åˆ°åˆ†ç»„åˆ— '{group_column}'ï¼Œæ·»åŠ ç›‘ç£åˆ†ææ­¥éª¤")
            
            # Step 4: Supervised Analysis - PLS-DA
            steps.append({
                "id": "metabolomics_plsda",
                "name": "PLS-DA åˆ†æ",
                "description": f"SOPè§„åˆ™ï¼šæ£€æµ‹åˆ°åˆ†ç»„åˆ— '{group_column}'ï¼Œå¿…é¡»è¿›è¡Œç›‘ç£åˆ†æï¼ˆPLS-DAï¼‰ä»¥è¯†åˆ«ç»„é—´å·®å¼‚",
                "selected": True,
                "params": {
                    "file_path": "<preprocess_data_output>",
                    "group_column": group_column,
                    "n_components": 2
                }
            })
            
            # Step 5: Differential Analysis
            steps.append({
                "id": "differential_analysis",
                "name": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",
                "description": f"SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡Œå·®å¼‚åˆ†æä»¥è¯†åˆ«æ˜¾è‘—å·®å¼‚çš„ä»£è°¢ç‰©ï¼ˆåˆ†ç»„åˆ—: {group_column}ï¼‰",
                "selected": True,
                "params": {
                    "file_path": "<preprocess_data_output>",
                    "group_column": group_column,
                    "method": "t-test",
                    "p_value_threshold": 0.05,
                    "fold_change_threshold": 1.5
                }
            })
            
            # Step 6: Visualization - Volcano Plot
            # æ³¨æ„ï¼švisualize_volcano éœ€è¦ diff_results å­—å…¸ï¼ŒåŒ…å« results åˆ—è¡¨
            steps.append({
                "id": "visualize_volcano",
                "name": "ç«å±±å›¾å¯è§†åŒ–",
                "description": "SOPè§„åˆ™ï¼šå¿…é¡»å¯è§†åŒ–å·®å¼‚åˆ†æç»“æœï¼Œå±•ç¤ºæ˜¾è‘—å·®å¼‚ä»£è°¢ç‰©",
                "selected": True,
                "params": {
                    "diff_results": "<differential_analysis_output>",  # å°†è‡ªåŠ¨ä»ä¸Šä¸€ä¸ªæ­¥éª¤è·å–
                    "output_path": None,  # å°†è‡ªåŠ¨ç”Ÿæˆ
                    "fdr_threshold": 0.05,
                    "log2fc_threshold": 1.0
                }
            })
            
            # Step 7: Functional Analysis - Pathway Enrichment
            # æ³¨æ„ï¼šmetabolomics_pathway_enrichment éœ€è¦ file_path, group_column, case_group, control_group
            # éœ€è¦ä»æ•°æ®ä¸­è‡ªåŠ¨æ£€æµ‹åˆ†ç»„å€¼
            potential_groups = file_metadata.get("potential_groups", {})
            case_group = None
            control_group = None
            
            # æ–¹æ³•1: ä» potential_groups å­—å…¸ä¸­æå–
            if isinstance(potential_groups, dict) and group_column in potential_groups:
                group_values = potential_groups[group_column]
                if isinstance(group_values, list) and len(group_values) >= 2:
                    # ä½¿ç”¨å‰ä¸¤ä¸ªåˆ†ç»„å€¼
                    case_group = group_values[0]
                    control_group = group_values[1]
                    logger.info(f"âœ… [SOPPlanner] ä» potential_groups æ£€æµ‹åˆ°åˆ†ç»„å€¼: {case_group} vs {control_group}")
            
            # æ–¹æ³•2: å¦‚æœ potential_groups æ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆæ—§æ ¼å¼å…¼å®¹ï¼‰
            if (not case_group or not control_group) and isinstance(potential_groups, dict):
                # å°è¯•ä»å…¶ä»–é”®ä¸­æŸ¥æ‰¾
                for key, values in potential_groups.items():
                    if isinstance(values, list) and len(values) >= 2:
                        case_group = values[0]
                        control_group = values[1]
                        logger.info(f"âœ… [SOPPlanner] ä»å…¶ä»–åˆ†ç»„åˆ—æ£€æµ‹åˆ°åˆ†ç»„å€¼: {case_group} vs {control_group}")
                        break
            
            # æ–¹æ³•3: å¦‚æœä»ç„¶æ²¡æœ‰æ£€æµ‹åˆ°ï¼Œå°è¯•ä» head æ•°æ®ä¸­æ¨æ–­
            if not case_group or not control_group:
                head_data = file_metadata.get("head", {})
                if isinstance(head_data, dict):
                    head_json = head_data.get("json", [])
                    if isinstance(head_json, list) and len(head_json) > 0:
                        # å°è¯•ä»ç¬¬ä¸€è¡Œæ•°æ®ä¸­è·å–åˆ†ç»„åˆ—çš„å€¼
                        first_row = head_json[0] if isinstance(head_json[0], dict) else {}
                        if group_column in first_row:
                            # éœ€è¦è¯»å–æ›´å¤šæ•°æ®æ¥è·å–å”¯ä¸€å€¼ï¼Œè¿™é‡Œå…ˆä½¿ç”¨å ä½ç¬¦
                            # å®é™…æ‰§è¡Œæ—¶ differential_analysis ä¼šè‡ªåŠ¨æ£€æµ‹
                            logger.info(f"âš ï¸ [SOPPlanner] æ— æ³•ä»å…ƒæ•°æ®ä¸­ç¡®å®šåˆ†ç»„å€¼ï¼Œå°†åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹")
            
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°åˆ†ç»„å€¼ï¼Œè®¾ç½®ä¸º Noneï¼ˆè®©å·¥å…·è‡ªåŠ¨æ£€æµ‹ï¼‰
            # æ³¨æ„ï¼šdifferential_analysis ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œä½† pathway_enrichment éœ€è¦æ˜ç¡®çš„å€¼
            # æ‰€ä»¥æˆ‘ä»¬ä» differential_analysis çš„ç»“æœä¸­æå–ï¼Œæˆ–è€…è®©ç”¨æˆ·æŒ‡å®š
            if not case_group or not control_group:
                # ä½¿ç”¨å ä½ç¬¦ï¼ŒExecutionLayer ä¼šå°è¯•ä»å‰ä¸€æ­¥éª¤çš„ç»“æœä¸­æå–
                # æˆ–è€…å·¥å…·ä¼šåœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹
                case_group = None  # è®©å·¥å…·è‡ªåŠ¨æ£€æµ‹
                control_group = None  # è®©å·¥å…·è‡ªåŠ¨æ£€æµ‹
                logger.warning(f"âš ï¸ [SOPPlanner] æœªæ£€æµ‹åˆ°åˆ†ç»„å€¼ï¼Œå°†åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹æˆ–ä» differential_analysis ç»“æœä¸­æå–")
            
            steps.append({
                "id": "metabolomics_pathway_enrichment",
                "name": "é€šè·¯å¯Œé›†åˆ†æ",
                "description": f"SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡Œé€šè·¯å¯Œé›†åˆ†æä»¥ç†è§£å·®å¼‚ä»£è°¢ç‰©çš„ç”Ÿç‰©å­¦æ„ä¹‰ï¼ˆåˆ†ç»„åˆ—: {group_column}ï¼‰",
                "selected": True,
                "params": {
                    "file_path": "<preprocess_data_output>",
                    "group_column": group_column,
                    "case_group": case_group if case_group else "<differential_analysis_case_group>",  # å ä½ç¬¦ï¼ŒExecutionLayer ä¼šå¤„ç†
                    "control_group": control_group if control_group else "<differential_analysis_control_group>",  # å ä½ç¬¦
                    "organism": "hsa",  # é»˜è®¤äººç±»ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
                    "p_value_threshold": 0.05
                }
            })
        else:
            logger.info("âš ï¸ [SOPPlanner] æœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼Œä»…æ‰§è¡Œæ— ç›‘ç£åˆ†æï¼ˆPCAï¼‰")
            # æ·»åŠ è­¦å‘Šæè¿°åˆ°æœ€åä¸€ä¸ªæ­¥éª¤
            steps[-1]["description"] += " âš ï¸ æ³¨æ„ï¼šæœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼Œæ— æ³•è¿›è¡Œç›‘ç£åˆ†æå’Œå·®å¼‚åˆ†æã€‚"
        
        # æ„å»ºå·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        workflow_name = "ä»£è°¢ç»„å­¦æ ‡å‡†åˆ†ææµç¨‹" + ("ï¼ˆå«åˆ†ç»„åˆ†æï¼‰" if has_groups else "ï¼ˆæ— ç›‘ç£åˆ†æï¼‰")
        
        # é€‚é…æ­¥éª¤æ ¼å¼ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        adapted_steps = []
        for i, step in enumerate(steps, 1):
            tool_id = step["id"]
            
            # éªŒè¯å·¥å…·æ˜¯å¦åœ¨æ³¨å†Œè¡¨ä¸­
            tool_metadata_obj = registry.get_metadata(tool_id)
            if not tool_metadata_obj:
                logger.warning(f"âš ï¸ [SOPPlanner] å·¥å…· '{tool_id}' ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œè·³è¿‡")
                continue
            
            # è·å–æ˜¾ç¤ºåç§°
            step_display_name = self._get_step_display_name(tool_id, tool_metadata_obj.description)
            
            # æ„å»ºé€‚é…åçš„æ­¥éª¤
            adapted_step = {
                "step_id": tool_id,
                "id": tool_id,
                "tool_id": tool_id,
                "name": step.get("name", step_display_name),
                "step_name": step.get("name", step_display_name),
                "description": step.get("description", tool_metadata_obj.description[:100]),
                "desc": step.get("description", tool_metadata_obj.description[:100])[:100],
                "selected": step.get("selected", True),
                "params": step.get("params", {})
            }
            
            adapted_steps.append(adapted_step)
        
        # æ„å»ºæœ€ç»ˆçš„å·¥ä½œæµé…ç½®
        workflow_config = {
            "type": "workflow_config",
            "workflow_data": {
                "workflow_name": workflow_name,
                "name": workflow_name,
                "steps": adapted_steps
            },
            "file_paths": [file_path] if file_path else []
        }
        
        logger.info(f"âœ… [SOPPlanner] ç¡®å®šæ€§æµç¨‹ç”Ÿæˆå®Œæˆ: {len(adapted_steps)} ä¸ªæ­¥éª¤")
        logger.info(f"   æ­¥éª¤åˆ—è¡¨: {[s['id'] for s in adapted_steps]}")
        
        return workflow_config
    
    def _build_sop_system_prompt(self) -> str:
        """
        æ„å»º SOP é©±åŠ¨çš„ç³»ç»Ÿæç¤ºè¯
        
        Returns:
            åŒ…å« SOP è§„åˆ™çš„ç³»ç»Ÿæç¤ºè¯
        """
        return """You are an expert Bioinformatics Pipeline Architect specializing in Metabolomics data analysis.

Your task is to generate executable workflow plans that STRICTLY follow the Metabolomics Standard Operating Procedure (SOP).

**CRITICAL SOP RULES (MUST FOLLOW):**

1. **Data Quality Assessment (MANDATORY FIRST STEP):**
   - IF missing_values > 50% â†’ MUST use a dropping/imputation tool.
   - IF missing_values > 0% AND missing_values <= 50% â†’ MUST use an imputation tool.
   - ALWAYS perform data inspection first (inspect_data).

2. **Data Preprocessing (ALWAYS REQUIRED):**
   - ALWAYS perform Normalization (Log2 transformation + Scaling).
   - Use preprocess_data tool for this step.
   - Parameters should be auto-filled based on file metadata.

3. **Analysis Type Selection (CONDITIONAL):**
   - IF group_columns exist (metadata_columns detected OR numeric columns with â‰¤5 unique values like 0/1) â†’ MUST perform:
     a. Unsupervised Analysis: PCA (pca_analysis) - ALWAYS perform PCA first
     b. Supervised Analysis: PLS-DA (metabolomics_plsda) - MUST add if groups detected
     c. Differential Analysis (differential_analysis) - MUST add if groups detected
     d. Pathway Enrichment (metabolomics_pathway_enrichment) - MUST be added if differential analysis is planned
   - IF NO group_columns â†’ Perform Unsupervised Analysis only:
     a. PCA (pca_analysis)

4. **Visualization Rules (CRITICAL - MUST FOLLOW):**
   - ğŸ”¥ ANTI-REDUNDANCY RULE: The tool `pca_analysis` ALREADY generates a plot. If you select `pca_analysis`, you MUST NOT select `visualize_pca`. They are mutually exclusive. Adding both will cause errors.
   - ğŸ”¥ DATA FLOW RULE: If you perform `differential_analysis`, you MUST follow it with `visualize_volcano` to plot the results.
   - ğŸ”¥ GROUP DETECTION: If a column (like 'Diet') has few unique values (e.g., 0 and 1, or 2-5 unique values), treat it as a Grouping Column. In this case, you MUST add `metabolomics_plsda` and `metabolomics_pathway_enrichment`.

**OUTPUT CONTRACT (CRITICAL - MUST MATCH EXACTLY):**

You MUST output a JSON object that matches the existing Frontend UI structure EXACTLY:

```json
{
  "name": "Generated Pipeline Name",
  "steps": [
    {
      "id": "tool_name_from_registry",
      "name": "Human Readable Step Name",
      "description": "Why this step is needed (based on SOP rules)",
      "selected": true,
      "params": {
        "file_path": "<auto-filled from metadata>",
        "param1": "value1",
        "param2": "value2"
      }
    }
  ]
}
```

**Step Structure Requirements:**
- "id": MUST be the exact tool name from the retrieved tools (e.g., "pca_analysis", "differential_analysis", "metabolomics_plsda")
- "name": Human-readable name in Chinese (e.g., "ä¸»æˆåˆ†åˆ†æ", "PLS-DA åˆ†æ")
- "description": Brief explanation of why this step is needed, referencing SOP rules
- "selected": Always true (all steps are selected by default)
- "params": Dictionary matching the tool's args_schema, with file_path auto-filled from metadata

**Parameter Auto-Filling Rules:**
- file_path: Use the file_path from file_metadata
- group_column: ğŸ”¥ CRITICAL - MUST be one of the strings in file_metadata['semantic_map']['group_cols']. If semantic_map['group_cols'] is empty, DO NOT add supervised steps (PLS-DA, Differential Analysis).
- n_components: Default to 2 for PCA/PLS-DA
- scale: Default to true for normalization
- Other parameters: Use sensible defaults from the tool's args_schema

**ğŸ”¥ SEMANTIC MAPPING CONSTRAINT (CRITICAL):**
- The group_column parameter MUST be selected from semantic_map['group_cols'] list.
- If semantic_map['group_cols'] is empty, SKIP all supervised analysis steps (metabolomics_plsda, differential_analysis, visualize_volcano, metabolomics_pathway_enrichment).
- Do NOT hallucinate or guess group column names. Only use what is explicitly provided in semantic_map['group_cols'].

**Example Output:**
{
  "name": "ä»£è°¢ç»„å­¦æ ‡å‡†åˆ†ææµç¨‹",
  "steps": [
    {
      "id": "inspect_data",
      "name": "æ•°æ®æ£€æŸ¥",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»é¦–å…ˆè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°ï¼Œæ£€æŸ¥ç¼ºå¤±å€¼ã€æ•°æ®èŒƒå›´ç­‰",
      "selected": true,
      "params": {
        "file_path": "/app/uploads/data.csv"
      }
    },
    {
      "id": "preprocess_data",
      "name": "æ•°æ®é¢„å¤„ç†",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒLog2è½¬æ¢å’Œæ ‡å‡†åŒ–ï¼Œç¼ºå¤±å€¼ç‡5%éœ€è¦æ’è¡¥",
      "selected": true,
      "params": {
        "file_path": "/app/uploads/data.csv",
        "log_transform": true,
        "standardize": true,
        "missing_imputation": "min"
      }
    }
  ]
}

Generate ONLY the JSON object, no markdown code blocks, no additional text."""
    
    def _build_sop_user_prompt(
        self,
        user_query: str,
        file_metadata: Dict[str, Any],
        retrieved_tools: List[Dict[str, Any]]
    ) -> str:
        """
        æ„å»º SOP é©±åŠ¨çš„ç”¨æˆ·æç¤ºè¯
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®
            retrieved_tools: æ£€ç´¢åˆ°çš„å·¥å…·åˆ—è¡¨
        
        Returns:
            ç”¨æˆ·æç¤ºè¯æ–‡æœ¬
        """
        # æ ¼å¼åŒ–æ–‡ä»¶å…ƒæ•°æ®
        metadata_text = self._format_file_metadata(file_metadata)
        
        # æ ¼å¼åŒ–å·¥å…·ä¿¡æ¯
        tools_text = []
        for i, tool in enumerate(retrieved_tools, 1):
            tool_info = f"""
Tool {i}: {tool['name']}
  Description: {tool['description']}
  Category: {tool['category']}
  Parameters Schema:
{json.dumps(tool['args_schema'], indent=2, ensure_ascii=False)}
"""
            tools_text.append(tool_info)
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Extract semantic_map for fast constraint
        semantic_map = file_metadata.get("semantic_map", {})
        group_cols = semantic_map.get("group_cols", [])
        
        # Fail-Fast: If no group columns, skip supervised steps
        has_groups = len(group_cols) > 0
        
        prompt = f"""**User Query:**
{user_query}

**File Metadata:**
{metadata_text}

**Available Tools:**
{''.join(tools_text)}

**Task (SIMPLIFIED - NO BIOINFORMATICS REASONING):**
Map the user's intent to the available group_cols. If user says 'analyze cachexia', and group_cols contains 'Muscle loss', pick 'Muscle loss'.

**ğŸ”¥ SEMANTIC MAPPING CONSTRAINT (CRITICAL):**
- semantic_map['group_cols'] = {group_cols}
- The group_column parameter in your JSON MUST be one of these strings: {group_cols if group_cols else '[]'}
- If group_cols is empty ([]), SKIP all supervised steps (metabolomics_plsda, differential_analysis, visualize_volcano, metabolomics_pathway_enrichment)
- Do NOT hallucinate or guess group column names. Only use what is explicitly provided.

**Workflow Decision:**
- Has groups: {has_groups}
- If has_groups=True: Add PCA + PLS-DA + Differential + Volcano + Pathway
- If has_groups=False: Add PCA only (unsupervised)

**CRITICAL: Visualization Rules (MUST FOLLOW STRICTLY)**
- ğŸ”¥ ANTI-REDUNDANCY RULE: The tool `pca_analysis` ALREADY generates a plot. If you select `pca_analysis`, you MUST NOT select `visualize_pca`. They are mutually exclusive.
- ğŸ”¥ DATA FLOW RULE: If you perform `differential_analysis`, you MUST follow it with `visualize_volcano` to plot the results.

**Output:**
Return ONLY a valid JSON object matching the structure:
{{
  "name": "Pipeline Name",
  "steps": [
    {{
      "id": "tool_name",
      "name": "Step Name",
      "description": "Why needed",
      "selected": true,
      "params": {{"file_path": "...", ...}}
    }}
  ]
}}

Remember: Output ONLY the JSON object, no markdown, no code blocks, no explanations."""
        
        return prompt
    
    def _format_file_metadata(self, file_metadata: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æ–‡ä»¶å…ƒæ•°æ®ä¸ºå¯è¯»æ–‡æœ¬
        
        Args:
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®å­—å…¸
        
        Returns:
            æ ¼å¼åŒ–çš„å…ƒæ•°æ®æ–‡æœ¬
        """
        if not file_metadata or file_metadata.get("status") != "success":
            return "File metadata not available or invalid."
        
        shape = file_metadata.get("shape", {})
        missing_rate = file_metadata.get("missing_rate", 0)
        metadata_cols = file_metadata.get("metadata_columns", [])
        feature_cols = file_metadata.get("feature_columns", [])
        file_path = file_metadata.get("file_path", "N/A")
        
        text = f"""File Path: {file_path}
Shape: {shape.get('rows', 'N/A')} rows Ã— {shape.get('cols', 'N/A')} columns
Missing Rate: {missing_rate}%
Metadata Columns: {', '.join(metadata_cols) if metadata_cols else 'None'}
Feature Columns (first 10): {', '.join(feature_cols[:10]) if feature_cols else 'None'}
Total Features: {file_metadata.get('total_feature_columns', 'N/A')}
"""
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Use semantic_map for clear constraints
        semantic_map = file_metadata.get("semantic_map", {})
        group_cols = semantic_map.get("group_cols", [])
        id_col = semantic_map.get("id_col", "N/A")
        feature_count = semantic_map.get("feature_count", "N/A")
        
        text += f"""
**Semantic Map (CRITICAL - USE THIS FOR group_column):**
- ID Column: {id_col}
- Group Columns: {group_cols if group_cols else '[] (NO GROUPS - SKIP SUPERVISED STEPS)'}
- Feature Count: {feature_count}

ğŸ”¥ CONSTRAINT: The group_column parameter MUST be one of: {group_cols if group_cols else '[]'}
ğŸ”¥ FAIL-FAST: If group_cols is empty, DO NOT add supervised steps (PLS-DA, Differential, Volcano, Pathway).
"""
        
        # ä¿ç•™æ—§æ ¼å¼ä»¥å…¼å®¹
        potential_groups = file_metadata.get("potential_groups", {})
        if isinstance(potential_groups, dict) and len(potential_groups) > 0:
            text += "\n**Grouping Columns (Legacy Format - Use semantic_map instead):**\n"
            for col_name, col_info in potential_groups.items():
                if isinstance(col_info, dict):
                    n_unique = col_info.get("n_unique", "?")
                    values = col_info.get("values", [])
                    values_str = ", ".join([str(v) for v in values[:10]])
                    text += f"  - {col_name}: {n_unique} unique values ({values_str})\n"
        
        # æ·»åŠ æ‰€æœ‰åˆ—åï¼ˆç”¨äºæ£€æµ‹æ•°å€¼å‹åˆ†ç»„åˆ—ï¼‰
        all_columns = file_metadata.get("columns", [])
        if all_columns:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å€¼å‹åˆ—å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯
            head_data = file_metadata.get("head", {})
            if head_data and isinstance(head_data, dict):
                head_json = head_data.get("json", [])
                if isinstance(head_json, list) and len(head_json) > 0:
                    try:
                        import pandas as pd
                        df_preview = pd.DataFrame(head_json)
                        text += "\n**Column Analysis (for Group Detection):**\n"
                        priority_keywords = ['Diet', 'diet', 'Group', 'group', 'Condition', 'condition']
                        for col in all_columns:
                            if col in df_preview.columns:
                                unique_count = df_preview[col].nunique()
                                # å¦‚æœåˆ—ååŒ…å«åˆ†ç»„å…³é”®è¯ä¸”å”¯ä¸€å€¼ <= 5ï¼Œæ ‡è®°ä¸ºæ½œåœ¨åˆ†ç»„åˆ—
                                if any(kw in col for kw in priority_keywords) and 2 <= unique_count <= 5:
                                    unique_values = sorted(df_preview[col].unique().tolist())
                                    text += f"  - {col}: {unique_count} unique values {unique_values} âš ï¸ POTENTIAL GROUPING COLUMN\n"
                    except Exception as e:
                        logger.debug(f"æ— æ³•åˆ†æåˆ—çš„å”¯ä¸€å€¼: {e}")
        
        # æ·»åŠ æ•°æ®èŒƒå›´ä¿¡æ¯
        data_range = file_metadata.get("data_range", {})
        if data_range:
            text += f"\nData Range: min={data_range.get('min', 'N/A')}, max={data_range.get('max', 'N/A')}\n"
        
        return text
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æ LLM å“åº”ï¼ˆå¤ç”¨ WorkflowPlanner çš„é€»è¾‘ï¼‰
        
        Args:
            response: LLM è¿”å›çš„æ–‡æœ¬
        
        Returns:
            è§£æåçš„å·¥ä½œæµè®¡åˆ’å­—å…¸
        """
        # å°è¯•æå– JSONï¼ˆå¯èƒ½è¢« markdown ä»£ç å—åŒ…è£¹ï¼‰
        response = response.strip()
        
        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        if response.startswith("```json"):
            response = response[7:]
        elif response.startswith("```"):
            response = response[3:]
        
        if response.endswith("```"):
            response = response[:-3]
        
        response = response.strip()
        
        try:
            plan = json.loads(response)
            return plan
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [SOPPlanner] JSON è§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response[:500]}")
            
            # å°è¯•æå– JSON å¯¹è±¡ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
            import re
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                try:
                    plan = json.loads(json_match.group())
                    logger.info("âœ… [SOPPlanner] ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆåŠŸæå– JSON")
                    return plan
                except:
                    pass
            
            raise ValueError(f"æ— æ³•è§£æ LLM å“åº”ä¸º JSON: {e}")
    
    def _validate_and_adapt_sop_plan(
        self,
        workflow_plan: Dict[str, Any],
        file_metadata: Dict[str, Any],
        retrieved_tools: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        éªŒè¯ SOP è®¡åˆ’å¹¶é€‚é…ä¸ºå‰ç«¯æ ¼å¼
        
        Args:
            workflow_plan: LLM ç”Ÿæˆçš„åŸå§‹è®¡åˆ’
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®
            retrieved_tools: æ£€ç´¢åˆ°çš„å·¥å…·åˆ—è¡¨
        
        Returns:
            éªŒè¯å’Œé€‚é…åçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        """
        # éªŒè¯åŸºæœ¬ç»“æ„
        if "name" not in workflow_plan:
            workflow_plan["name"] = "ä»£è°¢ç»„å­¦åˆ†ææµç¨‹"
        
        if "steps" not in workflow_plan or not isinstance(workflow_plan["steps"], list):
            raise ValueError("å·¥ä½œæµè®¡åˆ’å¿…é¡»åŒ…å« 'steps' æ•°ç»„")
        
        # è·å–æ–‡ä»¶è·¯å¾„
        file_path = file_metadata.get("file_path", "") if file_metadata else ""
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Fail-Fast Logic
        # æ£€æŸ¥ semantic_mapï¼Œå¦‚æœ group_cols ä¸ºç©ºï¼Œç§»é™¤æ‰€æœ‰ç›‘ç£æ­¥éª¤
        semantic_map = file_metadata.get("semantic_map", {}) if file_metadata else {}
        group_cols = semantic_map.get("group_cols", [])
        has_groups = len(group_cols) > 0
        
        if not has_groups:
            logger.warning("âš ï¸ [SOPPlanner] Fail-Fast: group_cols ä¸ºç©ºï¼Œå°†ç§»é™¤æ‰€æœ‰ç›‘ç£æ­¥éª¤")
        
        # åˆ›å»ºå·¥å…·åç§°åˆ°å·¥å…·ä¿¡æ¯çš„æ˜ å°„
        tool_map = {tool['name']: tool for tool in retrieved_tools}
        
        # ğŸ”¥ CRITICAL FIX: ç¡¬åˆ é™¤æ‰€æœ‰ visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰
        # ä¸å†éœ€è¦æ£€æŸ¥ï¼Œç›´æ¥ç§»é™¤æ‰€æœ‰ visualize_pca
        original_steps = workflow_plan.get("steps", [])
        workflow_plan["steps"] = [
            step for step in original_steps
            if step.get("id") != "visualize_pca" and 
               step.get("tool_id") != "visualize_pca" and 
               step.get("tool_name") != "visualize_pca"
        ]
        removed_count = len(original_steps) - len(workflow_plan["steps"])
        if removed_count > 0:
            logger.info(f"âœ… [SOPPlanner] ç¡¬åˆ é™¤ {removed_count} ä¸ª visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰")
        
        # ğŸ”¥ Fail-Fast: å¦‚æœ group_cols ä¸ºç©ºï¼Œç§»é™¤æ‰€æœ‰ç›‘ç£æ­¥éª¤
        if not has_groups:
            supervised_tools = ["metabolomics_plsda", "differential_analysis", "visualize_volcano", "metabolomics_pathway_enrichment"]
            before_failfast = len(workflow_plan["steps"])
            workflow_plan["steps"] = [
                step for step in workflow_plan["steps"]
                if step.get("id") not in supervised_tools and 
                   step.get("tool_id") not in supervised_tools and 
                   step.get("tool_name") not in supervised_tools
            ]
            removed_supervised = before_failfast - len(workflow_plan["steps"])
            if removed_supervised > 0:
                logger.info(f"âœ… [SOPPlanner] Fail-Fast: ç§»é™¤ {removed_supervised} ä¸ªç›‘ç£æ­¥éª¤ï¼ˆæ— åˆ†ç»„åˆ—ï¼‰")
        
        # é€‚é…æ­¥éª¤æ ¼å¼
        adapted_steps = []
        for i, step in enumerate(workflow_plan["steps"], 1):
            # éªŒè¯å·¥å…·å­˜åœ¨æ€§
            tool_id = step.get("id") or step.get("tool_id") or step.get("tool_name")
            if not tool_id:
                logger.warning(f"âš ï¸ [SOPPlanner] æ­¥éª¤ {i} ç¼ºå°‘ tool_idï¼Œè·³è¿‡")
                continue
            
            # ğŸ”¥ CRITICAL FIX: ç¡¬åˆ é™¤ visualize_pcaï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰
            if tool_id == "visualize_pca":
                logger.warning(f"âš ï¸ [SOPPlanner] ç¡¬åˆ é™¤ visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰")
                continue
            
            # æ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨æ³¨å†Œè¡¨ä¸­
            tool_metadata_obj = registry.get_metadata(tool_id)
            if not tool_metadata_obj:
                logger.warning(f"âš ï¸ [SOPPlanner] å·¥å…· '{tool_id}' ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œè·³è¿‡")
                continue
            
            # è·å–å·¥å…·ä¿¡æ¯
            tool_info = tool_map.get(tool_id, {})
            tool_desc = tool_metadata_obj.description
            
            # å¤„ç†å‚æ•°
            params = step.get("params", {})
            adapted_params = {}
            
            # è‡ªåŠ¨å¡«å…… file_path
            if "file_path" not in params and file_path:
                adapted_params["file_path"] = file_path
            elif "file_path" in params:
                adapted_params["file_path"] = params["file_path"]
            
            # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Use semantic_map for group_column
            # è‡ªåŠ¨å¡«å…… group_columnï¼ˆä¼˜å…ˆä½¿ç”¨ semantic_map['group_cols']ï¼‰
            if "group_column" not in params and file_metadata:
                semantic_map = file_metadata.get("semantic_map", {})
                group_cols = semantic_map.get("group_cols", [])
                if group_cols:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†ç»„åˆ—
                    adapted_params["group_column"] = group_cols[0]
                    logger.info(f"âœ… [SOPPlanner] ä» semantic_map è‡ªåŠ¨å¡«å…… group_column: {group_cols[0]}")
                else:
                    # å›é€€åˆ°æ—§é€»è¾‘ï¼ˆå…¼å®¹æ€§ï¼‰
                    metadata_cols = file_metadata.get("metadata_columns", [])
                    if metadata_cols:
                        adapted_params["group_column"] = metadata_cols[0]
            
            # ğŸ”¥ CRITICAL: éªŒè¯ group_column æ˜¯å¦åœ¨ semantic_map['group_cols'] ä¸­
            if "group_column" in adapted_params and file_metadata:
                semantic_map = file_metadata.get("semantic_map", {})
                group_cols = semantic_map.get("group_cols", [])
                planned_group_col = adapted_params.get("group_column")
                if group_cols and planned_group_col not in group_cols:
                    logger.warning(f"âš ï¸ [SOPPlanner] group_column '{planned_group_col}' ä¸åœ¨ semantic_map['group_cols'] ä¸­ï¼Œè‡ªåŠ¨æ›¿æ¢ä¸º: {group_cols[0]}")
                    adapted_params["group_column"] = group_cols[0]
            
            # å¤åˆ¶å…¶ä»–å‚æ•°
            for param_name, param_value in params.items():
                if param_name not in ["file_path", "group_column"]:  # é¿å…é‡å¤
                    adapted_params[param_name] = param_value
            
            # æ„å»ºé€‚é…åçš„æ­¥éª¤ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
            # ğŸ”¥ Frontend Contract: å¿…é¡»åŒ…å« step_id, tool_id, name, step_name, desc
            step_display_name = step.get("name", self._get_step_display_name(tool_id, tool_desc))
            step_description = step.get("description", tool_desc[:100] if tool_desc else "")
            
            adapted_step = {
                "step_id": tool_id,  # ğŸ”¥ Frontend requires step_id (not just id)
                "id": tool_id,  # ä¿ç•™ id ä½œä¸ºå…¼å®¹å­—æ®µ
                "tool_id": tool_id,  # Frontend requires tool_id
                "name": step_display_name,  # Frontend requires name
                "step_name": step_display_name,  # Frontend requires step_name (compatibility)
                "description": step_description,  # å®Œæ•´æè¿°
                "desc": step_description[:100] if len(step_description) > 100 else step_description,  # Frontend requires desc (truncated)
                "selected": step.get("selected", True),  # Frontend may use this
                "params": adapted_params
            }
            
            adapted_steps.append(adapted_step)
        
        # æ„å»ºæœ€ç»ˆçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        workflow_config = {
            "type": "workflow_config",
            "workflow_data": {
                "workflow_name": workflow_plan.get("name", "ä»£è°¢ç»„å­¦åˆ†ææµç¨‹"),
                "name": workflow_plan.get("name", "ä»£è°¢ç»„å­¦åˆ†ææµç¨‹"),  # å…¼å®¹å­—æ®µ
                "steps": adapted_steps
            },
            "file_paths": [file_path] if file_path else []
        }
        
        return workflow_config
    
    def _get_step_display_name(self, tool_name: str, tool_desc: str) -> str:
        """
        è·å–æ­¥éª¤çš„æ˜¾ç¤ºåç§°
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_desc: å·¥å…·æè¿°
        
        Returns:
            æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰
        """
        # åç§°æ˜ å°„ï¼ˆåŒ¹é…æ–°çš„å·¥å…· IDï¼‰
        name_mapping = {
            "inspect_data": "æ•°æ®æ£€æŸ¥",
            "preprocess_data": "æ•°æ®é¢„å¤„ç†",
            "pca_analysis": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "visualize_pca": "PCA å¯è§†åŒ–",
            "differential_analysis": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",
            "visualize_volcano": "ç«å±±å›¾",
            "metabolomics_plsda": "PLS-DA åˆ†æ",
            "metabolomics_pathway_enrichment": "é€šè·¯å¯Œé›†åˆ†æ",
            "metabolomics_heatmap": "çƒ­å›¾"
        }
        
        if tool_name in name_mapping:
            return name_mapping[tool_name]
        
        # é»˜è®¤ï¼šä½¿ç”¨å·¥å…·åç§°ï¼ˆç¾åŒ–ï¼‰
        return tool_name.replace("_", " ").title()


class RNAPlanner(SOPPlanner):
    """
    scRNA-seq ç‰¹å®šçš„ SOP è§„åˆ’å™¨
    
    ç»§æ‰¿è‡ª SOPPlannerï¼Œä½†ä½¿ç”¨ scRNA-seq ç‰¹å®šçš„ SOP è§„åˆ™ã€‚
    """
    
    def _build_sop_system_prompt(self) -> str:
        """
        æ„å»º scRNA-seq ç‰¹å®šçš„ SOP ç³»ç»Ÿæç¤ºè¯
        
        Returns:
            åŒ…å« scRNA-seq SOP è§„åˆ™çš„ç³»ç»Ÿæç¤ºè¯
        """
        return """You are an expert Bioinformatics Pipeline Architect specializing in Single-Cell RNA-seq (scRNA-seq) data analysis.

Your task is to generate executable workflow plans that STRICTLY follow the scRNA-seq Standard Operating Procedure (SOP).

**CRITICAL SOP RULES (MUST FOLLOW):**

1. **Input Type Detection (MANDATORY FIRST STEP):**
   - IF input is FASTQ files (.fastq, .fq) â†’ MUST start with CellRanger (rna_cellranger_count) - This runs ASYNCHRONOUSLY
   - IF input is Matrix/H5AD (.h5ad, .mtx, 10x directory) â†’ Start directly with QC (rna_qc_filter)

2. **Quality Control (ALWAYS REQUIRED AFTER INPUT PROCESSING):**
   - ALWAYS perform QC filtering (rna_qc_filter)
   - Filter cells based on: min_genes (default 200), max_mt (default 20%)
   - ALWAYS perform Doublet Detection (rna_doublet_detection) after QC
   - Generate QC visualization (rna_visualize_qc)

3. **Preprocessing Pipeline (MANDATORY SEQUENCE):**
   - ALWAYS perform Normalization (rna_normalize) - LogNormalize with target_sum=1e4
   - ALWAYS find Highly Variable Genes (rna_hvg) - default n_top_genes=2000
   - ALWAYS scale data (rna_scale) - for PCA preparation

4. **Dimensionality Reduction (REQUIRED):**
   - ALWAYS perform PCA (rna_pca) - default n_comps=50
   - ALWAYS compute Neighbors (rna_neighbors) - default n_neighbors=15
   - ALWAYS perform UMAP (rna_umap) - for visualization
   - OPTIONAL: t-SNE (rna_tsne) - alternative visualization

5. **Clustering (REQUIRED):**
   - ALWAYS perform Leiden Clustering (rna_clustering) - default resolution=0.5
   - Generate clustering visualization (rna_visualize_clustering)

6. **Marker Detection & Annotation (REQUIRED):**
   - ALWAYS find Marker Genes (rna_find_markers) - for each cluster
   - ALWAYS perform Cell Type Annotation (rna_cell_annotation) - using markers
   - Generate marker visualization (rna_visualize_markers)

7. **Export (FINAL STEP):**
   - ALWAYS export results (rna_export_results) - H5AD, CSVs, figures ZIP

**OUTPUT CONTRACT (CRITICAL - MUST MATCH EXACTLY):**

You MUST output a JSON object that matches the existing Frontend UI structure EXACTLY:

```json
{
  "name": "Generated Pipeline Name",
  "steps": [
    {
      "id": "tool_name_from_registry",
      "name": "Human Readable Step Name",
      "description": "Why this step is needed (based on SOP rules)",
      "selected": true,
      "params": {
        "adata_path": "<auto-filled from metadata>",
        "param1": "value1",
        "param2": "value2"
      }
    }
  ]
}
```

**Step Structure Requirements:**
- "id": MUST be the exact tool name from the retrieved tools (e.g., "rna_qc_filter", "rna_normalize")
- "name": Human-readable name in Chinese (e.g., "è´¨é‡æ§åˆ¶", "æ•°æ®æ ‡å‡†åŒ–")
- "description": Brief explanation of why this step is needed, referencing SOP rules
- "selected": Always true (all steps are selected by default)
- "params": Dictionary matching the tool's args_schema

**Parameter Auto-Filling Rules:**
- adata_path: Use the file_path from file_metadata (or output from previous step)
- For CellRanger: fastqs_path, transcriptome_path, output_dir should be auto-filled
- min_genes: Default to 200
- max_mt: Default to 20.0
- n_top_genes: Default to 2000
- resolution: Default to 0.5 for clustering
- Other parameters: Use sensible defaults from the tool's args_schema

**Example Output:**
{
  "name": "å•ç»†èƒè½¬å½•ç»„æ ‡å‡†åˆ†ææµç¨‹",
  "steps": [
    {
      "id": "rna_qc_filter",
      "name": "è´¨é‡æ§åˆ¶è¿‡æ»¤",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»é¦–å…ˆè¿›è¡Œè´¨é‡æ§åˆ¶ï¼Œè¿‡æ»¤ä½è´¨é‡ç»†èƒå’Œçº¿ç²’ä½“åŸºå› ",
      "selected": true,
      "params": {
        "adata_path": "/app/uploads/data.h5ad",
        "min_genes": 200,
        "max_mt": 20.0
      }
    },
    {
      "id": "rna_normalize",
      "name": "æ•°æ®æ ‡å‡†åŒ–",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒLogNormalizeæ ‡å‡†åŒ–ï¼Œä¸ºåç»­åˆ†æåšå‡†å¤‡",
      "selected": true,
      "params": {
        "adata_path": "<step1_output>",
        "target_sum": 10000
      }
    }
  ]
}

Generate ONLY the JSON object, no markdown code blocks, no additional text."""
    
    def _get_step_display_name(self, tool_name: str, tool_desc: str) -> str:
        """
        è·å–æ­¥éª¤çš„æ˜¾ç¤ºåç§°ï¼ˆscRNA-seq ç‰¹å®šï¼‰
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_desc: å·¥å…·æè¿°
        
        Returns:
            æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰
        """
        # scRNA-seq ç‰¹å®šçš„åç§°æ˜ å°„
        name_mapping = {
            "rna_cellranger_count": "Cell Ranger è®¡æ•°ï¼ˆå¼‚æ­¥ï¼‰",
            "rna_convert_cellranger_to_h5ad": "è½¬æ¢ä¸º H5AD æ ¼å¼",
            "rna_qc_filter": "è´¨é‡æ§åˆ¶è¿‡æ»¤",
            "rna_doublet_detection": "åŒè”ä½“æ£€æµ‹",
            "rna_normalize": "æ•°æ®æ ‡å‡†åŒ–",
            "rna_hvg": "é«˜å˜åŸºå› ç­›é€‰",
            "rna_scale": "æ•°æ®ç¼©æ”¾",
            "rna_pca": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "rna_neighbors": "æ„å»ºé‚»æ¥å›¾",
            "rna_umap": "UMAP é™ç»´",
            "rna_tsne": "t-SNE é™ç»´",
            "rna_clustering": "Leiden èšç±»",
            "rna_find_markers": "Marker åŸºå› æ£€æµ‹",
            "rna_cell_annotation": "ç»†èƒç±»å‹æ³¨é‡Š",
            "rna_visualize_qc": "QC å¯è§†åŒ–",
            "rna_visualize_clustering": "èšç±»å¯è§†åŒ–",
            "rna_visualize_markers": "Marker å¯è§†åŒ–",
            "rna_export_results": "ç»“æœå¯¼å‡º"
        }
        
        if tool_name in name_mapping:
            return name_mapping[tool_name]
        
        # é»˜è®¤ï¼šä½¿ç”¨å·¥å…·åç§°ï¼ˆç¾åŒ–ï¼‰
        return tool_name.replace("_", " ").title()
