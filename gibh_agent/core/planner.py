"""
åŠ¨æ€å·¥ä½œæµè§„åˆ’å™¨ - The Brain

ä½¿ç”¨ Tool-RAG æ¶æ„åŠ¨æ€ç”Ÿæˆå¯æ‰§è¡Œçš„å·¥ä½œæµè®¡åˆ’ã€‚
ç»“åˆå·¥å…·æ£€ç´¢å’Œ LLM æ¨ç†ï¼Œç”Ÿæˆç¬¦åˆå‰ç«¯æ ¼å¼çš„å·¥ä½œæµé…ç½®ã€‚

åŒ…å«ä¸¤ä¸ªè§„åˆ’å™¨ï¼š
1. WorkflowPlanner: é€šç”¨å·¥ä½œæµè§„åˆ’å™¨
2. SOPPlanner: åŸºäº SOPï¼ˆæ ‡å‡†æ“ä½œç¨‹åºï¼‰çš„é¢†åŸŸç‰¹å®šè§„åˆ’å™¨ï¼ˆå·²å‡çº§ä¸ºä½¿ç”¨ WorkflowRegistryï¼‰
"""
import json
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path

from .tool_retriever import ToolRetriever
from .tool_registry import registry
from .llm_client import LLMClient
from .workflows import WorkflowRegistry

logger = logging.getLogger(__name__)


class WorkflowPlanner:
    """
    å·¥ä½œæµè§„åˆ’å™¨
    
    èŒè´£ï¼š
    1. ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³å·¥å…·
    2. ä½¿ç”¨ LLM ç”Ÿæˆå·¥ä½œæµè®¡åˆ’
    3. éªŒè¯å’Œè½¬æ¢è¾“å‡ºæ ¼å¼
    """
    
    def __init__(
        self,
        tool_retriever: ToolRetriever,
        llm_client: LLMClient
    ):
        """
        åˆå§‹åŒ–å·¥ä½œæµè§„åˆ’å™¨
        
        Args:
            tool_retriever: å·¥å…·æ£€ç´¢å™¨å®ä¾‹
            llm_client: LLM å®¢æˆ·ç«¯å®ä¾‹
        """
        self.tool_retriever = tool_retriever
        self.llm_client = llm_client
    
    async def plan(
        self,
        user_query: str,
        context_files: List[str] = None,
        category_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµè®¡åˆ’
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            context_files: å¯ç”¨çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            category_filter: å¯é€‰çš„ç±»åˆ«è¿‡æ»¤å™¨ï¼ˆå¦‚ "Metabolomics"ï¼‰
        
        Returns:
            å·¥ä½œæµé…ç½®å­—å…¸ï¼Œç¬¦åˆå‰ç«¯æ ¼å¼
        """
        try:
            logger.info(f"ğŸ§  å¼€å§‹è§„åˆ’å·¥ä½œæµ: '{user_query}'")
            
            # Step 1: æ£€ç´¢ç›¸å…³å·¥å…·
            logger.info("ğŸ” Step 1: æ£€ç´¢ç›¸å…³å·¥å…·...")
            retrieved_tools = self.tool_retriever.retrieve(
                query=user_query,
                top_k=10,
                category_filter=category_filter
            )
            
            if not retrieved_tools:
                logger.warning("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³å·¥å…·")
                return {
                    "type": "error",
                    "error": "æœªæ‰¾åˆ°ç›¸å…³å·¥å…·ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢æˆ–å·¥å…·æ³¨å†Œ",
                    "message": "æ— æ³•ç”Ÿæˆå·¥ä½œæµè®¡åˆ’"
                }
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(retrieved_tools)} ä¸ªç›¸å…³å·¥å…·")
            for tool in retrieved_tools[:3]:  # åªæ‰“å°å‰3ä¸ª
                logger.info(f"   - {tool['name']} (ç›¸ä¼¼åº¦: {tool['similarity_score']:.4f})")
            
            # Step 2: æ„å»º LLM Prompt
            logger.info("ğŸ“ Step 2: æ„å»º LLM Prompt...")
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(
                user_query=user_query,
                retrieved_tools=retrieved_tools,
                context_files=context_files or []
            )
            
            # Step 3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’
            logger.info("ğŸ¤– Step 3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’...")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # å°è¯•ä½¿ç”¨ JSON modeï¼ˆå¦‚æœæ”¯æŒï¼‰
            response = await self.llm_client.achat(
                messages=messages,
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=2048
            )
            
            # Step 4: è§£æ LLM å“åº”
            logger.info("ğŸ”§ Step 4: è§£æ LLM å“åº”...")
            # ğŸ”¥ FIX: æå– ChatCompletion å¯¹è±¡çš„å†…å®¹
            if hasattr(response, 'choices') and response.choices:
                response_text = response.choices[0].message.content or ""
            else:
                response_text = str(response)
            workflow_plan = self._parse_llm_response(response_text)
            
            # Step 5: éªŒè¯å·¥å…·å­˜åœ¨æ€§
            logger.info("âœ… Step 5: éªŒè¯å·¥å…·...")
            validated_plan = self._validate_and_adapt(workflow_plan, context_files or [])
            
            logger.info(f"âœ… å·¥ä½œæµè§„åˆ’å®Œæˆ: {len(validated_plan.get('steps', []))} ä¸ªæ­¥éª¤")
            return validated_plan
        
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµè§„åˆ’å¤±è´¥: {e}", exc_info=True)
            return {
                "type": "error",
                "error": str(e),
                "message": f"å·¥ä½œæµè§„åˆ’å¤±è´¥: {str(e)}"
            }
    
    def _build_system_prompt(self) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯
        
        Returns:
            ç³»ç»Ÿæç¤ºè¯æ–‡æœ¬
        """
        return """You are a Senior Bioinformatics Workflow Architect.

Your task is to generate executable workflow plans based on user queries and available tools.

**CRITICAL RULES:**
1. Output MUST be a valid JSON object (no markdown code blocks, no extra text).
2. Structure: {"workflow_name": "...", "steps": [{"id": "step1", "tool_name": "...", "params": {...}, "dependency": "..."}]}
3. **Data Flow**: The output of Step N must match the input of Step N+1 (e.g., file paths).
4. If a parameter is a file path, use placeholders like `<step1_output>` or match the uploaded filename.
5. Use tool names EXACTLY as provided in the tool schemas.
6. Only include parameters that exist in the tool's args_schema.
7. For file paths, prefer using the actual uploaded filename if available.

**Step Structure:**
- "id": Unique step identifier (e.g., "step1", "step2")
- "tool_name": The exact tool name from the retrieved tools
- "params": Dictionary of parameters matching the tool's args_schema
- "dependency": Optional, the step ID this step depends on (e.g., "step1")

**Example Output:**
{
  "workflow_name": "PCA Analysis Pipeline",
  "steps": [
    {
      "id": "step1",
      "tool_name": "metabolomics_pca",
      "params": {
        "file_path": "cow_diet.csv",
        "n_components": 2,
        "scale": true
      },
      "dependency": null
    }
  ]
}

Generate ONLY the JSON object, no additional text."""
    
    def _build_user_prompt(
        self,
        user_query: str,
        retrieved_tools: List[Dict[str, Any]],
        context_files: List[str]
    ) -> str:
        """
        æ„å»ºç”¨æˆ·æç¤ºè¯
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_tools: æ£€ç´¢åˆ°çš„å·¥å…·åˆ—è¡¨
            context_files: å¯ç”¨æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            ç”¨æˆ·æç¤ºè¯æ–‡æœ¬
        """
        # æ ¼å¼åŒ–å·¥å…·ä¿¡æ¯
        tools_text = []
        for i, tool in enumerate(retrieved_tools, 1):
            tool_info = f"""
Tool {i}: {tool['name']}
  Description: {tool['description']}
  Category: {tool['category']}
  Output Type: {tool['output_type']}
  Parameters Schema:
{json.dumps(tool['args_schema'], indent=2, ensure_ascii=False)}
"""
            tools_text.append(tool_info)
        
        # æ ¼å¼åŒ–æ–‡ä»¶ä¿¡æ¯
        files_text = ""
        if context_files:
            files_text = "\n**Available Files:**\n"
            for file_path in context_files:
                filename = Path(file_path).name
                files_text += f"- {filename} (path: {file_path})\n"
        else:
            files_text = "\n**Available Files:** None (user may upload files later)\n"
        
        prompt = f"""**User Query:**
{user_query}

**Retrieved Tools:**
{''.join(tools_text)}

{files_text}

**Task:**
Generate a workflow plan that fulfills the user's request. Use the retrieved tools and available files.

**Instructions:**
1. Select the most relevant tools from the retrieved list.
2. Arrange them in a logical order (data flow: output of step N â†’ input of step N+1).
3. Fill in parameters using:
   - Actual file names from "Available Files" if they match the query
   - Default values from the tool's args_schema
   - Placeholders like `<step1_output>` if a step depends on another step's output
4. Keep the workflow concise - only include necessary steps.

**Output Format:**
Return ONLY a valid JSON object with this structure:
{{
  "workflow_name": "Descriptive workflow name",
  "steps": [
    {{
      "id": "step1",
      "tool_name": "exact_tool_name",
      "params": {{"param1": "value1", "param2": "value2"}},
      "dependency": null
    }},
    {{
      "id": "step2",
      "tool_name": "another_tool_name",
      "params": {{"file_path": "<step1_output>", "other_param": "value"}},
      "dependency": "step1"
    }}
  ]
}}

Remember: Output ONLY the JSON object, no markdown, no code blocks, no explanations."""
        
        return prompt
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æ LLM å“åº”
        
        Args:
            response: LLM è¿”å›çš„æ–‡æœ¬
        
        Returns:
            è§£æåçš„å·¥ä½œæµè®¡åˆ’å­—å…¸
        """
        # å°è¯•æå– JSONï¼ˆå¯èƒ½è¢« markdown ä»£ç å—åŒ…è£¹ï¼‰
        response = response.strip()
        
        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        if response.startswith("```json"):
            response = response[7:]
        elif response.startswith("```"):
            response = response[3:]
        
        if response.endswith("```"):
            response = response[:-3]
        
        response = response.strip()
        
        try:
            plan = json.loads(response)
            return plan
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response[:500]}")
            
            # å°è¯•æå– JSON å¯¹è±¡ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
            import re
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                try:
                    plan = json.loads(json_match.group())
                    logger.info("âœ… ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆåŠŸæå– JSON")
                    return plan
                except:
                    pass
            
            raise ValueError(f"æ— æ³•è§£æ LLM å“åº”ä¸º JSON: {e}")
    
    def _validate_and_adapt(
        self,
        workflow_plan: Dict[str, Any],
        context_files: List[str]
    ) -> Dict[str, Any]:
        """
        éªŒè¯å·¥ä½œæµè®¡åˆ’å¹¶é€‚é…ä¸ºå‰ç«¯æ ¼å¼
        
        Args:
            workflow_plan: LLM ç”Ÿæˆçš„åŸå§‹è®¡åˆ’
            context_files: å¯ç”¨æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            éªŒè¯å’Œé€‚é…åçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        """
        # éªŒè¯åŸºæœ¬ç»“æ„
        if "workflow_name" not in workflow_plan:
            workflow_plan["workflow_name"] = "Generated Workflow"
        
        if "steps" not in workflow_plan or not isinstance(workflow_plan["steps"], list):
            raise ValueError("å·¥ä½œæµè®¡åˆ’å¿…é¡»åŒ…å« 'steps' æ•°ç»„")
        
        # é€‚é…æ­¥éª¤æ ¼å¼
        adapted_steps = []
        for i, step in enumerate(workflow_plan["steps"], 1):
            # éªŒè¯å·¥å…·å­˜åœ¨æ€§
            tool_name = step.get("tool_name") or step.get("tool_id")
            if not tool_name:
                logger.warning(f"âš ï¸ æ­¥éª¤ {i} ç¼ºå°‘ tool_nameï¼Œè·³è¿‡")
                continue
            
            # æ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨æ³¨å†Œè¡¨ä¸­
            tool_metadata = registry.get_metadata(tool_name)
            if not tool_metadata:
                logger.warning(f"âš ï¸ å·¥å…· '{tool_name}' ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œè·³è¿‡")
                continue
            
            # è·å–å·¥å…·æè¿°
            tool_desc = tool_metadata.description
            
            # å¤„ç†æ–‡ä»¶è·¯å¾„å‚æ•°
            params = step.get("params", {})
            adapted_params = {}
            
            for param_name, param_value in params.items():
                # å¦‚æœå‚æ•°å€¼æ˜¯æ–‡ä»¶è·¯å¾„å ä½ç¬¦ï¼Œå°è¯•åŒ¹é…å®é™…æ–‡ä»¶
                if isinstance(param_value, str) and param_value.startswith("<") and param_value.endswith(">"):
                    # å ä½ç¬¦ï¼Œä¿æŒåŸæ ·ï¼ˆæ‰§è¡Œæ—¶ä¼šå¤„ç†ï¼‰
                    adapted_params[param_name] = param_value
                elif param_name == "file_path" and context_files:
                    # å¦‚æœæ˜¯ file_path å‚æ•°ï¼Œå°è¯•åŒ¹é…ä¸Šä¼ çš„æ–‡ä»¶
                    if isinstance(param_value, str):
                        # å°è¯•åŒ¹é…æ–‡ä»¶å
                        matched_file = None
                        for file_path in context_files:
                            if param_value.lower() in Path(file_path).name.lower():
                                matched_file = file_path
                                break
                        
                        if matched_file:
                            adapted_params[param_name] = matched_file
                        else:
                            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºé»˜è®¤å€¼
                            adapted_params[param_name] = context_files[0]
                    else:
                        adapted_params[param_name] = param_value
                else:
                    adapted_params[param_name] = param_value
            
            # æ„å»ºé€‚é…åçš„æ­¥éª¤ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
            # ğŸ”¥ Frontend Contract: å¿…é¡»åŒ…å« step_id, tool_id, name, step_name, desc
            step_display_name = self._get_step_display_name(tool_name, tool_desc)
            step_desc = tool_desc[:100] if tool_desc else ""
            
            adapted_step = {
                "step_id": tool_name,  # ğŸ”¥ Frontend requires step_id (must match tool_id)
                "id": tool_name,  # ä¿ç•™ id ä½œä¸ºå…¼å®¹å­—æ®µ
                "tool_id": tool_name,  # Frontend requires tool_id
                "name": step_display_name,  # Frontend requires name
                "step_name": step_display_name,  # Frontend requires step_name (compatibility)
                "description": tool_desc if tool_desc else "",  # å®Œæ•´æè¿°
                "desc": step_desc,  # Frontend requires desc (truncated to 100 chars)
                "selected": True,  # Frontend may use this
                "params": adapted_params
            }
            
            adapted_steps.append(adapted_step)
        
        # æ„å»ºæœ€ç»ˆçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        workflow_config = {
            "type": "workflow_config",
            "workflow_data": {
                "workflow_name": workflow_plan["workflow_name"],
                "steps": adapted_steps
            },
            "file_paths": context_files
        }
        
        return workflow_config
    
    def _get_step_display_name(self, tool_name: str, tool_desc: str) -> str:
        """
        è·å–æ­¥éª¤çš„æ˜¾ç¤ºåç§°
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_desc: å·¥å…·æè¿°
        
        Returns:
            æ˜¾ç¤ºåç§°
        """
        # ç®€å•çš„åç§°æ˜ å°„
        name_mapping = {
            "metabolomics_pca": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "metabolomics_differential_analysis": "å·®å¼‚åˆ†æ",
            "metabolomics_preprocess": "æ•°æ®é¢„å¤„ç†",
            "file_inspect": "æ–‡ä»¶æ£€æŸ¥"
        }
        
        if tool_name in name_mapping:
            return name_mapping[tool_name]
        
        # ä»æè¿°ä¸­æå–ï¼ˆç®€å•å¯å‘å¼ï¼‰
        if "PCA" in tool_desc or "principal component" in tool_desc.lower():
            return "ä¸»æˆåˆ†åˆ†æ"
        elif "differential" in tool_desc.lower():
            return "å·®å¼‚åˆ†æ"
        elif "preprocess" in tool_desc.lower() or "é¢„å¤„ç†" in tool_desc:
            return "æ•°æ®é¢„å¤„ç†"
        elif "inspect" in tool_desc.lower() or "æ£€æŸ¥" in tool_desc:
            return "æ–‡ä»¶æ£€æŸ¥"
        
        # é»˜è®¤ï¼šä½¿ç”¨å·¥å…·åç§°ï¼ˆç¾åŒ–ï¼‰
        return tool_name.replace("_", " ").title()



class SOPPlanner:
    """
    SOP é©±åŠ¨çš„åŠ¨æ€è§„åˆ’å™¨ï¼ˆå·²å‡çº§ï¼‰
    
    åŸºäºæ ‡å‡†æ“ä½œç¨‹åºï¼ˆSOPï¼‰è§„åˆ™ï¼Œä½¿ç”¨ LLM ç”Ÿæˆç¬¦åˆä¸“ä¸šæµç¨‹çš„å·¥ä½œæµè®¡åˆ’ã€‚
    
    ğŸ”¥ ARCHITECTURAL UPGRADE:
    - ä½¿ç”¨ WorkflowRegistry è¿›è¡Œä¸¥æ ¼çš„åŸŸç»‘å®šï¼ˆåªæ”¯æŒ Metabolomics å’Œ RNAï¼‰
    - ä½¿ç”¨ DAG ä¾èµ–è§£æï¼ˆä»£ç é€»è¾‘ï¼Œé LLM å¹»è§‰ï¼‰
    - æ”¯æŒè®¡åˆ’ä¼˜å…ˆï¼ˆplan-firstï¼‰ï¼šå¯ä»¥åœ¨æ²¡æœ‰æ–‡ä»¶çš„æƒ…å†µä¸‹ç”Ÿæˆå·¥ä½œæµ
    """
    
    def __init__(
        self,
        tool_retriever: ToolRetriever,
        llm_client: LLMClient
    ):
        """
        åˆå§‹åŒ– SOP è§„åˆ’å™¨
        
        Args:
            tool_retriever: å·¥å…·æ£€ç´¢å™¨å®ä¾‹
            llm_client: LLM å®¢æˆ·ç«¯å®ä¾‹
        """
        self.tool_retriever = tool_retriever
        self.llm_client = llm_client
        self.workflow_registry = WorkflowRegistry()
    
    async def generate_plan(
        self,
        user_query: str,
        file_metadata: Optional[Dict[str, Any]] = None,
        category_filter: Optional[str] = None,
        domain_name: Optional[str] = None,
        target_steps: Optional[List[str]] = None,
        is_template: bool = False  # ğŸ”¥ ARCHITECTURAL RESET: Explicit template flag
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆåŸºäº SOP è§„åˆ™çš„å·¥ä½œæµè®¡åˆ’ï¼ˆæ¶æ„é‡ç½®ç‰ˆ - ä¸¥æ ¼åˆ†ç¦»æ‰§è¡Œå’Œé¢„è§ˆï¼‰
        
        ğŸ”¥ ARCHITECTURAL RESET: Strict Separation of Execution and Preview
        - If is_template=False: MUST use _fill_parameters, MUST return template_mode=False
        - If is_template=True: MUST use _fill_placeholders, MUST return template_mode=True
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            category_filter: å·¥å…·ç±»åˆ«è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼‰
            domain_name: å¯é€‰çš„åŸŸåï¼ˆå¦‚æœæä¾›ï¼Œè·³è¿‡æ„å›¾åˆ†ç±»ï¼‰
            target_steps: å¯é€‰çš„ç›®æ ‡æ­¥éª¤åˆ—è¡¨ï¼ˆå¦‚æœæä¾›ï¼Œè·³è¿‡æ„å›¾åˆ†æï¼‰
            is_template: æ˜¯å¦ä¸ºæ¨¡æ¿æ¨¡å¼ï¼ˆTrue=é¢„è§ˆï¼ŒFalse=æ‰§è¡Œï¼‰
        
        Returns:
            ç¬¦åˆå‰ç«¯æ ¼å¼çš„å·¥ä½œæµé…ç½®å­—å…¸
        """
        try:
            logger.info(f"ğŸ§  [SOPPlanner] å¼€å§‹ç”Ÿæˆè®¡åˆ’: '{user_query}'")
            
            # Step 1: Intent Classification (LLM) - è¯†åˆ«åŸŸåå’Œæ¨¡å¼ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            execution_mode = None  # ğŸ”¥ NEW: Track execution mode from intent classification
            
            # ğŸ”¥ CRITICAL FIX: å¦‚æœæœ‰ file_metadataï¼Œé»˜è®¤åº”è¯¥æ˜¯ EXECUTION æ¨¡å¼ï¼ˆé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚é¢„è§ˆï¼‰
            has_file_metadata = file_metadata is not None
            
            if not domain_name:
                logger.info("ğŸ” [SOPPlanner] Step 1: æ„å›¾åˆ†ç±»ï¼ˆè¯†åˆ«åŸŸåå’Œæ¨¡å¼ï¼‰...")
                logger.info(f"ğŸ” [SOPPlanner] file_metadata å­˜åœ¨: {has_file_metadata}")
                intent_result = await self._classify_intent(user_query, file_metadata)
                domain_name = intent_result.get("domain_name")
                execution_mode = intent_result.get("mode", "PLANNING")  # ğŸ”¥ NEW: Extract mode
                
                # ğŸ”¥ CRITICAL FIX: å¦‚æœ file_metadata å­˜åœ¨ä½†æ¨¡å¼æ˜¯ PLANNINGï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯æ˜ç¡®çš„é¢„è§ˆè¯·æ±‚
                if has_file_metadata and execution_mode == "PLANNING":
                    query_lower = user_query.lower()
                    preview_keywords = ["preview", "é¢„è§ˆ", "show", "æ˜¾ç¤º", "æŸ¥çœ‹", "plan", "è§„åˆ’", "what", "ä»€ä¹ˆ"]
                    if not any(kw in query_lower for kw in preview_keywords):
                        # æ²¡æœ‰æ˜ç¡®çš„é¢„è§ˆå…³é”®è¯ï¼Œä½†æœ‰æ–‡ä»¶ -> å¼ºåˆ¶ EXECUTION
                        logger.warning(f"âš ï¸ [SOPPlanner] æœ‰æ–‡ä»¶ä½†æ¨¡å¼æ˜¯ PLANNINGï¼Œä¸”æ— é¢„è§ˆå…³é”®è¯ï¼Œå¼ºåˆ¶è®¾ç½®ä¸º EXECUTION")
                        execution_mode = "EXECUTION"
                
                logger.info(f"âœ… [SOPPlanner] æ„å›¾åˆ†ç±»ç»“æœ: domain={domain_name}, mode={execution_mode}")
            else:
                logger.info(f"âœ… [SOPPlanner] ä½¿ç”¨æä¾›çš„åŸŸå: {domain_name}")
                # ğŸ”¥ CRITICAL: If domain_name provided but no mode, infer from file_metadata
                if has_file_metadata:
                    # Default to EXECUTION if file exists and domain provided
                    execution_mode = "EXECUTION"
                    logger.info(f"âœ… [SOPPlanner] æœ‰ file_metadataï¼Œé»˜è®¤è®¾ç½®ä¸º EXECUTION æ¨¡å¼")
                else:
                    execution_mode = "PLANNING"
                    logger.info(f"âœ… [SOPPlanner] æ—  file_metadataï¼Œè®¾ç½®ä¸º PLANNING æ¨¡å¼")
            
            # Step 2: ä¸¥æ ¼åŸŸç»‘å®šæ£€æŸ¥
            if not self.workflow_registry.is_supported(domain_name):
                logger.warning(f"âš ï¸ [SOPPlanner] ä¸æ”¯æŒçš„åŸŸå: {domain_name}")
                return self.workflow_registry.get_unsupported_error(domain_name)
            
            # Step 3: è·å–å·¥ä½œæµå®ä¾‹
            workflow = self.workflow_registry.get_workflow(domain_name)
            if not workflow:
                return {
                    "type": "error",
                    "error": f"æ— æ³•è·å–å·¥ä½œæµ: {domain_name}",
                    "message": "å·¥ä½œæµæ³¨å†Œè¡¨é”™è¯¯"
                }
            
            # ğŸ”¥ ARCHITECTURAL FIX: ä¼˜å…ˆè¿è¡Œæ„å›¾åˆ†æï¼ˆPlan-Firstï¼‰
            # Step 4: Analyze User Intent (LLM) - ä»å¯ç”¨å·¥å…·é›†ä¸­é€‰æ‹©ç›®æ ‡æ­¥éª¤ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if target_steps is None:
                logger.info("ğŸ” [SOPPlanner] Step 2: åˆ†æç”¨æˆ·æ„å›¾ï¼ˆé€‰æ‹©ç›®æ ‡æ­¥éª¤ï¼‰...")
                target_steps = await self._analyze_user_intent(user_query, workflow)
            else:
                logger.info(f"âœ… [SOPPlanner] ä½¿ç”¨æä¾›çš„ç›®æ ‡æ­¥éª¤: {target_steps}")
            
            # ğŸ”¥ CRITICAL FIX: ç¡®ä¿ target_steps ä¸ä¸ºç©ºï¼ˆPlan-First å¿…é¡»è¿”å›å®Œæ•´æ ‡å‡†æµç¨‹ï¼‰
            # å¦‚æœæŸ¥è¯¢æ¨¡ç³Šï¼ˆå¦‚"Analyze this", "Full analysis", "å®Œæ•´åˆ†æ"ï¼‰ï¼Œä½¿ç”¨å®Œæ•´ SOP
            # å¦åˆ™ï¼Œä½¿ç”¨ç”¨æˆ·æ˜ç¡®è¯·æ±‚çš„æ­¥éª¤ï¼ˆå³ä½¿åªæœ‰ä¸€ä¸ªï¼‰
            # ğŸ”¥ URGENT: å¦‚æœæ²¡æœ‰æ–‡ä»¶ä¸”æŸ¥è¯¢æ˜¯è§„åˆ’ç±»ï¼ˆ"Plan", "é¢„è§ˆ", "show me"ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨å®Œæ•´ SOP
            if not target_steps:
                query_lower = user_query.lower()
                vague_keywords = ["analyze this", "full analysis", "å®Œæ•´åˆ†æ", "å…¨éƒ¨", "all", "complete"]
                planning_keywords = ["plan", "é¢„è§ˆ", "show me", "æ˜¾ç¤º", "ç”Ÿæˆ", "è§„åˆ’", "workflow", "æµç¨‹"]
                
                if any(kw in query_lower for kw in vague_keywords):
                    logger.info("â„¹ï¸ [SOPPlanner] æŸ¥è¯¢æ˜ç¡®è¦æ±‚å®Œæ•´åˆ†æï¼Œä½¿ç”¨å®Œæ•´ SOP")
                    target_steps = list(workflow.steps_dag.keys())
                elif not file_metadata and any(kw in query_lower for kw in planning_keywords):
                    # ğŸ”¥ CRITICAL FIX: Plan-First æ¨¡å¼ï¼ˆæ— æ–‡ä»¶ï¼‰ï¼Œé»˜è®¤è¿”å›å®Œæ•´æ ‡å‡†æµç¨‹
                    logger.info("â„¹ï¸ [SOPPlanner] Plan-First æ¨¡å¼ï¼ˆæ— æ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨å®Œæ•´æ ‡å‡†æµç¨‹")
                    target_steps = list(workflow.steps_dag.keys())
                else:
                    # å¦‚æœæ„å›¾åˆ†æå¤±è´¥ï¼Œå°è¯•å›é€€å…³é”®è¯åŒ¹é…
                    logger.info("â„¹ï¸ [SOPPlanner] æ„å›¾åˆ†ææœªè¿”å›æ­¥éª¤ï¼Œå°è¯•å…³é”®è¯åŒ¹é…...")
                    target_steps = self._fallback_intent_analysis(user_query, list(workflow.steps_dag.keys()))
                    if not target_steps:
                        # ğŸ”¥ CRITICAL FIX: å¦‚æœæ‰€æœ‰å›é€€éƒ½å¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´ SOPï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
                        logger.info("â„¹ï¸ [SOPPlanner] å…³é”®è¯åŒ¹é…ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´ SOPï¼ˆç¡®ä¿ Plan-First è¿”å›å®Œæ•´æµç¨‹ï¼‰")
                        target_steps = list(workflow.steps_dag.keys())
            
            # ğŸ”¥ CRITICAL FIX: å†æ¬¡ç¡®ä¿ target_steps ä¸ä¸ºç©º
            if not target_steps:
                logger.warning("âš ï¸ [SOPPlanner] target_steps ä»ç„¶ä¸ºç©ºï¼Œå¼ºåˆ¶ä½¿ç”¨å®Œæ•´ SOP")
                target_steps = list(workflow.steps_dag.keys())
            
            logger.info(f"âœ… [SOPPlanner] ç›®æ ‡æ­¥éª¤: {target_steps} (å…± {len(target_steps)} ä¸ª)")
            
            # Step 5: Resolve Dependencies (Code) - ä½¿ç”¨ç¡¬ç¼–ç  DAG è§£æä¾èµ–
            logger.info("ğŸ” [SOPPlanner] Step 3: è§£æä¾èµ–å…³ç³»...")
            resolved_steps = self._resolve_dependencies(target_steps, workflow)
            
            # ğŸ”¥ CRITICAL FIX: ç¡®ä¿ resolved_steps ä¸ä¸ºç©º
            if not resolved_steps:
                logger.error(f"âŒ [SOPPlanner] resolve_dependencies è¿”å›ç©ºåˆ—è¡¨ï¼target_steps: {target_steps}")
                logger.warning("âš ï¸ [SOPPlanner] å¼ºåˆ¶ä½¿ç”¨å®Œæ•´ SOP")
                resolved_steps = list(workflow.steps_dag.keys())
            
            logger.info(f"âœ… [SOPPlanner] ä¾èµ–è§£æå®Œæˆ: {target_steps} -> {resolved_steps} (å…± {len(resolved_steps)} ä¸ª)")
            
            # Step 6: Generate Template - ç”Ÿæˆå·¥ä½œæµæ¨¡æ¿ï¼ˆæ”¯æŒå ä½ç¬¦ï¼‰
            logger.info("ğŸ” [SOPPlanner] Step 4: ç”Ÿæˆå·¥ä½œæµæ¨¡æ¿...")
            workflow_config = workflow.generate_template(
                target_steps=resolved_steps,
                file_metadata=file_metadata
            )
            
            # ğŸ”¥ URGENT FIX: éªŒè¯ç”Ÿæˆçš„æ¨¡æ¿åŒ…å«æ­¥éª¤
            steps_count = len(workflow_config.get('workflow_data', {}).get('steps', []))
            if steps_count == 0:
                logger.error(f"âŒ [SOPPlanner] generate_template è¿”å›ç©ºæ­¥éª¤ï¼resolved_steps: {resolved_steps}")
                logger.error(f"âŒ [SOPPlanner] workflow_config: {workflow_config}")
                # å°è¯•ä¿®å¤ï¼šå¦‚æœ resolved_steps ä¸ä¸ºç©ºä½†æ¨¡æ¿ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ generate_template å®ç°æœ‰é—®é¢˜
                if resolved_steps:
                    logger.warning(f"âš ï¸ [SOPPlanner] resolved_steps ä¸ä¸ºç©ºä½†æ¨¡æ¿ä¸ºç©ºï¼Œå°è¯•æ‰‹åŠ¨æ„å»ºæ­¥éª¤...")
                    # è¿™é‡Œä¸åº”è¯¥æ‰‹åŠ¨æ„å»ºï¼Œåº”è¯¥ä¿®å¤ generate_template
                    # ä½†ä¸ºäº†ä¸ç ´åæµç¨‹ï¼Œæˆ‘ä»¬è¿”å›é”™è¯¯
                    return {
                        "type": "error",
                        "error": "å·¥ä½œæµæ¨¡æ¿ç”Ÿæˆå¤±è´¥",
                        "message": f"æ— æ³•ç”Ÿæˆå·¥ä½œæµæ¨¡æ¿ï¼šæ­¥éª¤åˆ—è¡¨ä¸ºç©ºã€‚resolved_steps: {resolved_steps}",
                        "workflow_data": workflow_config.get("workflow_data", {})
                    }
            
            logger.info(f"âœ… [SOPPlanner] æ¨¡æ¿ç”ŸæˆæˆåŠŸ: {steps_count} ä¸ªæ­¥éª¤")
            
            # ğŸ”¥ ARCHITECTURAL RESET: Step 7 - Strict Separation Based on is_template Flag
            logger.info("ğŸ” [SOPPlanner] Step 5: å¤„ç†å…ƒæ•°æ®...")
            logger.info(f"ğŸ” [SOPPlanner] is_template={is_template}, file_metadataå­˜åœ¨={file_metadata is not None}")
            
            # ğŸ”¥ CRITICAL: Remove ambiguity - Use is_template flag explicitly
            if is_template:
                # TEMPLATE MODE: Use placeholders, set template_mode = True
                workflow_config = self._fill_placeholders(workflow_config, user_query)
                logger.info("âœ… [SOPPlanner] TEMPLATE æ¨¡å¼ï¼šå·²ä½¿ç”¨å ä½ç¬¦ï¼Œtemplate_mode = True")
            else:
                # EXECUTION MODE: MUST use _fill_parameters, MUST return template_mode = False
                if not file_metadata:
                    logger.error("âŒ [SOPPlanner] EXECUTION æ¨¡å¼ä½† file_metadata ä¸å­˜åœ¨ï¼è¿™æ˜¯é€»è¾‘é”™è¯¯ã€‚")
                    # Fallback: Use placeholders but log error
                    workflow_config = self._fill_placeholders(workflow_config, user_query)
                    logger.warning("âš ï¸ [SOPPlanner] å›é€€åˆ°å ä½ç¬¦æ¨¡å¼ï¼ˆä½†è¿™æ˜¯é”™è¯¯çš„ï¼‰")
                else:
                    workflow_config = self._fill_parameters(workflow_config, file_metadata, workflow, template_mode=False)
                    logger.info("âœ… [SOPPlanner] EXECUTION æ¨¡å¼ï¼šå·²å¡«å……çœŸå®å‚æ•°ï¼Œtemplate_mode = False")
                    
                    # ğŸ”¥ CRITICAL: Validate that file_path in params is NOT <PENDING_UPLOAD>
                    steps = workflow_config.get("workflow_data", {}).get("steps", [])
                    for step in steps:
                        params = step.get("params", {})
                        for param_name in ["file_path", "adata_path"]:
                            if param_name in params:
                                param_value = params[param_name]
                                if param_value in ["<å¾…ä¸Šä¼ æ•°æ®>", "<PENDING_UPLOAD>", ""]:
                                    logger.error(f"âŒ [SOPPlanner] EXECUTION æ¨¡å¼ä½†æ­¥éª¤ {step.get('id')} çš„å‚æ•° {param_name} ä»æ˜¯å ä½ç¬¦: {param_value}")
                                    # Try to fix: use file_path from metadata
                                    if file_metadata.get("file_path"):
                                        params[param_name] = file_metadata.get("file_path")
                                        logger.warning(f"âš ï¸ [SOPPlanner] å·²ä¿®å¤ï¼šå°† {param_name} è®¾ç½®ä¸º {file_metadata.get('file_path')}")
            
            # ğŸ”¥ ARCHITECTURAL RESET: Final Validation - Enforce is_template flag
            final_steps_count = len(workflow_config.get('workflow_data', {}).get('steps', []))
            template_mode = workflow_config.get("template_mode", False)
            
            # ğŸ”¥ CRITICAL: Force validation based on is_template flag
            if is_template:
                # TEMPLATE MODE: MUST be True
                if not template_mode:
                    logger.warning(f"âš ï¸ [SOPPlanner] is_template=True ä½† template_mode=Falseï¼Œå¼ºåˆ¶è®¾ç½®ä¸º True")
                    template_mode = True
                    workflow_config["template_mode"] = True
                    if "workflow_data" in workflow_config:
                        workflow_config["workflow_data"]["template_mode"] = True
            else:
                # EXECUTION MODE: MUST be False
                if template_mode:
                    logger.error(f"âŒ [SOPPlanner] is_template=False ä½† template_mode=Trueï¼Œè¿™æ˜¯é€»è¾‘é”™è¯¯ï¼å¼ºåˆ¶è®¾ç½®ä¸º False")
                    template_mode = False
                    workflow_config["template_mode"] = False
                    if "workflow_data" in workflow_config:
                        workflow_config["workflow_data"]["template_mode"] = False
                
                # ğŸ”¥ CRITICAL: Validate file paths are NOT placeholders
                if file_metadata:
                    steps = workflow_config.get("workflow_data", {}).get("steps", [])
                    for step in steps:
                        params = step.get("params", {})
                        for param_name in ["file_path", "adata_path"]:
                            if param_name in params:
                                param_value = params[param_name]
                                if param_value in ["<å¾…ä¸Šä¼ æ•°æ®>", "<PENDING_UPLOAD>", ""]:
                                    logger.error(f"âŒ [SOPPlanner] EXECUTION æ¨¡å¼ä½†æ­¥éª¤ {step.get('id')} çš„å‚æ•° {param_name} ä»æ˜¯å ä½ç¬¦")
                                    # Try to fix
                                    if file_metadata.get("file_path"):
                                        params[param_name] = file_metadata.get("file_path")
                                        logger.warning(f"âš ï¸ [SOPPlanner] å·²ä¿®å¤ï¼šå°† {param_name} è®¾ç½®ä¸º {file_metadata.get('file_path')}")
            
            logger.info(f"âœ… [SOPPlanner] å·¥ä½œæµè§„åˆ’å®Œæˆ: {final_steps_count} ä¸ªæ­¥éª¤, template_mode = {template_mode}")
            logger.info(f"âœ… [SOPPlanner] file_metadata å­˜åœ¨: {file_metadata is not None}")
            
            if final_steps_count == 0:
                logger.error(f"âŒ [SOPPlanner] æœ€ç»ˆå·¥ä½œæµé…ç½®æ­¥éª¤ä¸ºç©ºï¼")
                return {
                    "type": "error",
                    "error": "å·¥ä½œæµæ­¥éª¤ä¸ºç©º",
                    "message": "å·¥ä½œæµè§„åˆ’å¤±è´¥ï¼šæœ€ç»ˆæ­¥éª¤åˆ—è¡¨ä¸ºç©ºã€‚",
                    "workflow_data": workflow_config.get("workflow_data", {})
                }
            
            # ğŸ”¥ CRITICAL FIX: æ„å»ºè¿”å›ç»“æœï¼Œæ¸…ç†å†—ä½™å­—æ®µ
            result = {
                "type": "workflow_config",
                "workflow_data": workflow_config.get("workflow_data"),
                "template_mode": template_mode
            }
            
            # ğŸ”¥ CRITICAL: åªåœ¨æ¨¡æ¿æ¨¡å¼æ—¶åŒ…å«è¯Šæ–­æ¶ˆæ¯
            # å¦‚æœæœ‰æ–‡ä»¶ï¼Œä¸åŒ…å«è¯Šæ–­ï¼ˆè®© Orchestrator ä» file_metadata ç”ŸæˆçœŸå®è¯Šæ–­ï¼‰
            if template_mode and "diagnosis" in workflow_config:
                result["diagnosis"] = workflow_config["diagnosis"]
            
            return result
        
        except Exception as e:
            logger.error(f"âŒ [SOPPlanner] å·¥ä½œæµè§„åˆ’å¤±è´¥: {e}", exc_info=True)
            return {
                "type": "error",
                "error": str(e),
                "message": f"å·¥ä½œæµè§„åˆ’å¤±è´¥: {str(e)}"
            }
    
    async def _analyze_user_intent(
        self,
        user_query: str,
        workflow: "BaseWorkflow"
    ) -> List[str]:
        """
        åˆ†æç”¨æˆ·æ„å›¾ï¼Œä»å¯ç”¨å·¥å…·é›†ä¸­é€‰æ‹©ç›®æ ‡æ­¥éª¤
        
        ğŸ”¥ ARCHITECTURAL REFACTOR: Dynamic Intent Filtering
        
        ä½¿ç”¨ LLM ä»å·¥ä½œæµçš„å¯ç”¨æ­¥éª¤ä¸­é€‰æ‹©ç”¨æˆ·æ˜ç¡®è¯·æ±‚çš„æ­¥éª¤ã€‚
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            workflow: å·¥ä½œæµå®ä¾‹ï¼ˆåŒ…å« steps_dagï¼‰
        
        Returns:
            ç›®æ ‡æ­¥éª¤IDåˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š["pca_analysis"]ï¼‰
            å¦‚æœæŸ¥è¯¢æ¨¡ç³Šï¼ˆå¦‚"Analyze this"ï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆè¡¨ç¤ºå®Œæ•´å·¥ä½œæµï¼‰
        """
        # è·å–å¯ç”¨æ­¥éª¤åˆ—è¡¨
        available_steps = list(workflow.steps_dag.keys())
        
        # æ„å»ºæç¤ºè¯
        system_prompt = """You are an Intent Analyzer for Bioinformatics Workflows.

Your task is to select the *Target Steps* the user explicitly asked for from the available toolset.

**Rules:**
1. If the user asks for a specific step (e.g., "Do PCA only", "Just do PCA"), return ONLY that step.
2. If the user asks for multiple specific steps (e.g., "Do PCA and differential analysis"), return those steps.
3. If the user's query is vague (e.g., "Analyze this", "Full analysis", "å®Œæ•´åˆ†æ"), return an empty list [] (which means full workflow).
4. Return ONLY a JSON array of tool_ids, no explanations.

**Output Format:**
Return ONLY a JSON array:
["step1", "step2", ...]  // Empty array [] means full workflow

**Example:**
- User: "Do PCA only." -> ["pca_analysis"]
- User: "I want PCA." -> ["pca_analysis"]
- User: "Just do PCA." -> ["pca_analysis"]
- User: "Do PCA and differential analysis." -> ["pca_analysis", "differential_analysis"]
- User: "Analyze this." -> []
- User: "Full analysis." -> []
- User: "å®Œæ•´åˆ†æ" -> []"""

        user_prompt = f"""**User Query:**
{user_query}

**Available Steps:**
{json.dumps(available_steps, ensure_ascii=False, indent=2)}

**Task:**
Select the target steps the user explicitly asked for. Return ONLY a JSON array of step IDs.

**Output:**
Return ONLY a JSON array (e.g., ["pca_analysis"] or [] for full workflow)."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.llm_client.achat(
            messages=messages,
            temperature=0.1,
            max_tokens=256
        )
        
        # ğŸ”¥ FIX: æå– ChatCompletion å¯¹è±¡çš„å†…å®¹
        if hasattr(response, 'choices') and response.choices:
            response_text = response.choices[0].message.content or ""
        else:
            response_text = str(response)
        
        # è§£æå“åº”
        try:
            target_steps = json.loads(response_text.strip())
            if not isinstance(target_steps, list):
                logger.warning(f"âš ï¸ LLM è¿”å›äº†éåˆ—è¡¨ç±»å‹: {type(target_steps)}ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
                return []
            
            # éªŒè¯æ­¥éª¤æ˜¯å¦å­˜åœ¨äºå·¥ä½œæµä¸­
            valid_steps = [s for s in target_steps if s in available_steps]
            invalid_steps = set(target_steps) - set(available_steps)
            if invalid_steps:
                logger.warning(f"âš ï¸ LLM è¿”å›äº†æ— æ•ˆçš„æ­¥éª¤ID: {invalid_steps}ï¼Œå·²è¿‡æ»¤")
            
            logger.info(f"âœ… [SOPPlanner] æ„å›¾åˆ†æå®Œæˆ: {valid_steps}")
            return valid_steps
        except json.JSONDecodeError as e:
            logger.error(f"âŒ æ„å›¾åˆ†æ JSON è§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response_text[:200] if 'response_text' in locals() else str(response)[:200]}")
            # å›é€€ï¼šå°è¯•ä»æŸ¥è¯¢ä¸­æ¨æ–­
            return self._fallback_intent_analysis(user_query, available_steps)
    
    def _fallback_intent_analysis(self, user_query: str, available_steps: List[str]) -> List[str]:
        """
        å›é€€çš„æ„å›¾åˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            available_steps: å¯ç”¨æ­¥éª¤åˆ—è¡¨
        
        Returns:
            ç›®æ ‡æ­¥éª¤åˆ—è¡¨
        """
        query_lower = user_query.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šæ­¥éª¤çš„å…³é”®è¯
        step_keywords = {
            "pca_analysis": ["pca", "ä¸»æˆåˆ†", "principal component"],
            "differential_analysis": ["differential", "å·®å¼‚", "diff"],
            "metabolomics_plsda": ["plsda", "pls-da", "pls da"],
            "visualize_volcano": ["volcano", "ç«å±±å›¾"],
            "metabolomics_pathway_enrichment": ["pathway", "é€šè·¯", "enrichment", "å¯Œé›†"],
            "preprocess_data": ["preprocess", "é¢„å¤„ç†"],
            "inspect_data": ["inspect", "æ£€æŸ¥"]
        }
        
        matched_steps = []
        for step_id, keywords in step_keywords.items():
            if step_id in available_steps:
                if any(kw in query_lower for kw in keywords):
                    matched_steps.append(step_id)
        
        # å¦‚æœåŒ¹é…åˆ°æ­¥éª¤ï¼Œè¿”å›åŒ¹é…çš„æ­¥éª¤ï¼›å¦åˆ™è¿”å›ç©ºåˆ—è¡¨ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰
        return matched_steps if matched_steps else []
    
    def _resolve_dependencies(
        self,
        target_steps: List[str],
        workflow: "BaseWorkflow"
    ) -> List[str]:
        """
        è§£æä¾èµ–å…³ç³»ï¼ˆä½¿ç”¨ç¡¬ç¼–ç  DAGï¼‰
        
        ğŸ”¥ ARCHITECTURAL REFACTOR: Hardcoded DAG Logic
        
        ä½¿ç”¨å·¥ä½œæµçš„ DAG é€’å½’è§£æä¾èµ–ï¼Œç¡®ä¿æ‰€æœ‰å‰ç½®æ­¥éª¤éƒ½è¢«åŒ…å«ã€‚
        
        Args:
            target_steps: ç”¨æˆ·è¯·æ±‚çš„ç›®æ ‡æ­¥éª¤åˆ—è¡¨
            workflow: å·¥ä½œæµå®ä¾‹ï¼ˆåŒ…å« steps_dagï¼‰
        
        Returns:
            å®Œæ•´çš„æ­¥éª¤åˆ—è¡¨ï¼ˆæŒ‰ä¾èµ–é¡ºåºæ’åºï¼‰
        """
        # ä½¿ç”¨ BaseWorkflow çš„ resolve_dependencies æ–¹æ³•
        resolved_steps = workflow.resolve_dependencies(target_steps)
        return resolved_steps
    
    def _fill_placeholders(
        self,
        workflow_config: Dict[str, Any],
        user_query: str
    ) -> Dict[str, Any]:
        """
        å¡«å……å ä½ç¬¦ï¼ˆå½“æ²¡æœ‰æ–‡ä»¶æ—¶ï¼‰
        
        ğŸ”¥ ARCHITECTURAL FIX: Plan-First Interactive Workflow
        
        å¦‚æœæ²¡æœ‰æ–‡ä»¶å…ƒæ•°æ®ï¼Œä½¿ç”¨å ä½ç¬¦å¡«å……å‚æ•°ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„è¯Šæ–­å†…å®¹ã€‚
        
        Args:
            workflow_config: å·¥ä½œæµé…ç½®
            user_query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆç”¨äºç”Ÿæˆè¯Šæ–­ä¿¡æ¯ï¼‰
        
        Returns:
            å¡«å……å ä½ç¬¦åçš„å·¥ä½œæµé…ç½®
        """
        steps = workflow_config.get("workflow_data", {}).get("steps", [])
        
        # ğŸ”¥ URGENT FIX: ç¡®ä¿ steps ä¸ä¸ºç©º
        if not steps or len(steps) == 0:
            logger.error(f"âŒ [SOPPlanner] _fill_placeholders: steps ä¸ºç©ºï¼workflow_config: {workflow_config}")
            # å°è¯•ä» workflow_data çš„å…¶ä»–ä½ç½®è·å–
            workflow_data = workflow_config.get("workflow_data", {})
            if isinstance(workflow_data, dict):
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å­—æ®µåŒ…å«æ­¥éª¤ä¿¡æ¯
                for key in ["steps", "workflow_steps", "pipeline_steps"]:
                    if key in workflow_data and workflow_data[key]:
                        steps = workflow_data[key]
                        logger.warning(f"âš ï¸ [SOPPlanner] ä» {key} è·å–æ­¥éª¤: {len(steps)} ä¸ª")
                        break
            
            # å¦‚æœä»ç„¶ä¸ºç©ºï¼Œè¿”å›é”™è¯¯
            if not steps or len(steps) == 0:
                logger.error(f"âŒ [SOPPlanner] _fill_placeholders: æ— æ³•è·å–æ­¥éª¤ï¼Œè¿”å›é”™è¯¯é…ç½®")
                return {
                    "type": "error",
                    "error": "å·¥ä½œæµæ­¥éª¤ä¸ºç©º",
                    "message": "æ— æ³•ç”Ÿæˆå·¥ä½œæµï¼šæ­¥éª¤åˆ—è¡¨ä¸ºç©ºã€‚è¯·æ£€æŸ¥å·¥ä½œæµé…ç½®ã€‚",
                    "workflow_data": workflow_config.get("workflow_data", {})
                }
        
        logger.info(f"âœ… [SOPPlanner] _fill_placeholders: å¤„ç† {len(steps)} ä¸ªæ­¥éª¤")
        
        # ğŸ”¥ ä½¿ç”¨ä¸­æ–‡å ä½ç¬¦
        placeholder_text = "<å¾…ä¸Šä¼ æ•°æ®>"
        
        for step in steps:
            params = step.get("params", {})
            
            # å¡«å…… file_path å ä½ç¬¦ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰
            if "file_path" in params or "adata_path" in params:
                param_name = "adata_path" if "adata_path" in params else "file_path"
                params[param_name] = placeholder_text
            
            # å¡«å…… group_column å ä½ç¬¦ï¼ˆå¦‚æœæ­¥éª¤éœ€è¦ï¼‰
            step_id = step.get("id") or step.get("tool_id")
            if step_id in ["metabolomics_plsda", "differential_analysis", "metabolomics_pathway_enrichment"]:
                if "group_column" in params:
                    params["group_column"] = "è‡ªåŠ¨æ£€æµ‹"
        
        # ğŸ”¥ ARCHITECTURAL FIX: ç”Ÿæˆç»“æ„åŒ–çš„è¯Šæ–­å†…å®¹ï¼ˆè€Œä¸æ˜¯é”™è¯¯ï¼‰
        # æ„å»ºæ­¥éª¤åˆ—è¡¨æ–‡æœ¬
        step_names = []
        for step in steps:
            step_name = step.get("name") or step.get("step_name") or step.get("id", "æœªçŸ¥æ­¥éª¤")
            step_names.append(step_name)
        
        step_list_text = "\n".join([f"{i+1}. {name}" for i, name in enumerate(step_names)])
        
        guide_message = f"""### ğŸ“‹ åˆ†ææ–¹æ¡ˆå·²ç”Ÿæˆ

æ ¹æ®æ‚¨çš„éœ€æ±‚ **'{user_query}'**ï¼Œæˆ‘ä¸ºæ‚¨è§„åˆ’äº†ä»¥ä¸‹æµç¨‹ï¼š

{step_list_text}

**å½“å‰çŠ¶æ€**ï¼šç­‰å¾…æ•°æ®ã€‚
**ä¸‹ä¸€æ­¥**ï¼šè¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸Šä¼ æ–‡ä»¶ã€‚"""
        
        workflow_config["diagnosis"] = {
            "status": "template_ready",
            "message": guide_message,
            "steps_count": len(steps),
            "template_mode": True
        }
        
        # ç¡®ä¿ workflow_data ä¸­åŒ…å«æ¨¡æ¿æ ‡è®°
        if "workflow_data" in workflow_config:
            workflow_config["workflow_data"]["template_mode"] = True
        
        # ğŸ”¥ CRITICAL: è®¾ç½®é¡¶å±‚ template_mode æ ‡è®°
        workflow_config["template_mode"] = True
        
        return workflow_config
    
    async def _classify_intent(
        self,
        user_query: str,
        file_metadata: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾åˆ†ç±»
        
        è¯†åˆ«ï¼š
        1. åŸŸåï¼ˆdomain_nameï¼‰ï¼š"Metabolomics" æˆ– "RNA"
        2. ç›®æ ‡æ­¥éª¤ï¼ˆtarget_stepsï¼‰ï¼šç”¨æˆ·è¯·æ±‚çš„å…·ä½“æ­¥éª¤åˆ—è¡¨
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            {
                "domain_name": "Metabolomics" | "RNA",
                "target_steps": ["step1", "step2", ...]  # å¦‚æœä¸ºç©ºï¼Œè¡¨ç¤ºå®Œæ•´å·¥ä½œæµ
            }
        """
        # ğŸ”¥ CONTEXT-AWARE INTENT CLASSIFICATION: Determine execution mode based on query + file context
        has_file = file_metadata is not None
        
        # æ„å»ºæ„å›¾åˆ†ç±»æç¤ºè¯
        system_prompt = """You are an Intent Classifier for Bioinformatics Workflows.

Your task is to classify user queries into:
1. Domain Name: "Metabolomics" or "RNA" (strictly one of these two)
2. Mode: "EXECUTION" or "PLANNING" (determines if user wants to run or preview)
3. Target Steps: List of specific steps the user wants (e.g., ["pca_analysis", "differential_analysis"])

**Available Domains:**
- Metabolomics: For metabolite data analysis (CSV files with metabolite measurements)
- RNA: For single-cell RNA-seq analysis (H5AD files, FASTQ files)

**Available Steps (Metabolomics):**
- inspect_data, preprocess_data, pca_analysis, metabolomics_plsda, differential_analysis, visualize_volcano, metabolomics_pathway_enrichment

**Available Steps (RNA):**
- rna_qc_filter, rna_normalize, rna_pca, rna_clustering, rna_find_markers, etc.

**Mode Classification Rules (CRITICAL):**
1. IF File is **False** (no file uploaded): ALWAYS return "PLANNING".
2. IF File is **True** (file uploaded):
   - Query implies ACTION ("analyze", "run", "do", "start", "æ‰§è¡Œ", "åˆ†æ", "è¿è¡Œ", "å¼€å§‹"): -> Return "EXECUTION"
   - Query implies INQUIRY ("show me the plan", "what steps?", "preview", "é¢„è§ˆ", "æ˜¾ç¤º", "æŸ¥çœ‹"): -> Return "PLANNING"
   - Query is VAGUE ("metabolomics", "RNA", "ä»£è°¢ç»„", "è½¬å½•ç»„"): -> Default to "EXECUTION" (Assume user wants to run the file they just uploaded)

**Output Format:**
Return ONLY a JSON object:
{
  "domain_name": "Metabolomics" | "RNA",
  "mode": "EXECUTION" | "PLANNING",
  "target_steps": ["step1", "step2", ...]  // Empty array [] means full workflow
}

**Rules:**
- If user asks for "PCA" or "ä¸»æˆåˆ†åˆ†æ", target_steps should include "pca_analysis" (Metabolomics) or "rna_pca" (RNA)
- If user asks for "full analysis" or "å®Œæ•´åˆ†æ", use empty array []
- Domain name MUST be exactly "Metabolomics" or "RNA" (case-sensitive)
- Mode MUST be exactly "EXECUTION" or "PLANNING" (case-sensitive)"""

        user_prompt = f"""**User Query:**
{user_query}

**File Context:**
File Uploaded: {has_file} ({'True' if has_file else 'False'})

**File Metadata (if available):**
{json.dumps(file_metadata, ensure_ascii=False, indent=2) if file_metadata else "No file metadata available"}

**Task:**
Classify the intent and return JSON only. Remember:
- If File=False: mode MUST be "PLANNING"
- If File=True + Action words: mode = "EXECUTION"
- If File=True + Inquiry words: mode = "PLANNING"
- If File=True + Vague: mode = "EXECUTION" (default)"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.llm_client.achat(
            messages=messages,
            temperature=0.1,
            max_tokens=512
        )
        
        # ğŸ”¥ FIX: æå– ChatCompletion å¯¹è±¡çš„å†…å®¹
        if hasattr(response, 'choices') and response.choices:
            response_text = response.choices[0].message.content or ""
        else:
            response_text = str(response)
        
        # è§£æå“åº”
        try:
            intent_result = json.loads(response_text.strip())
            domain_name = intent_result.get("domain_name", "Metabolomics")
            mode = intent_result.get("mode", "PLANNING")  # ğŸ”¥ NEW: Extract mode
            target_steps = intent_result.get("target_steps", [])
            
            # ğŸ”¥ CRITICAL: File-type-based fallback for domain classification
            # If file metadata exists, use file extension to override LLM decision if needed
            logger.info(f"ğŸ” [SOPPlanner] æ„å›¾åˆ†ç±»åæ£€æŸ¥: domain_name={domain_name}, file_metadata exists={file_metadata is not None}")
            if file_metadata:
                file_path = file_metadata.get("file_path", "")
                file_type = file_metadata.get("file_type", "")
                logger.info(f"ğŸ” [SOPPlanner] æ–‡ä»¶å…ƒæ•°æ®: file_path={file_path}, file_type={file_type}")
                
                # Check file extension
                if file_path:
                    file_ext = file_path.lower().split('.')[-1] if '.' in file_path else ""
                    logger.info(f"ğŸ” [SOPPlanner] æ–‡ä»¶æ‰©å±•å: {file_ext}")
                    
                    # CSV files are strongly associated with Metabolomics
                    if file_ext == "csv":
                        if domain_name == "RNA":
                            logger.warning(f"âš ï¸ LLM å°† CSV æ–‡ä»¶åˆ†ç±»ä¸º RNAï¼Œå¼ºåˆ¶è¦†ç›–ä¸º Metabolomics")
                            domain_name = "Metabolomics"
                        else:
                            logger.info(f"âœ… CSV æ–‡ä»¶å·²æ­£ç¡®åˆ†ç±»ä¸º {domain_name}")
                    
                    # H5AD files are strongly associated with RNA
                    if file_ext == "h5ad" and domain_name == "Metabolomics":
                        logger.warning(f"âš ï¸ LLM å°† H5AD æ–‡ä»¶åˆ†ç±»ä¸º Metabolomicsï¼Œå¼ºåˆ¶è¦†ç›–ä¸º RNA")
                        domain_name = "RNA"
                    
                    # FASTQ files are strongly associated with RNA
                    if file_ext in ["fastq", "fq"] and domain_name == "Metabolomics":
                        logger.warning(f"âš ï¸ LLM å°† FASTQ æ–‡ä»¶åˆ†ç±»ä¸º Metabolomicsï¼Œå¼ºåˆ¶è¦†ç›–ä¸º RNA")
                        domain_name = "RNA"
                else:
                    logger.warning(f"âš ï¸ [SOPPlanner] æ–‡ä»¶å…ƒæ•°æ®ä¸­æ²¡æœ‰ file_path")
                
                # Check file_type from metadata
                if file_type == "tabular":
                    if domain_name == "RNA":
                        logger.warning(f"âš ï¸ LLM å°† tabular æ–‡ä»¶åˆ†ç±»ä¸º RNAï¼Œå¼ºåˆ¶è¦†ç›–ä¸º Metabolomics")
                        domain_name = "Metabolomics"
                    else:
                        logger.info(f"âœ… tabular æ–‡ä»¶å·²æ­£ç¡®åˆ†ç±»ä¸º {domain_name}")
            else:
                logger.warning(f"âš ï¸ [SOPPlanner] æ²¡æœ‰æ–‡ä»¶å…ƒæ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ–‡ä»¶ç±»å‹æ£€æŸ¥")
            
            # éªŒè¯åŸŸå
            if domain_name not in ["Metabolomics", "RNA"]:
                logger.warning(f"âš ï¸ LLM è¿”å›äº†æ— æ•ˆçš„åŸŸå: {domain_name}ï¼Œä½¿ç”¨é»˜è®¤å€¼ Metabolomics")
                domain_name = "Metabolomics"
            
            # ğŸ”¥ CRITICAL: Validate and enforce mode rules
            if not has_file and mode == "EXECUTION":
                logger.warning(f"âš ï¸ LLM è¿”å›äº†ä¸ä¸€è‡´çš„æ¨¡å¼: æ— æ–‡ä»¶ä½† mode=EXECUTIONï¼Œå¼ºåˆ¶è®¾ç½®ä¸º PLANNING")
                mode = "PLANNING"
            
            # éªŒè¯æ¨¡å¼
            if mode not in ["EXECUTION", "PLANNING"]:
                logger.warning(f"âš ï¸ LLM è¿”å›äº†æ— æ•ˆçš„æ¨¡å¼: {mode}ï¼Œä½¿ç”¨é»˜è®¤å€¼ PLANNING")
                mode = "PLANNING"
            
            logger.info(f"âœ… [SOPPlanner] æ„å›¾åˆ†ç±»ç»“æœ: domain={domain_name}, mode={mode}, target_steps={len(target_steps)}")
            
            return {
                "domain_name": domain_name,
                "mode": mode,  # ğŸ”¥ NEW: Include mode in result
                "target_steps": target_steps if isinstance(target_steps, list) else []
            }
        except json.JSONDecodeError as e:
            logger.error(f"âŒ æ„å›¾åˆ†ç±» JSON è§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response_text[:200] if 'response_text' in locals() else str(response)[:200]}")
            # å›é€€ï¼šå°è¯•ä»æŸ¥è¯¢å’Œæ–‡ä»¶å…ƒæ•°æ®ä¸­æ¨æ–­
            return self._fallback_intent_classification(user_query, has_file, file_metadata)
    
    def _fallback_intent_classification(self, user_query: str, has_file: bool = False, file_metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å›é€€çš„æ„å›¾åˆ†ç±»ï¼ˆåŸºäºæ–‡ä»¶å†…å®¹å’Œå…³é”®è¯ï¼‰
        
        ğŸ”¥ STRATEGIC FIX: Content-Aware Routing
        - ä¼˜å…ˆä½¿ç”¨ FileInspector å…ƒæ•°æ®ï¼ˆåˆ—åã€æ•°æ®ç±»å‹ï¼‰è¿›è¡Œæ™ºèƒ½è·¯ç”±
        - å›é€€åˆ°æ–‡ä»¶æ‰©å±•å
        - æœ€åä½¿ç”¨å…³é”®è¯åŒ¹é…
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            has_file: æ˜¯å¦æœ‰æ–‡ä»¶ï¼ˆç”¨äºå†³å®šæ¨¡å¼ï¼‰
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ„å›¾åˆ†ç±»ç»“æœï¼ˆåŒ…å« modeï¼‰
        """
        query_lower = user_query.lower()
        domain_name = "Metabolomics"  # é»˜è®¤å€¼
        
        # ğŸ”¥ STRATEGIC FIX: Content-Aware Routing - åŸºäºæ–‡ä»¶å†…å®¹
        if file_metadata:
            file_path = file_metadata.get("file_path", "")
            file_type = file_metadata.get("file_type", "")
            columns = file_metadata.get("columns", [])
            feature_columns = file_metadata.get("feature_columns", [])
            metadata_columns = file_metadata.get("metadata_columns", [])
            
            # ç­–ç•¥1: æ£€æŸ¥æ–‡ä»¶ç±»å‹ï¼ˆæœ€å¯é ï¼‰
            if file_type == "anndata" or file_path.lower().endswith(('.h5ad', '.h5', '.loom')):
                logger.info("âœ… [SOPPlanner] æ£€æµ‹åˆ° RNA æ–‡ä»¶ç±»å‹ï¼ˆanndata/h5adï¼‰ï¼Œä½¿ç”¨ RNA åŸŸå")
                domain_name = "RNA"
            elif file_type == "tabular" or file_path.lower().endswith('.csv'):
                # ç­–ç•¥2: æ£€æŸ¥åˆ—åæ¨¡å¼ï¼ˆå†…å®¹æ„ŸçŸ¥ï¼‰
                # RNA ç‰¹å¾ï¼šåŒ…å« "gene", "barcode", "cell", "UMAP", "tSNE", "cluster" ç­‰
                # Metabolomics ç‰¹å¾ï¼šåŒ…å« "Metabolite", "Compound", "m/z", "RT" ç­‰ï¼Œæˆ–æ•°å€¼åˆ—å¾ˆå¤š
                rna_column_keywords = ["gene", "barcode", "cell", "umap", "tsne", "cluster", "leiden", "pca"]
                metabolomics_column_keywords = ["metabolite", "compound", "m/z", "rt", "retention", "mass"]
                
                column_names_lower = [col.lower() for col in columns] if columns else []
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ RNA ç‰¹å¾åˆ—å
                has_rna_keywords = any(kw in ' '.join(column_names_lower) for kw in rna_column_keywords)
                # æ£€æŸ¥æ˜¯å¦æœ‰ Metabolomics ç‰¹å¾åˆ—å
                has_metabolomics_keywords = any(kw in ' '.join(column_names_lower) for kw in metabolomics_column_keywords)
                
                # ç­–ç•¥3: æ£€æŸ¥æ•°æ®ç»“æ„
                # RNA: é€šå¸¸æœ‰å¤§é‡ç‰¹å¾åˆ—ï¼ˆåŸºå› ï¼‰ï¼Œå°‘é‡å…ƒæ•°æ®åˆ—
                # Metabolomics: é€šå¸¸æœ‰ä¸­ç­‰æ•°é‡çš„ç‰¹å¾åˆ—ï¼ˆä»£è°¢ç‰©ï¼‰ï¼Œå¯èƒ½æœ‰åˆ†ç»„åˆ—
                is_likely_rna = False
                is_likely_metabolomics = False
                
                if feature_columns and metadata_columns:
                    # å¦‚æœç‰¹å¾åˆ—æ•°é‡ > 1000ï¼Œå¾ˆå¯èƒ½æ˜¯ RNA
                    if len(feature_columns) > 1000:
                        is_likely_rna = True
                        logger.info(f"âœ… [SOPPlanner] æ£€æµ‹åˆ°å¤§é‡ç‰¹å¾åˆ— ({len(feature_columns)})ï¼Œæ¨æ–­ä¸º RNA")
                    # å¦‚æœç‰¹å¾åˆ—æ•°é‡åœ¨ 10-500 ä¹‹é—´ï¼Œä¸”æ²¡æœ‰ RNA å…³é”®è¯ï¼Œå¾ˆå¯èƒ½æ˜¯ Metabolomics
                    elif 10 <= len(feature_columns) <= 500 and not has_rna_keywords:
                        is_likely_metabolomics = True
                        logger.info(f"âœ… [SOPPlanner] æ£€æµ‹åˆ°ä¸­ç­‰æ•°é‡ç‰¹å¾åˆ— ({len(feature_columns)})ï¼Œæ¨æ–­ä¸º Metabolomics")
                
                # å†³ç­–é€»è¾‘
                if has_rna_keywords or is_likely_rna:
                    domain_name = "RNA"
                    logger.info("âœ… [SOPPlanner] åŸºäºåˆ—å/æ•°æ®ç»“æ„ï¼Œä½¿ç”¨ RNA åŸŸå")
                elif has_metabolomics_keywords or is_likely_metabolomics:
                    domain_name = "Metabolomics"
                    logger.info("âœ… [SOPPlanner] åŸºäºåˆ—å/æ•°æ®ç»“æ„ï¼Œä½¿ç”¨ Metabolomics åŸŸå")
                else:
                    # å›é€€åˆ°æ–‡ä»¶æ‰©å±•å
                    if file_path.lower().endswith('.csv'):
                        logger.info("âœ… [SOPPlanner] æ£€æµ‹åˆ° CSV æ–‡ä»¶ï¼Œé»˜è®¤ä½¿ç”¨ Metabolomics åŸŸåï¼ˆå›é€€ï¼‰")
                        domain_name = "Metabolomics"
                    else:
                        # æœ€åä½¿ç”¨å…³é”®è¯åŒ¹é…
                        rna_keywords = ["rna", "scrna", "single cell", "å•ç»†èƒ", "è½¬å½•ç»„", "cellranger"]
                        domain_name = "RNA" if any(kw in query_lower for kw in rna_keywords) else "Metabolomics"
            else:
                # æœªçŸ¥æ–‡ä»¶ç±»å‹ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…
                rna_keywords = ["rna", "scrna", "single cell", "å•ç»†èƒ", "è½¬å½•ç»„", "cellranger", "h5ad"]
                domain_name = "RNA" if any(kw in query_lower for kw in rna_keywords) else "Metabolomics"
        else:
            # No file metadata - check RNA keywords
            rna_keywords = ["rna", "scrna", "single cell", "å•ç»†èƒ", "è½¬å½•ç»„", "cellranger", "h5ad"]
            domain_name = "RNA" if any(kw in query_lower for kw in rna_keywords) else "Metabolomics"
        
        # ğŸ”¥ CRITICAL: Determine mode based on query and file presence
        # Action keywords
        action_keywords = ["analyze", "run", "do", "start", "æ‰§è¡Œ", "åˆ†æ", "è¿è¡Œ", "å¼€å§‹"]
        # Inquiry keywords
        inquiry_keywords = ["show", "preview", "plan", "what", "é¢„è§ˆ", "æ˜¾ç¤º", "æŸ¥çœ‹", "è§„åˆ’"]
        
        if not has_file:
            mode = "PLANNING"  # No file -> always planning
        elif any(kw in query_lower for kw in action_keywords):
            mode = "EXECUTION"  # Action words + file -> execution
        elif any(kw in query_lower for kw in inquiry_keywords):
            mode = "PLANNING"  # Inquiry words + file -> planning
        else:
            mode = "EXECUTION"  # Vague query + file -> default to execution
        
        return {
            "domain_name": domain_name,
            "mode": mode,
            "target_steps": []
        }
    
    def _fill_parameters(
        self,
        workflow_config: Dict[str, Any],
        file_metadata: Dict[str, Any],
        workflow: "BaseWorkflow",
        template_mode: bool = False  # ğŸ”¥ NEW: Allow PLANNING mode with file paths filled
    ) -> Dict[str, Any]:
        """
        å¡«å……å·¥ä½œæµå‚æ•°ï¼ˆåŸºäºæ–‡ä»¶å…ƒæ•°æ®ï¼‰
        
        Args:
            workflow_config: å·¥ä½œæµé…ç½®
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®
            workflow: å·¥ä½œæµå®ä¾‹
            
        Returns:
            å¡«å……å‚æ•°åçš„å·¥ä½œæµé…ç½®
        """
        # ğŸ”¥ CRITICAL FIX: Ensure we work on a copy to avoid modifying the original
        workflow_config = workflow_config.copy() if isinstance(workflow_config, dict) else workflow_config
        
        # ğŸ”¥ CRITICAL FIX: Ensure workflow_data exists and is a dict
        if "workflow_data" not in workflow_config:
            workflow_config["workflow_data"] = {}
        elif not isinstance(workflow_config["workflow_data"], dict):
            workflow_config["workflow_data"] = {}
        
        steps = workflow_config.get("workflow_data", {}).get("steps", [])
        
        # ğŸ”¥ CRITICAL FIX: Ensure steps is a list (not None)
        if not isinstance(steps, list):
            logger.warning(f"âš ï¸ [SOPPlanner] steps ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(steps)}ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨")
            steps = []
            workflow_config["workflow_data"]["steps"] = steps
        
        file_path = file_metadata.get("file_path")
        
        # ğŸ”¥ CRITICAL FIX: æ£€æµ‹åˆ†ç»„åˆ—ï¼ˆç”¨äºä»£è°¢ç»„å­¦ï¼‰
        # é¦–å…ˆå°è¯•ä» semantic_map è·å–
        semantic_map = file_metadata.get("semantic_map", {})
        group_cols = semantic_map.get("group_cols", [])
        
        # å¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•æ£€æµ‹
        if not group_cols:
            detected_group_col = self._detect_group_column_heuristic(file_metadata)
            if detected_group_col:
                group_cols = [detected_group_col]
                logger.info(f"âœ… [SOPPlanner] å¯å‘å¼æ£€æµ‹åˆ°åˆ†ç»„åˆ—: {detected_group_col}")
                # æ›´æ–° semantic_map
                if "semantic_map" not in file_metadata:
                    file_metadata["semantic_map"] = {}
                file_metadata["semantic_map"]["group_cols"] = group_cols
        
        for step in steps:
            params = step.get("params", {})
            step_id = step.get("id")
            
            # ğŸ”¥ CRITICAL FIX: å¡«å…… file_path æˆ– adata_pathï¼ˆè¦†ç›–å ä½ç¬¦ï¼‰
            if "file_path" in params or "adata_path" in params:
                param_name = "adata_path" if "adata_path" in params else "file_path"
                if file_path:
                    # ğŸ”¥ CRITICAL: è¦†ç›–å ä½ç¬¦ï¼ˆå¦‚ "<å¾…ä¸Šä¼ æ•°æ®>"ï¼‰
                    old_value = params.get(param_name, "")
                    params[param_name] = file_path
                    if old_value and old_value != file_path:
                        logger.info(f"âœ… [SOPPlanner] è¦†ç›–å ä½ç¬¦: {old_value} -> {file_path}")
                elif params.get(param_name) in ["<å¾…ä¸Šä¼ æ•°æ®>", "<PENDING_UPLOAD>", ""]:
                    # å¦‚æœæ²¡æœ‰æ–‡ä»¶è·¯å¾„ä½†å‚æ•°æ˜¯å ä½ç¬¦ï¼Œè®°å½•è­¦å‘Š
                    logger.warning(f"âš ï¸ [SOPPlanner] æ­¥éª¤ {step_id} çš„å‚æ•° {param_name} ä»ç„¶æ˜¯å ä½ç¬¦ï¼Œä½† file_metadata å­˜åœ¨")
            
            # ğŸ”¥ CRITICAL FIX: å¼ºåˆ¶å¡«å…… group_columnï¼ˆå¯¹äºéœ€è¦åˆ†ç»„åˆ—çš„æ­¥éª¤ï¼‰
            if step_id in ["metabolomics_plsda", "differential_analysis", "metabolomics_pathway_enrichment"]:
                if group_cols:
                    # å¼ºåˆ¶è®¾ç½® group_column å‚æ•°
                    if "group_column" not in params:
                        params["group_column"] = group_cols[0]
                        logger.info(f"âœ… [SOPPlanner] å¼ºåˆ¶å¡«å…… group_column: {group_cols[0]} -> {step_id}")
                    elif params.get("group_column") != group_cols[0]:
                        # å¦‚æœå·²å­˜åœ¨ä½†å€¼ä¸åŒï¼Œæ›´æ–°å®ƒ
                        params["group_column"] = group_cols[0]
                        logger.info(f"âœ… [SOPPlanner] æ›´æ–° group_column: {params.get('group_column')} -> {group_cols[0]} ({step_id})")
                else:
                    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼Œæ ‡è®°ä¸ºéœ€è¦ç”¨æˆ·è¾“å…¥
                    logger.warning(f"âš ï¸ [SOPPlanner] æ­¥éª¤ {step_id} éœ€è¦åˆ†ç»„åˆ—ï¼Œä½†æœªæ£€æµ‹åˆ°")
                    step["status"] = "waiting_for_upload"
                    step["description"] += " âš ï¸ éœ€è¦åˆ†ç»„ä¿¡æ¯"
            elif "group_column" in params and group_cols:
                # å¯¹äºå…¶ä»–æ­¥éª¤ï¼Œå¦‚æœæœ‰ group_column å‚æ•°ä¸”æœ‰åˆ†ç»„åˆ—ï¼Œå¡«å……å®ƒ
                params["group_column"] = group_cols[0]
        
        # ğŸ”¥ CONTEXT-AWARE FIX: Set template_mode based on parameter (allows PLANNING mode with file)
        workflow_config["template_mode"] = template_mode
        
        # ğŸ”¥ CRITICAL FIX: æ¸…é™¤æ¨¡æ¿è¯Šæ–­æ¶ˆæ¯ï¼ˆä»…åœ¨ EXECUTION æ¨¡å¼ï¼‰
        # å¦‚æœ diagnosis å­˜åœ¨ä¸”æ˜¯æ¨¡æ¿æ¶ˆæ¯ï¼Œä¸”æ˜¯ EXECUTION æ¨¡å¼ï¼Œåˆ™æ¸…é™¤å®ƒ
        if not template_mode and "diagnosis" in workflow_config:
            diagnosis = workflow_config.get("diagnosis")
            if isinstance(diagnosis, dict) and diagnosis.get("status") == "template_ready":
                # æ¸…é™¤æ¨¡æ¿è¯Šæ–­ï¼Œè®© Orchestrator ç”ŸæˆçœŸå®è¯Šæ–­ï¼ˆä»… EXECUTION æ¨¡å¼ï¼‰
                workflow_config.pop("diagnosis", None)
                logger.info("âœ… [SOPPlanner] EXECUTION æ¨¡å¼ï¼šå·²æ¸…é™¤æ¨¡æ¿è¯Šæ–­æ¶ˆæ¯ï¼Œç­‰å¾… Orchestrator ç”ŸæˆçœŸå®è¯Šæ–­")
        
        # ç¡®ä¿ workflow_data ä¸­åŒ…å«æ¨¡å¼æ ‡è®°
        if "workflow_data" in workflow_config:
            workflow_config["workflow_data"]["template_mode"] = template_mode
        
        return workflow_config
    
    def _detect_group_column_heuristic(self, file_metadata: Dict[str, Any]) -> Optional[str]:
        """
        å¯å‘å¼æ£€æµ‹åˆ†ç»„åˆ—
        
        ğŸ”¥ CRITICAL FIX: å³ä½¿åˆ—æ˜¯æ•°å€¼å‹ï¼ˆint/floatï¼‰ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œä¹Ÿå½“ä½œåˆ†ç±»å˜é‡
        
        Args:
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®
        
        Returns:
            æ£€æµ‹åˆ°çš„åˆ†ç»„åˆ—åï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
        """
        # ä¼˜å…ˆçº§å…³é”®è¯åˆ—è¡¨
        priority_keywords = ['Diet', 'diet', 'Group', 'group', 'Condition', 'condition', 
                            'Treatment', 'treatment', 'Class', 'class', 'Category', 'category',
                            'Type', 'type', 'Label', 'label', 'Status', 'status']
        
        # æ–¹æ³•1: æ£€æŸ¥ metadata_columnsï¼ˆFileInspector å¯èƒ½å·²ç»æ£€æµ‹åˆ°ï¼‰
        metadata_cols = file_metadata.get("metadata_columns", [])
        if metadata_cols:
            # ä¼˜å…ˆæ£€æŸ¥å…³é”®è¯åŒ¹é…
            for col in metadata_cols:
                if any(keyword in col for keyword in priority_keywords):
                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰: {col}")
                    return col
            # å¦‚æœæ²¡æœ‰å…³é”®è¯åŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå…ƒæ•°æ®åˆ—
            logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆmetadata_columnsï¼‰: {metadata_cols[0]}")
            return metadata_cols[0]
        
        # æ–¹æ³•2: æ£€æŸ¥ potential_groups
        potential_groups = file_metadata.get("potential_groups", {})
        if isinstance(potential_groups, dict) and len(potential_groups) > 0:
            first_group_col = list(potential_groups.keys())[0]
            logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆpotential_groupsï¼‰: {first_group_col}")
            return first_group_col
        
        # æ–¹æ³•3: ğŸ”¥ CRITICAL FIX - æ£€æŸ¥æ•°å€¼åˆ—ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œå½“ä½œåˆ†ç±»å˜é‡
        columns = file_metadata.get("columns", [])
        head_data = file_metadata.get("head", {})
        
        if columns and head_data:
            try:
                import pandas as pd
                head_json = head_data.get("json", [])
                if isinstance(head_json, list) and len(head_json) > 0:
                    df_preview = pd.DataFrame(head_json)
                    
                    # é¦–å…ˆæ£€æŸ¥å…³é”®è¯åŒ¹é…çš„åˆ—
                    for col in columns:
                        if col in df_preview.columns:
                            if any(keyword in col for keyword in priority_keywords):
                                # æ£€æŸ¥å”¯ä¸€å€¼æ•°é‡
                                unique_count = df_preview[col].nunique()
                                if 2 <= unique_count <= 5:  # ğŸ”¥ å…³é”®ï¼šå³ä½¿æ•°å€¼å‹ï¼Œå”¯ä¸€å€¼ <= 5 ä¹Ÿå½“ä½œåˆ†ç±»
                                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆæ•°å€¼å‹å…³é”®è¯åŒ¹é…ï¼‰: {col} (å”¯ä¸€å€¼: {unique_count})")
                                    return col
                    
                    # ç„¶åæ£€æŸ¥æ‰€æœ‰åˆ—ï¼ˆåŒ…æ‹¬æ•°å€¼åˆ—ï¼‰
                    for col in columns:
                        if col in df_preview.columns:
                            unique_count = df_preview[col].nunique()
                            # ğŸ”¥ CRITICAL: å³ä½¿åˆ—æ˜¯æ•°å€¼å‹ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œå½“ä½œåˆ†ç±»å˜é‡
                            if 2 <= unique_count <= 5:
                                # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«åˆ†ç»„å…³é”®è¯
                                if any(keyword in col for keyword in priority_keywords):
                                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆæ•°å€¼å‹ï¼Œå”¯ä¸€å€¼ <= 5ï¼‰: {col} (å”¯ä¸€å€¼: {unique_count})")
                                    return col
                                # æˆ–è€…å¦‚æœå”¯ä¸€å€¼æ­£å¥½æ˜¯ 2ï¼ˆå…¸å‹çš„äºŒå…ƒåˆ†ç»„ï¼‰
                                elif unique_count == 2:
                                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆäºŒå…ƒæ•°å€¼å‹ï¼‰: {col} (å”¯ä¸€å€¼: {unique_count})")
                                    return col
            except Exception as e:
                logger.warning(f"âš ï¸ [Heuristic] æ•°å€¼åˆ—æ£€æµ‹å¤±è´¥: {e}")
        
        # æ–¹æ³•4: æ£€æŸ¥æ‰€æœ‰åˆ—åï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        if columns:
            for col in columns:
                if any(keyword in col for keyword in priority_keywords):
                    logger.info(f"âœ… [Heuristic] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆåˆ—åå…³é”®è¯åŒ¹é…ï¼‰: {col}")
                    return col
        
        logger.info("âš ï¸ [Heuristic] æœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—")
        return None
    
    def _generate_metabolomics_plan(self, file_metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆç¡®å®šæ€§çš„ä»£è°¢ç»„å­¦ SOP æµç¨‹
        
        ä¸¥æ ¼æŒ‰ç…§çº¿æ€§æµç¨‹å›¾é€»è¾‘ï¼š
        1. inspect_data (Always)
        2. preprocess_data (Always)
        3. pca_analysis (Always)
        4. Decision: å¦‚æœæœ‰åˆ†ç»„åˆ—
           - metabolomics_plsda
           - differential_analysis
           - visualize_volcano
           - metabolomics_pathway_enrichment
        5. å¦‚æœæ²¡æœ‰åˆ†ç»„åˆ—ï¼šåœæ­¢
        
        Args:
            file_metadata: FileInspector è¿”å›çš„æ–‡ä»¶å…ƒæ•°æ®ï¼ˆNoneè¡¨ç¤ºæ¨¡æ¿æ¨¡å¼ï¼‰
        
        Returns:
            ç¬¦åˆå‰ç«¯æ ¼å¼çš„å·¥ä½œæµé…ç½®å­—å…¸
        """
        logger.info("ğŸ“‹ [SOPPlanner] ç”Ÿæˆç¡®å®šæ€§ä»£è°¢ç»„å­¦ SOP æµç¨‹")
        
        # ğŸ”¥ CRITICAL FIX: ç§»é™¤æ–‡ä»¶ç¼ºå¤±æ£€æŸ¥é€»è¾‘
        # Planneråº”è¯¥åªæ¥å—file_metadataä½œä¸ºè¾“å…¥ï¼ˆNoneæˆ–Dictï¼‰å¹¶ç›¸åº”åœ°è¾“å‡ºè®¡åˆ’
        # Orchestratorå·²ç»å¤„ç†äº†åˆ†æ”¯é€»è¾‘ï¼Œè¿™é‡Œä¸éœ€è¦å†æ¬¡æ£€æŸ¥
        
        # è·å–æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
        file_path = file_metadata.get("file_path", "") if file_metadata else ""
        
        # æ£€æµ‹åˆ†ç»„åˆ—ï¼ˆå¦‚æœæœ‰æ–‡ä»¶å…ƒæ•°æ®ï¼‰
        group_column = None
        has_groups = False
        if file_metadata:
            group_column = self._detect_group_column_heuristic(file_metadata)
            has_groups = group_column is not None
        
        logger.info(f"ğŸ” [SOPPlanner] åˆ†ç»„æ£€æµ‹ç»“æœ: {group_column if has_groups else 'æ— åˆ†ç»„åˆ—'}")
        
        # æ„å»ºæ­¥éª¤åˆ—è¡¨
        steps = []
        
        # Step 1: Data Inspection (Always)
        steps.append({
            "id": "inspect_data",
            "name": "æ•°æ®æ£€æŸ¥",
            "description": "SOPè§„åˆ™ï¼šå¿…é¡»é¦–å…ˆè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°ï¼Œæ£€æŸ¥ç¼ºå¤±å€¼ã€æ•°æ®èŒƒå›´ç­‰",
            "selected": True,
            "params": {
                "file_path": file_path if file_path else "<PENDING_UPLOAD>"
            }
        })
        
        # Step 2: Preprocessing (Always)
        steps.append({
            "id": "preprocess_data",
            "name": "æ•°æ®é¢„å¤„ç†",
            "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒLog2è½¬æ¢å’Œæ ‡å‡†åŒ–ï¼Œç¼ºå¤±å€¼å¤„ç†",
            "selected": True,
            "params": {
                "file_path": file_path if file_path else "<PENDING_UPLOAD>",  # å°†è‡ªåŠ¨æ›´æ–°ä¸ºé¢„å¤„ç†åçš„æ–‡ä»¶
                "log_transform": True,
                "standardize": True,
                "missing_imputation": "min"
            }
        })
        
        # Step 3: Unsupervised Analysis - PCA (Always)
        steps.append({
            "id": "pca_analysis",
            "name": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒPCAåˆ†æä»¥æ¢ç´¢æ•°æ®ç»“æ„å’Œé™ç»´",
            "selected": True,
            "params": {
                "file_path": "<preprocess_data_output>",  # å°†è‡ªåŠ¨æ›´æ–°
                "n_components": 2,
                "scale": True
            }
        })
        
        # Decision Node: Check for Group Column
        if has_groups:
            logger.info(f"âœ… [SOPPlanner] æ£€æµ‹åˆ°åˆ†ç»„åˆ— '{group_column}'ï¼Œæ·»åŠ ç›‘ç£åˆ†ææ­¥éª¤")
            
            # Step 4: Supervised Analysis - PLS-DA
            steps.append({
                "id": "metabolomics_plsda",
                "name": "PLS-DA åˆ†æ",
                "description": f"SOPè§„åˆ™ï¼šæ£€æµ‹åˆ°åˆ†ç»„åˆ— '{group_column}'ï¼Œå¿…é¡»è¿›è¡Œç›‘ç£åˆ†æï¼ˆPLS-DAï¼‰ä»¥è¯†åˆ«ç»„é—´å·®å¼‚",
                "selected": True,
                "params": {
                    "file_path": "<preprocess_data_output>",
                    "group_column": group_column,
                    "n_components": 2
                }
            })
            
            # Step 5: Differential Analysis
            steps.append({
                "id": "differential_analysis",
                "name": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",
                "description": f"SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡Œå·®å¼‚åˆ†æä»¥è¯†åˆ«æ˜¾è‘—å·®å¼‚çš„ä»£è°¢ç‰©ï¼ˆåˆ†ç»„åˆ—: {group_column}ï¼‰",
                "selected": True,
                "params": {
                    "file_path": "<preprocess_data_output>",
                    "group_column": group_column,
                    "method": "t-test",
                    "p_value_threshold": 0.05,
                    "fold_change_threshold": 1.5
                }
            })
            
            # Step 6: Visualization - Volcano Plot
            # æ³¨æ„ï¼švisualize_volcano éœ€è¦ diff_results å­—å…¸ï¼ŒåŒ…å« results åˆ—è¡¨
            steps.append({
                "id": "visualize_volcano",
                "name": "ç«å±±å›¾å¯è§†åŒ–",
                "description": "SOPè§„åˆ™ï¼šå¿…é¡»å¯è§†åŒ–å·®å¼‚åˆ†æç»“æœï¼Œå±•ç¤ºæ˜¾è‘—å·®å¼‚ä»£è°¢ç‰©",
                "selected": True,
                "params": {
                    "diff_results": "<differential_analysis_output>",  # å°†è‡ªåŠ¨ä»ä¸Šä¸€ä¸ªæ­¥éª¤è·å–
                    "output_path": None,  # å°†è‡ªåŠ¨ç”Ÿæˆ
                    "fdr_threshold": 0.05,
                    "log2fc_threshold": 1.0
                }
            })
            
            # Step 7: Functional Analysis - Pathway Enrichment
            # æ³¨æ„ï¼šmetabolomics_pathway_enrichment éœ€è¦ file_path, group_column, case_group, control_group
            # éœ€è¦ä»æ•°æ®ä¸­è‡ªåŠ¨æ£€æµ‹åˆ†ç»„å€¼
            potential_groups = file_metadata.get("potential_groups", {}) if file_metadata else {}
            case_group = None
            control_group = None
            
            # æ–¹æ³•1: ä» potential_groups å­—å…¸ä¸­æå–
            if isinstance(potential_groups, dict) and group_column in potential_groups:
                group_values = potential_groups[group_column]
                if isinstance(group_values, list) and len(group_values) >= 2:
                    # ä½¿ç”¨å‰ä¸¤ä¸ªåˆ†ç»„å€¼
                    case_group = group_values[0]
                    control_group = group_values[1]
                    logger.info(f"âœ… [SOPPlanner] ä» potential_groups æ£€æµ‹åˆ°åˆ†ç»„å€¼: {case_group} vs {control_group}")
            
            # æ–¹æ³•2: å¦‚æœ potential_groups æ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆæ—§æ ¼å¼å…¼å®¹ï¼‰
            if (not case_group or not control_group) and isinstance(potential_groups, dict):
                # å°è¯•ä»å…¶ä»–é”®ä¸­æŸ¥æ‰¾
                for key, values in potential_groups.items():
                    if isinstance(values, list) and len(values) >= 2:
                        case_group = values[0]
                        control_group = values[1]
                        logger.info(f"âœ… [SOPPlanner] ä»å…¶ä»–åˆ†ç»„åˆ—æ£€æµ‹åˆ°åˆ†ç»„å€¼: {case_group} vs {control_group}")
                        break
            
            # æ–¹æ³•3: å¦‚æœä»ç„¶æ²¡æœ‰æ£€æµ‹åˆ°ï¼Œå°è¯•ä» head æ•°æ®ä¸­æ¨æ–­
            if not case_group or not control_group:
                head_data = file_metadata.get("head", {})
                if isinstance(head_data, dict):
                    head_json = head_data.get("json", [])
                    if isinstance(head_json, list) and len(head_json) > 0:
                        # å°è¯•ä»ç¬¬ä¸€è¡Œæ•°æ®ä¸­è·å–åˆ†ç»„åˆ—çš„å€¼
                        first_row = head_json[0] if isinstance(head_json[0], dict) else {}
                        if group_column in first_row:
                            # éœ€è¦è¯»å–æ›´å¤šæ•°æ®æ¥è·å–å”¯ä¸€å€¼ï¼Œè¿™é‡Œå…ˆä½¿ç”¨å ä½ç¬¦
                            # å®é™…æ‰§è¡Œæ—¶ differential_analysis ä¼šè‡ªåŠ¨æ£€æµ‹
                            logger.info(f"âš ï¸ [SOPPlanner] æ— æ³•ä»å…ƒæ•°æ®ä¸­ç¡®å®šåˆ†ç»„å€¼ï¼Œå°†åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹")
            
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°åˆ†ç»„å€¼ï¼Œè®¾ç½®ä¸º Noneï¼ˆè®©å·¥å…·è‡ªåŠ¨æ£€æµ‹ï¼‰
            # æ³¨æ„ï¼šdifferential_analysis ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œä½† pathway_enrichment éœ€è¦æ˜ç¡®çš„å€¼
            # æ‰€ä»¥æˆ‘ä»¬ä» differential_analysis çš„ç»“æœä¸­æå–ï¼Œæˆ–è€…è®©ç”¨æˆ·æŒ‡å®š
            if not case_group or not control_group:
                # ä½¿ç”¨å ä½ç¬¦ï¼ŒExecutionLayer ä¼šå°è¯•ä»å‰ä¸€æ­¥éª¤çš„ç»“æœä¸­æå–
                # æˆ–è€…å·¥å…·ä¼šåœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹
                case_group = None  # è®©å·¥å…·è‡ªåŠ¨æ£€æµ‹
                control_group = None  # è®©å·¥å…·è‡ªåŠ¨æ£€æµ‹
                logger.warning(f"âš ï¸ [SOPPlanner] æœªæ£€æµ‹åˆ°åˆ†ç»„å€¼ï¼Œå°†åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹æˆ–ä» differential_analysis ç»“æœä¸­æå–")
            
            steps.append({
                "id": "metabolomics_pathway_enrichment",
                "name": "é€šè·¯å¯Œé›†åˆ†æ",
                "description": f"SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡Œé€šè·¯å¯Œé›†åˆ†æä»¥ç†è§£å·®å¼‚ä»£è°¢ç‰©çš„ç”Ÿç‰©å­¦æ„ä¹‰ï¼ˆåˆ†ç»„åˆ—: {group_column}ï¼‰",
                "selected": True,
                "params": {
                    "file_path": "<preprocess_data_output>",
                    "group_column": group_column,
                    "case_group": case_group if case_group else "<differential_analysis_case_group>",  # å ä½ç¬¦ï¼ŒExecutionLayer ä¼šå¤„ç†
                    "control_group": control_group if control_group else "<differential_analysis_control_group>",  # å ä½ç¬¦
                    "organism": "hsa",  # é»˜è®¤äººç±»ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
                    "p_value_threshold": 0.05
                }
            })
        else:
            logger.info("âš ï¸ [SOPPlanner] æœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼Œä»…æ‰§è¡Œæ— ç›‘ç£åˆ†æï¼ˆPCAï¼‰")
            # æ·»åŠ è­¦å‘Šæè¿°åˆ°æœ€åä¸€ä¸ªæ­¥éª¤
            steps[-1]["description"] += " âš ï¸ æ³¨æ„ï¼šæœªæ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼Œæ— æ³•è¿›è¡Œç›‘ç£åˆ†æå’Œå·®å¼‚åˆ†æã€‚"
        
        # æ„å»ºå·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        workflow_name = "ä»£è°¢ç»„å­¦æ ‡å‡†åˆ†ææµç¨‹" + ("ï¼ˆå«åˆ†ç»„åˆ†æï¼‰" if has_groups else "ï¼ˆæ— ç›‘ç£åˆ†æï¼‰")
        
        # é€‚é…æ­¥éª¤æ ¼å¼ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        adapted_steps = []
        for i, step in enumerate(steps, 1):
            tool_id = step["id"]
            
            # éªŒè¯å·¥å…·æ˜¯å¦åœ¨æ³¨å†Œè¡¨ä¸­
            tool_metadata_obj = registry.get_metadata(tool_id)
            if not tool_metadata_obj:
                logger.warning(f"âš ï¸ [SOPPlanner] å·¥å…· '{tool_id}' ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œè·³è¿‡")
                continue
            
            # è·å–æ˜¾ç¤ºåç§°
            step_display_name = self._get_step_display_name(tool_id, tool_metadata_obj.description)
            
            # æ„å»ºé€‚é…åçš„æ­¥éª¤
            adapted_step = {
                "step_id": tool_id,
                "id": tool_id,
                "tool_id": tool_id,
                "name": step.get("name", step_display_name),
                "step_name": step.get("name", step_display_name),
                "description": step.get("description", tool_metadata_obj.description[:100]),
                "desc": step.get("description", tool_metadata_obj.description[:100])[:100],
                "selected": step.get("selected", True),
                "params": step.get("params", {})
            }
            
            adapted_steps.append(adapted_step)
        
        # æ„å»ºæœ€ç»ˆçš„å·¥ä½œæµé…ç½®
        workflow_config = {
            "type": "workflow_config",
            "workflow_data": {
                "workflow_name": workflow_name,
                "name": workflow_name,
                "steps": adapted_steps
            },
            "file_paths": [file_path] if file_path else []
        }
        
        logger.info(f"âœ… [SOPPlanner] ç¡®å®šæ€§æµç¨‹ç”Ÿæˆå®Œæˆ: {len(adapted_steps)} ä¸ªæ­¥éª¤")
        logger.info(f"   æ­¥éª¤åˆ—è¡¨: {[s['id'] for s in adapted_steps]}")
        
        return workflow_config
    
    def _build_sop_system_prompt(self) -> str:
        """
        æ„å»º SOP é©±åŠ¨çš„ç³»ç»Ÿæç¤ºè¯
        
        Returns:
            åŒ…å« SOP è§„åˆ™çš„ç³»ç»Ÿæç¤ºè¯
        """
        return """You are an expert Bioinformatics Pipeline Architect specializing in Metabolomics data analysis.

Your task is to generate executable workflow plans that STRICTLY follow the Metabolomics Standard Operating Procedure (SOP).

**CRITICAL SOP RULES (MUST FOLLOW):**

1. **Data Quality Assessment (MANDATORY FIRST STEP):**
   - IF missing_values > 50% â†’ MUST use a dropping/imputation tool.
   - IF missing_values > 0% AND missing_values <= 50% â†’ MUST use an imputation tool.
   - ALWAYS perform data inspection first (inspect_data).

2. **Data Preprocessing (ALWAYS REQUIRED):**
   - ALWAYS perform Normalization (Log2 transformation + Scaling).
   - Use preprocess_data tool for this step.
   - Parameters should be auto-filled based on file metadata.

3. **Analysis Type Selection (CONDITIONAL):**
   - IF group_columns exist (metadata_columns detected OR numeric columns with â‰¤5 unique values like 0/1) â†’ MUST perform:
     a. Unsupervised Analysis: PCA (pca_analysis) - ALWAYS perform PCA first
     b. Supervised Analysis: PLS-DA (metabolomics_plsda) - MUST add if groups detected
     c. Differential Analysis (differential_analysis) - MUST add if groups detected
     d. Pathway Enrichment (metabolomics_pathway_enrichment) - MUST be added if differential analysis is planned
   - IF NO group_columns â†’ Perform Unsupervised Analysis only:
     a. PCA (pca_analysis)

4. **Visualization Rules (CRITICAL - MUST FOLLOW):**
   - ğŸ”¥ ANTI-REDUNDANCY RULE: The tool `pca_analysis` ALREADY generates a plot. If you select `pca_analysis`, you MUST NOT select `visualize_pca`. They are mutually exclusive. Adding both will cause errors.
   - ğŸ”¥ DATA FLOW RULE: If you perform `differential_analysis`, you MUST follow it with `visualize_volcano` to plot the results.
   - ğŸ”¥ GROUP DETECTION: If a column (like 'Diet') has few unique values (e.g., 0 and 1, or 2-5 unique values), treat it as a Grouping Column. In this case, you MUST add `metabolomics_plsda` and `metabolomics_pathway_enrichment`.

**OUTPUT CONTRACT (CRITICAL - MUST MATCH EXACTLY):**

You MUST output a JSON object that matches the existing Frontend UI structure EXACTLY:

```json
{
  "name": "Generated Pipeline Name",
  "steps": [
    {
      "id": "tool_name_from_registry",
      "name": "Human Readable Step Name",
      "description": "Why this step is needed (based on SOP rules)",
      "selected": true,
      "params": {
        "file_path": "<auto-filled from metadata>",
        "param1": "value1",
        "param2": "value2"
      }
    }
  ]
}
```

**Step Structure Requirements:**
- "id": MUST be the exact tool name from the retrieved tools (e.g., "pca_analysis", "differential_analysis", "metabolomics_plsda")
- "name": Human-readable name in Chinese (e.g., "ä¸»æˆåˆ†åˆ†æ", "PLS-DA åˆ†æ")
- "description": Brief explanation of why this step is needed, referencing SOP rules
- "selected": Always true (all steps are selected by default)
- "params": Dictionary matching the tool's args_schema, with file_path auto-filled from metadata

**Parameter Auto-Filling Rules:**
- file_path: Use the file_path from file_metadata
- group_column: ğŸ”¥ CRITICAL - MUST be one of the strings in file_metadata['semantic_map']['group_cols']. If semantic_map['group_cols'] is empty, DO NOT add supervised steps (PLS-DA, Differential Analysis).
- n_components: Default to 2 for PCA/PLS-DA
- scale: Default to true for normalization
- Other parameters: Use sensible defaults from the tool's args_schema

**ğŸ”¥ SEMANTIC MAPPING CONSTRAINT (CRITICAL):**
- The group_column parameter MUST be selected from semantic_map['group_cols'] list.
- If semantic_map['group_cols'] is empty, SKIP all supervised analysis steps (metabolomics_plsda, differential_analysis, visualize_volcano, metabolomics_pathway_enrichment).
- Do NOT hallucinate or guess group column names. Only use what is explicitly provided in semantic_map['group_cols'].

**Example Output:**
{
  "name": "ä»£è°¢ç»„å­¦æ ‡å‡†åˆ†ææµç¨‹",
  "steps": [
    {
      "id": "inspect_data",
      "name": "æ•°æ®æ£€æŸ¥",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»é¦–å…ˆè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°ï¼Œæ£€æŸ¥ç¼ºå¤±å€¼ã€æ•°æ®èŒƒå›´ç­‰",
      "selected": true,
      "params": {
        "file_path": "/app/uploads/data.csv"
      }
    },
    {
      "id": "preprocess_data",
      "name": "æ•°æ®é¢„å¤„ç†",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒLog2è½¬æ¢å’Œæ ‡å‡†åŒ–ï¼Œç¼ºå¤±å€¼ç‡5%éœ€è¦æ’è¡¥",
      "selected": true,
      "params": {
        "file_path": "/app/uploads/data.csv",
        "log_transform": true,
        "standardize": true,
        "missing_imputation": "min"
      }
    }
  ]
}

Generate ONLY the JSON object, no markdown code blocks, no additional text."""
    
    def _build_sop_user_prompt(
        self,
        user_query: str,
        file_metadata: Dict[str, Any],
        retrieved_tools: List[Dict[str, Any]]
    ) -> str:
        """
        æ„å»º SOP é©±åŠ¨çš„ç”¨æˆ·æç¤ºè¯
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®
            retrieved_tools: æ£€ç´¢åˆ°çš„å·¥å…·åˆ—è¡¨
        
        Returns:
            ç”¨æˆ·æç¤ºè¯æ–‡æœ¬
        """
        # æ ¼å¼åŒ–æ–‡ä»¶å…ƒæ•°æ®
        metadata_text = self._format_file_metadata(file_metadata)
        
        # æ ¼å¼åŒ–å·¥å…·ä¿¡æ¯
        tools_text = []
        for i, tool in enumerate(retrieved_tools, 1):
            tool_info = f"""
Tool {i}: {tool['name']}
  Description: {tool['description']}
  Category: {tool['category']}
  Parameters Schema:
{json.dumps(tool['args_schema'], indent=2, ensure_ascii=False)}
"""
            tools_text.append(tool_info)
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Extract semantic_map for fast constraint
        semantic_map = file_metadata.get("semantic_map", {})
        group_cols = semantic_map.get("group_cols", [])
        
        # Fail-Fast: If no group columns, skip supervised steps
        has_groups = len(group_cols) > 0
        
        prompt = f"""**User Query:**
{user_query}

**File Metadata:**
{metadata_text}

**Available Tools:**
{''.join(tools_text)}

**Task (SIMPLIFIED - NO BIOINFORMATICS REASONING):**
Map the user's intent to the available group_cols. If user says 'analyze cachexia', and group_cols contains 'Muscle loss', pick 'Muscle loss'.

**ğŸ”¥ SEMANTIC MAPPING CONSTRAINT (CRITICAL):**
- semantic_map['group_cols'] = {group_cols}
- The group_column parameter in your JSON MUST be one of these strings: {group_cols if group_cols else '[]'}
- If group_cols is empty ([]), SKIP all supervised steps (metabolomics_plsda, differential_analysis, visualize_volcano, metabolomics_pathway_enrichment)
- Do NOT hallucinate or guess group column names. Only use what is explicitly provided.

**Workflow Decision:**
- Has groups: {has_groups}
- If has_groups=True: Add PCA + PLS-DA + Differential + Volcano + Pathway
- If has_groups=False: Add PCA only (unsupervised)

**CRITICAL: Visualization Rules (MUST FOLLOW STRICTLY)**
- ğŸ”¥ ANTI-REDUNDANCY RULE: The tool `pca_analysis` ALREADY generates a plot. If you select `pca_analysis`, you MUST NOT select `visualize_pca`. They are mutually exclusive.
- ğŸ”¥ DATA FLOW RULE: If you perform `differential_analysis`, you MUST follow it with `visualize_volcano` to plot the results.

**Output:**
Return ONLY a valid JSON object matching the structure:
{{
  "name": "Pipeline Name",
  "steps": [
    {{
      "id": "tool_name",
      "name": "Step Name",
      "description": "Why needed",
      "selected": true,
      "params": {{"file_path": "...", ...}}
    }}
  ]
}}

Remember: Output ONLY the JSON object, no markdown, no code blocks, no explanations."""
        
        return prompt
    
    def _format_file_metadata(self, file_metadata: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æ–‡ä»¶å…ƒæ•°æ®ä¸ºå¯è¯»æ–‡æœ¬
        
        Args:
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®å­—å…¸
        
        Returns:
            æ ¼å¼åŒ–çš„å…ƒæ•°æ®æ–‡æœ¬
        """
        if not file_metadata or file_metadata.get("status") != "success":
            return "File metadata not available or invalid."
        
        shape = file_metadata.get("shape", {})
        missing_rate = file_metadata.get("missing_rate", 0)
        metadata_cols = file_metadata.get("metadata_columns", [])
        feature_cols = file_metadata.get("feature_columns", [])
        file_path = file_metadata.get("file_path", "N/A")
        
        text = f"""File Path: {file_path}
Shape: {shape.get('rows', 'N/A')} rows Ã— {shape.get('cols', 'N/A')} columns
Missing Rate: {missing_rate}%
Metadata Columns: {', '.join(metadata_cols) if metadata_cols else 'None'}
Feature Columns (first 10): {', '.join(feature_cols[:10]) if feature_cols else 'None'}
Total Features: {file_metadata.get('total_feature_columns', 'N/A')}
"""
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Use semantic_map for clear constraints
        semantic_map = file_metadata.get("semantic_map", {})
        group_cols = semantic_map.get("group_cols", [])
        id_col = semantic_map.get("id_col", "N/A")
        feature_count = semantic_map.get("feature_count", "N/A")
        
        text += f"""
**Semantic Map (CRITICAL - USE THIS FOR group_column):**
- ID Column: {id_col}
- Group Columns: {group_cols if group_cols else '[] (NO GROUPS - SKIP SUPERVISED STEPS)'}
- Feature Count: {feature_count}

ğŸ”¥ CONSTRAINT: The group_column parameter MUST be one of: {group_cols if group_cols else '[]'}
ğŸ”¥ FAIL-FAST: If group_cols is empty, DO NOT add supervised steps (PLS-DA, Differential, Volcano, Pathway).
"""
        
        # ä¿ç•™æ—§æ ¼å¼ä»¥å…¼å®¹
        potential_groups = file_metadata.get("potential_groups", {})
        if isinstance(potential_groups, dict) and len(potential_groups) > 0:
            text += "\n**Grouping Columns (Legacy Format - Use semantic_map instead):**\n"
            for col_name, col_info in potential_groups.items():
                if isinstance(col_info, dict):
                    n_unique = col_info.get("n_unique", "?")
                    values = col_info.get("values", [])
                    values_str = ", ".join([str(v) for v in values[:10]])
                    text += f"  - {col_name}: {n_unique} unique values ({values_str})\n"
        
        # æ·»åŠ æ‰€æœ‰åˆ—åï¼ˆç”¨äºæ£€æµ‹æ•°å€¼å‹åˆ†ç»„åˆ—ï¼‰
        all_columns = file_metadata.get("columns", [])
        if all_columns:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å€¼å‹åˆ—å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯
            head_data = file_metadata.get("head", {})
            if head_data and isinstance(head_data, dict):
                head_json = head_data.get("json", [])
                if isinstance(head_json, list) and len(head_json) > 0:
                    try:
                        import pandas as pd
                        df_preview = pd.DataFrame(head_json)
                        text += "\n**Column Analysis (for Group Detection):**\n"
                        priority_keywords = ['Diet', 'diet', 'Group', 'group', 'Condition', 'condition']
                        for col in all_columns:
                            if col in df_preview.columns:
                                unique_count = df_preview[col].nunique()
                                # å¦‚æœåˆ—ååŒ…å«åˆ†ç»„å…³é”®è¯ä¸”å”¯ä¸€å€¼ <= 5ï¼Œæ ‡è®°ä¸ºæ½œåœ¨åˆ†ç»„åˆ—
                                if any(kw in col for kw in priority_keywords) and 2 <= unique_count <= 5:
                                    unique_values = sorted(df_preview[col].unique().tolist())
                                    text += f"  - {col}: {unique_count} unique values {unique_values} âš ï¸ POTENTIAL GROUPING COLUMN\n"
                    except Exception as e:
                        logger.debug(f"æ— æ³•åˆ†æåˆ—çš„å”¯ä¸€å€¼: {e}")
        
        # æ·»åŠ æ•°æ®èŒƒå›´ä¿¡æ¯
        data_range = file_metadata.get("data_range", {})
        if data_range:
            text += f"\nData Range: min={data_range.get('min', 'N/A')}, max={data_range.get('max', 'N/A')}\n"
        
        return text
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æ LLM å“åº”ï¼ˆå¤ç”¨ WorkflowPlanner çš„é€»è¾‘ï¼‰
        
        Args:
            response: LLM è¿”å›çš„æ–‡æœ¬
        
        Returns:
            è§£æåçš„å·¥ä½œæµè®¡åˆ’å­—å…¸
        """
        # å°è¯•æå– JSONï¼ˆå¯èƒ½è¢« markdown ä»£ç å—åŒ…è£¹ï¼‰
        response = response.strip()
        
        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        if response.startswith("```json"):
            response = response[7:]
        elif response.startswith("```"):
            response = response[3:]
        
        if response.endswith("```"):
            response = response[:-3]
        
        response = response.strip()
        
        try:
            plan = json.loads(response)
            return plan
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [SOPPlanner] JSON è§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response[:500]}")
            
            # å°è¯•æå– JSON å¯¹è±¡ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
            import re
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                try:
                    plan = json.loads(json_match.group())
                    logger.info("âœ… [SOPPlanner] ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆåŠŸæå– JSON")
                    return plan
                except:
                    pass
            
            raise ValueError(f"æ— æ³•è§£æ LLM å“åº”ä¸º JSON: {e}")
    
    def _validate_and_adapt_sop_plan(
        self,
        workflow_plan: Dict[str, Any],
        file_metadata: Dict[str, Any],
        retrieved_tools: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        éªŒè¯ SOP è®¡åˆ’å¹¶é€‚é…ä¸ºå‰ç«¯æ ¼å¼
        
        Args:
            workflow_plan: LLM ç”Ÿæˆçš„åŸå§‹è®¡åˆ’
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®
            retrieved_tools: æ£€ç´¢åˆ°çš„å·¥å…·åˆ—è¡¨
        
        Returns:
            éªŒè¯å’Œé€‚é…åçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        """
        # éªŒè¯åŸºæœ¬ç»“æ„
        if "name" not in workflow_plan:
            workflow_plan["name"] = "ä»£è°¢ç»„å­¦åˆ†ææµç¨‹"
        
        if "steps" not in workflow_plan or not isinstance(workflow_plan["steps"], list):
            raise ValueError("å·¥ä½œæµè®¡åˆ’å¿…é¡»åŒ…å« 'steps' æ•°ç»„")
        
        # è·å–æ–‡ä»¶è·¯å¾„
        file_path = file_metadata.get("file_path", "") if file_metadata else ""
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Fail-Fast Logic
        # æ£€æŸ¥ semantic_mapï¼Œå¦‚æœ group_cols ä¸ºç©ºï¼Œç§»é™¤æ‰€æœ‰ç›‘ç£æ­¥éª¤
        semantic_map = file_metadata.get("semantic_map", {}) if file_metadata else {}
        group_cols = semantic_map.get("group_cols", [])
        has_groups = len(group_cols) > 0
        
        if not has_groups:
            logger.warning("âš ï¸ [SOPPlanner] Fail-Fast: group_cols ä¸ºç©ºï¼Œå°†ç§»é™¤æ‰€æœ‰ç›‘ç£æ­¥éª¤")
        
        # åˆ›å»ºå·¥å…·åç§°åˆ°å·¥å…·ä¿¡æ¯çš„æ˜ å°„
        tool_map = {tool['name']: tool for tool in retrieved_tools}
        
        # ğŸ”¥ CRITICAL FIX: ç¡¬åˆ é™¤æ‰€æœ‰ visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰
        # ä¸å†éœ€è¦æ£€æŸ¥ï¼Œç›´æ¥ç§»é™¤æ‰€æœ‰ visualize_pca
        original_steps = workflow_plan.get("steps", [])
        workflow_plan["steps"] = [
            step for step in original_steps
            if step.get("id") != "visualize_pca" and 
               step.get("tool_id") != "visualize_pca" and 
               step.get("tool_name") != "visualize_pca"
        ]
        removed_count = len(original_steps) - len(workflow_plan["steps"])
        if removed_count > 0:
            logger.info(f"âœ… [SOPPlanner] ç¡¬åˆ é™¤ {removed_count} ä¸ª visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰")
        
        # ğŸ”¥ Fail-Fast: å¦‚æœ group_cols ä¸ºç©ºï¼Œç§»é™¤æ‰€æœ‰ç›‘ç£æ­¥éª¤
        if not has_groups:
            supervised_tools = ["metabolomics_plsda", "differential_analysis", "visualize_volcano", "metabolomics_pathway_enrichment"]
            before_failfast = len(workflow_plan["steps"])
            workflow_plan["steps"] = [
                step for step in workflow_plan["steps"]
                if step.get("id") not in supervised_tools and 
                   step.get("tool_id") not in supervised_tools and 
                   step.get("tool_name") not in supervised_tools
            ]
            removed_supervised = before_failfast - len(workflow_plan["steps"])
            if removed_supervised > 0:
                logger.info(f"âœ… [SOPPlanner] Fail-Fast: ç§»é™¤ {removed_supervised} ä¸ªç›‘ç£æ­¥éª¤ï¼ˆæ— åˆ†ç»„åˆ—ï¼‰")
        
        # é€‚é…æ­¥éª¤æ ¼å¼
        adapted_steps = []
        for i, step in enumerate(workflow_plan["steps"], 1):
            # éªŒè¯å·¥å…·å­˜åœ¨æ€§
            tool_id = step.get("id") or step.get("tool_id") or step.get("tool_name")
            if not tool_id:
                logger.warning(f"âš ï¸ [SOPPlanner] æ­¥éª¤ {i} ç¼ºå°‘ tool_idï¼Œè·³è¿‡")
                continue
            
            # ğŸ”¥ CRITICAL FIX: ç¡¬åˆ é™¤ visualize_pcaï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰
            if tool_id == "visualize_pca":
                logger.warning(f"âš ï¸ [SOPPlanner] ç¡¬åˆ é™¤ visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰")
                continue
            
            # æ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨æ³¨å†Œè¡¨ä¸­
            tool_metadata_obj = registry.get_metadata(tool_id)
            if not tool_metadata_obj:
                logger.warning(f"âš ï¸ [SOPPlanner] å·¥å…· '{tool_id}' ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œè·³è¿‡")
                continue
            
            # è·å–å·¥å…·ä¿¡æ¯
            tool_info = tool_map.get(tool_id, {})
            tool_desc = tool_metadata_obj.description
            
            # å¤„ç†å‚æ•°
            params = step.get("params", {})
            adapted_params = {}
            
            # è‡ªåŠ¨å¡«å…… file_path
            if "file_path" not in params and file_path:
                adapted_params["file_path"] = file_path
            elif "file_path" in params:
                adapted_params["file_path"] = params["file_path"]
            
            # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 2 - Use semantic_map for group_column
            # è‡ªåŠ¨å¡«å…… group_columnï¼ˆä¼˜å…ˆä½¿ç”¨ semantic_map['group_cols']ï¼‰
            if "group_column" not in params and file_metadata:
                semantic_map = file_metadata.get("semantic_map", {})
                group_cols = semantic_map.get("group_cols", [])
                if group_cols:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†ç»„åˆ—
                    adapted_params["group_column"] = group_cols[0]
                    logger.info(f"âœ… [SOPPlanner] ä» semantic_map è‡ªåŠ¨å¡«å…… group_column: {group_cols[0]}")
                else:
                    # å›é€€åˆ°æ—§é€»è¾‘ï¼ˆå…¼å®¹æ€§ï¼‰
                    metadata_cols = file_metadata.get("metadata_columns", [])
                    if metadata_cols:
                        adapted_params["group_column"] = metadata_cols[0]
            
            # ğŸ”¥ CRITICAL: éªŒè¯ group_column æ˜¯å¦åœ¨ semantic_map['group_cols'] ä¸­
            if "group_column" in adapted_params and file_metadata:
                semantic_map = file_metadata.get("semantic_map", {})
                group_cols = semantic_map.get("group_cols", [])
                planned_group_col = adapted_params.get("group_column")
                if group_cols and planned_group_col not in group_cols:
                    logger.warning(f"âš ï¸ [SOPPlanner] group_column '{planned_group_col}' ä¸åœ¨ semantic_map['group_cols'] ä¸­ï¼Œè‡ªåŠ¨æ›¿æ¢ä¸º: {group_cols[0]}")
                    adapted_params["group_column"] = group_cols[0]
            
            # å¤åˆ¶å…¶ä»–å‚æ•°
            for param_name, param_value in params.items():
                if param_name not in ["file_path", "group_column"]:  # é¿å…é‡å¤
                    adapted_params[param_name] = param_value
            
            # æ„å»ºé€‚é…åçš„æ­¥éª¤ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
            # ğŸ”¥ Frontend Contract: å¿…é¡»åŒ…å« step_id, tool_id, name, step_name, desc
            step_display_name = step.get("name", self._get_step_display_name(tool_id, tool_desc))
            step_description = step.get("description", tool_desc[:100] if tool_desc else "")
            
            adapted_step = {
                "step_id": tool_id,  # ğŸ”¥ Frontend requires step_id (not just id)
                "id": tool_id,  # ä¿ç•™ id ä½œä¸ºå…¼å®¹å­—æ®µ
                "tool_id": tool_id,  # Frontend requires tool_id
                "name": step_display_name,  # Frontend requires name
                "step_name": step_display_name,  # Frontend requires step_name (compatibility)
                "description": step_description,  # å®Œæ•´æè¿°
                "desc": step_description[:100] if len(step_description) > 100 else step_description,  # Frontend requires desc (truncated)
                "selected": step.get("selected", True),  # Frontend may use this
                "params": adapted_params
            }
            
            adapted_steps.append(adapted_step)
        
        # æ„å»ºæœ€ç»ˆçš„å·¥ä½œæµé…ç½®ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        workflow_config = {
            "type": "workflow_config",
            "workflow_data": {
                "workflow_name": workflow_plan.get("name", "ä»£è°¢ç»„å­¦åˆ†ææµç¨‹"),
                "name": workflow_plan.get("name", "ä»£è°¢ç»„å­¦åˆ†ææµç¨‹"),  # å…¼å®¹å­—æ®µ
                "steps": adapted_steps
            },
            "file_paths": [file_path] if file_path else []
        }
        
        return workflow_config
    
    def _get_step_display_name(self, tool_name: str, tool_desc: str) -> str:
        """
        è·å–æ­¥éª¤çš„æ˜¾ç¤ºåç§°
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_desc: å·¥å…·æè¿°
        
        Returns:
            æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰
        """
        # åç§°æ˜ å°„ï¼ˆåŒ¹é…æ–°çš„å·¥å…· IDï¼‰
        name_mapping = {
            "inspect_data": "æ•°æ®æ£€æŸ¥",
            "preprocess_data": "æ•°æ®é¢„å¤„ç†",
            "pca_analysis": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "visualize_pca": "PCA å¯è§†åŒ–",
            "differential_analysis": "å·®å¼‚ä»£è°¢ç‰©åˆ†æ",
            "visualize_volcano": "ç«å±±å›¾",
            "metabolomics_plsda": "PLS-DA åˆ†æ",
            "metabolomics_pathway_enrichment": "é€šè·¯å¯Œé›†åˆ†æ",
            "metabolomics_heatmap": "çƒ­å›¾"
        }
        
        if tool_name in name_mapping:
            return name_mapping[tool_name]
        
        # é»˜è®¤ï¼šä½¿ç”¨å·¥å…·åç§°ï¼ˆç¾åŒ–ï¼‰
        return tool_name.replace("_", " ").title()


class RNAPlanner(SOPPlanner):
    """
    scRNA-seq ç‰¹å®šçš„ SOP è§„åˆ’å™¨
    
    ç»§æ‰¿è‡ª SOPPlannerï¼Œä½†ä½¿ç”¨ scRNA-seq ç‰¹å®šçš„ SOP è§„åˆ™ã€‚
    """
    
    def _build_sop_system_prompt(self) -> str:
        """
        æ„å»º scRNA-seq ç‰¹å®šçš„ SOP ç³»ç»Ÿæç¤ºè¯
        
        Returns:
            åŒ…å« scRNA-seq SOP è§„åˆ™çš„ç³»ç»Ÿæç¤ºè¯
        """
        return """You are an expert Bioinformatics Pipeline Architect specializing in Single-Cell RNA-seq (scRNA-seq) data analysis.

Your task is to generate executable workflow plans that STRICTLY follow the scRNA-seq Standard Operating Procedure (SOP).

**CRITICAL SOP RULES (MUST FOLLOW):**

1. **Input Type Detection (MANDATORY FIRST STEP):**
   - IF input is FASTQ files (.fastq, .fq) â†’ MUST start with CellRanger (rna_cellranger_count) - This runs ASYNCHRONOUSLY
   - IF input is Matrix/H5AD (.h5ad, .mtx, 10x directory) â†’ Start directly with QC (rna_qc_filter)

2. **Quality Control (ALWAYS REQUIRED AFTER INPUT PROCESSING):**
   - ALWAYS perform QC filtering (rna_qc_filter)
   - Filter cells based on: min_genes (default 200), max_mt (default 20%)
   - ALWAYS perform Doublet Detection (rna_doublet_detection) after QC
   - Generate QC visualization (rna_visualize_qc)

3. **Preprocessing Pipeline (MANDATORY SEQUENCE):**
   - ALWAYS perform Normalization (rna_normalize) - LogNormalize with target_sum=1e4
   - ALWAYS find Highly Variable Genes (rna_hvg) - default n_top_genes=2000
   - ALWAYS scale data (rna_scale) - for PCA preparation

4. **Dimensionality Reduction (REQUIRED):**
   - ALWAYS perform PCA (rna_pca) - default n_comps=50
   - ALWAYS compute Neighbors (rna_neighbors) - default n_neighbors=15
   - ALWAYS perform UMAP (rna_umap) - for visualization
   - OPTIONAL: t-SNE (rna_tsne) - alternative visualization

5. **Clustering (REQUIRED):**
   - ALWAYS perform Leiden Clustering (rna_clustering) - default resolution=0.5
   - Generate clustering visualization (rna_visualize_clustering)

6. **Marker Detection & Annotation (REQUIRED):**
   - ALWAYS find Marker Genes (rna_find_markers) - for each cluster
   - ALWAYS perform Cell Type Annotation (rna_cell_annotation) - using markers
   - Generate marker visualization (rna_visualize_markers)

7. **Export (FINAL STEP):**
   - ALWAYS export results (rna_export_results) - H5AD, CSVs, figures ZIP

**OUTPUT CONTRACT (CRITICAL - MUST MATCH EXACTLY):**

You MUST output a JSON object that matches the existing Frontend UI structure EXACTLY:

```json
{
  "name": "Generated Pipeline Name",
  "steps": [
    {
      "id": "tool_name_from_registry",
      "name": "Human Readable Step Name",
      "description": "Why this step is needed (based on SOP rules)",
      "selected": true,
      "params": {
        "adata_path": "<auto-filled from metadata>",
        "param1": "value1",
        "param2": "value2"
      }
    }
  ]
}
```

**Step Structure Requirements:**
- "id": MUST be the exact tool name from the retrieved tools (e.g., "rna_qc_filter", "rna_normalize")
- "name": Human-readable name in Chinese (e.g., "è´¨é‡æ§åˆ¶", "æ•°æ®æ ‡å‡†åŒ–")
- "description": Brief explanation of why this step is needed, referencing SOP rules
- "selected": Always true (all steps are selected by default)
- "params": Dictionary matching the tool's args_schema

**Parameter Auto-Filling Rules:**
- adata_path: Use the file_path from file_metadata (or output from previous step)
- For CellRanger: fastqs_path, transcriptome_path, output_dir should be auto-filled
- min_genes: Default to 200
- max_mt: Default to 20.0
- n_top_genes: Default to 2000
- resolution: Default to 0.5 for clustering
- Other parameters: Use sensible defaults from the tool's args_schema

**Example Output:**
{
  "name": "å•ç»†èƒè½¬å½•ç»„æ ‡å‡†åˆ†ææµç¨‹",
  "steps": [
    {
      "id": "rna_qc_filter",
      "name": "è´¨é‡æ§åˆ¶è¿‡æ»¤",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»é¦–å…ˆè¿›è¡Œè´¨é‡æ§åˆ¶ï¼Œè¿‡æ»¤ä½è´¨é‡ç»†èƒå’Œçº¿ç²’ä½“åŸºå› ",
      "selected": true,
      "params": {
        "adata_path": "/app/uploads/data.h5ad",
        "min_genes": 200,
        "max_mt": 20.0
      }
    },
    {
      "id": "rna_normalize",
      "name": "æ•°æ®æ ‡å‡†åŒ–",
      "description": "SOPè§„åˆ™ï¼šå¿…é¡»è¿›è¡ŒLogNormalizeæ ‡å‡†åŒ–ï¼Œä¸ºåç»­åˆ†æåšå‡†å¤‡",
      "selected": true,
      "params": {
        "adata_path": "<step1_output>",
        "target_sum": 10000
      }
    }
  ]
}

Generate ONLY the JSON object, no markdown code blocks, no additional text."""
    
    def _get_step_display_name(self, tool_name: str, tool_desc: str) -> str:
        """
        è·å–æ­¥éª¤çš„æ˜¾ç¤ºåç§°ï¼ˆscRNA-seq ç‰¹å®šï¼‰
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_desc: å·¥å…·æè¿°
        
        Returns:
            æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰
        """
        # scRNA-seq ç‰¹å®šçš„åç§°æ˜ å°„
        name_mapping = {
            "rna_cellranger_count": "Cell Ranger è®¡æ•°ï¼ˆå¼‚æ­¥ï¼‰",
            "rna_convert_cellranger_to_h5ad": "è½¬æ¢ä¸º H5AD æ ¼å¼",
            "rna_qc_filter": "è´¨é‡æ§åˆ¶è¿‡æ»¤",
            "rna_doublet_detection": "åŒè”ä½“æ£€æµ‹",
            "rna_normalize": "æ•°æ®æ ‡å‡†åŒ–",
            "rna_hvg": "é«˜å˜åŸºå› ç­›é€‰",
            "rna_scale": "æ•°æ®ç¼©æ”¾",
            "rna_pca": "ä¸»æˆåˆ†åˆ†æ (PCA)",
            "rna_neighbors": "æ„å»ºé‚»æ¥å›¾",
            "rna_umap": "UMAP é™ç»´",
            "rna_tsne": "t-SNE é™ç»´",
            "rna_clustering": "Leiden èšç±»",
            "rna_find_markers": "Marker åŸºå› æ£€æµ‹",
            "rna_cell_annotation": "ç»†èƒç±»å‹æ³¨é‡Š",
            "rna_visualize_qc": "QC å¯è§†åŒ–",
            "rna_visualize_clustering": "èšç±»å¯è§†åŒ–",
            "rna_visualize_markers": "Marker å¯è§†åŒ–",
            "rna_export_results": "ç»“æœå¯¼å‡º"
        }
        
        if tool_name in name_mapping:
            return name_mapping[tool_name]
        
        # é»˜è®¤ï¼šä½¿ç”¨å·¥å…·åç§°ï¼ˆç¾åŒ–ï¼‰
        return tool_name.replace("_", " ").title()
