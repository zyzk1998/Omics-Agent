"""
é€šç”¨å·¥ä½œæµæ‰§è¡Œå™¨ - The Hands

åŠ¨æ€æ‰§è¡Œå·¥ä½œæµï¼Œä¸ä¾èµ–ç¡¬ç¼–ç çš„å·¥å…·é€»è¾‘ã€‚
ä½¿ç”¨ ToolRegistry æŸ¥æ‰¾å’Œæ‰§è¡Œå·¥å…·ã€‚
"""
import os
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
from datetime import datetime

from .tool_registry import registry
from .utils import sanitize_for_json

logger = logging.getLogger(__name__)


class WorkflowExecutor:
    """
    å·¥ä½œæµæ‰§è¡Œå™¨
    
    èŒè´£ï¼š
    1. ä» ToolRegistry æŸ¥æ‰¾å·¥å…·
    2. éªŒè¯å‚æ•°
    3. æ‰§è¡Œå·¥å…·
    4. å¤„ç†æ­¥éª¤é—´çš„æ•°æ®æµ
    5. ç”Ÿæˆç¬¦åˆå‰ç«¯æ ¼å¼çš„æ‰§è¡ŒæŠ¥å‘Š
    """
    
    def __init__(self, output_dir: Optional[str] = None, upload_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµæ‰§è¡Œå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œå°†åœ¨æ‰§è¡Œæ—¶åˆ›å»ºï¼‰
            upload_dir: ä¸Šä¼ ç›®å½•ï¼ˆç”¨äºè§£æç›¸å¯¹è·¯å¾„ï¼‰
        """
        self.output_dir = output_dir
        self.upload_dir = upload_dir or os.getenv("UPLOAD_DIR", "/app/uploads")
        self.step_results: Dict[str, Any] = {}  # å­˜å‚¨æ­¥éª¤ç»“æœï¼Œç”¨äºæ•°æ®æµä¼ é€’
    
    def _resolve_file_path(self, file_path: str) -> str:
        """
        ğŸ”¥ CRITICAL REGRESSION FIX: è§£ææ–‡ä»¶è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
        
        æ£€æŸ¥é¡ºåºï¼š
        1. å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ä¸”å­˜åœ¨ -> ç›´æ¥ä½¿ç”¨
        2. å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ -> å‰ç½® UPLOAD_DIR
        3. å¦‚æœåªæ˜¯æ–‡ä»¶å -> åœ¨ UPLOAD_DIR ä¸­æœç´¢
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½æ˜¯ç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„æˆ–æ–‡ä»¶åï¼‰
            
        Returns:
            è§£æåçš„ç»å¯¹è·¯å¾„
        """
        if not file_path or file_path in ["<å¾…ä¸Šä¼ æ•°æ®>", "<PENDING_UPLOAD>", ""]:
            return file_path
        
        original_path = file_path
        upload_dir_path = Path(self.upload_dir)
        
        # Check 1: Is it already absolute and existing?
        if os.path.isabs(file_path):
            path_obj = Path(file_path)
            if path_obj.exists():
                logger.info(f"âœ… [Executor] è·¯å¾„å·²ä¸ºç»å¯¹è·¯å¾„ä¸”å­˜åœ¨: {file_path}")
                return str(path_obj.resolve())
            else:
                logger.warning(f"âš ï¸ [Executor] ç»å¯¹è·¯å¾„ä¸å­˜åœ¨: {file_path}ï¼Œå°è¯•åœ¨ UPLOAD_DIR ä¸­æŸ¥æ‰¾")
        
        # Check 2: Is it relative? (e.g., "guest/.../cow_diet.csv")
        # Try prepending UPLOAD_DIR
        if not os.path.isabs(file_path):
            # Remove leading slash if present (e.g., "/guest/..." -> "guest/...")
            file_path_clean = file_path.lstrip('/')
            potential_path = upload_dir_path / file_path_clean
            
            if potential_path.exists():
                resolved = str(potential_path.resolve())
                logger.info(f"âœ… [Executor] è§£æç›¸å¯¹è·¯å¾„: {original_path} -> {resolved}")
                return resolved
            
            # Try with original relative path
            potential_path2 = upload_dir_path / file_path
            if potential_path2.exists():
                resolved = str(potential_path2.resolve())
                logger.info(f"âœ… [Executor] è§£æç›¸å¯¹è·¯å¾„ï¼ˆåŸå§‹ï¼‰: {original_path} -> {resolved}")
                return resolved
        
        # Check 3: Is it just a filename? (e.g., "cow_diet.csv")
        # Search in UPLOAD_DIR recursively
        filename = Path(file_path).name
        if filename == file_path or '/' not in file_path.replace('\\', '/'):
            logger.info(f"ğŸ” [Executor] æœç´¢æ–‡ä»¶å: {filename} åœ¨ {self.upload_dir}")
            for found_path in upload_dir_path.rglob(filename):
                if found_path.is_file():
                    resolved = str(found_path.resolve())
                    logger.info(f"âœ… [Executor] æ‰¾åˆ°æ–‡ä»¶: {original_path} -> {resolved}")
                    return resolved
        
        # If all checks fail, try to construct absolute path anyway
        if not os.path.isabs(file_path):
            final_path = upload_dir_path / file_path.lstrip('/')
            resolved = str(final_path.resolve())
            logger.warning(f"âš ï¸ [Executor] æ— æ³•éªŒè¯è·¯å¾„å­˜åœ¨ï¼Œä½†æ„é€ ç»å¯¹è·¯å¾„: {original_path} -> {resolved}")
            return resolved
        
        # Return original if already absolute (even if doesn't exist)
        logger.warning(f"âš ï¸ [Executor] æ— æ³•è§£æè·¯å¾„ï¼Œè¿”å›åŸå§‹å€¼: {original_path}")
        return original_path
    
    def execute_step(
        self,
        step_data: Dict[str, Any],
        step_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤
        
        Args:
            step_data: æ­¥éª¤æ•°æ®ï¼ŒåŒ…å« tool_id å’Œ params
            step_context: æ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å‰åºæ­¥éª¤çš„è¾“å‡ºç­‰ï¼‰
        
        Returns:
            æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        step_id = step_data.get("step_id", "unknown")
        tool_id = step_data.get("tool_id")
        params = step_data.get("params", {})
        step_name = step_data.get("name", tool_id)
        
        logger.info(f"ğŸ”§ æ‰§è¡Œæ­¥éª¤: {step_id} ({tool_id})")
        
        if not tool_id:
            error_msg = f"æ­¥éª¤ {step_id} ç¼ºå°‘ tool_id"
            logger.error(f"âŒ {error_msg}")
            return {
                "status": "error",
                "step_id": step_id,
                "step_name": step_name,
                "error": error_msg,
                "message": error_msg
            }
        
        # æŸ¥æ‰¾å·¥å…·
        tool_func = registry.get_tool(tool_id)
        if not tool_func:
            error_msg = f"å·¥å…· '{tool_id}' æœªåœ¨æ³¨å†Œè¡¨ä¸­æ‰¾åˆ°"
            logger.error(f"âŒ {error_msg}")
            return {
                "status": "error",
                "step_id": step_id,
                "step_name": step_name,
                "error": error_msg,
                "message": error_msg
            }
        
        # ğŸ”¥ å‚æ•°æ˜ å°„ï¼šæ ¹æ®å·¥å…·ç±»åˆ«æ˜ å°„æ–‡ä»¶è·¯å¾„å‚æ•°
        tool_metadata = registry.get_metadata(tool_id)
        tool_category = tool_metadata.category if tool_metadata else None
        
        # ç¡®å®šå·¥å…·æœŸæœ›çš„æ–‡ä»¶è·¯å¾„å‚æ•°å
        if tool_category == "scRNA-seq":
            # RNA å·¥å…·ä½¿ç”¨ adata_path
            file_param_name = "adata_path"
            # å¦‚æœæä¾›äº† file_path ä½†æ²¡æœ‰ adata_pathï¼Œè¿›è¡Œæ˜ å°„
            if "file_path" in params and file_param_name not in params:
                params[file_param_name] = params.pop("file_path")
                logger.info(f"ğŸ”„ [Executor] å‚æ•°æ˜ å°„: file_path -> {file_param_name} (å·¥å…·: {tool_id})")
        else:
            # å…¶ä»–å·¥å…·ï¼ˆå¦‚ä»£è°¢ç»„å­¦ï¼‰ä½¿ç”¨ file_path
            file_param_name = "file_path"
        
        # éªŒè¯å‚æ•°ï¼ˆå¯é€‰ä½†æ¨èï¼‰
        try:
            if tool_metadata:
                # ä½¿ç”¨ Pydantic schema éªŒè¯å‚æ•°
                validated_params = tool_metadata.args_schema(**params)
                params = validated_params.model_dump()
                logger.debug(f"âœ… å‚æ•°éªŒè¯é€šè¿‡: {step_id}")
        except Exception as validation_err:
            logger.warning(f"âš ï¸ å‚æ•°éªŒè¯å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰: {validation_err}")
            # ç»§ç»­æ‰§è¡Œï¼Œä¸å› éªŒè¯å¤±è´¥è€Œä¸­æ–­
        
        # å¤„ç†æ•°æ®æµï¼šæ›¿æ¢å ä½ç¬¦ï¼ˆä¼ é€’å·¥å…·ç±»åˆ«ä¿¡æ¯ï¼‰
        processed_params = self._process_data_flow(params, step_context, tool_category=tool_category)
        
        # ğŸ”¥ CRITICAL FIX: å¯¹äº scRNA-seq å·¥å…·ï¼Œç¡®ä¿ç§»é™¤ file_path å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if tool_category == "scRNA-seq" and "file_path" in processed_params:
            # å¦‚æœå·²ç»æœ‰ adata_pathï¼Œç§»é™¤ file_path
            if "adata_path" in processed_params:
                del processed_params["file_path"]
                logger.info(f"ğŸ”„ [Executor] ç§»é™¤å¤šä½™çš„ file_path å‚æ•°ï¼ˆå·¥å…·å·²æœ‰ adata_pathï¼‰")
            else:
                # å¦‚æœæ²¡æœ‰ adata_pathï¼Œå°† file_path æ˜ å°„ä¸º adata_path
                processed_params["adata_path"] = processed_params.pop("file_path")
                logger.info(f"ğŸ”„ [Executor] å‚æ•°æ˜ å°„: file_path -> adata_path (å·¥å…·: {tool_id})")
        
        # ğŸ”¥ CRITICAL FIX: å¯¹äº visualize_volcanoï¼Œè‡ªåŠ¨æ³¨å…¥ diff_resultsï¼ˆå¦‚æœç¼ºå¤±ï¼‰
        if tool_id == "visualize_volcano":
            # å¦‚æœ diff_results ç¼ºå¤±æˆ–ä»ç„¶æ˜¯å ä½ç¬¦ï¼Œå°è¯•ä» differential_analysis æ­¥éª¤è·å–
            if "diff_results" not in processed_params or (
                isinstance(processed_params.get("diff_results"), str) and 
                processed_params.get("diff_results", "").startswith("<")
            ):
                # æŸ¥æ‰¾ differential_analysis æ­¥éª¤çš„ç»“æœ
                diff_result = None
                for step_id, step_result in self.step_results.items():
                    if step_id == "differential_analysis" or "differential" in step_id.lower():
                        # step_results å­˜å‚¨çš„æ˜¯å·¥å…·è¿”å›çš„åŸå§‹ç»“æœ
                        if isinstance(step_result, dict):
                            diff_result = step_result
                            break
                
                if isinstance(diff_result, dict):
                    processed_params["diff_results"] = diff_result
                    logger.info(f"âœ… [Executor] è‡ªåŠ¨æ³¨å…¥ diff_results åˆ° visualize_volcano")
                else:
                    logger.error(f"âŒ [Executor] æ— æ³•æ‰¾åˆ° differential_analysis ç»“æœï¼Œvisualize_volcano å°†å¤±è´¥")
            
            # è¿‡æ»¤æ‰å·¥å…·ä¸æ¥å—çš„å‚æ•°ï¼ˆä¿ç•™ diff_resultsï¼‰
            allowed_params = {"diff_results", "output_path", "fdr_threshold", "log2fc_threshold"}
            filtered_params = {k: v for k, v in processed_params.items() if k in allowed_params}
            if len(filtered_params) < len(processed_params):
                removed = set(processed_params.keys()) - allowed_params
                logger.warning(f"âš ï¸ [Executor] ç§»é™¤ {tool_id} ä¸æ¥å—çš„å‚æ•°: {removed}")
            processed_params = filtered_params
        
        # ğŸ”¥ CRITICAL REGRESSION FIX: Normalize all path-like parameters before tool execution
        path_params = ["file_path", "adata_path", "output_path", "output_file", "fastq_path", "reference_path", "output_h5ad"]
        for param_name in path_params:
            if param_name in processed_params:
                original_path = processed_params[param_name]
                if original_path and isinstance(original_path, str):
                    # Skip placeholder values
                    if original_path not in ["<å¾…ä¸Šä¼ æ•°æ®>", "<PENDING_UPLOAD>", ""]:
                        resolved_path = self._resolve_file_path(original_path)
                        if resolved_path != original_path:
                            logger.info(f"ğŸ”„ [Executor] è§£æè·¯å¾„å‚æ•° {param_name}: {original_path} -> {resolved_path}")
                        processed_params[param_name] = resolved_path
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 3 - Pre-Flight Check & Auto-Correction
        # å¯¹äºéœ€è¦ group_column çš„å·¥å…·ï¼ŒéªŒè¯åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ semantic_map è‡ªåŠ¨ä¿®æ­£
        tools_requiring_group_column = ["differential_analysis", "metabolomics_plsda", "metabolomics_pathway_enrichment"]
        if tool_id in tools_requiring_group_column and "group_column" in processed_params:
            group_column = processed_params.get("group_column")
            file_path = processed_params.get("file_path")
            
            if file_path and os.path.exists(file_path):
                try:
                    import pandas as pd
                    # Pre-Flight Check: è¯»å–æ–‡ä»¶æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                    df = pd.read_csv(file_path, index_col=0, nrows=1)  # åªè¯»ç¬¬ä¸€è¡Œæ£€æŸ¥åˆ—å
                    
                    # ğŸ”¥ ä¿®å¤ï¼šå…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ï¼‰
                    group_column_found = group_column in df.columns
                    matched_column = None
                    
                    if not group_column_found:
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿
                        group_column_normalized = group_column.lower().replace(' ', '').replace('_', '').replace('-', '')
                        for col in df.columns:
                            col_normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                            if col_normalized == group_column_normalized:
                                matched_column = col
                                logger.info(f"ğŸ”„ [Executor] æ¨¡ç³ŠåŒ¹é…åˆ†ç»„åˆ—: '{group_column}' -> '{col}'")
                                break
                        
                        if matched_column:
                            group_column = matched_column
                            group_column_found = True
                            processed_params["group_column"] = matched_column
                    
                    if not group_column_found:
                        logger.warning(f"âš ï¸ [Executor] Pre-Flight Check: åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­")
                        
                        # Auto-Correction: å°è¯•è·å– semantic_map
                        semantic_map = None
                        
                        # æ–¹æ³•1: ä» step_context è·å–ï¼ˆå¦‚æœä¼ é€’äº†ï¼‰
                        if step_context:
                            file_metadata = step_context.get("file_metadata")
                            if file_metadata:
                                semantic_map = file_metadata.get("semantic_map", {})
                        
                        # æ–¹æ³•2: å¦‚æœæœªæ‰¾åˆ°ï¼Œé‡æ–°è¯»å–æ–‡ä»¶å…ƒæ•°æ®ï¼ˆä½¿ç”¨ FileInspectorï¼‰
                        if not semantic_map:
                            try:
                                from .file_inspector import FileInspector
                                # os å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œæ— éœ€é‡å¤å¯¼å…¥
                                upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
                                inspector = FileInspector(upload_dir)
                                file_metadata = inspector.inspect_file(file_path)
                                if file_metadata.get("status") == "success":
                                    semantic_map = file_metadata.get("semantic_map", {})
                                    logger.info(f"âœ… [Executor] é‡æ–°è¯»å– file_metadata è·å– semantic_map")
                            except Exception as e:
                                logger.debug(f"âš ï¸ [Executor] æ— æ³•é‡æ–°è¯»å– file_metadata: {e}")
                        
                        # å¦‚æœæ‰¾åˆ° semantic_mapï¼Œä½¿ç”¨å®ƒè¿›è¡Œè‡ªåŠ¨ä¿®æ­£
                        if semantic_map:
                            group_cols = semantic_map.get("group_cols", [])
                            if len(group_cols) == 1:
                                # å¦‚æœ FileInspector æ‰¾åˆ°äº†æ°å¥½ä¸€ä¸ªåˆ†ç»„åˆ—ï¼Œè‡ªåŠ¨ä½¿ç”¨å®ƒ
                                auto_group_col = group_cols[0]
                                logger.info(f"âœ… [Executor] Auto-Correction: ä½¿ç”¨ semantic_map ä¸­çš„å”¯ä¸€åˆ†ç»„åˆ—: {auto_group_col}")
                                processed_params["group_column"] = auto_group_col
                            elif len(group_cols) > 1:
                                # å¦‚æœæœ‰å¤šä¸ªï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
                                auto_group_col = group_cols[0]
                                logger.info(f"âœ… [Executor] Auto-Correction: ä½¿ç”¨ semantic_map ä¸­çš„ç¬¬ä¸€ä¸ªåˆ†ç»„åˆ—: {auto_group_col}")
                                processed_params["group_column"] = auto_group_col
                            else:
                                logger.error(f"âŒ [Executor] semantic_map['group_cols'] ä¸ºç©ºï¼Œæ— æ³•è‡ªåŠ¨ä¿®æ­£")
                        else:
                            # å›é€€åˆ°å¯å‘å¼æ£€æµ‹
                            logger.warning(f"âš ï¸ [Executor] æœªæ‰¾åˆ° semantic_mapï¼Œä½¿ç”¨å¯å‘å¼æ£€æµ‹...")
                            detected_group_column = self._detect_group_column_from_file(file_path)
                            
                            if detected_group_column:
                                logger.info(f"âœ… [Executor] å¯å‘å¼æ£€æµ‹åˆ°åˆ†ç»„åˆ—: {detected_group_column}ï¼Œæ›¿æ¢ '{group_column}'")
                                processed_params["group_column"] = detected_group_column
                            else:
                                # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ—å
                                available_cols = list(df.columns)
                                logger.error(f"âŒ [Executor] æ— æ³•è‡ªåŠ¨æ£€æµ‹åˆ†ç»„åˆ—ã€‚å¯ç”¨åˆ—: {available_cols[:10]}")
                                # ä¸ç«‹å³å¤±è´¥ï¼Œè®©å·¥å…·è‡ªå·±å¤„ç†é”™è¯¯ï¼ˆå·¥å…·ä¼šè¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼‰
                except Exception as e:
                    logger.warning(f"âš ï¸ [Executor] Pre-Flight Check å¤±è´¥: {e}ï¼Œç»§ç»­æ‰§è¡Œ")
        
        # æ‰§è¡Œå·¥å…·
        try:
            logger.info(f"ğŸš€ è°ƒç”¨å·¥å…·: {tool_id} with params: {list(processed_params.keys())}")
            result = tool_func(**processed_params)
            
            # ç¡®ä¿ç»“æœæ˜¯å­—å…¸æ ¼å¼
            if not isinstance(result, dict):
                result = {
                    "status": "success",
                    "data": result,
                    "message": f"æ­¥éª¤ {step_name} æ‰§è¡Œå®Œæˆ"
                }
            
            # ç¡®ä¿åŒ…å« status å­—æ®µ
            if "status" not in result:
                result["status"] = "success"
            
            # ğŸ”¥ æ ¹æ®å·¥å…·è¿”å›çš„çŠ¶æ€è®°å½•æ­£ç¡®çš„æ—¥å¿—å’Œæ¶ˆæ¯
            tool_status = result.get("status", "success")
            if tool_status == "error":
                error_msg = result.get("error") or result.get("message") or f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥"
                logger.error(f"âŒ æ­¥éª¤ {step_id} æ‰§è¡Œå¤±è´¥: {error_msg}")
                # å­˜å‚¨ç»“æœä¾›åç»­æ­¥éª¤ä½¿ç”¨ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿå­˜å‚¨ï¼Œç”¨äºè°ƒè¯•ï¼‰
                self.step_results[step_id] = result
                return {
                    "status": "error",
                    "step_id": step_id,
                    "step_name": step_name,
                    "tool_id": tool_id,
                    "result": result,
                    "error": error_msg,
                    "message": error_msg  # ğŸ”¥ ä½¿ç”¨é”™è¯¯æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯"æ‰§è¡Œå®Œæˆ"
                }
            else:
                logger.info(f"âœ… æ­¥éª¤ {step_id} æ‰§è¡ŒæˆåŠŸ")
                # å­˜å‚¨ç»“æœä¾›åç»­æ­¥éª¤ä½¿ç”¨
                self.step_results[step_id] = result
                return {
                    "status": result.get("status", "success"),
                    "step_id": step_id,
                    "step_name": step_name,
                    "tool_id": tool_id,
                    "result": result,
                    "message": result.get("message", f"æ­¥éª¤ {step_name} æ‰§è¡Œå®Œæˆ")
                }
        
        except Exception as e:
            error_msg = f"æ­¥éª¤ {step_id} æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}", exc_info=True)
            
            return {
                "status": "error",
                "step_id": step_id,
                "step_name": step_name,
                "tool_id": tool_id,
                "error": str(e),
                "message": error_msg
            }
    
    def _process_data_flow(
        self,
        params: Dict[str, Any],
        step_context: Optional[Dict[str, Any]] = None,
        tool_category: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ•°æ®æµï¼šæ›¿æ¢å ä½ç¬¦ï¼ˆå¦‚ <step1_output>ï¼‰å’Œè‡ªåŠ¨æ³¨å…¥æ–‡ä»¶è·¯å¾„
        
        Args:
            params: åŸå§‹å‚æ•°
            step_context: æ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å« current_file_pathï¼‰
            tool_category: å·¥å…·ç±»åˆ«ï¼ˆç”¨äºç¡®å®šæ–‡ä»¶è·¯å¾„å‚æ•°åï¼‰
        
        Returns:
            å¤„ç†åçš„å‚æ•°
        """
        processed = {}
        
        # ğŸ”¥ æ ¹æ®å·¥å…·ç±»åˆ«ç¡®å®šæ–‡ä»¶è·¯å¾„å‚æ•°å
        if tool_category == "scRNA-seq":
            file_param_name = "adata_path"
        else:
            file_param_name = "file_path"
        
        # ğŸ”¥ å¦‚æœæ–‡ä»¶è·¯å¾„å‚æ•°ç¼ºå¤±ï¼Œå°è¯•ä»ä¸Šä¸‹æ–‡æ³¨å…¥
        if file_param_name not in params and step_context:
            current_file_path = step_context.get("current_file_path")
            if current_file_path:
                processed[file_param_name] = current_file_path
                logger.info(f"ğŸ”„ æ•°æ®æµ: è‡ªåŠ¨æ³¨å…¥ {file_param_name} = {current_file_path}")
        
        # ğŸ”¥ å¦‚æœæä¾›äº† file_path ä½†å·¥å…·éœ€è¦ adata_pathï¼Œè¿›è¡Œæ˜ å°„
        if tool_category == "scRNA-seq" and "file_path" in params and "adata_path" not in params:
            processed["adata_path"] = params.pop("file_path")
            logger.info(f"ğŸ”„ æ•°æ®æµ: å‚æ•°æ˜ å°„ file_path -> adata_path")
        
        for key, value in params.items():
            if isinstance(value, str) and value.startswith("<") and value.endswith(">"):
                # å ä½ç¬¦ï¼Œå°è¯•ä»ä¸Šä¸‹æ–‡æˆ–æ­¥éª¤ç»“æœä¸­è·å–
                placeholder = value[1:-1]  # ç§»é™¤ < >
                
                # ğŸ”¥ CRITICAL FIX: ç‰¹æ®Šå¤„ç† differential_analysis_output å ä½ç¬¦
                # ç”¨äº visualize_volcanoï¼Œéœ€è¦ä¼ é€’å®Œæ•´çš„ diff_results å­—å…¸
                if placeholder == "differential_analysis_output" or "differential_analysis" in placeholder.lower():
                    # æŸ¥æ‰¾ differential_analysis æ­¥éª¤çš„ç»“æœ
                    diff_result = None
                    for step_id, step_result in self.step_results.items():
                        if step_id == "differential_analysis" or "differential" in step_id.lower():
                            # step_results å­˜å‚¨çš„æ˜¯å·¥å…·è¿”å›çš„åŸå§‹ç»“æœ
                            if isinstance(step_result, dict):
                                diff_result = step_result
                                break
                    
                    if isinstance(diff_result, dict):
                        # å¯¹äº visualize_volcanoï¼Œéœ€è¦ä¼ é€’å®Œæ•´çš„ diff_results å­—å…¸
                        if key == "diff_results":
                            processed[key] = diff_result
                            logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> å®Œæ•´ diff_results å­—å…¸")
                            continue
                        else:
                            # å…¶ä»–æƒ…å†µï¼Œå°è¯•æå–ç‰¹å®šå­—æ®µ
                            output_path = (
                                diff_result.get("output_file") or
                                diff_result.get("output_path") or
                                diff_result.get("file_path")
                            )
                            if output_path:
                                processed[key] = output_path
                                logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {output_path}")
                                continue
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ° differential_analysis æ­¥éª¤ç»“æœï¼Œæ— æ³•è§£æ <{placeholder}>")
                
                # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼šä» differential_analysis ç»“æœä¸­æå–åˆ†ç»„ä¿¡æ¯
                if placeholder in ["differential_analysis_case_group", "differential_analysis_control_group"]:
                    # æŸ¥æ‰¾ differential_analysis æ­¥éª¤çš„ç»“æœ
                    diff_result = None
                    for step_id, step_result in self.step_results.items():
                        if step_id == "differential_analysis" or "differential" in step_id.lower():
                            # æ£€æŸ¥ step_result çš„ç»“æ„ï¼ˆå¯èƒ½æ˜¯åŒ…è£…åœ¨ result å­—æ®µä¸­ï¼‰
                            if isinstance(step_result, dict):
                                actual_result = step_result.get("result", step_result)
                                if isinstance(actual_result, dict):
                                    diff_result = actual_result
                                    break
                    
                    if isinstance(diff_result, dict):
                        # å°è¯•ä»å¤šä¸ªä½ç½®æå–åˆ†ç»„ä¿¡æ¯
                        summary = diff_result.get("summary", {})
                        if placeholder == "differential_analysis_case_group":
                            extracted_value = (
                                diff_result.get("case_group") or
                                diff_result.get("group1") or
                                summary.get("case_group") or
                                diff_result.get("experimental_group")
                            )
                        else:  # differential_analysis_control_group
                            extracted_value = (
                                diff_result.get("control_group") or
                                diff_result.get("group2") or
                                summary.get("control_group") or
                                diff_result.get("control_group")
                            )
                        
                        if extracted_value:
                            processed[key] = extracted_value
                            logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {extracted_value}")
                            continue
                        else:
                            logger.warning(f"âš ï¸ æ— æ³•ä» differential_analysis ç»“æœä¸­æå– {placeholder}")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ° differential_analysis æ­¥éª¤ç»“æœï¼Œæ— æ³•æå– {placeholder}")
                
                # å°è¯•ä» step_results ä¸­è·å–
                if placeholder in self.step_results:
                    step_result = self.step_results[placeholder]
                    # ğŸ”¥ CRITICAL FIX: å¯¹äº scRNA-seq å·¥å…·ï¼Œä¼˜å…ˆæå– output_h5ad
                    if isinstance(step_result, dict):
                        # å¯¹äº scRNA-seq å·¥å…·ï¼Œä¼˜å…ˆæŸ¥æ‰¾ output_h5ad
                        if tool_category == "scRNA-seq":
                            output_path = (
                                step_result.get("output_h5ad") or  # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ output_h5ad
                                step_result.get("output_file") or
                                step_result.get("output_path") or
                                step_result.get("file_path")
                            )
                        else:
                            # å…¶ä»–å·¥å…·ä½¿ç”¨æ ‡å‡†å­—æ®µ
                            output_path = (
                                step_result.get("output_file") or
                                step_result.get("output_path") or
                                step_result.get("file_path") or
                                step_result.get("plot_path") or
                                step_result.get("result_path") or
                                step_result.get("preprocessed_file")
                            )
                        if output_path:
                            processed[key] = output_path
                            logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {output_path}")
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è·¯å¾„ï¼Œä½¿ç”¨æ•´ä¸ªç»“æœ
                            processed[key] = step_result
                    else:
                        processed[key] = step_result
                elif step_context and placeholder in step_context:
                    processed[key] = step_context[placeholder]
                else:
                    # å ä½ç¬¦æœªè§£æï¼Œä¿æŒåŸæ ·ï¼ˆå¯èƒ½åç»­æ­¥éª¤ä¼šå¤„ç†ï¼‰
                    logger.warning(f"âš ï¸ æ— æ³•è§£æå ä½ç¬¦: {value}")
                    processed[key] = value
            else:
                processed[key] = value
        
        return processed
    
    def _is_image_file(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            å¦‚æœæ˜¯å›¾ç‰‡æ–‡ä»¶è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        if not file_path:
            return False
        
        # æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶æ‰©å±•å
        image_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.pdf', '.tiff', '.tif'}
        
        # è·å–æ–‡ä»¶æ‰©å±•åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        file_ext = os.path.splitext(file_path.lower())[1]
        
        return file_ext in image_extensions
    
    def _detect_group_column_from_file(self, file_path: str) -> Optional[str]:
        """
        ä»æ–‡ä»¶ä¸­è‡ªåŠ¨æ£€æµ‹åˆ†ç»„åˆ—
        
        ä½¿ç”¨å¯å‘å¼æ–¹æ³•ï¼š
        1. ä¼˜å…ˆæ£€æŸ¥åŒ…å«åˆ†ç»„å…³é”®è¯çš„éæ•°å€¼åˆ—
        2. æ£€æŸ¥å”¯ä¸€å€¼ <= 5 çš„æ•°å€¼åˆ—
        3. æ£€æŸ¥å”¯ä¸€å€¼ <= 5 çš„éæ•°å€¼åˆ—
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ£€æµ‹åˆ°çš„åˆ†ç»„åˆ—åï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
        """
        try:
            import pandas as pd
            
            # è¯»å–æ–‡ä»¶ï¼ˆé‡‡æ ·è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶é—®é¢˜ï¼‰
            df = pd.read_csv(file_path, index_col=0, nrows=1000)
            
            # ä¼˜å…ˆçº§å…³é”®è¯åˆ—è¡¨ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼Œå¦‚"Muscle loss"ä¼šåŒ¹é…åŒ…å«"Muscle"æˆ–"Loss"çš„åˆ—ï¼‰
            priority_keywords = ['Diet', 'diet', 'Group', 'group', 'Condition', 'condition', 
                                'Treatment', 'treatment', 'Class', 'class', 'Category', 'category',
                                'Type', 'type', 'Label', 'label', 'Status', 'status',
                                'Muscle', 'muscle', 'Loss', 'loss', 'MuscleLoss', 'muscleloss',
                                'Muscle_loss', 'muscle_loss']  # æ·»åŠ ç”¨æˆ·æ•°æ®ç›¸å…³çš„å…³é”®è¯
            
            # è¯†åˆ«åˆ—ç±»å‹
            metadata_cols = []
            numeric_cols = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    numeric_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            # æ–¹æ³•1: æ£€æŸ¥éæ•°å€¼åˆ—ï¼ˆmetadata_colsï¼‰ï¼Œä¼˜å…ˆå…³é”®è¯åŒ¹é…
            # ğŸ”¥ æ”¹è¿›ï¼šæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼Œå¦‚"Muscle loss"ä¼šåŒ¹é…"MuscleLoss"æˆ–"Muscle_loss"
            for col in metadata_cols:
                col_lower = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å…³é”®è¯ï¼ˆå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ï¼‰
                if any(keyword.lower().replace(' ', '').replace('_', '').replace('-', '') in col_lower 
                       for keyword in priority_keywords):
                    unique_count = df[col].nunique()
                    if 2 <= unique_count <= 20:  # åˆç†çš„åˆ†ç»„æ•°é‡
                        logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰: {col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                        return col
            
            # æ–¹æ³•2: æ£€æŸ¥æ•°å€¼åˆ—ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œå½“ä½œåˆ†ç±»å˜é‡
            for col in numeric_cols:
                unique_count = df[col].nunique()
                if 2 <= unique_count <= 5:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                    if any(keyword in col for keyword in priority_keywords):
                        logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆæ•°å€¼å‹å…³é”®è¯åŒ¹é…ï¼‰: {col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                        return col
                    # æˆ–è€…æ£€æŸ¥æ˜¯å¦æ˜¯äºŒå…ƒå˜é‡ï¼ˆ0/1ï¼‰
                    unique_values = sorted(df[col].dropna().unique().tolist())
                    if unique_values == [0, 1] or unique_values == [1, 0]:
                        logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆäºŒå…ƒæ•°å€¼å‹ï¼‰: {col}")
                        return col
            
            # æ–¹æ³•3: æ£€æŸ¥éæ•°å€¼åˆ—ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5
            for col in metadata_cols:
                unique_count = df[col].nunique()
                if 2 <= unique_count <= 5:
                    logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆéæ•°å€¼å‹ï¼Œå”¯ä¸€å€¼ <= 5ï¼‰: {col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                    return col
            
            # æ–¹æ³•4: å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ç¬¬ä¸€ä¸ª metadata_colsï¼ˆå¦‚æœæœ‰ï¼‰
            if metadata_cols:
                first_meta_col = metadata_cols[0]
                unique_count = df[first_meta_col].nunique()
                if 2 <= unique_count <= 20:
                    logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆé»˜è®¤ metadata_colsï¼‰: {first_meta_col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                    return first_meta_col
            
            logger.warning("âš ï¸ [Executor] æœªæ£€æµ‹åˆ°åˆé€‚çš„åˆ†ç»„åˆ—")
            return None
            
        except Exception as e:
            logger.error(f"âŒ [Executor] æ£€æµ‹åˆ†ç»„åˆ—å¤±è´¥: {e}", exc_info=True)
            return None
    
    def execute_workflow(
        self,
        workflow_data: Dict[str, Any],
        file_paths: List[str] = None,
        output_dir: Optional[str] = None,
        agent: Optional[Any] = None  # å¯é€‰çš„ Agent å®ä¾‹ï¼Œç”¨äºç”Ÿæˆè¯Šæ–­
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•´ä¸ªå·¥ä½œæµ
        
        Args:
            workflow_data: å·¥ä½œæµé…ç½®ï¼ˆåŒ…å« workflow_name å’Œ stepsï¼‰
            file_paths: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œå°†è‡ªåŠ¨åˆ›å»ºï¼‰
        
        Returns:
            æ‰§è¡ŒæŠ¥å‘Šï¼ˆç¬¦åˆå‰ç«¯ analysis_report æ ¼å¼ï¼‰
        """
        workflow_name = workflow_data.get("workflow_name", "Unknown Workflow")
        steps = workflow_data.get("steps", [])
        
        logger.info("=" * 80)
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: {workflow_name}")
        logger.info(f"ğŸ“‹ æ­¥éª¤æ•°: {len(steps)}")
        logger.info("=" * 80)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"./results/run_{timestamp}"
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        self.output_dir = str(output_path)
        
        logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åˆå§‹åŒ–æ­¥éª¤ç»“æœåˆ—è¡¨
        steps_details = []
        steps_results = []
        
        # ğŸ”¥ ä¸Šä¸‹æ–‡é“¾ï¼šè·Ÿè¸ªå½“å‰æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè‡ªåŠ¨ä¼ é€’ç»™ä¸‹ä¸€ä¸ªæ­¥éª¤
        # ğŸ”¥ CRITICAL REGRESSION FIX: Resolve file paths to absolute paths
        resolved_file_paths = []
        if file_paths:
            for fp in file_paths:
                if fp and isinstance(fp, str):
                    resolved = self._resolve_file_path(fp)
                    resolved_file_paths.append(resolved)
                    if resolved != fp:
                        logger.info(f"ğŸ”„ [Executor] è§£æè¾“å…¥æ–‡ä»¶è·¯å¾„: {fp} -> {resolved}")
        current_file_path = resolved_file_paths[0] if resolved_file_paths else None
        
        # æ‰§è¡Œæ¯ä¸ªæ­¥éª¤
        for i, step in enumerate(steps, 1):
            step_id = step.get("step_id", f"step{i}")
            step_name = step.get("name", step.get("step_name", step_id))
            tool_id = step.get("tool_id", step_id)
            params = step.get("params", {})
            
            # ğŸ”¥ CRITICAL FIX: å®Œå…¨ç§»é™¤ visualize_pca æ­¥éª¤ï¼ˆä¸åœ¨æµç¨‹ä¸­æ˜¾ç¤ºï¼‰
            if tool_id == "visualize_pca" or step_id == "visualize_pca":
                logger.warning(f"âš ï¸ [Executor] å®Œå…¨ç§»é™¤ visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰")
                continue  # ç›´æ¥è·³è¿‡ï¼Œä¸æ·»åŠ åˆ°æ­¥éª¤è¯¦æƒ…ä¸­
            
            logger.info(f"\n{'=' * 80}")
            logger.info(f"ğŸ“Œ æ­¥éª¤ {i}/{len(steps)}: {step_name} ({step_id})")
            logger.info(f"{'=' * 80}")
            
            # ğŸ”¥ æ™ºèƒ½å‚æ•°æ˜ å°„ï¼šæ ¹æ®å·¥å…·ç±»å‹è‡ªåŠ¨æ˜ å°„æ–‡ä»¶è·¯å¾„å‚æ•°
            tool_metadata = registry.get_metadata(tool_id)
            tool_category = tool_metadata.category if tool_metadata else None
            
            # ç¡®å®šå·¥å…·æœŸæœ›çš„æ–‡ä»¶è·¯å¾„å‚æ•°å
            if tool_category == "scRNA-seq":
                # RNA å·¥å…·ä½¿ç”¨ adata_path
                file_param_name = "adata_path"
            else:
                # å…¶ä»–å·¥å…·ï¼ˆå¦‚ä»£è°¢ç»„å­¦ï¼‰ä½¿ç”¨ file_path
                file_param_name = "file_path"
            
            # è‡ªåŠ¨æ³¨å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœç¼ºå¤±ä¸”æˆ‘ä»¬æœ‰å½“å‰æ–‡ä»¶è·¯å¾„ï¼‰
            if file_param_name not in params and current_file_path:
                params[file_param_name] = current_file_path
                logger.info(f"ğŸ”„ è‡ªåŠ¨æ³¨å…¥ {file_param_name}: {current_file_path}")
            
            # ğŸ”¥ å‚æ•°æ˜ å°„ï¼šå¦‚æœå·¥å…·æœŸæœ› adata_path ä½†æä¾›äº† file_pathï¼Œè¿›è¡Œæ˜ å°„
            if file_param_name == "adata_path" and "file_path" in params and file_param_name not in params:
                params[file_param_name] = params.pop("file_path")
                logger.info(f"ğŸ”„ å‚æ•°æ˜ å°„: file_path -> {file_param_name}")
            
            # å¦‚æœå·¥å…·éœ€è¦ output_dirï¼Œä¹Ÿè‡ªåŠ¨æ³¨å…¥
            if "output_dir" not in params and self.output_dir:
                params["output_dir"] = self.output_dir
            
            # æ›´æ–°æ­¥éª¤çš„ params
            step["params"] = params
            
            # æ„å»ºæ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ–‡ä»¶è·¯å¾„ç­‰ï¼‰
            step_context = {
                "file_paths": file_paths or [],
                "output_dir": self.output_dir,
                "workflow_name": workflow_name,
                "current_file_path": current_file_path  # ä¼ é€’å½“å‰æ–‡ä»¶è·¯å¾„
            }
            
            # æ‰§è¡Œæ­¥éª¤
            step_result = self.execute_step(step, step_context)
            
            # ğŸ”¥ CRITICAL FIX: æ›´æ–° current_file_path ä¾›ä¸‹ä¸€ä¸ªæ­¥éª¤ä½¿ç”¨
            # å¯¹äº scRNA-seq å·¥å…·ï¼Œä¼˜å…ˆä½¿ç”¨ output_h5ad
            result_data = step_result.get("result", {})
            if isinstance(result_data, dict):
                # ğŸ”¥ å¯¹äº scRNA-seq å·¥å…·ï¼Œä¼˜å…ˆæŸ¥æ‰¾ output_h5ad
                tool_metadata = registry.get_metadata(step.get("tool_id", ""))
                tool_category = tool_metadata.category if tool_metadata else None
                
                if tool_category == "scRNA-seq":
                    # scRNA-seq å·¥å…·ä¼˜å…ˆä½¿ç”¨ output_h5ad
                    next_file_path = (
                        result_data.get("output_h5ad") or  # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ output_h5ad
                        result_data.get("output_file") or
                        result_data.get("output_path") or
                        result_data.get("file_path")
                    )
                else:
                    # å…¶ä»–å·¥å…·ä½¿ç”¨æ ‡å‡†å­—æ®µ
                    next_file_path = (
                        result_data.get("output_file") or
                        result_data.get("output_path") or
                        result_data.get("file_path") or
                        result_data.get("preprocessed_file")
                    )
                
                if next_file_path:
                    # ğŸ”¥ ä¿®å¤ï¼šå³ä½¿æ–‡ä»¶ä¸å­˜åœ¨ä¹Ÿæ›´æ–°è·¯å¾„ï¼ˆæ–‡ä»¶å¯èƒ½ç¨ååˆ›å»ºï¼‰
                    current_file_path = next_file_path
                    if os.path.exists(next_file_path):
                        logger.info(f"âœ… æ›´æ–°å½“å‰æ–‡ä»¶è·¯å¾„: {current_file_path}")
                    else:
                        logger.warning(f"âš ï¸ è¾“å‡ºè·¯å¾„ä¸å­˜åœ¨ï¼Œä½†ä¼šä½¿ç”¨: {next_file_path} (æ–‡ä»¶å¯èƒ½ç¨ååˆ›å»º)")
            
            # æ„å»ºæ­¥éª¤è¯¦æƒ…ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
            step_detail = {
                "step_id": step_id,
                "tool_id": step.get("tool_id"),
                "name": step_name,
                "status": step_result.get("status", "error"),
                "summary": step_result.get("message", ""),
                "step_result": {
                    "step_name": step_name,
                    "status": step_result.get("status", "error"),
                    "logs": step_result.get("message", ""),
                    "data": step_result.get("result", {})
                }
            }
            
            # ğŸ”¥ æå–å›¾ç‰‡è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰- ä¸¥æ ¼æ£€æŸ¥æ–‡ä»¶ç±»å‹
            result_data = step_result.get("result", {})
            if isinstance(result_data, dict):
                # ä¼˜å…ˆæ£€æŸ¥æ˜ç¡®çš„å›¾ç‰‡è·¯å¾„å­—æ®µ
                plot_path = result_data.get("plot_path") or result_data.get("image_path")
                
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å›¾ç‰‡è·¯å¾„å­—æ®µï¼Œæ£€æŸ¥ output_path çš„æ–‡ä»¶æ‰©å±•å
                if not plot_path:
                    output_path = result_data.get("output_path") or result_data.get("output_file") or result_data.get("file_path")
                    if output_path and self._is_image_file(output_path):
                        plot_path = output_path
                
                # åªæœ‰ç¡®è®¤æ˜¯å›¾ç‰‡æ–‡ä»¶æ‰æ·»åŠ åˆ° plot å­—æ®µ
                if plot_path and self._is_image_file(plot_path):
                    step_detail["plot"] = plot_path
                    logger.info(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾ç‰‡æ–‡ä»¶: {plot_path}")
                elif plot_path:
                    # å¦‚æœä¸æ˜¯å›¾ç‰‡æ–‡ä»¶ï¼ˆå¦‚ CSVï¼‰ï¼Œè®°å½•åˆ° data å­—æ®µè€Œä¸æ˜¯ plot
                    logger.debug(f"ğŸ“„ æ£€æµ‹åˆ°éå›¾ç‰‡æ–‡ä»¶: {plot_path}ï¼Œä¸æ·»åŠ åˆ° plot å­—æ®µ")
            
            steps_details.append(step_detail)
            steps_results.append(step_detail["step_result"])
            
            # ğŸ”¥ CRITICAL REGRESSION FIX: If async job started, stop execution
            if step_result.get("status") == "async_job_started":
                logger.info(f"ğŸš€ [Executor] æ£€æµ‹åˆ°å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨: {step_id}, job_id: {step_result.get('job_id', 'N/A')}")
                logger.info(f"ğŸš€ [Executor] åœæ­¢æ‰§è¡Œåç»­æ­¥éª¤ï¼Œç­‰å¾…å¼‚æ­¥ä½œä¸šå®Œæˆ")
                # Add job_id to step_detail if present
                if "job_id" in step_result:
                    step_detail["job_id"] = step_result["job_id"]
                break  # STOP HERE - Do not execute next steps
            
            # å¦‚æœæ­¥éª¤å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ
            if step_result.get("status") == "error":
                logger.error(f"âŒ æ­¥éª¤ {step_id} å¤±è´¥ï¼Œåœæ­¢å·¥ä½œæµæ‰§è¡Œ")
                break
        
        # ç¡®å®šæœ€ç»ˆçŠ¶æ€
        all_success = all(
            detail.get("status") == "success"
            for detail in steps_details
        )
        workflow_status = "success" if all_success else "error"
        
        # æå–æœ€ç»ˆå›¾ç‰‡ï¼ˆæœ€åä¸€ä¸ªæˆåŠŸæ­¥éª¤çš„å›¾ç‰‡ï¼‰
        final_plot = None
        for detail in reversed(steps_details):
            if detail.get("plot"):
                final_plot = detail["plot"]
                break
        
        # æ„å»ºæ‰§è¡ŒæŠ¥å‘Šï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        report_data = {
            "status": workflow_status,
            "workflow_name": workflow_name,
            "steps_details": steps_details,
            "steps_results": steps_results,
            "output_dir": self.output_dir
        }
        
        if final_plot:
            report_data["final_plot"] = final_plot
        
        logger.info("=" * 80)
        logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {workflow_name} (çŠ¶æ€: {workflow_status})")
        logger.info(f"ğŸ“Š æˆåŠŸæ­¥éª¤: {sum(1 for d in steps_details if d.get('status') == 'success')}/{len(steps_details)}")
        logger.info("=" * 80)
        
        # ğŸ”¥ ç”Ÿæˆ AI Expert Diagnosisï¼ˆå¦‚æœæä¾›äº† Agent å®ä¾‹ï¼‰
        # æ³¨æ„ï¼šç”±äº execute_workflow æ˜¯åŒæ­¥æ–¹æ³•ï¼Œè¯Šæ–­ç”Ÿæˆå°†åœ¨åå°è¿›è¡Œæˆ–è·³è¿‡
        # å®é™…è¯Šæ–­ç”Ÿæˆåº”è¯¥åœ¨ Agent çš„ execute_workflow æ–¹æ³•ä¸­è°ƒç”¨
        # è¿™é‡Œåªæ ‡è®°éœ€è¦è¯Šæ–­ï¼Œä¸å®é™…ç”Ÿæˆï¼ˆé¿å…å¼‚æ­¥/åŒæ­¥æ··ç”¨é—®é¢˜ï¼‰
        if agent and hasattr(agent, '_generate_analysis_summary'):
            logger.info("ğŸ“ [Executor] Agent å®ä¾‹å·²æä¾›ï¼Œè¯Šæ–­å°†åœ¨ Agent å±‚ç”Ÿæˆ")
        
        # ğŸ”¥ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨ï¼ˆå¤„ç† Numpy ç±»å‹ã€NaN/Infinity ç­‰ï¼‰
        logger.info("ğŸ§¹ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨...")
        sanitized_report = sanitize_for_json(report_data)
        logger.info("âœ… æ•°æ®æ¸…ç†å®Œæˆ")
        
        return sanitized_report

