"""
é€šç”¨å·¥ä½œæµæ‰§è¡Œå™¨ - The Hands

åŠ¨æ€æ‰§è¡Œå·¥ä½œæµï¼Œä¸ä¾èµ–ç¡¬ç¼–ç çš„å·¥å…·é€»è¾‘ã€‚
ä½¿ç”¨ ToolRegistry æŸ¥æ‰¾å’Œæ‰§è¡Œå·¥å…·ã€‚
"""
import os
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
from datetime import datetime

from .tool_registry import registry
from .utils import sanitize_for_json

logger = logging.getLogger(__name__)


class WorkflowExecutor:
    """
    å·¥ä½œæµæ‰§è¡Œå™¨
    
    èŒè´£ï¼š
    1. ä» ToolRegistry æŸ¥æ‰¾å·¥å…·
    2. éªŒè¯å‚æ•°
    3. æ‰§è¡Œå·¥å…·
    4. å¤„ç†æ­¥éª¤é—´çš„æ•°æ®æµ
    5. ç”Ÿæˆç¬¦åˆå‰ç«¯æ ¼å¼çš„æ‰§è¡ŒæŠ¥å‘Š
    """
    
    def __init__(self, output_dir: Optional[str] = None, upload_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµæ‰§è¡Œå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œå°†åœ¨æ‰§è¡Œæ—¶åˆ›å»ºï¼‰
            upload_dir: ä¸Šä¼ ç›®å½•ï¼ˆç”¨äºè§£æç›¸å¯¹è·¯å¾„ï¼‰
        """
        self.output_dir = output_dir
        self.upload_dir = upload_dir or os.getenv("UPLOAD_DIR", "/app/uploads")
        self.results_dir = os.getenv("RESULTS_DIR", "/app/results")
        self.step_results: Dict[str, Any] = {}  # å­˜å‚¨æ­¥éª¤ç»“æœï¼Œç”¨äºæ•°æ®æµä¼ é€’
    
    def _resolve_file_path(self, file_path: str) -> str:
        """
        ğŸ”¥ SYSTEM-WIDE REFACTOR: Smart Path Resolver
        
        æŒ‰ç…§ä¸¥æ ¼çš„é€»è¾‘é¡ºåºè§£ææ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿ä¸ä¼šé”™è¯¯åœ°ä¿®æ”¹å·²å­˜åœ¨çš„ç»å¯¹è·¯å¾„ã€‚
        
        æ£€æŸ¥é¡ºåºï¼ˆä¸¥æ ¼æŒ‰ç…§æ­¤é¡ºåºï¼Œä¸è¦æ”¹å˜ï¼‰ï¼š
        1. ç»å¯¹è·¯å¾„ä¸”å­˜åœ¨ -> ç›´æ¥è¿”å›ï¼ˆä¸ä¿®æ”¹ï¼‰
        2. Results ç›®å½• -> å°è¯•åœ¨ RESULTS_DIR ä¸­æŸ¥æ‰¾
        3. Uploads ç›®å½• -> å°è¯•åœ¨ UPLOAD_DIR ä¸­æŸ¥æ‰¾
        4. å½“å‰å·¥ä½œç›®å½• -> å°è¯•ç›¸å¯¹è·¯å¾„
        5. å¤±è´¥ -> æŠ›å‡º FileNotFoundError
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½æ˜¯ç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„æˆ–æ–‡ä»¶åï¼‰
            
        Returns:
            è§£æåçš„ç»å¯¹è·¯å¾„
            
        Raises:
            FileNotFoundError: å¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½å¤±è´¥
        """
        if not file_path or file_path in ["<å¾…ä¸Šä¼ æ•°æ®>", "<PENDING_UPLOAD>", ""]:
            return file_path
        
        original_path = file_path
        attempted_paths = []
        
        # Check 1: Absolute & Exists
        # ğŸ”¥ CRITICAL: If absolute and exists, RETURN IMMEDIATELY (Do not touch it!)
        if Path(file_path).is_absolute():
            path_obj = Path(file_path)
            if path_obj.exists():
                resolved = str(path_obj.resolve())
                logger.info(f"âœ… [Path Resolver] ç»å¯¹è·¯å¾„å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›: {original_path} -> {resolved}")
                return resolved
            else:
                attempted_paths.append(str(path_obj.resolve()))
                logger.debug(f"ğŸ” [Path Resolver] ç»å¯¹è·¯å¾„ä¸å­˜åœ¨: {file_path}")
        
        # Check 2: Results Directory
        # Construct path = RESULTS_DIR / file_path
        results_dir_path = Path(self.results_dir)
        # Remove leading slash if present
        file_path_clean = file_path.lstrip('/')
        potential_results_path = results_dir_path / file_path_clean
        
        if potential_results_path.exists():
            resolved = str(potential_results_path.resolve())
            logger.info(f"âœ… [Path Resolver] åœ¨ Results ç›®å½•æ‰¾åˆ°: {original_path} -> {resolved}")
            return resolved
        attempted_paths.append(str(potential_results_path.resolve()))
        
        # Also try with original path (in case it's already relative to results)
        if not Path(file_path).is_absolute():
            potential_results_path2 = results_dir_path / file_path
            if potential_results_path2.exists():
                resolved = str(potential_results_path2.resolve())
                logger.info(f"âœ… [Path Resolver] åœ¨ Results ç›®å½•æ‰¾åˆ°ï¼ˆåŸå§‹è·¯å¾„ï¼‰: {original_path} -> {resolved}")
                return resolved
            attempted_paths.append(str(potential_results_path2.resolve()))
        
        # Check 3: Uploads Directory
        # Construct path = UPLOAD_DIR / file_path
        upload_dir_path = Path(self.upload_dir)
        potential_upload_path = upload_dir_path / file_path_clean
        
        if potential_upload_path.exists():
            resolved = str(potential_upload_path.resolve())
            logger.info(f"âœ… [Path Resolver] åœ¨ Uploads ç›®å½•æ‰¾åˆ°: {original_path} -> {resolved}")
            return resolved
        attempted_paths.append(str(potential_upload_path.resolve()))
        
        # Try with original relative path
        if not Path(file_path).is_absolute():
            potential_upload_path2 = upload_dir_path / file_path
            if potential_upload_path2.exists():
                resolved = str(potential_upload_path2.resolve())
                logger.info(f"âœ… [Path Resolver] åœ¨ Uploads ç›®å½•æ‰¾åˆ°ï¼ˆåŸå§‹è·¯å¾„ï¼‰: {original_path} -> {resolved}")
                return resolved
            attempted_paths.append(str(potential_upload_path2.resolve()))
        
        # Check 4: Relative to Current Work Dir
        # Try as relative path from current working directory
        if not Path(file_path).is_absolute():
            try:
                cwd_path = Path.cwd() / file_path
                if cwd_path.exists():
                    resolved = str(cwd_path.resolve())
                    logger.info(f"âœ… [Path Resolver] åœ¨å½“å‰å·¥ä½œç›®å½•æ‰¾åˆ°: {original_path} -> {resolved}")
                    return resolved
                attempted_paths.append(str(cwd_path.resolve()))
            except Exception as e:
                logger.debug(f"âš ï¸ [Path Resolver] æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•å¤±è´¥: {e}")
        
        # Check 5: Filename search (only if it's just a filename)
        filename = Path(file_path).name
        if filename == file_path or '/' not in file_path.replace('\\', '/'):
            # Search in results directory recursively
            logger.debug(f"ğŸ” [Path Resolver] æœç´¢æ–‡ä»¶å: {filename} åœ¨ {self.results_dir}")
            for found_path in results_dir_path.rglob(filename):
                if found_path.is_file():
                    resolved = str(found_path.resolve())
                    logger.info(f"âœ… [Path Resolver] åœ¨ Results ç›®å½•é€’å½’æ‰¾åˆ°: {original_path} -> {resolved}")
                    return resolved
            
            # Search in uploads directory recursively
            logger.debug(f"ğŸ” [Path Resolver] æœç´¢æ–‡ä»¶å: {filename} åœ¨ {self.upload_dir}")
            for found_path in upload_dir_path.rglob(filename):
                if found_path.is_file():
                    resolved = str(found_path.resolve())
                    logger.info(f"âœ… [Path Resolver] åœ¨ Uploads ç›®å½•é€’å½’æ‰¾åˆ°: {original_path} -> {resolved}")
                    return resolved
        
        # Failure: All checks failed
        error_msg = (
            f"Could not resolve path: {original_path}. "
            f"Checked: {attempted_paths[:5]}"  # Limit to first 5 for readability
        )
        logger.error(f"âŒ [Path Resolver] {error_msg}")
        raise FileNotFoundError(error_msg)
    
    def execute_step(
        self,
        step_data: Dict[str, Any],
        step_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤
        
        Args:
            step_data: æ­¥éª¤æ•°æ®ï¼ŒåŒ…å« tool_id å’Œ params
            step_context: æ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å‰åºæ­¥éª¤çš„è¾“å‡ºç­‰ï¼‰
        
        Returns:
            æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        step_id = step_data.get("step_id", "unknown")
        tool_id = step_data.get("tool_id")
        params = step_data.get("params", {})
        step_name = step_data.get("name", tool_id)
        
        logger.info(f"ğŸ”§ æ‰§è¡Œæ­¥éª¤: {step_id} ({tool_id})")
        
        if not tool_id:
            error_msg = f"æ­¥éª¤ {step_id} ç¼ºå°‘ tool_id"
            logger.error(f"âŒ {error_msg}")
            return {
                "status": "error",
                "step_id": step_id,
                "step_name": step_name,
                "error": error_msg,
                "message": error_msg
            }
        
        # æŸ¥æ‰¾å·¥å…·
        tool_func = registry.get_tool(tool_id)
        if not tool_func:
            error_msg = f"å·¥å…· '{tool_id}' æœªåœ¨æ³¨å†Œè¡¨ä¸­æ‰¾åˆ°"
            logger.error(f"âŒ {error_msg}")
            return {
                "status": "error",
                "step_id": step_id,
                "step_name": step_name,
                "error": error_msg,
                "message": error_msg
            }
        
        # ğŸ”¥ å‚æ•°æ˜ å°„ï¼šæ ¹æ®å·¥å…·ç±»åˆ«æ˜ å°„æ–‡ä»¶è·¯å¾„å‚æ•°
        tool_metadata = registry.get_metadata(tool_id)
        tool_category = tool_metadata.category if tool_metadata else None
        
        # ç¡®å®šå·¥å…·æœŸæœ›çš„æ–‡ä»¶è·¯å¾„å‚æ•°å
        if tool_category == "scRNA-seq":
            # RNA å·¥å…·ä½¿ç”¨ adata_path
            file_param_name = "adata_path"
            # å¦‚æœæä¾›äº† file_path ä½†æ²¡æœ‰ adata_pathï¼Œè¿›è¡Œæ˜ å°„
            if "file_path" in params and file_param_name not in params:
                params[file_param_name] = params.pop("file_path")
                logger.info(f"ğŸ”„ [Executor] å‚æ•°æ˜ å°„: file_path -> {file_param_name} (å·¥å…·: {tool_id})")
        else:
            # å…¶ä»–å·¥å…·ï¼ˆå¦‚ä»£è°¢ç»„å­¦ï¼‰ä½¿ç”¨ file_path
            file_param_name = "file_path"
        
        # éªŒè¯å‚æ•°ï¼ˆå¯é€‰ä½†æ¨èï¼‰
        # ğŸ”¥ CRITICAL FIX: å‚æ•°éªŒè¯å¤±è´¥æ—¶ä¸åº”è¯¥æ¸…ç©º paramsï¼Œåº”è¯¥ä¿ç•™åŸå§‹å‚æ•°
        # ğŸ”¥ CRITICAL FIX: ä¿å­˜å…³é”®å‚æ•°ï¼ˆå¦‚ group_columnï¼‰ä»¥é˜²éªŒè¯åä¸¢å¤±
        critical_params_backup = {}
        tools_requiring_group_column = ["differential_analysis", "metabolomics_plsda", "metabolomics_pathway_enrichment"]
        if tool_id in tools_requiring_group_column and "group_column" in params:
            critical_params_backup["group_column"] = params["group_column"]
        
        try:
            if tool_metadata:
                # ä½¿ç”¨ Pydantic schema éªŒè¯å‚æ•°ï¼ˆå…è®¸é¢å¤–å­—æ®µï¼‰
                # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨ model_validate å¹¶è®¾ç½® extra='ignore' æ¥å¿½ç•¥é¢å¤–å­—æ®µ
                try:
                    validated_params = tool_metadata.args_schema.model_validate(params, strict=False)
                    params = validated_params.model_dump(exclude_unset=False)
                    logger.debug(f"âœ… å‚æ•°éªŒè¯é€šè¿‡: {step_id}")
                except Exception as e:
                    # å¦‚æœ model_validate å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ __init__ ä½†æ•è·é¢å¤–å­—æ®µ
                    try:
                        # åªæå–æ¨¡å‹å®šä¹‰çš„å­—æ®µ
                        schema_fields = tool_metadata.args_schema.model_fields.keys()
                        filtered_params = {k: v for k, v in params.items() if k in schema_fields}
                        validated_params = tool_metadata.args_schema(**filtered_params)
                        params = validated_params.model_dump(exclude_unset=False)
                        # ä¿ç•™ä¸åœ¨ schema ä¸­çš„é¢å¤–å‚æ•°ï¼ˆå¦‚ kwargsï¼‰
                        extra_params = {k: v for k, v in params.items() if k not in schema_fields}
                        params.update(extra_params)
                        logger.debug(f"âœ… å‚æ•°éªŒè¯é€šè¿‡ï¼ˆä¿ç•™é¢å¤–å‚æ•°ï¼‰: {step_id}")
                    except Exception as e2:
                        logger.warning(f"âš ï¸ å‚æ•°éªŒè¯å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼Œä¿ç•™åŸå§‹å‚æ•°ï¼‰: {e2}")
                        # ä¿ç•™åŸå§‹ params
                
                # ğŸ”¥ CRITICAL FIX: æ¢å¤å…³é”®å‚æ•°ï¼ˆå¦‚æœéªŒè¯åä¸¢å¤±ï¼‰
                for key, value in critical_params_backup.items():
                    if key not in params:
                        params[key] = value
                        logger.warning(f"âš ï¸ [Executor] éªŒè¯åæ¢å¤å…³é”®å‚æ•° {key}: {value}")
        except Exception as validation_err:
            logger.warning(f"âš ï¸ å‚æ•°éªŒè¯å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼Œä¿ç•™åŸå§‹å‚æ•°ï¼‰: {validation_err}")
            # ğŸ”¥ CRITICAL: éªŒè¯å¤±è´¥æ—¶ä¿ç•™åŸå§‹ paramsï¼Œä¸å› éªŒè¯å¤±è´¥è€Œä¸­æ–­æˆ–æ¸…ç©ºå‚æ•°
            # params ä¿æŒä¸å˜ï¼Œç»§ç»­æ‰§è¡Œ
        
        # ğŸ”¥ CRITICAL DEBUG: è®°å½•åŸå§‹å‚æ•°
        if tool_id in ["metabolomics_plsda", "differential_analysis", "metabolomics_pathway_enrichment"]:
            logger.info(f"ğŸ” [Executor] {tool_id} åŸå§‹å‚æ•°: {list(params.keys())}")
            if "group_column" in params:
                logger.info(f"âœ… [Executor] åŸå§‹å‚æ•°ä¸­åŒ…å« group_column: {params['group_column']}")
            else:
                logger.warning(f"âš ï¸ [Executor] åŸå§‹å‚æ•°ä¸­ç¼ºå°‘ group_column")
        
        # å¤„ç†æ•°æ®æµï¼šæ›¿æ¢å ä½ç¬¦ï¼ˆä¼ é€’å·¥å…·ç±»åˆ«ä¿¡æ¯ï¼‰
        processed_params = self._process_data_flow(params, step_context, tool_category=tool_category)
        
        # ğŸ”¥ CRITICAL DEBUG: è®°å½•å¤„ç†åçš„å‚æ•°
        if tool_id in ["metabolomics_plsda", "differential_analysis", "metabolomics_pathway_enrichment"]:
            logger.info(f"ğŸ” [Executor] {tool_id} å¤„ç†åå‚æ•°: {list(processed_params.keys())}")
            if "group_column" in processed_params:
                logger.info(f"âœ… [Executor] å¤„ç†åå‚æ•°ä¸­åŒ…å« group_column: {processed_params['group_column']}")
            else:
                logger.error(f"âŒ [Executor] å¤„ç†åå‚æ•°ä¸­ç¼ºå°‘ group_columnï¼")
        
        # ğŸ”¥ CRITICAL FIX: å¯¹äº scRNA-seq å·¥å…·ï¼Œç¡®ä¿ç§»é™¤ file_path å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if tool_category == "scRNA-seq" and "file_path" in processed_params:
            # å¦‚æœå·²ç»æœ‰ adata_pathï¼Œç§»é™¤ file_path
            if "adata_path" in processed_params:
                del processed_params["file_path"]
                logger.info(f"ğŸ”„ [Executor] ç§»é™¤å¤šä½™çš„ file_path å‚æ•°ï¼ˆå·¥å…·å·²æœ‰ adata_pathï¼‰")
            else:
                # å¦‚æœæ²¡æœ‰ adata_pathï¼Œå°† file_path æ˜ å°„ä¸º adata_path
                processed_params["adata_path"] = processed_params.pop("file_path")
                logger.info(f"ğŸ”„ [Executor] å‚æ•°æ˜ å°„: file_path -> adata_path (å·¥å…·: {tool_id})")
        
        # ğŸ”¥ CRITICAL FIX: å¯¹äº visualize_volcanoï¼Œè‡ªåŠ¨æ³¨å…¥ diff_resultsï¼ˆå¦‚æœç¼ºå¤±ï¼‰
        if tool_id == "visualize_volcano":
            # å¦‚æœ diff_results ç¼ºå¤±æˆ–ä»ç„¶æ˜¯å ä½ç¬¦ï¼Œå°è¯•ä» differential_analysis æ­¥éª¤è·å–
            if "diff_results" not in processed_params or (
                isinstance(processed_params.get("diff_results"), str) and 
                processed_params.get("diff_results", "").startswith("<")
            ):
                # æŸ¥æ‰¾ differential_analysis æ­¥éª¤çš„ç»“æœ
                diff_result = None
                for step_id, step_result in self.step_results.items():
                    if step_id == "differential_analysis" or "differential" in step_id.lower():
                        # step_results å­˜å‚¨çš„æ˜¯å·¥å…·è¿”å›çš„åŸå§‹ç»“æœ
                        if isinstance(step_result, dict):
                            diff_result = step_result
                            break
                
                if isinstance(diff_result, dict):
                    processed_params["diff_results"] = diff_result
                    logger.info(f"âœ… [Executor] è‡ªåŠ¨æ³¨å…¥ diff_results åˆ° visualize_volcano")
                else:
                    logger.error(f"âŒ [Executor] æ— æ³•æ‰¾åˆ° differential_analysis ç»“æœï¼Œvisualize_volcano å°†å¤±è´¥")
            
            # è¿‡æ»¤æ‰å·¥å…·ä¸æ¥å—çš„å‚æ•°ï¼ˆä¿ç•™ diff_resultsï¼‰
            allowed_params = {"diff_results", "output_path", "fdr_threshold", "log2fc_threshold"}
            filtered_params = {k: v for k, v in processed_params.items() if k in allowed_params}
            if len(filtered_params) < len(processed_params):
                removed = set(processed_params.keys()) - allowed_params
                logger.warning(f"âš ï¸ [Executor] ç§»é™¤ {tool_id} ä¸æ¥å—çš„å‚æ•°: {removed}")
            processed_params = filtered_params
        
        # ğŸ”¥ CRITICAL REGRESSION FIX: Normalize all path-like parameters before tool execution
        # ğŸ”¥ TASK 2: Only resolve paths that are NOT already absolute and existing
        path_params = ["file_path", "adata_path", "output_path", "output_file", "fastq_path", "reference_path", "output_h5ad"]
        for param_name in path_params:
            if param_name in processed_params:
                original_path = processed_params[param_name]
                if original_path and isinstance(original_path, str):
                    # Skip placeholder values
                    if original_path not in ["<å¾…ä¸Šä¼ æ•°æ®>", "<PENDING_UPLOAD>", ""]:
                        # ğŸ”¥ CRITICAL: If already absolute and exists, do NOT modify it
                        if Path(original_path).is_absolute() and Path(original_path).exists():
                            logger.debug(f"âœ… [Executor] è·¯å¾„å‚æ•° {param_name} å·²æ˜¯ç»å¯¹è·¯å¾„ä¸”å­˜åœ¨ï¼Œä¸ä¿®æ”¹: {original_path}")
                            # Keep it as is - do not call _resolve_file_path
                        else:
                            # Only resolve if not absolute or doesn't exist
                            try:
                                resolved_path = self._resolve_file_path(original_path)
                                if resolved_path != original_path:
                                    logger.info(f"ğŸ”„ [Executor] è§£æè·¯å¾„å‚æ•° {param_name}: {original_path} -> {resolved_path}")
                                processed_params[param_name] = resolved_path
                            except FileNotFoundError as e:
                                # If resolution fails, keep original and let tool handle the error
                                logger.warning(f"âš ï¸ [Executor] è·¯å¾„è§£æå¤±è´¥ï¼Œä¿ç•™åŸå§‹è·¯å¾„: {original_path} (é”™è¯¯: {e})")
                                # Keep original_path - tool will handle the error
        
        # ğŸ”¥ ARCHITECTURAL UPGRADE: Phase 3 - Pre-Flight Check & Auto-Correction
        # å¯¹äºéœ€è¦ group_column çš„å·¥å…·ï¼ŒéªŒè¯åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ semantic_map è‡ªåŠ¨ä¿®æ­£
        tools_requiring_group_column = ["differential_analysis", "metabolomics_plsda", "metabolomics_pathway_enrichment"]
        if tool_id in tools_requiring_group_column and "group_column" in processed_params:
            group_column = processed_params.get("group_column")
            file_path = processed_params.get("file_path")
            
            if file_path and os.path.exists(file_path):
                try:
                    import pandas as pd
                    # Pre-Flight Check: è¯»å–æ–‡ä»¶æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                    df = pd.read_csv(file_path, index_col=0, nrows=1)  # åªè¯»ç¬¬ä¸€è¡Œæ£€æŸ¥åˆ—å
                    
                    # ğŸ”¥ ä¿®å¤ï¼šå…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ï¼‰
                    group_column_found = group_column in df.columns
                    matched_column = None
                    
                    if not group_column_found:
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿
                        group_column_normalized = group_column.lower().replace(' ', '').replace('_', '').replace('-', '')
                        for col in df.columns:
                            col_normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                            if col_normalized == group_column_normalized:
                                matched_column = col
                                logger.info(f"ğŸ”„ [Executor] æ¨¡ç³ŠåŒ¹é…åˆ†ç»„åˆ—: '{group_column}' -> '{col}'")
                                break
                        
                        if matched_column:
                            group_column = matched_column
                            group_column_found = True
                            processed_params["group_column"] = matched_column
                    
                    if not group_column_found:
                        logger.warning(f"âš ï¸ [Executor] Pre-Flight Check: åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­")
                        
                        # Auto-Correction: å°è¯•è·å– semantic_map
                        semantic_map = None
                        
                        # æ–¹æ³•1: ä» step_context è·å–ï¼ˆå¦‚æœä¼ é€’äº†ï¼‰
                        if step_context:
                            file_metadata = step_context.get("file_metadata")
                            if file_metadata:
                                semantic_map = file_metadata.get("semantic_map", {})
                        
                        # æ–¹æ³•2: å¦‚æœæœªæ‰¾åˆ°ï¼Œé‡æ–°è¯»å–æ–‡ä»¶å…ƒæ•°æ®ï¼ˆä½¿ç”¨ FileInspectorï¼‰
                        if not semantic_map:
                            try:
                                from .file_inspector import FileInspector
                                # os å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œæ— éœ€é‡å¤å¯¼å…¥
                                upload_dir = os.getenv("UPLOAD_DIR", "/app/uploads")
                                inspector = FileInspector(upload_dir)
                                file_metadata = inspector.inspect_file(file_path)
                                if file_metadata.get("status") == "success":
                                    semantic_map = file_metadata.get("semantic_map", {})
                                    logger.info(f"âœ… [Executor] é‡æ–°è¯»å– file_metadata è·å– semantic_map")
                            except Exception as e:
                                logger.debug(f"âš ï¸ [Executor] æ— æ³•é‡æ–°è¯»å– file_metadata: {e}")
                        
                        # å¦‚æœæ‰¾åˆ° semantic_mapï¼Œä½¿ç”¨å®ƒè¿›è¡Œè‡ªåŠ¨ä¿®æ­£
                        if semantic_map:
                            group_cols = semantic_map.get("group_cols", [])
                            if len(group_cols) == 1:
                                # å¦‚æœ FileInspector æ‰¾åˆ°äº†æ°å¥½ä¸€ä¸ªåˆ†ç»„åˆ—ï¼Œè‡ªåŠ¨ä½¿ç”¨å®ƒ
                                auto_group_col = group_cols[0]
                                logger.info(f"âœ… [Executor] Auto-Correction: ä½¿ç”¨ semantic_map ä¸­çš„å”¯ä¸€åˆ†ç»„åˆ—: {auto_group_col}")
                                processed_params["group_column"] = auto_group_col
                            elif len(group_cols) > 1:
                                # å¦‚æœæœ‰å¤šä¸ªï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
                                auto_group_col = group_cols[0]
                                logger.info(f"âœ… [Executor] Auto-Correction: ä½¿ç”¨ semantic_map ä¸­çš„ç¬¬ä¸€ä¸ªåˆ†ç»„åˆ—: {auto_group_col}")
                                processed_params["group_column"] = auto_group_col
                            else:
                                logger.error(f"âŒ [Executor] semantic_map['group_cols'] ä¸ºç©ºï¼Œæ— æ³•è‡ªåŠ¨ä¿®æ­£")
                        else:
                            # å›é€€åˆ°å¯å‘å¼æ£€æµ‹
                            logger.warning(f"âš ï¸ [Executor] æœªæ‰¾åˆ° semantic_mapï¼Œä½¿ç”¨å¯å‘å¼æ£€æµ‹...")
                            detected_group_column = self._detect_group_column_from_file(file_path)
                            
                            if detected_group_column:
                                logger.info(f"âœ… [Executor] å¯å‘å¼æ£€æµ‹åˆ°åˆ†ç»„åˆ—: {detected_group_column}ï¼Œæ›¿æ¢ '{group_column}'")
                                processed_params["group_column"] = detected_group_column
                            else:
                                # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ—å
                                available_cols = list(df.columns)
                                logger.error(f"âŒ [Executor] æ— æ³•è‡ªåŠ¨æ£€æµ‹åˆ†ç»„åˆ—ã€‚å¯ç”¨åˆ—: {available_cols[:10]}")
                                # ä¸ç«‹å³å¤±è´¥ï¼Œè®©å·¥å…·è‡ªå·±å¤„ç†é”™è¯¯ï¼ˆå·¥å…·ä¼šè¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼‰
                except Exception as e:
                    logger.warning(f"âš ï¸ [Executor] Pre-Flight Check å¤±è´¥: {e}ï¼Œç»§ç»­æ‰§è¡Œ")
        
        # ğŸ”¥ CRITICAL FIX: å¯¹äºéœ€è¦ group_column çš„å·¥å…·ï¼Œå¼ºåˆ¶ç¡®ä¿å‚æ•°å­˜åœ¨
        # è¿™æ˜¯æ‰§è¡Œå‰çš„æœ€åä¸€é“é˜²çº¿ï¼Œç¡®ä¿å‚æ•°ç»å¯¹ä¸ä¼šä¸¢å¤±
        tools_requiring_group_column = ["differential_analysis", "metabolomics_plsda", "metabolomics_pathway_enrichment"]
        if tool_id in tools_requiring_group_column:
            # å¤šé‡æ£€æŸ¥ï¼šä»å¤šä¸ªæ¥æºå°è¯•è·å– group_column
            group_column_value = None
            
            # æ£€æŸ¥1: processed_params ä¸­æ˜¯å¦æœ‰
            if "group_column" in processed_params:
                group_column_value = processed_params["group_column"]
                logger.info(f"âœ… [Executor] {tool_id} group_column å·²å­˜åœ¨: {group_column_value}")
            
            # æ£€æŸ¥2: åŸå§‹ step_data ä¸­æ˜¯å¦æœ‰
            elif "group_column" in step_data.get("params", {}):
                group_column_value = step_data["params"]["group_column"]
                processed_params["group_column"] = group_column_value
                logger.warning(f"âš ï¸ [Executor] {tool_id} ä»åŸå§‹æ­¥éª¤æ•°æ®æ¢å¤ group_column: {group_column_value}")
            
            # æ£€æŸ¥3: step_context ä¸­çš„ file_metadata
            elif step_context and step_context.get("file_metadata"):
                semantic_map = step_context["file_metadata"].get("semantic_map", {})
                group_cols = semantic_map.get("group_cols", [])
                if group_cols:
                    group_column_value = group_cols[0]
                    processed_params["group_column"] = group_column_value
                    logger.warning(f"âš ï¸ [Executor] {tool_id} ä» file_metadata è‡ªåŠ¨æ³¨å…¥ group_column: {group_column_value}")
            
            # å¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½å¤±è´¥ï¼Œè¿”å›æ˜ç¡®é”™è¯¯
            if not group_column_value:
                logger.error(f"âŒ [Executor] CRITICAL: {tool_id} éœ€è¦ group_column å‚æ•°ï¼Œä½†æ‰€æœ‰æ¥æºéƒ½æœªæ‰¾åˆ°ï¼")
                logger.error(f"   processed_params: {list(processed_params.keys())}")
                logger.error(f"   åŸå§‹æ­¥éª¤å‚æ•°: {list(step_data.get('params', {}).keys())}")
                logger.error(f"   step_context keys: {list(step_context.keys()) if step_context else 'None'}")
                return {
                    "status": "error",
                    "step_id": step_id,
                    "step_name": step_name,
                    "error": f"å·¥å…· {tool_id} éœ€è¦ group_column å‚æ•°ï¼Œä½†å‚æ•°ç¼ºå¤±ä¸”æ— æ³•è‡ªåŠ¨è·å–ã€‚è¯·æ£€æŸ¥ Planner æ˜¯å¦æ­£ç¡®ç”Ÿæˆäº† group_column å‚æ•°ã€‚",
                    "message": f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥ï¼šç¼ºå°‘å¿…éœ€å‚æ•° group_column"
                }
        
        # æ‰§è¡Œå·¥å…·
        try:
            # ğŸ”¥ CRITICAL DEBUG: è®°å½•æ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬ group_columnï¼‰
            logger.info(f"ğŸš€ è°ƒç”¨å·¥å…·: {tool_id} with params: {list(processed_params.keys())}")
            if "group_column" in processed_params:
                logger.info(f"âœ… [Executor] group_column å‚æ•°å­˜åœ¨: {processed_params['group_column']}")
            else:
                if tool_id in tools_requiring_group_column:
                    logger.warning(f"âš ï¸ [Executor] group_column å‚æ•°ç¼ºå¤±ï¼ˆä½†å·²å°è¯•ä¿®å¤ï¼‰ï¼å¯ç”¨å‚æ•°: {list(processed_params.keys())}")
            result = tool_func(**processed_params)
            
            # ç¡®ä¿ç»“æœæ˜¯å­—å…¸æ ¼å¼
            if not isinstance(result, dict):
                result = {
                    "status": "success",
                    "data": result,
                    "message": f"æ­¥éª¤ {step_name} æ‰§è¡Œå®Œæˆ"
                }
            
            # ç¡®ä¿åŒ…å« status å­—æ®µ
            if "status" not in result:
                result["status"] = "success"
            
            # ğŸ”¥ æ ¹æ®å·¥å…·è¿”å›çš„çŠ¶æ€è®°å½•æ­£ç¡®çš„æ—¥å¿—å’Œæ¶ˆæ¯
            tool_status = result.get("status", "success")
            if tool_status == "error":
                error_msg = result.get("error") or result.get("message") or f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥"
                logger.error(f"âŒ æ­¥éª¤ {step_id} æ‰§è¡Œå¤±è´¥: {error_msg}")
                # å­˜å‚¨ç»“æœä¾›åç»­æ­¥éª¤ä½¿ç”¨ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿå­˜å‚¨ï¼Œç”¨äºè°ƒè¯•ï¼‰
                self.step_results[step_id] = result
                return {
                    "status": "error",
                    "step_id": step_id,
                    "step_name": step_name,
                    "tool_id": tool_id,
                    "result": result,
                    "error": error_msg,
                    "message": error_msg  # ğŸ”¥ ä½¿ç”¨é”™è¯¯æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯"æ‰§è¡Œå®Œæˆ"
                }
            else:
                logger.info(f"âœ… æ­¥éª¤ {step_id} æ‰§è¡ŒæˆåŠŸ")
                # å­˜å‚¨ç»“æœä¾›åç»­æ­¥éª¤ä½¿ç”¨
                self.step_results[step_id] = result
                return {
                    "status": result.get("status", "success"),
                    "step_id": step_id,
                    "step_name": step_name,
                    "tool_id": tool_id,
                    "result": result,
                    "message": result.get("message", f"æ­¥éª¤ {step_name} æ‰§è¡Œå®Œæˆ")
                }
        
        except Exception as e:
            error_msg = f"æ­¥éª¤ {step_id} æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}", exc_info=True)
            
            return {
                "status": "error",
                "step_id": step_id,
                "step_name": step_name,
                "tool_id": tool_id,
                "error": str(e),
                "message": error_msg
            }
    
    def _process_data_flow(
        self,
        params: Dict[str, Any],
        step_context: Optional[Dict[str, Any]] = None,
        tool_category: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ•°æ®æµï¼šæ›¿æ¢å ä½ç¬¦ï¼ˆå¦‚ <step1_output>ï¼‰å’Œè‡ªåŠ¨æ³¨å…¥æ–‡ä»¶è·¯å¾„
        
        Args:
            params: åŸå§‹å‚æ•°
            step_context: æ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å« current_file_pathï¼‰
            tool_category: å·¥å…·ç±»åˆ«ï¼ˆç”¨äºç¡®å®šæ–‡ä»¶è·¯å¾„å‚æ•°åï¼‰
        
        Returns:
            å¤„ç†åçš„å‚æ•°
        """
        processed = {}
        
        # ğŸ”¥ CRITICAL FIX: å…ˆå¤‡ä»½å…³é”®å‚æ•°ï¼Œé˜²æ­¢åœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¸¢å¤±
        critical_params_backup = {}
        if "group_column" in params:
            critical_params_backup["group_column"] = params["group_column"]
            logger.debug(f"ğŸ” [æ•°æ®æµå¤„ç†] å¤‡ä»½ group_column: {params['group_column']}")
        
        # ğŸ”¥ CRITICAL FIX: å…ˆå¤åˆ¶æ‰€æœ‰éå ä½ç¬¦å‚æ•°ï¼ˆåŒ…æ‹¬ group_columnï¼‰ï¼Œç¡®ä¿ä¸ä¼šä¸¢å¤±
        # è¿™æ ·å¯ä»¥ä¿è¯å³ä½¿å ä½ç¬¦å¤„ç†å¤±è´¥ï¼Œå…³é”®å‚æ•°ä¹Ÿä¸ä¼šä¸¢å¤±
        for key, value in params.items():
            # è·³è¿‡å ä½ç¬¦ï¼Œç¨åå¤„ç†
            if isinstance(value, str) and value.startswith("<") and value.endswith(">"):
                continue
            # ç«‹å³å¤åˆ¶éå ä½ç¬¦å‚æ•°ï¼ˆåŒ…æ‹¬ group_columnï¼‰
            processed[key] = value
            if key == "group_column":
                logger.debug(f"âœ… [æ•°æ®æµå¤„ç†] å·²å¤åˆ¶ group_column: {value}")
        
        # ğŸ”¥ æ ¹æ®å·¥å…·ç±»åˆ«ç¡®å®šæ–‡ä»¶è·¯å¾„å‚æ•°å
        if tool_category == "scRNA-seq":
            file_param_name = "adata_path"
        else:
            file_param_name = "file_path"
        
        # ğŸ”¥ å¦‚æœæ–‡ä»¶è·¯å¾„å‚æ•°ç¼ºå¤±ï¼Œå°è¯•ä»ä¸Šä¸‹æ–‡æ³¨å…¥
        if file_param_name not in params and step_context:
            current_file_path = step_context.get("current_file_path")
            if current_file_path:
                processed[file_param_name] = current_file_path
                logger.info(f"ğŸ”„ æ•°æ®æµ: è‡ªåŠ¨æ³¨å…¥ {file_param_name} = {current_file_path}")
        
        # ğŸ”¥ å¦‚æœæä¾›äº† file_path ä½†å·¥å…·éœ€è¦ adata_pathï¼Œè¿›è¡Œæ˜ å°„
        if tool_category == "scRNA-seq" and "file_path" in params and "adata_path" not in params:
            processed["adata_path"] = params.pop("file_path")
            logger.info(f"ğŸ”„ æ•°æ®æµ: å‚æ•°æ˜ å°„ file_path -> adata_path")
        
        # ç°åœ¨å¤„ç†å ä½ç¬¦ï¼ˆå¯èƒ½ä¼šè¦†ç›–å·²å¤åˆ¶çš„å€¼ï¼‰
        for key, value in params.items():
            if isinstance(value, str) and value.startswith("<") and value.endswith(">"):
                # å ä½ç¬¦ï¼Œå°è¯•ä»ä¸Šä¸‹æ–‡æˆ–æ­¥éª¤ç»“æœä¸­è·å–
                placeholder = value[1:-1]  # ç§»é™¤ < >
                
                # ğŸ”¥ CRITICAL FIX: ç‰¹æ®Šå¤„ç† preprocess_data_output å ä½ç¬¦
                # ç”¨äº PCAã€PLS-DAã€å·®å¼‚åˆ†æç­‰æ­¥éª¤ï¼Œéœ€è¦ä» preprocess_data æ­¥éª¤è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
                if placeholder == "preprocess_data_output" or "preprocess" in placeholder.lower():
                    # æŸ¥æ‰¾ preprocess_data æ­¥éª¤çš„ç»“æœ
                    preprocess_result = None
                    for step_id, step_result in self.step_results.items():
                        if step_id == "preprocess_data" or "preprocess" in step_id.lower():
                            # step_results å­˜å‚¨çš„æ˜¯å·¥å…·è¿”å›çš„åŸå§‹ç»“æœ
                            if isinstance(step_result, dict):
                                preprocess_result = step_result
                                break
                    
                    if isinstance(preprocess_result, dict):
                        # æå–è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆé¡ºåºï¼šoutput_file, output_path, file_pathï¼‰
                        output_path = (
                            preprocess_result.get("output_file") or
                            preprocess_result.get("output_path") or
                            preprocess_result.get("file_path")
                        )
                        if output_path:
                            # ğŸ”¥ TASK 2: The previous step MUST return an Absolute Path
                            # If it's already absolute and exists, use it directly (Check 1 in _resolve_file_path will handle it)
                            # Only resolve if it's not absolute or doesn't exist
                            if Path(output_path).is_absolute() and Path(output_path).exists():
                                # Already absolute and exists, use directly (no modification)
                                processed[key] = output_path
                                logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {output_path} (ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨)")
                            else:
                                # Try to resolve (may be relative or non-existent absolute)
                                try:
                                    resolved_path = self._resolve_file_path(output_path)
                                    processed[key] = resolved_path
                                    logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {resolved_path} (å·²è§£æ)")
                                except FileNotFoundError:
                                    # If resolution fails, use original (let tool handle the error)
                                    processed[key] = output_path
                                    logger.warning(f"âš ï¸ æ•°æ®æµ: {key} = <{placeholder}> -> {output_path} (è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„)")
                            continue
                        else:
                            logger.warning(f"âš ï¸ preprocess_data ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚å¯ç”¨å­—æ®µ: {list(preprocess_result.keys())}")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ° preprocess_data æ­¥éª¤ç»“æœï¼Œæ— æ³•è§£æ <{placeholder}>ã€‚å¯ç”¨æ­¥éª¤: {list(self.step_results.keys())}")
                
                # ğŸ”¥ CRITICAL FIX: ç‰¹æ®Šå¤„ç† differential_analysis_output å ä½ç¬¦
                # ç”¨äº visualize_volcanoï¼Œéœ€è¦ä¼ é€’å®Œæ•´çš„ diff_results å­—å…¸
                if placeholder == "differential_analysis_output" or "differential_analysis" in placeholder.lower():
                    # æŸ¥æ‰¾ differential_analysis æ­¥éª¤çš„ç»“æœ
                    diff_result = None
                    for step_id, step_result in self.step_results.items():
                        if step_id == "differential_analysis" or "differential" in step_id.lower():
                            # step_results å­˜å‚¨çš„æ˜¯å·¥å…·è¿”å›çš„åŸå§‹ç»“æœ
                            if isinstance(step_result, dict):
                                diff_result = step_result
                                break
                    
                    if isinstance(diff_result, dict):
                        # å¯¹äº visualize_volcanoï¼Œéœ€è¦ä¼ é€’å®Œæ•´çš„ diff_results å­—å…¸
                        if key == "diff_results":
                            processed[key] = diff_result
                            logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> å®Œæ•´ diff_results å­—å…¸")
                            continue
                        else:
                            # å…¶ä»–æƒ…å†µï¼Œå°è¯•æå–ç‰¹å®šå­—æ®µ
                            output_path = (
                                diff_result.get("output_file") or
                                diff_result.get("output_path") or
                                diff_result.get("file_path")
                            )
                            if output_path:
                                processed[key] = output_path
                                logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {output_path}")
                                continue
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ° differential_analysis æ­¥éª¤ç»“æœï¼Œæ— æ³•è§£æ <{placeholder}>")
                
                # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼šä» differential_analysis ç»“æœä¸­æå–åˆ†ç»„ä¿¡æ¯
                if placeholder in ["differential_analysis_case_group", "differential_analysis_control_group"]:
                    # æŸ¥æ‰¾ differential_analysis æ­¥éª¤çš„ç»“æœ
                    diff_result = None
                    for step_id, step_result in self.step_results.items():
                        if step_id == "differential_analysis" or "differential" in step_id.lower():
                            # æ£€æŸ¥ step_result çš„ç»“æ„ï¼ˆå¯èƒ½æ˜¯åŒ…è£…åœ¨ result å­—æ®µä¸­ï¼‰
                            if isinstance(step_result, dict):
                                actual_result = step_result.get("result", step_result)
                                if isinstance(actual_result, dict):
                                    diff_result = actual_result
                                    break
                    
                    if isinstance(diff_result, dict):
                        # å°è¯•ä»å¤šä¸ªä½ç½®æå–åˆ†ç»„ä¿¡æ¯
                        summary = diff_result.get("summary", {})
                        if placeholder == "differential_analysis_case_group":
                            extracted_value = (
                                diff_result.get("case_group") or
                                diff_result.get("group1") or
                                summary.get("case_group") or
                                diff_result.get("experimental_group")
                            )
                        else:  # differential_analysis_control_group
                            extracted_value = (
                                diff_result.get("control_group") or
                                diff_result.get("group2") or
                                summary.get("control_group") or
                                diff_result.get("control_group")
                            )
                        
                        if extracted_value:
                            processed[key] = extracted_value
                            logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {extracted_value}")
                            continue
                        else:
                            logger.warning(f"âš ï¸ æ— æ³•ä» differential_analysis ç»“æœä¸­æå– {placeholder}")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ° differential_analysis æ­¥éª¤ç»“æœï¼Œæ— æ³•æå– {placeholder}")
                
                # å°è¯•ä» step_results ä¸­è·å–
                if placeholder in self.step_results:
                    step_result = self.step_results[placeholder]
                    # ğŸ”¥ CRITICAL FIX: å¯¹äº scRNA-seq å·¥å…·ï¼Œä¼˜å…ˆæå– output_h5ad
                    if isinstance(step_result, dict):
                        # å¯¹äº scRNA-seq å·¥å…·ï¼Œä¼˜å…ˆæŸ¥æ‰¾ output_h5ad
                        if tool_category == "scRNA-seq":
                            output_path = (
                                step_result.get("output_h5ad") or  # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ output_h5ad
                                step_result.get("output_file") or
                                step_result.get("output_path") or
                                step_result.get("file_path")
                            )
                        else:
                            # å…¶ä»–å·¥å…·ä½¿ç”¨æ ‡å‡†å­—æ®µ
                            output_path = (
                                step_result.get("output_file") or
                                step_result.get("output_path") or
                                step_result.get("file_path") or
                                step_result.get("plot_path") or
                                step_result.get("result_path") or
                                step_result.get("preprocessed_file")
                            )
                        if output_path:
                            processed[key] = output_path
                            logger.info(f"ğŸ”„ æ•°æ®æµ: {key} = <{placeholder}> -> {output_path}")
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è·¯å¾„ï¼Œä½¿ç”¨æ•´ä¸ªç»“æœ
                            processed[key] = step_result
                    else:
                        processed[key] = step_result
                elif step_context and placeholder in step_context:
                    processed[key] = step_context[placeholder]
                else:
                    # å ä½ç¬¦æœªè§£æï¼Œä¿æŒåŸæ ·ï¼ˆå¯èƒ½åç»­æ­¥éª¤ä¼šå¤„ç†ï¼‰
                    logger.warning(f"âš ï¸ æ— æ³•è§£æå ä½ç¬¦: {value}")
                    processed[key] = value
            else:
                # éå ä½ç¬¦å‚æ•°å·²ç»åœ¨å¾ªç¯å¼€å§‹å‰å¤åˆ¶ï¼Œè¿™é‡Œä¸éœ€è¦å†æ¬¡å¤åˆ¶
                # ä½†å¦‚æœè¿™ä¸ª key ä¸åœ¨ processed ä¸­ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œè¿˜æ˜¯å¤åˆ¶ä¸€ä¸‹
                if key not in processed:
                    processed[key] = value
        
        # ğŸ”¥ CRITICAL FIX: å¼ºåˆ¶ç¡®ä¿ group_column ç­‰å…³é”®å‚æ•°æ²¡æœ‰è¢«æ„å¤–ç§»é™¤
        # è¿™æ˜¯æœ€åçš„ä¿æŠ¤æªæ–½ï¼Œç¡®ä¿å³ä½¿å‰é¢çš„é€»è¾‘æœ‰é—®é¢˜ï¼Œgroup_column ä¹Ÿä¸ä¼šä¸¢å¤±
        for key, value in critical_params_backup.items():
            if key not in processed:
                processed[key] = value
                logger.error(f"âŒ [æ•°æ®æµå¤„ç†] CRITICAL: {key} ä¸¢å¤±ï¼Œå·²å¼ºåˆ¶æ¢å¤: {value}")
            else:
                logger.debug(f"âœ… [æ•°æ®æµå¤„ç†] {key} å·²ä¿ç•™: {processed[key]}")
        
        # åŒé‡æ£€æŸ¥ï¼šå¦‚æœåŸå§‹ params ä¸­æœ‰ group_columnï¼Œä½† processed ä¸­æ²¡æœ‰ï¼Œå†æ¬¡æ¢å¤
        if "group_column" in params and "group_column" not in processed:
            processed["group_column"] = params["group_column"]
            logger.error(f"âŒ [æ•°æ®æµå¤„ç†] CRITICAL: group_column åœ¨åŒé‡æ£€æŸ¥ä¸­ä¸¢å¤±ï¼Œå·²å¼ºåˆ¶æ¢å¤: {params['group_column']}")
        
        return processed
    
    def _is_image_file(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            å¦‚æœæ˜¯å›¾ç‰‡æ–‡ä»¶è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        if not file_path:
            return False
        
        # æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶æ‰©å±•å
        image_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.pdf', '.tiff', '.tif'}
        
        # è·å–æ–‡ä»¶æ‰©å±•åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        file_ext = os.path.splitext(file_path.lower())[1]
        
        return file_ext in image_extensions
    
    def _detect_group_column_from_file(self, file_path: str) -> Optional[str]:
        """
        ä»æ–‡ä»¶ä¸­è‡ªåŠ¨æ£€æµ‹åˆ†ç»„åˆ—
        
        ä½¿ç”¨å¯å‘å¼æ–¹æ³•ï¼š
        1. ä¼˜å…ˆæ£€æŸ¥åŒ…å«åˆ†ç»„å…³é”®è¯çš„éæ•°å€¼åˆ—
        2. æ£€æŸ¥å”¯ä¸€å€¼ <= 5 çš„æ•°å€¼åˆ—
        3. æ£€æŸ¥å”¯ä¸€å€¼ <= 5 çš„éæ•°å€¼åˆ—
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ£€æµ‹åˆ°çš„åˆ†ç»„åˆ—åï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
        """
        try:
            import pandas as pd
            
            # è¯»å–æ–‡ä»¶ï¼ˆé‡‡æ ·è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶é—®é¢˜ï¼‰
            df = pd.read_csv(file_path, index_col=0, nrows=1000)
            
            # ä¼˜å…ˆçº§å…³é”®è¯åˆ—è¡¨ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼Œå¦‚"Muscle loss"ä¼šåŒ¹é…åŒ…å«"Muscle"æˆ–"Loss"çš„åˆ—ï¼‰
            priority_keywords = ['Diet', 'diet', 'Group', 'group', 'Condition', 'condition', 
                                'Treatment', 'treatment', 'Class', 'class', 'Category', 'category',
                                'Type', 'type', 'Label', 'label', 'Status', 'status',
                                'Muscle', 'muscle', 'Loss', 'loss', 'MuscleLoss', 'muscleloss',
                                'Muscle_loss', 'muscle_loss']  # æ·»åŠ ç”¨æˆ·æ•°æ®ç›¸å…³çš„å…³é”®è¯
            
            # è¯†åˆ«åˆ—ç±»å‹
            metadata_cols = []
            numeric_cols = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    numeric_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            # æ–¹æ³•1: æ£€æŸ¥éæ•°å€¼åˆ—ï¼ˆmetadata_colsï¼‰ï¼Œä¼˜å…ˆå…³é”®è¯åŒ¹é…
            # ğŸ”¥ æ”¹è¿›ï¼šæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼Œå¦‚"Muscle loss"ä¼šåŒ¹é…"MuscleLoss"æˆ–"Muscle_loss"
            for col in metadata_cols:
                col_lower = col.lower().replace(' ', '').replace('_', '').replace('-', '')
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å…³é”®è¯ï¼ˆå¿½ç•¥å¤§å°å†™ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ï¼‰
                if any(keyword.lower().replace(' ', '').replace('_', '').replace('-', '') in col_lower 
                       for keyword in priority_keywords):
                    unique_count = df[col].nunique()
                    if 2 <= unique_count <= 20:  # åˆç†çš„åˆ†ç»„æ•°é‡
                        logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰: {col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                        return col
            
            # æ–¹æ³•2: æ£€æŸ¥æ•°å€¼åˆ—ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5ï¼Œå½“ä½œåˆ†ç±»å˜é‡
            for col in numeric_cols:
                unique_count = df[col].nunique()
                if 2 <= unique_count <= 5:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                    if any(keyword in col for keyword in priority_keywords):
                        logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆæ•°å€¼å‹å…³é”®è¯åŒ¹é…ï¼‰: {col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                        return col
                    # æˆ–è€…æ£€æŸ¥æ˜¯å¦æ˜¯äºŒå…ƒå˜é‡ï¼ˆ0/1ï¼‰
                    unique_values = sorted(df[col].dropna().unique().tolist())
                    if unique_values == [0, 1] or unique_values == [1, 0]:
                        logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆäºŒå…ƒæ•°å€¼å‹ï¼‰: {col}")
                        return col
            
            # æ–¹æ³•3: æ£€æŸ¥éæ•°å€¼åˆ—ï¼Œå¦‚æœå”¯ä¸€å€¼ <= 5
            for col in metadata_cols:
                unique_count = df[col].nunique()
                if 2 <= unique_count <= 5:
                    logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆéæ•°å€¼å‹ï¼Œå”¯ä¸€å€¼ <= 5ï¼‰: {col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                    return col
            
            # æ–¹æ³•4: å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ç¬¬ä¸€ä¸ª metadata_colsï¼ˆå¦‚æœæœ‰ï¼‰
            if metadata_cols:
                first_meta_col = metadata_cols[0]
                unique_count = df[first_meta_col].nunique()
                if 2 <= unique_count <= 20:
                    logger.info(f"âœ… [Executor] æ£€æµ‹åˆ°åˆ†ç»„åˆ—ï¼ˆé»˜è®¤ metadata_colsï¼‰: {first_meta_col} ({unique_count} ä¸ªå”¯ä¸€å€¼)")
                    return first_meta_col
            
            logger.warning("âš ï¸ [Executor] æœªæ£€æµ‹åˆ°åˆé€‚çš„åˆ†ç»„åˆ—")
            return None
            
        except Exception as e:
            logger.error(f"âŒ [Executor] æ£€æµ‹åˆ†ç»„åˆ—å¤±è´¥: {e}", exc_info=True)
            return None
    
    def execute_workflow(
        self,
        workflow_data: Dict[str, Any],
        file_paths: List[str] = None,
        output_dir: Optional[str] = None,
        agent: Optional[Any] = None  # å¯é€‰çš„ Agent å®ä¾‹ï¼Œç”¨äºç”Ÿæˆè¯Šæ–­
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•´ä¸ªå·¥ä½œæµ
        
        Args:
            workflow_data: å·¥ä½œæµé…ç½®ï¼ˆåŒ…å« workflow_name å’Œ stepsï¼‰
            file_paths: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œå°†è‡ªåŠ¨åˆ›å»ºï¼‰
        
        Returns:
            æ‰§è¡ŒæŠ¥å‘Šï¼ˆç¬¦åˆå‰ç«¯ analysis_report æ ¼å¼ï¼‰
        """
        workflow_name = workflow_data.get("workflow_name", "Unknown Workflow")
        steps = workflow_data.get("steps", [])
        
        logger.info("=" * 80)
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: {workflow_name}")
        logger.info(f"ğŸ“‹ æ­¥éª¤æ•°: {len(steps)}")
        logger.info("=" * 80)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"./results/run_{timestamp}"
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        self.output_dir = str(output_path)
        
        logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åˆå§‹åŒ–æ­¥éª¤ç»“æœåˆ—è¡¨
        steps_details = []
        steps_results = []
        
        # ğŸ”¥ ä¸Šä¸‹æ–‡é“¾ï¼šè·Ÿè¸ªå½“å‰æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè‡ªåŠ¨ä¼ é€’ç»™ä¸‹ä¸€ä¸ªæ­¥éª¤
        # ğŸ”¥ CRITICAL REGRESSION FIX: Resolve file paths to absolute paths
        resolved_file_paths = []
        if file_paths:
            for fp in file_paths:
                if fp and isinstance(fp, str):
                    resolved = self._resolve_file_path(fp)
                    resolved_file_paths.append(resolved)
                    if resolved != fp:
                        logger.info(f"ğŸ”„ [Executor] è§£æè¾“å…¥æ–‡ä»¶è·¯å¾„: {fp} -> {resolved}")
        current_file_path = resolved_file_paths[0] if resolved_file_paths else None
        
        # æ‰§è¡Œæ¯ä¸ªæ­¥éª¤
        for i, step in enumerate(steps, 1):
            step_id = step.get("step_id", f"step{i}")
            step_name = step.get("name", step.get("step_name", step_id))
            tool_id = step.get("tool_id", step_id)
            params = step.get("params", {})
            
            # ğŸ”¥ CRITICAL FIX: å®Œå…¨ç§»é™¤ visualize_pca æ­¥éª¤ï¼ˆä¸åœ¨æµç¨‹ä¸­æ˜¾ç¤ºï¼‰
            if tool_id == "visualize_pca" or step_id == "visualize_pca":
                logger.warning(f"âš ï¸ [Executor] å®Œå…¨ç§»é™¤ visualize_pca æ­¥éª¤ï¼ˆpca_analysis å·²åŒ…å«å¯è§†åŒ–ï¼‰")
                continue  # ç›´æ¥è·³è¿‡ï¼Œä¸æ·»åŠ åˆ°æ­¥éª¤è¯¦æƒ…ä¸­
            
            logger.info(f"\n{'=' * 80}")
            logger.info(f"ğŸ“Œ æ­¥éª¤ {i}/{len(steps)}: {step_name} ({step_id})")
            logger.info(f"{'=' * 80}")
            
            # ğŸ”¥ æ™ºèƒ½å‚æ•°æ˜ å°„ï¼šæ ¹æ®å·¥å…·ç±»å‹è‡ªåŠ¨æ˜ å°„æ–‡ä»¶è·¯å¾„å‚æ•°
            tool_metadata = registry.get_metadata(tool_id)
            tool_category = tool_metadata.category if tool_metadata else None
            
            # ç¡®å®šå·¥å…·æœŸæœ›çš„æ–‡ä»¶è·¯å¾„å‚æ•°å
            if tool_category == "scRNA-seq":
                # RNA å·¥å…·ä½¿ç”¨ adata_path
                file_param_name = "adata_path"
            else:
                # å…¶ä»–å·¥å…·ï¼ˆå¦‚ä»£è°¢ç»„å­¦ï¼‰ä½¿ç”¨ file_path
                file_param_name = "file_path"
            
            # ğŸ”¥ CRITICAL FIX: æ£€æŸ¥æ˜¯å¦æœ‰å ä½ç¬¦éœ€è¦å¤„ç†
            # å ä½ç¬¦ï¼ˆå¦‚ <preprocess_data_output>ï¼‰å¿…é¡»ä¼˜å…ˆäºè‡ªåŠ¨æ³¨å…¥çš„ current_file_path
            has_placeholder = any(
                isinstance(v, str) and v.startswith("<") and v.endswith(">")
                for v in params.values()
            )
            
            # ğŸ”¥ CRITICAL FIX: å¦‚æœå­˜åœ¨å ä½ç¬¦ï¼ŒDO NOT è‡ªåŠ¨æ³¨å…¥æ–‡ä»¶è·¯å¾„
            # å ä½ç¬¦ä¼šåœ¨ execute_step å†…éƒ¨çš„ _process_data_flow ä¸­è§£æ
            # åªæœ‰åœ¨æ²¡æœ‰å ä½ç¬¦ä¸”å‚æ•°ç¼ºå¤±æ—¶ï¼Œæ‰è‡ªåŠ¨æ³¨å…¥
            if not has_placeholder:
                # è‡ªåŠ¨æ³¨å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœç¼ºå¤±ä¸”æˆ‘ä»¬æœ‰å½“å‰æ–‡ä»¶è·¯å¾„ï¼‰
                if file_param_name not in params and current_file_path:
                    params[file_param_name] = current_file_path
                    logger.info(f"ğŸ”„ è‡ªåŠ¨æ³¨å…¥ {file_param_name}: {current_file_path}")
            else:
                # æœ‰å ä½ç¬¦ï¼Œè®°å½•æ—¥å¿—ä½†ä¸è‡ªåŠ¨æ³¨å…¥
                placeholder_keys = [k for k, v in params.items() if isinstance(v, str) and v.startswith("<") and v.endswith(">")]
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å ä½ç¬¦ {placeholder_keys}ï¼Œè·³è¿‡è‡ªåŠ¨æ³¨å…¥ï¼Œç­‰å¾…å ä½ç¬¦è§£æ")
            
            # æ„å»ºæ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ–‡ä»¶è·¯å¾„ç­‰ï¼‰
            step_context = {
                "file_paths": file_paths or [],
                "output_dir": self.output_dir,
                "workflow_name": workflow_name,
                "current_file_path": current_file_path  # ä¼ é€’å½“å‰æ–‡ä»¶è·¯å¾„
            }
            
            # ğŸ”¥ å‚æ•°æ˜ å°„ï¼šå¦‚æœå·¥å…·æœŸæœ› adata_path ä½†æä¾›äº† file_pathï¼Œè¿›è¡Œæ˜ å°„
            if file_param_name == "adata_path" and "file_path" in params and file_param_name not in params:
                params[file_param_name] = params.pop("file_path")
                logger.info(f"ğŸ”„ å‚æ•°æ˜ å°„: file_path -> {file_param_name}")
            
            # å¦‚æœå·¥å…·éœ€è¦ output_dirï¼Œä¹Ÿè‡ªåŠ¨æ³¨å…¥
            if "output_dir" not in params and self.output_dir:
                params["output_dir"] = self.output_dir
            
            # æ›´æ–°æ­¥éª¤çš„ params
            step["params"] = params
            
            # æ‰§è¡Œæ­¥éª¤ï¼ˆå†…éƒ¨ä¼šè°ƒç”¨ _process_data_flow å¤„ç†å ä½ç¬¦ï¼‰
            step_result = self.execute_step(step, step_context)
            
            # ğŸ”¥ CRITICAL FIX: æ›´æ–° current_file_path ä¾›ä¸‹ä¸€ä¸ªæ­¥éª¤ä½¿ç”¨
            # ä½†æ˜¯ï¼šåªæœ‰ preprocess_data æ­¥éª¤çš„è¾“å‡ºæ‰åº”è¯¥æ›´æ–° current_file_path
            # å…¶ä»–æ­¥éª¤ï¼ˆå¦‚ differential_analysisï¼‰çš„è¾“å‡ºä¸åº”è¯¥å½±å“åç»­æ­¥éª¤çš„æ–‡ä»¶è·¯å¾„
            # å› ä¸ºåç»­æ­¥éª¤ï¼ˆå¦‚ PLS-DAã€PCAï¼‰åº”è¯¥ä½¿ç”¨ <preprocess_data_output> å ä½ç¬¦
            result_data = step_result.get("result", {})
            if isinstance(result_data, dict):
                tool_id = step.get("tool_id", "")
                
                # ğŸ”¥ CRITICAL FIX: åªæœ‰ preprocess_data æ­¥éª¤çš„è¾“å‡ºæ‰æ›´æ–° current_file_path
                # å…¶ä»–æ­¥éª¤çš„è¾“å‡ºä¸åº”è¯¥å½±å“åç»­æ­¥éª¤çš„æ–‡ä»¶è·¯å¾„
                if tool_id == "preprocess_data" or "preprocess" in tool_id.lower():
                    # å¯¹äº scRNA-seq å·¥å…·ï¼Œä¼˜å…ˆæŸ¥æ‰¾ output_h5ad
                    tool_metadata = registry.get_metadata(tool_id)
                    tool_category = tool_metadata.category if tool_metadata else None
                    
                    if tool_category == "scRNA-seq":
                        # scRNA-seq å·¥å…·ä¼˜å…ˆä½¿ç”¨ output_h5ad
                        next_file_path = (
                            result_data.get("output_h5ad") or
                            result_data.get("output_file") or
                            result_data.get("output_path") or
                            result_data.get("file_path")
                        )
                    else:
                        # å…¶ä»–å·¥å…·ä½¿ç”¨æ ‡å‡†å­—æ®µ
                        next_file_path = (
                            result_data.get("output_file") or
                            result_data.get("output_path") or
                            result_data.get("file_path") or
                            result_data.get("preprocessed_file")
                        )
                    
                    if next_file_path:
                        # ğŸ”¥ ä¿®å¤ï¼šå³ä½¿æ–‡ä»¶ä¸å­˜åœ¨ä¹Ÿæ›´æ–°è·¯å¾„ï¼ˆæ–‡ä»¶å¯èƒ½ç¨ååˆ›å»ºï¼‰
                        current_file_path = next_file_path
                        if os.path.exists(next_file_path):
                            logger.info(f"âœ… [Executor] æ›´æ–°å½“å‰æ–‡ä»¶è·¯å¾„ï¼ˆæ¥è‡ª preprocess_dataï¼‰: {current_file_path}")
                        else:
                            logger.warning(f"âš ï¸ [Executor] è¾“å‡ºè·¯å¾„ä¸å­˜åœ¨ï¼Œä½†ä¼šä½¿ç”¨: {next_file_path} (æ–‡ä»¶å¯èƒ½ç¨ååˆ›å»º)")
                else:
                    # å…¶ä»–æ­¥éª¤çš„è¾“å‡ºä¸æ›´æ–° current_file_path
                    # åç»­æ­¥éª¤åº”è¯¥ä½¿ç”¨å ä½ç¬¦ï¼ˆå¦‚ <preprocess_data_output>ï¼‰è€Œä¸æ˜¯ current_file_path
                    logger.debug(f"ğŸ” [Executor] æ­¥éª¤ {tool_id} çš„è¾“å‡ºä¸æ›´æ–° current_file_pathï¼ˆåç»­æ­¥éª¤åº”ä½¿ç”¨å ä½ç¬¦ï¼‰")
            
            # æ„å»ºæ­¥éª¤è¯¦æƒ…ï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
            step_detail = {
                "step_id": step_id,
                "tool_id": step.get("tool_id"),
                "name": step_name,
                "status": step_result.get("status", "error"),
                "summary": step_result.get("message", ""),
                "step_result": {
                    "step_name": step_name,
                    "status": step_result.get("status", "error"),
                    "logs": step_result.get("message", ""),
                    "data": step_result.get("result", {})
                }
            }
            
            # ğŸ”¥ æå–å›¾ç‰‡è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰- ä¸¥æ ¼æ£€æŸ¥æ–‡ä»¶ç±»å‹
            result_data = step_result.get("result", {})
            if isinstance(result_data, dict):
                # ä¼˜å…ˆæ£€æŸ¥æ˜ç¡®çš„å›¾ç‰‡è·¯å¾„å­—æ®µ
                plot_path = result_data.get("plot_path") or result_data.get("image_path")
                
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å›¾ç‰‡è·¯å¾„å­—æ®µï¼Œæ£€æŸ¥ output_path çš„æ–‡ä»¶æ‰©å±•å
                if not plot_path:
                    output_path = result_data.get("output_path") or result_data.get("output_file") or result_data.get("file_path")
                    if output_path and self._is_image_file(output_path):
                        plot_path = output_path
                
                # åªæœ‰ç¡®è®¤æ˜¯å›¾ç‰‡æ–‡ä»¶æ‰æ·»åŠ åˆ° plot å­—æ®µ
                if plot_path and self._is_image_file(plot_path):
                    step_detail["plot"] = plot_path
                    logger.info(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾ç‰‡æ–‡ä»¶: {plot_path}")
                elif plot_path:
                    # å¦‚æœä¸æ˜¯å›¾ç‰‡æ–‡ä»¶ï¼ˆå¦‚ CSVï¼‰ï¼Œè®°å½•åˆ° data å­—æ®µè€Œä¸æ˜¯ plot
                    logger.debug(f"ğŸ“„ æ£€æµ‹åˆ°éå›¾ç‰‡æ–‡ä»¶: {plot_path}ï¼Œä¸æ·»åŠ åˆ° plot å­—æ®µ")
            
            steps_details.append(step_detail)
            steps_results.append(step_detail["step_result"])
            
            # ğŸ”¥ CRITICAL REGRESSION FIX: If async job started, stop execution
            if step_result.get("status") == "async_job_started":
                logger.info(f"ğŸš€ [Executor] æ£€æµ‹åˆ°å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨: {step_id}, job_id: {step_result.get('job_id', 'N/A')}")
                logger.info(f"ğŸš€ [Executor] åœæ­¢æ‰§è¡Œåç»­æ­¥éª¤ï¼Œç­‰å¾…å¼‚æ­¥ä½œä¸šå®Œæˆ")
                # Add job_id to step_detail if present
                if "job_id" in step_result:
                    step_detail["job_id"] = step_result["job_id"]
                break  # STOP HERE - Do not execute next steps
            
            # ğŸ”¥ CRITICAL FIX: å³ä½¿æ­¥éª¤å¤±è´¥ï¼Œä¹Ÿç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤
            # è¿™æ ·å¯ä»¥ç¡®ä¿å‰é¢çš„æ­¥éª¤ç»“æœæ­£å¸¸æ˜¾ç¤ºï¼Œä¸ä¼šå› ä¸ºåç»­æ­¥éª¤å¤±è´¥è€Œå½±å“å‰é¢çš„æ˜¾ç¤º
            if step_result.get("status") == "error":
                error_msg = step_result.get("error") or step_result.get("message") or "æœªçŸ¥é”™è¯¯"
                logger.error(f"âŒ æ­¥éª¤ {step_id} å¤±è´¥: {error_msg}")
                logger.warning(f"âš ï¸ ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤ï¼Œç¡®ä¿å‰é¢çš„æ­¥éª¤ç»“æœæ­£å¸¸è¿”å›")
                # ä¸ breakï¼Œç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤
                # æ³¨æ„ï¼šå¦‚æœåç»­æ­¥éª¤ä¾èµ–äºå½“å‰å¤±è´¥çš„æ­¥éª¤ï¼Œå®ƒä»¬å¯èƒ½ä¼šå¤±è´¥ï¼Œä½†è‡³å°‘ä¼šå°è¯•æ‰§è¡Œ
        
        # ç¡®å®šæœ€ç»ˆçŠ¶æ€
        all_success = all(
            detail.get("status") == "success"
            for detail in steps_details
        )
        workflow_status = "success" if all_success else "error"
        
        # æå–æœ€ç»ˆå›¾ç‰‡ï¼ˆæœ€åä¸€ä¸ªæˆåŠŸæ­¥éª¤çš„å›¾ç‰‡ï¼‰
        final_plot = None
        for detail in reversed(steps_details):
            if detail.get("plot"):
                final_plot = detail["plot"]
                break
        
        # æ„å»ºæ‰§è¡ŒæŠ¥å‘Šï¼ˆç¬¦åˆå‰ç«¯æ ¼å¼ï¼‰
        report_data = {
            "status": workflow_status,
            "workflow_name": workflow_name,
            "steps_details": steps_details,
            "steps_results": steps_results,
            "output_dir": self.output_dir
        }
        
        if final_plot:
            report_data["final_plot"] = final_plot
        
        logger.info("=" * 80)
        logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {workflow_name} (çŠ¶æ€: {workflow_status})")
        logger.info(f"ğŸ“Š æˆåŠŸæ­¥éª¤: {sum(1 for d in steps_details if d.get('status') == 'success')}/{len(steps_details)}")
        logger.info("=" * 80)
        
        # ğŸ”¥ ç”Ÿæˆ AI Expert Diagnosisï¼ˆå¦‚æœæä¾›äº† Agent å®ä¾‹ï¼‰
        # æ³¨æ„ï¼šç”±äº execute_workflow æ˜¯åŒæ­¥æ–¹æ³•ï¼Œè¯Šæ–­ç”Ÿæˆå°†åœ¨åå°è¿›è¡Œæˆ–è·³è¿‡
        # å®é™…è¯Šæ–­ç”Ÿæˆåº”è¯¥åœ¨ Agent çš„ execute_workflow æ–¹æ³•ä¸­è°ƒç”¨
        # è¿™é‡Œåªæ ‡è®°éœ€è¦è¯Šæ–­ï¼Œä¸å®é™…ç”Ÿæˆï¼ˆé¿å…å¼‚æ­¥/åŒæ­¥æ··ç”¨é—®é¢˜ï¼‰
        if agent and hasattr(agent, '_generate_analysis_summary'):
            logger.info("ğŸ“ [Executor] Agent å®ä¾‹å·²æä¾›ï¼Œè¯Šæ–­å°†åœ¨ Agent å±‚ç”Ÿæˆ")
        
        # ğŸ”¥ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨ï¼ˆå¤„ç† Numpy ç±»å‹ã€NaN/Infinity ç­‰ï¼‰
        logger.info("ğŸ§¹ æ¸…ç†æ•°æ®ä»¥ç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨...")
        sanitized_report = sanitize_for_json(report_data)
        logger.info("âœ… æ•°æ®æ¸…ç†å®Œæˆ")
        
        return sanitized_report

