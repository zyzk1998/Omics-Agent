"""
Agentic ç»„ä»¶ - æ™ºèƒ½æŸ¥è¯¢å¤„ç†

åŒ…å«ï¼š
1. QueryRewriter: æŸ¥è¯¢é‡å†™å™¨ï¼ˆå°†æ¨¡ç³ŠæŸ¥è¯¢è½¬æ¢ä¸ºæŠ€æœ¯æœ¯è¯­ï¼‰
2. Clarifier: æ¾„æ¸…å™¨ï¼ˆä¸»åŠ¨è¯¢é—®ç¼ºå¤±ä¿¡æ¯ï¼‰
3. Reflector: åæ€å™¨ï¼ˆè‡ªæˆ‘æ£€æŸ¥å’Œçº æ­£å·¥ä½œæµè®¡åˆ’ï¼‰
"""
import json
import logging
from typing import Dict, Any, List, Optional
from .llm_client import LLMClient

logger = logging.getLogger(__name__)


class QueryRewriter:
    """
    æŸ¥è¯¢é‡å†™å™¨
    
    å°†ç”¨æˆ·çš„æ¨¡ç³ŠæŸ¥è¯¢è½¬æ¢ä¸ºç²¾ç¡®çš„ç”Ÿç‰©ä¿¡æ¯å­¦æœ¯è¯­ã€‚
    ä¾‹å¦‚ï¼š"run that cell thing" -> "Execute scRNA-seq pipeline starting from CellRanger"
    """
    
    def __init__(self, llm_client: LLMClient):
        """
        åˆå§‹åŒ–æŸ¥è¯¢é‡å†™å™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯å®žä¾‹
        """
        self.llm_client = llm_client
    
    async def rewrite(self, raw_query: str, history: List[Dict[str, str]] = None) -> str:
        """
        é‡å†™æŸ¥è¯¢
        
        Args:
            raw_query: ç”¨æˆ·çš„åŽŸå§‹æŸ¥è¯¢
            history: å¯¹è¯åŽ†å²ï¼ˆå¯é€‰ï¼Œç”¨äºŽä¸Šä¸‹æ–‡ï¼‰
            
        Returns:
            é‡å†™åŽçš„æŸ¥è¯¢
        """
        try:
            logger.info(f"ðŸ”„ [QueryRewriter] é‡å†™æŸ¥è¯¢: '{raw_query}'")
            
            system_prompt = """You are a Bioinformatics Query Translator.

Your task is to translate vague user queries into precise bioinformatics technical terms.

**Examples:**
- "run that cell thing" -> "Execute scRNA-seq pipeline starting from CellRanger"
- "do pca" -> "Perform Principal Component Analysis (PCA)"
- "analyze this file" -> "Analyze the uploaded file using standard metabolomics pipeline"
- "é‚£ä¸ªç»†èƒžåˆ†æž" -> "æ‰§è¡Œå•ç»†èƒžè½¬å½•ç»„åˆ†æžæµç¨‹"

**Rules:**
1. Translate vague terms into specific bioinformatics operations
2. Preserve the user's intent
3. Use standard bioinformatics terminology
4. If the query is already clear, return it as-is
5. Output ONLY the refined query, no explanations

**Output Format:**
Return ONLY the refined query text, no markdown, no code blocks."""

            user_prompt = f"""**User Query:**
{raw_query}

**Context:**
{self._format_history(history) if history else "No conversation history"}

**Task:**
Translate this query into precise bioinformatics technical terms."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            response = await self.llm_client.achat(
                messages=messages,
                temperature=0.3,
                max_tokens=256
            )
            
            # ðŸ”¥ FIX: æå– ChatCompletion å¯¹è±¡çš„å†…å®¹
            if hasattr(response, 'choices') and response.choices:
                refined_query = response.choices[0].message.content or ""
            else:
                refined_query = str(response)
            refined_query = refined_query.strip()
            logger.info(f"âœ… [QueryRewriter] é‡å†™å®Œæˆ: '{raw_query}' -> '{refined_query}'")
            
            return refined_query
        
        except Exception as e:
            logger.error(f"âŒ [QueryRewriter] é‡å†™å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶è¿”å›žåŽŸå§‹æŸ¥è¯¢
            return raw_query
    
    def _format_history(self, history: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯åŽ†å²"""
        if not history:
            return ""
        
        formatted = []
        for item in history[-5:]:  # åªä½¿ç”¨æœ€è¿‘5æ¡
            role = item.get("role", "user")
            content = item.get("content", "")
            formatted.append(f"{role.capitalize()}: {content}")
        
        return "\n".join(formatted)


class Clarifier:
    """
    æ¾„æ¸…å™¨
    
    ä¸»åŠ¨æ£€æŸ¥ç¼ºå¤±çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¯¢é—®ç”¨æˆ·ã€‚
    ä¾‹å¦‚ï¼šä»£è°¢ç»„å­¦åˆ†æžéœ€è¦åˆ†ç»„åˆ—ï¼Œå¦‚æžœç¼ºå¤±åˆ™è¯¢é—®ç”¨æˆ·ã€‚
    """
    
    def __init__(self, llm_client: LLMClient):
        """
        åˆå§‹åŒ–æ¾„æ¸…å™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯å®žä¾‹
        """
        self.llm_client = llm_client
    
    async def check_and_clarify(
        self,
        refined_query: str,
        file_metadata: Optional[Dict[str, Any]] = None,
        domain: Optional[str] = None
    ) -> Optional[str]:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…
        
        ðŸ”¥ ARCHITECTURAL MERGE: ä¿®å¤ Plan-First é€»è¾‘
        - åŒºåˆ†è§„åˆ’å’Œæ‰§è¡Œæ„å›¾
        - å¦‚æžœæ˜¯è§„åˆ’æ„å›¾ï¼Œä¸è¦æ±‚æ–‡ä»¶
        
        Args:
            refined_query: é‡å†™åŽçš„æŸ¥è¯¢
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            domain: åŸŸåï¼ˆMetabolomics æˆ– RNAï¼‰
            
        Returns:
            å¦‚æžœéœ€è¦æ¾„æ¸…ï¼Œè¿”å›žæ¾„æ¸…é—®é¢˜ï¼›å¦åˆ™è¿”å›ž None
        """
        try:
            logger.info(f"ðŸ” [Clarifier] æ£€æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…: '{refined_query}'")
            
            # ðŸ”¥ ARCHITECTURAL MERGE: é¦–å…ˆæ£€æŸ¥ç”¨æˆ·æ„å›¾ï¼ˆè§„åˆ’ vs æ‰§è¡Œï¼‰
            intent = await self._check_intent(refined_query, file_metadata)
            
            if intent == "planning":
                # ç”¨æˆ·åªæ˜¯æƒ³è§„åˆ’å·¥ä½œæµï¼Œä¸éœ€è¦æ–‡ä»¶
                logger.info("âœ… [Clarifier] ç”¨æˆ·æ„å›¾ï¼šè§„åˆ’å·¥ä½œæµï¼Œæ— éœ€æ¾„æ¸…")
                return None
            
            # æž„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = self._build_context(file_metadata, domain)
            
            system_prompt = """You are a Bioinformatics Clarification Assistant.

Your task is to check if critical information is missing for executing the SOP (Standard Operating Procedure).

**Critical Checks:**
1. **Metabolomics Analysis:**
   - Requires a group column for differential analysis
   - If group column is missing, ask: "è¯·é—®æ‚¨æƒ³ä½¿ç”¨å“ªä¸€åˆ—ä½œä¸ºåˆ†ç»„åˆ—ï¼Ÿ"
   
2. **scRNA-seq Analysis:**
   - Requires input file type (FASTQ vs H5AD)
   - If ambiguous, ask: "è¯·é—®æ‚¨çš„è¾“å…¥æ•°æ®æ˜¯ FASTQ æ–‡ä»¶è¿˜æ˜¯ H5AD æ–‡ä»¶ï¼Ÿ"

3. **General:**
   - Check if file is uploaded (if file operations are needed)
   - Check if parameters are specified (if tool requires parameters)

**Output Format:**
- If clarification is needed: Return ONLY the clarification question in Chinese
- If no clarification needed: Return "OK"

**Rules:**
- Be polite and concise
- Ask ONE question at a time
- Use Simplified Chinese
- Only ask if information is truly critical"""

            user_prompt = f"""**Refined Query:**
{refined_query}

**Context:**
{context_info}

**Task:**
Check if critical information is missing. If yes, generate a polite clarification question."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            response = await self.llm_client.achat(
                messages=messages,
                temperature=0.2,
                max_tokens=128
            )
            
            # ðŸ”¥ FIX: æå– ChatCompletion å¯¹è±¡çš„å†…å®¹
            if hasattr(response, 'choices') and response.choices:
                clarification = response.choices[0].message.content or ""
            else:
                clarification = str(response)
            clarification = clarification.strip()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…
            if clarification.upper() == "OK" or clarification.lower() == "ok":
                logger.info("âœ… [Clarifier] æ— éœ€æ¾„æ¸…")
                return None
            else:
                logger.info(f"â“ [Clarifier] éœ€è¦æ¾„æ¸…: {clarification}")
                return clarification
        
        except Exception as e:
            logger.error(f"âŒ [Clarifier] æ¾„æ¸…æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶å‡è®¾ä¸éœ€è¦æ¾„æ¸…
            return None
    
    async def _check_intent(self, query: str, file_metadata: Optional[Dict[str, Any]]) -> str:
        """
        æ£€æŸ¥ç”¨æˆ·æ„å›¾ï¼šè§„åˆ’å·¥ä½œæµ vs æ‰§è¡Œå·¥ä½œæµ
        
        Returns:
            "planning" æˆ– "execution"
        """
        try:
            # å¦‚æžœå·²ç»æœ‰æ–‡ä»¶ï¼Œé€šå¸¸æ˜¯æ‰§è¡Œæ„å›¾
            if file_metadata:
                return "execution"
            
            # ðŸ”¥ URGENT FIX: ä½¿ç”¨ LLM åˆ¤æ–­æ„å›¾ - æ›´ä¸¥æ ¼è¯†åˆ« planning
            system_prompt = """You are an Intent Classifier.

Your task is to determine if the user wants to PLAN a workflow or EXECUTE a workflow.

**Planning Intent Keywords (STRICT):**
- "plan", "è§„åˆ’", "è®¾è®¡", "ç”Ÿæˆå·¥ä½œæµ", "create workflow", "ç”Ÿæˆæ–¹æ¡ˆ"
- "what steps", "å“ªäº›æ­¥éª¤", "how to", "æ€Žä¹ˆåš"
- "without file", "å…ˆè§„åˆ’", "å…ˆçœ‹çœ‹"
- "I want PCA", "I want to do PCA", "æˆ‘æƒ³åšPCA" (when no file)
- "show me", "æ˜¾ç¤º", "é¢„è§ˆ"
- Any query that asks for a workflow template WITHOUT immediate execution

**Execution Intent Keywords:**
- "run", "æ‰§è¡Œ", "åˆ†æž", "analyze", "process", "å¼€å§‹åˆ†æž"
- "do", "åš", "å¼€å§‹", "ç«‹å³"
- "proceed", "ç»§ç»­", "go ahead" (when plan already exists)
- Implies immediate action WITH a file

**CRITICAL RULE:**
- If user says "I want PCA" or "Do PCA" WITHOUT a file, classify as "planning"
- If user says "Plan PCA" or "è§„åˆ’PCA", classify as "planning"
- If user says "Proceed" or "ç»§ç»­" AFTER a plan exists, classify as "execution"

**Output Format:**
Return ONLY one word: "planning" or "execution"."""

            user_prompt = f"""**User Query:**
{query}

**File Available:** {"Yes" if file_metadata else "No"}

**Task:**
Classify the user's intent."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            response = await self.llm_client.achat(
                messages=messages,
                temperature=0.1,
                max_tokens=16
            )
            
            # ðŸ”¥ FIX: æå– ChatCompletion å¯¹è±¡çš„å†…å®¹
            if hasattr(response, 'choices') and response.choices:
                intent = response.choices[0].message.content or ""
            else:
                intent = str(response)
            intent = intent.strip().lower()
            if "plan" in intent:
                return "planning"
            else:
                return "execution"
        
        except Exception as e:
            logger.warning(f"âš ï¸ [Clarifier] æ„å›¾æ£€æŸ¥å¤±è´¥: {e}ï¼Œé»˜è®¤è¿”å›ž execution")
            return "execution"
    
    def _build_context(self, file_metadata: Optional[Dict[str, Any]], domain: Optional[str]) -> str:
        """æž„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []
        
        if domain:
            context_parts.append(f"Domain: {domain}")
        
        if file_metadata:
            context_parts.append("File Metadata Available:")
            
            # æ£€æŸ¥åˆ†ç»„åˆ—
            semantic_map = file_metadata.get("semantic_map", {})
            group_cols = semantic_map.get("group_cols", [])
            if group_cols:
                context_parts.append(f"  - Group columns: {', '.join(group_cols)}")
            else:
                context_parts.append("  - Group columns: None")
            
            # æ£€æŸ¥æ–‡ä»¶ç±»åž‹
            file_type = file_metadata.get("file_type", "unknown")
            context_parts.append(f"  - File type: {file_type}")
        else:
            context_parts.append("File Metadata: Not available")
        
        return "\n".join(context_parts)


class Reflector:
    """
    åæ€å™¨
    
    è‡ªæˆ‘æ£€æŸ¥å’Œçº æ­£å·¥ä½œæµè®¡åˆ’ï¼Œç¡®ä¿ç¬¦åˆ SOP è§„åˆ™ã€‚
    """
    
    def __init__(self, llm_client: LLMClient):
        """
        åˆå§‹åŒ–åæ€å™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯å®žä¾‹
        """
        self.llm_client = llm_client
    
    async def reflect_and_correct(
        self,
        workflow_plan: Dict[str, Any],
        domain: str,
        file_metadata: Optional[Dict[str, Any]] = None,
        dag_issues: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        åæ€å¹¶çº æ­£å·¥ä½œæµè®¡åˆ’
        
        Args:
            workflow_plan: ç”Ÿæˆçš„å·¥ä½œæµè®¡åˆ’
            domain: åŸŸåï¼ˆMetabolomics æˆ– RNAï¼‰
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            çº æ­£åŽçš„å·¥ä½œæµè®¡åˆ’ï¼ˆå¦‚æžœå‘çŽ°é—®é¢˜ï¼‰æˆ–åŽŸå§‹è®¡åˆ’
        """
        try:
            logger.info(f"ðŸ” [Reflector] åæ€å·¥ä½œæµè®¡åˆ’: {domain}")
            
            # èŽ·å– SOP è§„åˆ™
            sop_rules = self._get_sop_rules(domain)
            
            # å¤„ç† DAG é—®é¢˜
            dag_issues_text = ""
            if dag_issues:
                dag_issues_text = "\n".join([f"- {issue}" for issue in dag_issues])
            else:
                dag_issues_text = "None (DAG validation passed)"
            
            system_prompt = """You are a Bioinformatics Workflow Validator.

Your task is to review a generated workflow plan against SOP (Standard Operating Procedure) rules and identify any flaws.

**SOP Rules:**
{sop_rules}

            **Common Flaws to Check:**
1. Missing required steps (e.g., Normalization before PCA)
2. Incorrect step order (e.g., Clustering before Neighbors)
3. Missing prerequisites (e.g., QC before Normalization)
4. Incompatible parameters

**DAG Issues (if provided):**
{dag_issues_text}

**Output Format:**
Return JSON only:
{{
  "is_valid": true/false,
  "issues": ["issue1", "issue2", ...],
  "corrected_steps": [{{"id": "...", "name": "...", ...}}, ...]  // Only if is_valid=false
}}

If the plan is valid, return:
{{
  "is_valid": true,
  "issues": []
}}"""

            user_prompt = f"""**Workflow Plan:**
{json.dumps(workflow_plan, ensure_ascii=False, indent=2)}

**Domain:** {domain}

**File Metadata:**
{json.dumps(file_metadata, ensure_ascii=False, indent=2) if file_metadata else "Not available"}

**Task:**
Review this workflow plan against SOP rules. If flawed, provide corrected steps."""

            messages = [
                {"role": "system", "content": system_prompt.format(sop_rules=sop_rules)},
                {"role": "user", "content": user_prompt}
            ]
            
            response = await self.llm_client.achat(
                messages=messages,
                temperature=0.1,
                max_tokens=1024
            )
            
            # ðŸ”¥ FIX: æå– ChatCompletion å¯¹è±¡çš„å†…å®¹
            if hasattr(response, 'choices') and response.choices:
                response_text = response.choices[0].message.content or ""
            else:
                response_text = str(response)
            
            # è§£æžå“åº”
            try:
                reflection_result = json.loads(response_text.strip())
                
                if reflection_result.get("is_valid", True):
                    logger.info("âœ… [Reflector] å·¥ä½œæµè®¡åˆ’æœ‰æ•ˆ")
                    return workflow_plan
                else:
                    issues = reflection_result.get("issues", [])
                    logger.warning(f"âš ï¸ [Reflector] å‘çŽ° {len(issues)} ä¸ªé—®é¢˜: {issues}")
                    
                    # å¦‚æžœæœ‰çº æ­£åŽçš„æ­¥éª¤ï¼Œä½¿ç”¨å®ƒä»¬
                    corrected_steps = reflection_result.get("corrected_steps")
                    if corrected_steps:
                        # å¤„ç†ä¸åŒçš„å·¥ä½œæµè®¡åˆ’æ ¼å¼
                        if "workflow_data" in workflow_plan:
                            workflow_plan["workflow_data"]["steps"] = corrected_steps
                        elif "steps" in workflow_plan:
                            workflow_plan["steps"] = corrected_steps
                        else:
                            # å¦‚æžœæ ¼å¼ä¸åŒ¹é…ï¼Œå°è¯•åˆ›å»º workflow_data ç»“æž„
                            workflow_plan["workflow_data"] = {
                                "workflow_name": workflow_plan.get("name", "Corrected Workflow"),
                                "steps": corrected_steps
                            }
                        logger.info("âœ… [Reflector] å·²åº”ç”¨çº æ­£")
                    
                    return workflow_plan
            
            except json.JSONDecodeError as e:
                logger.error(f"âŒ [Reflector] JSON è§£æžå¤±è´¥: {e}")
                logger.error(f"å“åº”å†…å®¹: {response[:200]}")
                # è§£æžå¤±è´¥æ—¶è¿”å›žåŽŸå§‹è®¡åˆ’
                return workflow_plan
        
        except Exception as e:
            logger.error(f"âŒ [Reflector] åæ€å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶è¿”å›žåŽŸå§‹è®¡åˆ’
            return workflow_plan
    
    def _get_sop_rules(self, domain: str) -> str:
        """èŽ·å– SOP è§„åˆ™"""
        if domain == "Metabolomics":
            return """**Metabolomics SOP Rules:**
1. MUST start with inspect_data (data quality assessment)
2. MUST perform preprocess_data (Log2 transformation + Scaling)
3. MUST perform pca_analysis (unsupervised analysis)
4. IF group columns exist:
   - MUST perform metabolomics_plsda (supervised analysis)
   - MUST perform differential_analysis
   - MUST perform visualize_volcano
   - MUST perform metabolomics_pathway_enrichment
5. Step order: inspect -> preprocess -> pca -> (if groups) plsda -> diff -> volcano -> pathway"""
        
        elif domain == "RNA":
            return """**scRNA-seq SOP Rules:**
1. IF input is FASTQ: MUST start with rna_cellranger_count
2. IF input is FASTQ: MUST convert with rna_convert_cellranger_to_h5ad
3. MUST perform rna_qc_filter (quality control)
4. MUST perform rna_doublet_detection (after QC)
5. MUST perform rna_normalize (LogNormalize)
6. MUST perform rna_hvg (highly variable genes)
7. MUST perform rna_scale (for PCA)
8. MUST perform rna_pca (before neighbors)
9. MUST perform rna_neighbors (before UMAP/clustering)
10. MUST perform rna_umap (visualization)
11. MUST perform rna_clustering (Leiden)
12. MUST perform rna_find_markers (after clustering)
13. MUST perform rna_cell_annotation (after markers)
14. Step order: QC -> Doublet -> Normalize -> HVG -> Scale -> PCA -> Neighbors -> UMAP/Clustering -> Markers -> Annotation"""
        
        else:
            return f"**{domain} SOP Rules:**\n(No specific rules defined)"

