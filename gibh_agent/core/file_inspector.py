"""
æ–‡ä»¶æ£€æµ‹å™¨ - Universal Eyes of the System
å¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥å™¨ï¼Œæ”¯æŒè¡¨æ ¼æ•°æ®ã€å•ç»†èƒæ•°æ®ã€å›¾åƒç­‰

æ¶æ„ï¼šç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰
- InspectorRegistry: å•ä¾‹æ³¨å†Œè¡¨
- BaseFileHandler: æŠ½è±¡åŸºç±»
- å…·ä½“ç­–ç•¥ï¼šTenXDirectoryHandler, AnnDataHandler, TabularHandler
- FileInspector: é—¨é¢ç±»
"""
import os
import json
import gzip
import logging
from pathlib import Path
from typing import Dict, Optional, Any, List, Tuple
from abc import ABC, abstractmethod
import numpy as np

logger = logging.getLogger(__name__)


# ============================================
# Part 1: Registry & Base Class
# ============================================

class InspectorRegistry:
    """
    æ£€æŸ¥å™¨æ³¨å†Œè¡¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    ç®¡ç†æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥ç­–ç•¥ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    """
    _instance = None
    _handlers: List[Tuple[int, type]] = []  # [(priority, handler_class), ...]
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def register(self, priority: int, handler_class: type):
        """æ³¨å†Œæ£€æŸ¥å™¨"""
        self._handlers.append((priority, handler_class))
        # æŒ‰ä¼˜å…ˆçº§é™åºæ’åºï¼ˆä¼˜å…ˆçº§é«˜çš„å…ˆæ‰§è¡Œï¼‰
        self._handlers.sort(key=lambda x: x[0], reverse=True)
        logger.debug(f"ğŸ“ Registered handler: {handler_class.__name__} (priority={priority})")
    
    def get_handlers(self) -> List[type]:
        """è·å–æ‰€æœ‰å·²æ³¨å†Œçš„æ£€æŸ¥å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰"""
        return [handler_class for _, handler_class in self._handlers]
    
    def clear(self):
        """æ¸…ç©ºæ³¨å†Œè¡¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        self._handlers.clear()


# å…¨å±€æ³¨å†Œè¡¨å®ä¾‹
_registry = InspectorRegistry()


def register_inspector(priority: int):
    """
    è£…é¥°å™¨ï¼šæ³¨å†Œæ–‡ä»¶æ£€æŸ¥å™¨
    
    Args:
        priority: ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    """
    def decorator(cls):
        _registry.register(priority, cls)
        return cls
    return decorator


class BaseFileHandler(ABC):
    """
    æ–‡ä»¶æ£€æŸ¥å™¨åŸºç±»
    
    æ‰€æœ‰å…·ä½“çš„æ–‡ä»¶æ£€æŸ¥å™¨éƒ½åº”è¯¥ç»§æ‰¿æ­¤ç±»å¹¶å®ç°ï¼š
    - can_handle(path) -> bool: æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†è¯¥è·¯å¾„
    - inspect(path) -> dict: æ‰§è¡Œæ£€æŸ¥å¹¶è¿”å›ç»“æœ
    """
    
    @abstractmethod
    def can_handle(self, path: Path) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†è¯¥è·¯å¾„
        
        Args:
            path: æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
            
        Returns:
            True å¦‚æœå¯ä»¥å¤„ç†ï¼ŒFalse å¦åˆ™
            
        Note:
            MUST check file content/structure, not just extension
        """
        pass
    
    @abstractmethod
    def inspect(self, path: Path) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–‡ä»¶æ£€æŸ¥
        
        Args:
            path: æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
            
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸ï¼Œå¿…é¡»åŒ…å«ï¼š
            - status: "success" æˆ– "error"
            - shape: {"rows": int, "cols": int} æˆ– {"n_obs": int, "n_vars": int}
            - columns: List[str] æˆ– None
            - file_type: str
            - preview: Optional[Dict] æˆ– str
            - error: Optional[str] (å¦‚æœ status == "error")
        """
        pass


# ============================================
# Part 2: Concrete Strategies
# ============================================

@register_inspector(priority=10)
class TenXDirectoryHandler(BaseFileHandler):
    """
    10x Genomics ç›®å½•æ£€æŸ¥å™¨ï¼ˆä¼˜å…ˆçº§ï¼š10ï¼‰
    
    æ£€æŸ¥åŒ…å« matrix.mtx, barcodes.tsv, features.tsv çš„ç›®å½•
    æ”¯æŒé€’å½’æœç´¢åµŒå¥—ç›®å½•
    """
    
    def _find_10x_data_path(self, root_path: Path) -> Optional[Path]:
        """
        é€’å½’æœç´¢ 10x æ•°æ®ç›®å½•
        
        ğŸ”¥ CRITICAL: å¿…é¡»æ‰¾åˆ°åŒ…å« matrix.mtx çš„å®é™…æ•°æ®ç›®å½•
        æ”¯æŒåµŒå¥—ç›®å½•ç»“æ„ï¼ˆå¦‚ MyData/Sample1/outs/ï¼‰
        
        Args:
            root_path: æ ¹ç›®å½•è·¯å¾„
            
        Returns:
            åŒ…å« matrix.mtx çš„å­ç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        print(f"DEBUG: Searching for 10x data in: {root_path}")  # ğŸ”¥ Loud logging
        logger.debug(f"DEBUG: Inspecting 10x path: {root_path}")
        
        # Step 1: é¦–å…ˆæ£€æŸ¥æ ¹ç›®å½•
        try:
            dir_contents = os.listdir(root_path)
            has_matrix = any(f in dir_contents for f in ['matrix.mtx', 'matrix.mtx.gz'])
            has_barcodes = any(f in dir_contents for f in ['barcodes.tsv', 'barcodes.tsv.gz'])
            has_features = any(f in dir_contents for f in ['features.tsv', 'features.tsv.gz', 'genes.tsv', 'genes.tsv.gz'])
            
            if has_matrix and (has_barcodes or has_features):
                print(f"DEBUG: Found 10x data at root: {root_path}")  # ğŸ”¥ Loud logging
                logger.debug(f"DEBUG: Found 10x data at root: {root_path}")
                return root_path
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to list root directory: {e}")
        
        # Step 2: é€’å½’æœç´¢å­ç›®å½•ï¼ˆä½¿ç”¨ os.walkï¼‰
        print(f"DEBUG: Recursively searching subdirectories...")  # ğŸ”¥ Loud logging
        for root, dirs, files in os.walk(root_path):
            root_path_obj = Path(root)
            
            # ğŸ”¥ CRITICAL: æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦åŒ…å« matrix.mtxï¼ˆæœªå‹ç¼©æˆ–å‹ç¼©ï¼‰
            has_matrix_local = 'matrix.mtx' in files or 'matrix.mtx.gz' in files
            
            if has_matrix_local:
                # ğŸ”¥ å¥å£®çš„æ–‡ä»¶ååŒ¹é…ï¼šæ£€æŸ¥ barcodes å’Œ featuresï¼ˆæ”¯æŒå¤šç§å‘½åï¼‰
                has_barcodes_local = any(f in files for f in ['barcodes.tsv', 'barcodes.tsv.gz'])
                has_features_local = any(f in files for f in ['features.tsv', 'features.tsv.gz', 'genes.tsv', 'genes.tsv.gz'])
                
                # ğŸ”¥ å¦‚æœæ‰¾åˆ° matrix.mtxï¼Œå³ä½¿æ²¡æœ‰ barcodes/features ä¹Ÿè¿”å›ï¼ˆå¯èƒ½åœ¨å…¶ä»–ä½ç½®ï¼‰
                # ä½†ä¼˜å…ˆè¿”å›åŒæ—¶åŒ…å«æ‰€æœ‰æ–‡ä»¶çš„ç›®å½•
                if has_barcodes_local or has_features_local:
                    print(f"DEBUG: Found data root at: {root_path_obj}")  # ğŸ”¥ Loud logging
                    logger.debug(f"DEBUG: Found matrix at: {root_path_obj}")
                    return root_path_obj
                else:
                    # å¦‚æœåªæœ‰ matrix.mtxï¼Œä¹Ÿè¿”å›ï¼ˆbarcodes/features å¯èƒ½åœ¨çˆ¶ç›®å½•æˆ–å­ç›®å½•ï¼‰
                    print(f"DEBUG: Found matrix.mtx at: {root_path_obj} (checking for barcodes/features nearby)")  # ğŸ”¥ Loud logging
                    # æ£€æŸ¥çˆ¶ç›®å½•å’Œå½“å‰ç›®å½•
                    parent_dir = root_path_obj.parent
                    if parent_dir.exists():
                        try:
                            parent_files = os.listdir(parent_dir)
                            if any(f in parent_files for f in ['barcodes.tsv', 'barcodes.tsv.gz', 'features.tsv', 'features.tsv.gz', 'genes.tsv', 'genes.tsv.gz']):
                                print(f"DEBUG: Found barcodes/features in parent: {parent_dir}")  # ğŸ”¥ Loud logging
                                return root_path_obj
                        except Exception:
                            pass
                    
                    # å¦‚æœå½“å‰ç›®å½•æœ‰ matrix.mtxï¼Œè¿”å›å®ƒï¼ˆè®©åç»­é€»è¾‘å¤„ç†æ–‡ä»¶æŸ¥æ‰¾ï¼‰
                    print(f"DEBUG: Using directory with matrix.mtx: {root_path_obj}")  # ğŸ”¥ Loud logging
                    return root_path_obj
        
        print(f"DEBUG: No 10x data found in {root_path}")  # ğŸ”¥ Loud logging
        logger.debug(f"DEBUG: No 10x data found in {root_path}")
        return None
    
    def can_handle(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸º 10x Genomics ç›®å½•ï¼ˆæ”¯æŒåµŒå¥—ç›®å½•ï¼‰"""
        if not path.is_dir():
            return False
        
        try:
            real_data_path = self._find_10x_data_path(path)
            return real_data_path is not None
        except Exception as e:
            logger.debug(f"DEBUG: can_handle failed: {e}")
            return False
    
    def _smart_open(self, filepath: Path):
        """
        æ™ºèƒ½æ‰“å¼€æ–‡ä»¶ï¼Œè‡ªåŠ¨å¤„ç† gzip å‹ç¼©
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„ï¼ˆPath å¯¹è±¡ï¼‰
            
        Returns:
            æ–‡ä»¶å¯¹è±¡ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
        """
        filepath_str = str(filepath)
        if filepath_str.endswith('.gz'):
            # ğŸ”¥ CRITICAL: 'rt' æ¨¡å¼ + encoding='utf-8' å¯¹äº gzip æ–‡ä»¶è‡³å…³é‡è¦
            return gzip.open(filepath_str, 'rt', encoding='utf-8')
        else:
            return open(filepath_str, 'r', encoding='utf-8')
    
    def _count_lines_skip_comments(self, file_path: Path) -> int:
        """
        ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼Œè·³è¿‡æ³¨é‡Šè¡Œï¼ˆä»¥ % æˆ– # å¼€å¤´ï¼‰
        
        æ”¯æŒ gzip å‹ç¼©æ–‡ä»¶ï¼ˆé€æ˜å¤„ç†ï¼‰
        """
        count = 0
        try:
            with self._smart_open(file_path) as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('%') and not line.startswith('#'):
                        count += 1
            logger.debug(f"DEBUG: Counted {count} lines in {file_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to count lines in {file_path}: {e}", exc_info=True)
            # è¿”å› 0 è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…å†³å®šå¦‚ä½•å¤„ç†
        return count
    
    def inspect(self, path: Path) -> Dict[str, Any]:
        """
        æ£€æŸ¥ 10x Genomics ç›®å½•ï¼ˆæ”¯æŒåµŒå¥—ç›®å½•ï¼‰
        
        è¿”å›åŒ…å« debug_trace å­—æ®µçš„å­—å…¸ï¼Œç”¨äºè°ƒè¯•
        """
        debug_logs = []  # ğŸ”¥ åˆ›å»ºè°ƒè¯•æ—¥å¿—åˆ—è¡¨
        debug_logs.append(f"Input path: {path}")
        
        try:
            logger.debug(f"DEBUG: Inspecting 10x path: {path}")
            debug_logs.append(f"Starting inspection of: {path}")
            
            # è®°å½•æ ¹ç›®å½•å†…å®¹
            try:
                dir_contents = os.listdir(path)
                debug_logs.append(f"Directory contents: {dir_contents}")
                logger.debug(f"DEBUG: Directory contents: {dir_contents}")
            except Exception as e:
                debug_logs.append(f"Failed to list directory: {str(e)}")
                logger.warning(f"âš ï¸ Failed to list directory: {e}")
            
            # ğŸ”¥ é€’å½’æœç´¢æ‰¾åˆ°å®é™…çš„æ•°æ®ç›®å½•
            debug_logs.append(f"Starting recursive search for matrix.mtx...")
            real_data_path = self._find_10x_data_path(path)
            
            if real_data_path is None:
                debug_logs.append(f"ERROR: Could not find matrix.mtx in directory tree")
                debug_trace = "\n".join(debug_logs)
                return {
                    "status": "error",
                    "error": f"åœ¨ç›®å½• {path} ä¸­æœªæ‰¾åˆ° 10x æ•°æ®æ–‡ä»¶ï¼ˆmatrix.mtx, barcodes.tsv, features.tsvï¼‰",
                    "file_type": "directory",
                    "debug_trace": debug_trace  # ğŸ”¥ è¿”å›è°ƒè¯•è·Ÿè¸ª
                }
            
            debug_logs.append(f"Found data root at: {real_data_path}")
            logger.debug(f"DEBUG: Found 10x root at: {real_data_path}")
            print(f"DEBUG: Found data root at: {real_data_path}")  # ğŸ”¥ Loud logging
            
            logger.debug(f"DEBUG: Found 10x root at: {real_data_path}")
            print(f"DEBUG: Found data root at: {real_data_path}")  # ğŸ”¥ Loud logging
            
            # Step B: åœ¨æ‰¾åˆ°çš„ç›®å½•ä¸­æŸ¥æ‰¾å¿…éœ€æ–‡ä»¶ï¼ˆæ”¯æŒå‹ç¼©å’Œæœªå‹ç¼©ï¼‰
            # ğŸ”¥ å¥å£®çš„æ–‡ä»¶ååŒ¹é…ï¼šæ”¯æŒå¤šç§å‘½åçº¦å®š
            matrix_path = None
            barcodes_path = None
            features_path = None
            
            # æŸ¥æ‰¾ matrix.mtxï¼ˆå‹ç¼©æˆ–æœªå‹ç¼©ï¼‰
            debug_logs.append(f"Looking for matrix.mtx in: {real_data_path}")
            print(f"DEBUG: Looking for matrix.mtx in: {real_data_path}")  # ğŸ”¥ Loud logging
            try:
                data_dir_contents = os.listdir(real_data_path)
                debug_logs.append(f"Data directory contents: {data_dir_contents}")
            except Exception as e:
                debug_logs.append(f"Failed to list data directory: {str(e)}")
            
            for f in ['matrix.mtx', 'matrix.mtx.gz']:
                candidate = real_data_path / f
                if candidate.exists():
                    matrix_path = candidate
                    debug_logs.append(f"Matrix found at: {matrix_path}")
                    print(f"DEBUG: Found matrix at: {matrix_path}")  # ğŸ”¥ Loud logging
                    break
            
            if not matrix_path:
                debug_logs.append(f"ERROR: matrix.mtx not found in {real_data_path}")
            
            # æŸ¥æ‰¾ barcodes.tsvï¼ˆå‹ç¼©æˆ–æœªå‹ç¼©ï¼‰
            # ğŸ”¥ å¦‚æœä¸åœ¨å½“å‰ç›®å½•ï¼Œä¹Ÿæ£€æŸ¥çˆ¶ç›®å½•
            debug_logs.append(f"Looking for barcodes.tsv in: {real_data_path}")
            print(f"DEBUG: Looking for barcodes.tsv in: {real_data_path}")  # ğŸ”¥ Loud logging
            search_dirs = [real_data_path]
            if real_data_path.parent.exists():
                search_dirs.append(real_data_path.parent)
                debug_logs.append(f"Also checking parent directory: {real_data_path.parent}")
            
            for search_dir in search_dirs:
                debug_logs.append(f"Scanning subdir: {search_dir}")
                for f in ['barcodes.tsv', 'barcodes.tsv.gz']:
                    candidate = search_dir / f
                    if candidate.exists():
                        barcodes_path = candidate
                        debug_logs.append(f"Barcodes found at: {barcodes_path}")
                        print(f"DEBUG: Found barcodes at: {barcodes_path}")  # ğŸ”¥ Loud logging
                        break
                if barcodes_path:
                    break
            
            if not barcodes_path:
                debug_logs.append(f"ERROR: barcodes.tsv not found in search directories")
            
            # æŸ¥æ‰¾ features.tsv æˆ– genes.tsvï¼ˆå‹ç¼©æˆ–æœªå‹ç¼©ï¼‰
            # ğŸ”¥ å¥å£®åŒ¹é…ï¼šæ”¯æŒ features.tsv, genes.tsvï¼ˆæ ‡å‡† 10x æ ¼å¼ï¼‰
            debug_logs.append(f"Looking for features.tsv/genes.tsv in: {real_data_path}")
            print(f"DEBUG: Looking for features.tsv/genes.tsv in: {real_data_path}")  # ğŸ”¥ Loud logging
            for search_dir in search_dirs:
                debug_logs.append(f"Scanning subdir for features: {search_dir}")
                for f in ['features.tsv', 'features.tsv.gz', 'genes.tsv', 'genes.tsv.gz']:
                    candidate = search_dir / f
                    if candidate.exists():
                        features_path = candidate
                        debug_logs.append(f"Features found at: {features_path}")
                        print(f"DEBUG: Found features at: {features_path}")  # ğŸ”¥ Loud logging
                        break
                if features_path:
                    break
            
            if not features_path:
                debug_logs.append(f"ERROR: features.tsv/genes.tsv not found in search directories")
            
            if not all([matrix_path, barcodes_path, features_path]):
                missing = []
                if not matrix_path:
                    missing.append("matrix.mtx")
                if not barcodes_path:
                    missing.append("barcodes.tsv")
                if not features_path:
                    missing.append("features.tsv/genes.tsv")
                
                debug_logs.append(f"ERROR: Missing required files: {', '.join(missing)}")
                debug_trace = "\n".join(debug_logs)
                return {
                    "status": "error",
                    "error": f"10xç›®å½•ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {', '.join(missing)}ã€‚æœç´¢è·¯å¾„: {real_data_path}",
                    "file_type": "directory",
                    "debug_trace": debug_trace  # ğŸ”¥ è¿”å›è°ƒè¯•è·Ÿè¸ª
                }
            
            # Step C: ç»Ÿè®¡ç»†èƒæ•°ï¼ˆbarcodes.tsv çš„è¡Œæ•°ï¼Œè·³è¿‡æ³¨é‡Šï¼‰
            debug_logs.append(f"Attempting to read barcodes file: {barcodes_path}")
            logger.debug(f"DEBUG: Counting cells from: {barcodes_path}")
            print(f"DEBUG: Counting cells from: {barcodes_path}")  # ğŸ”¥ Loud logging
            try:
                n_cells = self._count_lines_skip_comments(barcodes_path)
                debug_logs.append(f"Successfully counted {n_cells} cells from barcodes.tsv")
            except Exception as e:
                debug_logs.append(f"Error reading barcodes file: {str(e)}")
                logger.error(f"âŒ Error reading barcodes: {e}", exc_info=True)
                n_cells = 0
            
            # ğŸ”¥ CRITICAL FIX: å¦‚æœè¡Œæ•°ç»Ÿè®¡å¤±è´¥ï¼ˆè¿”å›0ï¼‰ï¼Œå›é€€åˆ° scanpy.read_10x_mtx
            if n_cells == 0:
                debug_logs.append(f"WARNING: Line counting returned 0 cells, falling back to scanpy.read_10x_mtx")
                logger.warning(f"âš ï¸ Line counting returned 0, falling back to scanpy.read_10x_mtx")
                try:
                    import scanpy as sc
                    from .rna_utils import read_10x_data
                    debug_logs.append(f"Attempting scanpy.read_10x_mtx fallback...")
                    logger.info(f"ğŸ”„ Using scanpy.read_10x_mtx as fallback...")
                    adata = read_10x_data(str(real_data_path), var_names='gene_symbols', cache=False)
                    n_cells = adata.n_obs
                    n_genes = adata.n_vars
                    debug_logs.append(f"Fallback successful: {n_cells} cells, {n_genes} genes")
                    logger.info(f"âœ… Fallback successful: {n_cells} cells, {n_genes} genes")
                    print(f"DEBUG: Detected {n_cells} cells (via scanpy fallback)")  # ğŸ”¥ Loud logging
                    
                    # ä½¿ç”¨ scanpy çš„ç»“æœï¼Œè·³è¿‡æ‰‹åŠ¨ç»Ÿè®¡
                    feature_types = ['Gene Expression']  # é»˜è®¤å€¼
                    columns = list(adata.var_names[:20]) if hasattr(adata, 'var_names') else []
                    
                    if n_cells == 0 or n_genes == 0:
                        debug_logs.append(f"ERROR: Fallback also returned 0 cells or genes")
                        debug_trace = "\n".join(debug_logs)
                        return {
                            "status": "error",
                            "error": f"10xæ•°æ®ä¸ºç©º: {n_cells} ä¸ªç»†èƒ, {n_genes} ä¸ªåŸºå› ",
                            "file_type": "directory",
                            "n_obs": n_cells,
                            "n_vars": n_genes,
                            "n_samples": n_cells,
                            "n_features": n_genes,
                            "debug_trace": debug_trace  # ğŸ”¥ è¿”å›è°ƒè¯•è·Ÿè¸ª
                        }
                    
                    # æ„å»ºé¢„è§ˆ
                    preview = {
                        "n_cells": n_cells,
                        "n_genes": n_genes,
                        "feature_types": feature_types,
                        "sample_genes": columns[:10]
                    }
                    
                    debug_trace = "\n".join(debug_logs)  # ğŸ”¥ ç”Ÿæˆè°ƒè¯•è·Ÿè¸ª
                    
                    # ç›´æ¥è¿”å›ç»“æœï¼ˆè·³è¿‡ä¸‹é¢çš„æ‰‹åŠ¨ç»Ÿè®¡é€»è¾‘ï¼‰
                    result = {
                        "status": "success",
                        "file_path": str(path.resolve()),
                        "real_data_path": str(real_data_path.resolve()),
                        "file_type": "anndata",
                        "shape": {
                            "rows": n_cells,
                            "cols": n_genes
                        },
                        "n_obs": n_cells,
                        "n_vars": n_genes,
                        "n_samples": n_cells,  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿å­˜åœ¨
                        "n_features": n_genes,  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿å­˜åœ¨
                        "columns": columns[:20] if columns else None,
                        "head": {
                            "markdown": f"10x Genomics æ•°æ®\n- ç»†èƒæ•°: {n_cells}\n- åŸºå› æ•°: {n_genes}\n- æ•°æ®è·¯å¾„: {real_data_path}",
                            "json": preview
                        },
                        "feature_types": feature_types,
                        "debug_trace": debug_trace,  # ğŸ”¥ è¿”å›è°ƒè¯•è·Ÿè¸ª
                        "data": {
                            "summary": {
                                "n_samples": n_cells,
                                "n_features": n_genes,
                                "feature_types": feature_types
                            }
                        }
                    }
                    
                    logger.info(f"âœ… [TenXDirectoryHandler] Success (fallback): {n_cells} cells, {n_genes} genes")
                    return result
                    
                except Exception as fallback_error:
                    debug_logs.append(f"EXCEPTION in fallback: {str(fallback_error)}")
                    debug_trace = "\n".join(debug_logs)
                    logger.error(f"âŒ Fallback to scanpy also failed: {fallback_error}", exc_info=True)
                    return {
                        "status": "error",
                        "error": f"æ— æ³•è¯»å–10xæ•°æ®: {str(fallback_error)}",
                        "file_type": "directory",
                        "debug_trace": debug_trace  # ğŸ”¥ è¿”å›è°ƒè¯•è·Ÿè¸ª
                    }
            
            print(f"DEBUG: Detected {n_cells} cells.")  # ğŸ”¥ Loud logging
            
            # è¯»å– features.tsv è·å–åŸºå› ä¿¡æ¯ï¼ˆä½¿ç”¨ _smart_openï¼‰
            debug_logs.append(f"Attempting to read features file: {features_path}")
            n_genes = 0
            feature_types = []
            columns = []
            
            try:
                logger.debug(f"DEBUG: Reading features from: {features_path}")
                with self._smart_open(features_path) as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('%') and not line.startswith('#'):
                            n_genes += 1
                            parts = line.split('\t')
                            if len(parts) >= 1:
                                gene_id = parts[0]
                                gene_symbol = parts[1] if len(parts) > 1 else gene_id
                                feature_type = parts[2] if len(parts) > 2 else 'Gene Expression'
                                
                                if feature_type not in feature_types:
                                    feature_types.append(feature_type)
                                
                                # åªä¿å­˜å‰100ä¸ªåŸºå› åï¼ˆé¿å…è¿”å›è¿‡å¤šæ•°æ®ï¼‰
                                if len(columns) < 100:
                                    columns.append(gene_symbol or gene_id)
                debug_logs.append(f"Successfully read {n_genes} genes from features.tsv")
            except Exception as e:
                debug_logs.append(f"Error reading features file: {str(e)}")
                logger.warning(f"âš ï¸ Failed to read features.tsv: {e}")
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œè‡³å°‘ç»Ÿè®¡è¡Œæ•°
                try:
                    n_genes = self._count_lines_skip_comments(features_path)
                    debug_logs.append(f"Fallback: counted {n_genes} genes using line counting")
                except Exception as count_err:
                    debug_logs.append(f"Error counting lines in features file: {str(count_err)}")
                    n_genes = 0
            
            print(f"DEBUG: Detected {n_genes} genes.")  # ğŸ”¥ Loud logging
            debug_logs.append(f"Final counts: {n_cells} cells, {n_genes} genes")
            
            if n_cells == 0 or n_genes == 0:
                return {
                    "status": "error",
                    "error": f"10xæ•°æ®ä¸ºç©º: {n_cells} ä¸ªç»†èƒ, {n_genes} ä¸ªåŸºå› ",
                    "file_type": "directory",
                    "n_obs": n_cells,
                    "n_vars": n_genes
                }
            
            # æ„å»ºé¢„è§ˆï¼ˆå‰5ä¸ªç»†èƒå’ŒåŸºå› ï¼‰
            preview = {
                "n_cells": n_cells,
                "n_genes": n_genes,
                "feature_types": feature_types,
                "sample_genes": columns[:10]
            }
            
            # ğŸ”¥ CRITICAL FIX: ç¡®ä¿è¿”å›æ‰€æœ‰å¿…éœ€çš„é”®ï¼ˆå…¼å®¹ DataDiagnosticianï¼‰
            # åŒæ—¶è®¾ç½® n_obs/n_vars å’Œ n_samples/n_features
            debug_trace = "\n".join(debug_logs)  # ğŸ”¥ ç”Ÿæˆè°ƒè¯•è·Ÿè¸ªå­—ç¬¦ä¸²
            
            result = {
                "status": "success",
                "file_path": str(path.resolve()),  # è¿”å›åŸå§‹è·¯å¾„ï¼ˆç”¨æˆ·ä¸Šä¼ çš„è·¯å¾„ï¼‰
                "real_data_path": str(real_data_path.resolve()),  # å®é™…æ•°æ®æ‰€åœ¨è·¯å¾„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                "file_type": "anndata",
                "shape": {
                    "rows": n_cells,
                    "cols": n_genes
                },
                # scRNA-seq æ ¼å¼ï¼ˆScanpyTool æœŸæœ›çš„ï¼‰
                "n_obs": n_cells,  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿å­˜åœ¨
                "n_vars": n_genes,  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿å­˜åœ¨
                # é€šç”¨æ ¼å¼ï¼ˆDataDiagnostician æœŸæœ›çš„ï¼‰
                "n_samples": n_cells,  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿å­˜åœ¨
                "n_features": n_genes,  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿å­˜åœ¨
                "columns": columns[:20] if columns else None,  # å‰20ä¸ªåŸºå› å
                "head": {
                    "markdown": f"10x Genomics æ•°æ®\n- ç»†èƒæ•°: {n_cells}\n- åŸºå› æ•°: {n_genes}\n- ç‰¹å¾ç±»å‹: {', '.join(feature_types)}\n- æ•°æ®è·¯å¾„: {real_data_path}",
                    "json": preview
                },
                "feature_types": feature_types,
                "debug_trace": debug_trace,  # ğŸ”¥ è¿”å›è°ƒè¯•è·Ÿè¸ª
                "data": {
                    "summary": {
                        "n_samples": n_cells,  # ğŸ”¥ ç¡®ä¿åµŒå¥—å­—å…¸ä¸­ä¹Ÿå­˜åœ¨
                        "n_features": n_genes,  # ğŸ”¥ ç¡®ä¿åµŒå¥—å­—å…¸ä¸­ä¹Ÿå­˜åœ¨
                        "feature_types": feature_types
                    }
                }
            }
            
            logger.info(f"âœ… [TenXDirectoryHandler] Success: {n_cells} cells, {n_genes} genes (data_path: {real_data_path})")
            print(f"DEBUG: Final result - {n_cells} cells, {n_genes} genes")  # ğŸ”¥ Loud logging
            return result
            
        except Exception as e:
            debug_logs.append(f"EXCEPTION: {str(e)}")
            debug_trace = "\n".join(debug_logs)
            logger.error(f"âŒ [TenXDirectoryHandler] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": f"æ— æ³•è¯»å–10xæ•°æ®: {str(e)}",
                "file_type": "directory",
                "debug_trace": debug_trace  # ğŸ”¥ è¿”å›è°ƒè¯•è·Ÿè¸ª
            }


@register_inspector(priority=8)
class FastqDirectoryHandler(BaseFileHandler):
    """
    FASTQ ç›®å½•æ£€æŸ¥å™¨ï¼ˆä¼˜å…ˆçº§ï¼š8ï¼Œé«˜äºå…¶ä»–ç›®å½•æ£€æŸ¥å™¨ï¼‰
    
    æ£€æŸ¥åŒ…å« FASTQ æ–‡ä»¶çš„ç›®å½•ï¼ˆ.fastq, .fq, .fastq.gz, .fq.gzï¼‰
    æä¾›è½»é‡çº§è¯Šæ–­ï¼šæ–‡ä»¶æ•°é‡ã€æ€»å¤§å°ã€æ–‡ä»¶ç±»å‹ç­‰
    """
    
    def can_handle(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸º FASTQ ç›®å½•"""
        if not path.is_dir():
            return False
        
        try:
            # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦åŒ…å« FASTQ æ–‡ä»¶
            fastq_extensions = ('.fastq', '.fq', '.fastq.gz', '.fq.gz')
            for item in path.iterdir():
                if item.is_file() and item.name.lower().endswith(fastq_extensions):
                    return True
                # ä¹Ÿæ£€æŸ¥å­ç›®å½•ï¼ˆé€’å½’ä¸€å±‚ï¼‰
                elif item.is_dir():
                    try:
                        for subitem in item.iterdir():
                            if subitem.is_file() and subitem.name.lower().endswith(fastq_extensions):
                                return True
                    except (PermissionError, OSError):
                        continue
            return False
        except (PermissionError, OSError) as e:
            logger.warning(f"âš ï¸ æ— æ³•è®¿é—®ç›®å½• {path}: {e}")
            return False
    
    def inspect(self, path: Path) -> Dict[str, Any]:
        """
        æ£€æŸ¥ FASTQ ç›®å½•
        
        è¿”å›è½»é‡çº§è¯Šæ–­ä¿¡æ¯ï¼š
        - æ–‡ä»¶æ•°é‡
        - æ€»å¤§å°
        - æ–‡ä»¶ç±»å‹ï¼ˆR1, R2, I1ç­‰ï¼‰
        - æ˜¯å¦åŒ…å«å®Œæ•´çš„é…å¯¹ç«¯æ•°æ®
        """
        try:
            fastq_files = []
            total_size = 0
            file_types = set()  # R1, R2, I1ç­‰
            
            fastq_extensions = ('.fastq', '.fq', '.fastq.gz', '.fq.gz')
            
            # æ”¶é›†æ‰€æœ‰FASTQæ–‡ä»¶
            for item in path.rglob('*'):  # é€’å½’æœç´¢
                if item.is_file() and item.name.lower().endswith(fastq_extensions):
                    fastq_files.append(item)
                    total_size += item.stat().st_size
                    
                    # è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼ˆR1, R2, I1ç­‰ï¼‰
                    name_lower = item.name.lower()
                    if '_r1_' in name_lower or '_r1.' in name_lower or name_lower.endswith('_r1.fastq') or name_lower.endswith('_r1.fq'):
                        file_types.add('R1')
                    elif '_r2_' in name_lower or '_r2.' in name_lower or name_lower.endswith('_r2.fastq') or name_lower.endswith('_r2.fq'):
                        file_types.add('R2')
                    elif '_i1_' in name_lower or '_i1.' in name_lower or name_lower.endswith('_i1.fastq') or name_lower.endswith('_i1.fq'):
                        file_types.add('I1')
                    elif '_i2_' in name_lower or '_i2.' in name_lower:
                        file_types.add('I2')
            
            if not fastq_files:
                return {
                    "status": "error",
                    "error": f"ç›®å½• {path} ä¸­æœªæ‰¾åˆ° FASTQ æ–‡ä»¶",
                    "file_type": "directory"
                }
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é…å¯¹ç«¯æ•°æ®
            has_paired_end = 'R1' in file_types and 'R2' in file_types
            has_index = 'I1' in file_types or 'I2' in file_types
            
            # æ„å»ºè¯Šæ–­ä¿¡æ¯
            diagnosis_info = {
                "file_count": len(fastq_files),
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
                "total_size_gb": round(total_size / (1024 * 1024 * 1024), 2),
                "file_types": sorted(list(file_types)),
                "has_paired_end": has_paired_end,
                "has_index": has_index,
                "is_10x_format": has_paired_end and has_index,  # 10xæ ¼å¼é€šå¸¸åŒ…å«R1, R2, I1
                "sample_names": sorted(list(set(f.name.split('_')[0] for f in fastq_files)))[:10]  # å‰10ä¸ªæ ·æœ¬å
            }
            
            return {
                "status": "success",
                "success": True,
                "file_type": "fastq",
                "file_path": str(path.resolve()),
                "shape": {"n_files": len(fastq_files)},  # å…¼å®¹æ€§
                "columns": None,
                "preview": None,
                "diagnosis_info": diagnosis_info,
                "message": f"æ£€æµ‹åˆ° {len(fastq_files)} ä¸ª FASTQ æ–‡ä»¶ï¼Œæ€»å¤§å° {diagnosis_info['total_size_gb']:.2f} GB"
            }
            
        except Exception as e:
            logger.error(f"âŒ [FastqDirectoryHandler] æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "error",
                "error": f"æ— æ³•æ£€æŸ¥ FASTQ ç›®å½•: {str(e)}",
                "file_type": "directory"
            }


@register_inspector(priority=5)
class AnnDataHandler(BaseFileHandler):
    """
    AnnData (H5AD) æ–‡ä»¶æ£€æŸ¥å™¨ï¼ˆä¼˜å…ˆçº§ï¼š5ï¼‰
    
    æ£€æŸ¥ .h5ad æ–‡ä»¶ï¼Œä½¿ç”¨ backed='r' æ¨¡å¼é¿å…åŠ è½½å…¨éƒ¨æ•°æ®
    """
    
    def can_handle(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸º H5AD æ–‡ä»¶"""
        if not path.is_file():
            return False
        
        if not path.suffix.lower() == '.h5ad':
            return False
        
        # å†…å®¹æ£€æŸ¥ï¼šéªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ HDF5 æ–‡ä»¶
        try:
            import h5py
            return h5py.is_hdf5(str(path))
        except ImportError:
            # å¦‚æœæ²¡æœ‰ h5pyï¼Œè‡³å°‘æ£€æŸ¥æ‰©å±•å
            logger.debug("âš ï¸ h5py not available, using extension check only")
            return True
        except Exception:
            return False
    
    def inspect(self, path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥ H5AD æ–‡ä»¶"""
        try:
            import scanpy as sc
            
            logger.info(f"ğŸ” [AnnDataHandler] Loading H5AD: {path}")
            
            # ä½¿ç”¨ backed='r' æ¨¡å¼ï¼ˆåªè¯»ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
            try:
                adata = sc.read_h5ad(str(path), backed='r')
            except Exception as e:
                logger.warning(f"âš ï¸ Backed mode failed: {e}, using normal mode")
                adata = sc.read_h5ad(str(path))
            
            if adata is None:
                return {
                    "status": "error",
                    "error": "æ•°æ®è¯»å–å¤±è´¥ï¼šè¿”å›äº†ç©ºå¯¹è±¡",
                    "file_type": "anndata"
                }
            
            if adata.n_obs == 0 or adata.n_vars == 0:
                return {
                    "status": "error",
                    "error": f"æ•°æ®ä¸ºç©º: {adata.n_obs} ä¸ªç»†èƒ, {adata.n_vars} ä¸ªåŸºå› ",
                    "file_type": "anndata",
                    "n_obs": adata.n_obs,
                    "n_vars": adata.n_vars
                }
            
            # æå–åŸºæœ¬ä¿¡æ¯
            n_obs = adata.n_obs
            n_vars = adata.n_vars
            obs_keys = list(adata.obs.columns) if hasattr(adata.obs, 'columns') else []
            var_keys = list(adata.var.columns) if hasattr(adata.var, 'columns') else []
            
            # è·å–åŸºå› åï¼ˆå‰20ä¸ªï¼‰
            columns = list(adata.var_names[:20]) if hasattr(adata, 'var_names') else []
            
            # è®¡ç®—ç¨€ç–åº¦
            sparsity = 0
            if hasattr(adata.X, 'nnz'):
                total_cells = n_obs * n_vars
                sparsity = (1 - adata.X.nnz / total_cells) * 100 if total_cells > 0 else 0
            
            # æ£€æŸ¥æ•°æ®å€¼èŒƒå›´ï¼ˆé‡‡æ ·ï¼‰
            data_range = {}
            try:
                sample_size = min(1000, n_obs * n_vars)
                if sample_size > 0:
                    if hasattr(adata.X, 'toarray'):
                        sample_data = adata.X[:min(100, n_obs), :min(100, n_vars)].toarray()
                    else:
                        sample_data = adata.X[:min(100, n_obs), :min(100, n_vars)]
                    
                    data_range = {
                        "min": float(np.min(sample_data)),
                        "max": float(np.max(sample_data)),
                        "mean": float(np.mean(sample_data)),
                        "median": float(np.median(sample_data))
                    }
            except Exception as e:
                logger.warning(f"âš ï¸ Could not calculate data range: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰èšç±»ç»“æœ
            has_clusters = 'leiden' in obs_keys or 'louvain' in obs_keys
            has_umap = 'X_umap' in adata.obsm_keys() if hasattr(adata, 'obsm_keys') else False
            
            # æ„å»ºé¢„è§ˆ
            preview = {
                "n_obs": n_obs,
                "n_vars": n_vars,
                "obs_keys": obs_keys[:10],
                "var_keys": var_keys[:10],
                "sparsity": round(sparsity, 2),
                "has_clusters": has_clusters,
                "has_umap": has_umap
            }
            
            return {
                "status": "success",
                "file_path": str(path.resolve()),
                "file_type": "anndata",
                "shape": {
                    "rows": n_obs,
                    "cols": n_vars
                },
                "n_obs": n_obs,
                "n_vars": n_vars,
                "n_samples": n_obs,  # å…¼å®¹æ€§
                "n_features": n_vars,  # å…¼å®¹æ€§
                "columns": columns,
                "head": {
                    "markdown": f"H5AD æ–‡ä»¶\n- ç»†èƒæ•°: {n_obs}\n- åŸºå› æ•°: {n_vars}\n- ç¨€ç–åº¦: {sparsity:.2f}%",
                    "json": preview
                },
                "obs_keys": obs_keys,
                "var_keys": var_keys,
                "sparsity": sparsity,
                "has_clusters": has_clusters,
                "has_umap": has_umap,
                "data_range": data_range,
                "data": {
                    "summary": {
                        "n_samples": n_obs,
                        "n_features": n_vars,
                        "sparsity": round(sparsity, 2)
                    }
                }
            }
            
        except ImportError:
            return {
                "status": "error",
                "error": "scanpy not installed. Please install: pip install scanpy",
                "file_type": "anndata"
            }
        except Exception as e:
            logger.error(f"âŒ [AnnDataHandler] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": f"æ— æ³•è¯»å–H5ADæ–‡ä»¶: {str(e)}",
                "file_type": "anndata"
            }


@register_inspector(priority=1)
class TabularHandler(BaseFileHandler):
    """
    è¡¨æ ¼æ–‡ä»¶æ£€æŸ¥å™¨ï¼ˆä¼˜å…ˆçº§ï¼š1ï¼‰
    
    æ”¯æŒ CSV, TSV, TXT, XLSX æ–‡ä»¶
    """
    
    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    SUPPORTED_EXTENSIONS = {'.csv', '.tsv', '.txt', '.xlsx', '.xls'}
    
    def can_handle(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è¡¨æ ¼æ–‡ä»¶"""
        if not path.is_file():
            return False
        return path.suffix.lower() in self.SUPPORTED_EXTENSIONS
    
    def _count_csv_lines(self, file_path: Path, separator: str = ',') -> Optional[int]:
        """ç»Ÿè®¡ CSV/TSV æ–‡ä»¶è¡Œæ•°ï¼ˆé¿å…åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰"""
        try:
            count = 0
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆè¡¨å¤´ï¼‰
                next(f, None)
                for _ in f:
                    count += 1
            return count
        except Exception:
            return None
    
    def inspect(self, path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥è¡¨æ ¼æ–‡ä»¶"""
        try:
            import pandas as pd
            
            logger.info(f"ğŸ” [TabularHandler] Loading: {path}")
            
            absolute_path = str(path.resolve())
            file_size_mb = path.stat().st_size / (1024 * 1024)
            
            # æ£€æµ‹åˆ†éš”ç¬¦
            separator = ','
            try:
                with open(absolute_path, 'r', encoding='utf-8', errors='ignore') as f:
                    first_line = f.readline()
                    if '\t' in first_line:
                        separator = '\t'
                    elif ',' in first_line:
                        separator = ','
                    elif ';' in first_line:
                        separator = ';'
            except Exception:
                pass
            
            # è¯»å–é¢„è§ˆï¼ˆå‰10è¡Œï¼‰
            LARGE_FILE_THRESHOLD_MB = 200
            preview_rows = 10
            
            if file_size_mb < LARGE_FILE_THRESHOLD_MB:
                # å°æ–‡ä»¶ï¼šå®Œæ•´è¯»å–
                df = pd.read_csv(absolute_path, sep=separator)
                is_sampled = False
                total_rows = len(df)
            else:
                # å¤§æ–‡ä»¶ï¼šé‡‡æ ·è¯»å–
                df = pd.read_csv(absolute_path, sep=separator, nrows=preview_rows)
                is_sampled = True
                # å°è¯•ç»Ÿè®¡æ€»è¡Œæ•°
                total_rows = self._count_csv_lines(path, separator)
                if total_rows is None:
                    total_rows = len(df)  # å›é€€åˆ°é‡‡æ ·è¡Œæ•°
            
            # è¯†åˆ«åˆ—ç±»å‹
            metadata_cols = []
            numeric_cols = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    numeric_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            n_samples = total_rows if not is_sampled else (total_rows if total_rows else len(df))
            n_features = len(numeric_cols)
            
            # ç¼ºå¤±å€¼ç»Ÿè®¡
            missing_rate = 0
            if len(numeric_cols) > 0:
                numeric_data = df[numeric_cols]
                total_cells = numeric_data.size
                missing_cells = numeric_data.isnull().sum().sum()
                missing_rate = (missing_cells / total_cells * 100) if total_cells > 0 else 0
            
            # æ•°æ®èŒƒå›´
            data_range = {}
            if len(numeric_cols) > 0:
                numeric_data = df[numeric_cols]
                data_range = {
                    "min": float(numeric_data.min().min()),
                    "max": float(numeric_data.max().max()),
                    "mean": float(numeric_data.mean().mean()),
                    "median": float(numeric_data.median().median())
                }
            
            # æ„å»ºé¢„è§ˆ
            try:
                from tabulate import tabulate
                head_markdown = tabulate(df.head(preview_rows), headers='keys', tablefmt='grid', showindex=False)
            except ImportError:
                # å›é€€åˆ° CSV æ ¼å¼
                head_markdown = df.head(preview_rows).to_csv(index=False)
            
            head_json = df.head(preview_rows).to_dict(orient='records')
            
            return {
                "status": "success",
                "file_path": absolute_path,
                "file_type": "tabular",
                "file_size_mb": round(file_size_mb, 2),
                "is_sampled": is_sampled,
                "separator": separator,
                "columns": list(df.columns),
                "shape": {
                    "rows": n_samples,
                    "cols": len(df.columns)
                },
                "n_samples": n_samples,
                "n_features": n_features,
                "head": {
                    "markdown": head_markdown,
                    "json": head_json
                },
                "metadata_columns": metadata_cols,
                "feature_columns": numeric_cols[:20],
                "total_feature_columns": len(numeric_cols),
                "missing_rate": round(missing_rate, 2),
                "data_range": data_range,
                "data": {
                    "summary": {
                        "n_samples": n_samples,
                        "n_features": n_features,
                        "missing_rate": round(missing_rate, 2),
                        "data_range": data_range,
                        "is_sampled": is_sampled
                    }
                }
            }
            
        except ImportError:
            return {
                "status": "error",
                "error": "pandas not installed. Please install: pip install pandas",
                "file_type": "tabular"
            }
        except Exception as e:
            logger.error(f"âŒ [TabularHandler] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_type": "tabular"
            }


# ============================================
# Part 3: The Facade (FileInspector)
# ============================================

class FileInspector:
    """
    æ–‡ä»¶æ£€æµ‹å™¨ - ç³»ç»Ÿçš„"é€šç”¨çœ¼ç›"ï¼ˆé—¨é¢æ¨¡å¼ï¼‰
    
    ä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ£€æŸ¥å™¨å¤„ç†æ–‡ä»¶
    """
    
    def __init__(self, upload_dir: str):
        """
        åˆå§‹åŒ–æ–‡ä»¶æ£€æµ‹å™¨
        
        Args:
            upload_dir: ä¸Šä¼ æ–‡ä»¶ç›®å½•
        """
        self.upload_dir = Path(upload_dir)
        try:
            self.upload_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError) as e:
            logger.warning(f"âš ï¸ æ— æ³•åˆ›å»ºä¸Šä¼ ç›®å½• {self.upload_dir}: {e}")
        
        # å¸¸è§ Docker æŒ‚è½½è·¯å¾„åˆ—è¡¨
        self.common_mount_paths = [
            "/app/uploads",
            "/app/data/uploads",
            "/app/data",
            "/workspace/uploads",
            "./uploads",
            "./data"
        ]
        
        # è·å–æ‰€æœ‰å·²æ³¨å†Œçš„æ£€æŸ¥å™¨
        self.handlers = [handler_class() for handler_class in _registry.get_handlers()]
        logger.info(f"âœ… [FileInspector] Loaded {len(self.handlers)} handlers")
    
    def _resolve_actual_path(self, file_path: str) -> Tuple[Optional[str], List[str]]:
        """
        æ™ºèƒ½è·¯å¾„è§£æï¼šå°è¯•åœ¨å¤šä¸ªå¸¸è§è·¯å¾„ä¸­æŸ¥æ‰¾æ–‡ä»¶æˆ–ç›®å½•
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            (actual_path, searched_paths)
        """
        searched_paths = []
        original_path = Path(file_path)
        
        # Step 1: æ£€æŸ¥åŸå§‹è·¯å¾„
        if original_path.exists():
            resolved_path = str(original_path.resolve())
            searched_paths.append(resolved_path)
            logger.info(f"âœ… [Smart Path Resolution] Found: {resolved_path}")
            return resolved_path, searched_paths
        
        if original_path.is_absolute():
            searched_paths.append(str(original_path))
        else:
            cwd_path = Path(os.getcwd()) / original_path
            searched_paths.append(str(cwd_path.resolve()))
        
        # Step 2: æå–æ–‡ä»¶åæˆ–ç›®å½•å
        path_name = original_path.name if original_path.name else str(original_path)
        if not path_name or path_name == '.':
            return None, searched_paths
        
        # Step 3: åœ¨å¸¸è§æŒ‚è½½è·¯å¾„ä¸­æœç´¢
        for mount_path in self.common_mount_paths:
            mount_path_obj = Path(mount_path)
            if not mount_path_obj.is_absolute():
                mount_path_obj = Path(os.getcwd()) / mount_path_obj
            
            try:
                resolved_mount = mount_path_obj.resolve()
                candidate_path = resolved_mount / path_name
                searched_paths.append(str(candidate_path))
                
                if candidate_path.exists():
                    logger.info(f"âœ… [Smart Path Resolution] Found: {candidate_path}")
                    return str(candidate_path), searched_paths
            except (OSError, ValueError) as e:
                logger.debug(f"âš ï¸ Invalid path {mount_path}: {e}")
                continue
        
        logger.warning(f"âŒ [Smart Path Resolution] Not found: {path_name}")
        return None, searched_paths
    
    def inspect_file(self, file_path: str) -> Dict[str, Any]:
        """
        å¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥ä¸»å…¥å£ï¼ˆåˆ†å‘å™¨ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        # Step 1: ä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æ
        actual_path, searched_paths = self._resolve_actual_path(file_path)
        
        if actual_path is None:
            current_cwd = os.getcwd()
            error_msg = (
                f"File or directory not found: '{file_path}'\n\n"
                f"**Searched locations ({len(searched_paths)}):**\n"
                + "\n".join(f"  - {path}" for path in searched_paths[:10])
                + f"\n\n**Current working directory:** {current_cwd}\n"
                f"**Upload directory (configured):** {self.upload_dir}\n"
                f"**Environment UPLOAD_DIR:** {os.getenv('UPLOAD_DIR', 'Not set')}"
            )
            
            logger.error(f"âŒ [FileInspector] {error_msg}")
            
            return {
                "status": "error",
                "success": False,
                "error": error_msg,
                "file_type": "unknown",
                "file_path": file_path,
                "searched_paths": searched_paths,
                "current_cwd": current_cwd
            }
        
        # Step 2: è½¬æ¢ä¸º Path å¯¹è±¡
        path = Path(actual_path)
        
        # Step 3: éå†æ‰€æœ‰æ£€æŸ¥å™¨ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ä»¥å¤„ç†çš„
        for handler in self.handlers:
            try:
                if handler.can_handle(path):
                    logger.info(f"âœ… [FileInspector] Using handler: {handler.__class__.__name__}")
                    result = handler.inspect(path)
                    # ç¡®ä¿è¿”å›ç»å¯¹è·¯å¾„
                    if result.get("status") == "success":
                        result["file_path"] = str(path.resolve())
                        result["success"] = True
                    return result
            except Exception as e:
                logger.warning(f"âš ï¸ Handler {handler.__class__.__name__} failed: {e}")
                # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ£€æŸ¥å™¨
                continue
        
        # Step 4: ç‰¹æ®Šå¤„ç†ï¼šå•ç‹¬çš„ .mtx æ–‡ä»¶
        if path.suffix == '.mtx' or path.name.lower() == 'matrix.mtx':
            error_msg = (
                "âš ï¸ **å•ç‹¬çš„ matrix.mtx æ–‡ä»¶æ— æ³•å•ç‹¬ä½¿ç”¨**\n\n"
                "10x Genomics æ•°æ®éœ€è¦ä¸‰ä¸ªæ–‡ä»¶ï¼š\n"
                "1. `matrix.mtx` - è¡¨è¾¾çŸ©é˜µ\n"
                "2. `barcodes.tsv` - ç»†èƒæ¡å½¢ç \n"
                "3. `features.tsv` æˆ– `genes.tsv` - åŸºå› ä¿¡æ¯\n\n"
                "**è§£å†³æ–¹æ¡ˆï¼š**\n"
                "- è¯·åŒæ—¶ä¸Šä¼ è¿™ä¸‰ä¸ªæ–‡ä»¶ï¼ˆæˆ–åŒ…å«è¿™ä¸‰ä¸ªæ–‡ä»¶çš„ç›®å½•ï¼‰\n"
                "- å¦‚æœæ–‡ä»¶å·²è§£å‹ï¼Œè¯·å°†å®ƒä»¬æ”¾åœ¨åŒä¸€ä¸ªç›®å½•ä¸­ä¸Šä¼ \n"
                "- å¦‚æœæ–‡ä»¶å·²å‹ç¼©ï¼Œè¯·ä¸Šä¼ å‹ç¼©åçš„æ–‡ä»¶ï¼ˆ.gz æ ¼å¼ï¼‰\n\n"
                f"**å½“å‰æ–‡ä»¶ï¼š** {path.name}"
            )
            logger.warning(f"âš ï¸ {error_msg}")
            return {
                "status": "error",
                "success": False,
                "error": error_msg,
                "file_type": "mtx",
                "file_path": str(path.resolve())
            }
        
        # Step 5: å¦‚æœæ²¡æœ‰æ£€æŸ¥å™¨å¯ä»¥å¤„ç†ï¼Œè¿”å›é”™è¯¯
        error_msg = f"æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹: {path.name}"
        logger.error(f"âŒ [FileInspector] {error_msg}")
        
        return {
            "status": "error",
            "success": False,
            "error": error_msg,
            "file_type": "unknown",
            "file_path": str(path.resolve())
        }
