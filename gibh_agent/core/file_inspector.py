"""
æ–‡ä»¶æ£€æµ‹å™¨ - Universal Eyes of the System
å¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥å™¨ï¼Œæ”¯æŒè¡¨æ ¼æ•°æ®ã€å•ç»†èƒæ•°æ®ã€å›¾åƒç­‰
"""
import os
import json
import gzip
import logging
from pathlib import Path
from typing import Dict, Optional, Any, List
import numpy as np

logger = logging.getLogger(__name__)


class FileInspector:
    """
    æ–‡ä»¶æ£€æµ‹å™¨ - ç³»ç»Ÿçš„"é€šç”¨çœ¼ç›"
    
    æ”¯æŒå¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥ï¼š
    - Tabular: CSV, TSV, TXT, XLSX
    - Single-Cell: H5AD, MTX, 10x Genomics
    - Image: JPG, PNG, TIFF
    """
    
    # æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼ˆMBï¼‰
    LARGE_FILE_THRESHOLD_MB = 200
    SAMPLE_SIZE_LARGE_FILE = 10000
    
    def __init__(self, upload_dir: str):
        """
        åˆå§‹åŒ–æ–‡ä»¶æ£€æµ‹å™¨
        
        Args:
            upload_dir: ä¸Šä¼ æ–‡ä»¶ç›®å½•
        """
        self.upload_dir = Path(upload_dir)
        self.upload_dir.mkdir(parents=True, exist_ok=True)
    
    def inspect_file(self, file_path: str) -> Dict[str, Any]:
        """
        å¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥ä¸»å…¥å£ï¼ˆåˆ†å‘å™¨ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        filepath = Path(file_path)
        
        if not filepath.exists():
            return {
                "status": "error",
                "error": f"File not found: {file_path}",
                "file_type": "unknown"
            }
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹å¹¶åˆ†å‘åˆ°ç›¸åº”çš„æ£€æŸ¥å™¨
        if filepath.is_dir():
            # 10x Genomics ç›®å½•
            return self._inspect_anndata(file_path)
        
        file_ext = filepath.suffix.lower()
        
        # Tabular æ•°æ®
        if file_ext in ['.csv', '.tsv', '.txt', '.xlsx']:
            return self._inspect_tabular(file_path)
        
        # Single-Cell æ•°æ®
        elif file_ext in ['.h5ad', '.mtx'] or file_ext.endswith('.mtx.gz'):
            return self._inspect_anndata(file_path)
        
        # å›¾åƒæ•°æ®
        elif file_ext in ['.jpg', '.jpeg', '.png', '.tiff', '.tif']:
            return self._inspect_image(file_path)
        
        else:
            return {
                "status": "error",
                "error": f"Unsupported file type: {file_ext}",
                "file_type": "unknown"
            }
    
    def _inspect_tabular(
        self,
        file_path: str
    ) -> Dict[str, Any]:
        """
        æ£€æŸ¥è¡¨æ ¼æ•°æ®ï¼ˆCSV, TSV, TXT, XLSXï¼‰
        
        ç­–ç•¥ï¼š
        - å°æ–‡ä»¶ï¼ˆ<200MBï¼‰ï¼šå®Œæ•´è¯»å–ä»¥è·å¾—å‡†ç¡®ç»Ÿè®¡
        - å¤§æ–‡ä»¶ï¼ˆ>200MBï¼‰ï¼šé‡‡æ ·10000è¡Œé˜²æ­¢OOM
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        import pandas as pd
        
        try:
            filepath = Path(file_path)
            file_size_mb = filepath.stat().st_size / (1024 * 1024)
            
            logger.info(f"ğŸ” [Tabular Inspector] File: {file_path}, Size: {file_size_mb:.2f} MB")
            
            # å†³å®šè¯»å–ç­–ç•¥
            if file_size_mb < self.LARGE_FILE_THRESHOLD_MB:
                # å°æ–‡ä»¶ï¼šå®Œæ•´è¯»å–
                logger.info(f"   ğŸ“– Reading full file (size < {self.LARGE_FILE_THRESHOLD_MB}MB)")
                df = pd.read_csv(file_path)
                is_sampled = False
            else:
                # å¤§æ–‡ä»¶ï¼šé‡‡æ ·è¯»å–
                logger.info(f"   ğŸ“– Sampling {self.SAMPLE_SIZE_LARGE_FILE} rows (size >= {self.LARGE_FILE_THRESHOLD_MB}MB)")
                df = pd.read_csv(file_path, nrows=self.SAMPLE_SIZE_LARGE_FILE)
                is_sampled = True
                # ä¼°ç®—æ€»è¡Œæ•°
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        total_lines = sum(1 for _ in f) - 1  # å‡å»è¡¨å¤´
                except:
                    total_lines = None
            
            # è¯†åˆ«åˆ—ç±»å‹
            metadata_cols = []
            numeric_cols = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    numeric_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            n_samples = len(df) if not is_sampled else (total_lines if total_lines else len(df))
            n_features = len(numeric_cols)
            
            # ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆå…³é”®ï¼šéœ€è¦å®Œæ•´æ•°æ®æˆ–è¶³å¤Ÿå¤§çš„é‡‡æ ·ï¼‰
            if len(numeric_cols) > 0:
                numeric_data = df[numeric_cols]
                total_cells = numeric_data.size
                missing_cells = numeric_data.isnull().sum().sum()
                missing_rate = (missing_cells / total_cells * 100) if total_cells > 0 else 0
            else:
                missing_rate = 0
            
            # æ•°æ®èŒƒå›´ï¼ˆå…³é”®ï¼šç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ Log2 å˜æ¢ï¼‰
            data_range = {}
            if len(numeric_cols) > 0:
                numeric_data = df[numeric_cols]
                data_range = {
                    "min": float(numeric_data.min().min()),
                    "max": float(numeric_data.max().max()),
                    "mean": float(numeric_data.mean().mean()),
                    "median": float(numeric_data.median().median())
                }
            
            # è¯†åˆ«æ½œåœ¨çš„åˆ†ç»„åˆ—ï¼ˆå”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ï¼‰
            potential_groups = {}
            for col in metadata_cols:
                unique_count = df[col].nunique()
                if unique_count > 1 and unique_count <= min(20, len(df) // 2):
                    potential_groups[col] = {
                        "n_unique": int(unique_count),
                        "values": df[col].unique().tolist()[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
                    }
            
            # æ„å»ºç»“æœï¼ˆåªåŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸åŒ…å«åŸå§‹æ•°æ®è¡Œï¼‰
            result = {
                "status": "success",
                "file_path": file_path,
                "file_type": "tabular",
                "file_size_mb": round(file_size_mb, 2),
                "is_sampled": is_sampled,
                "n_samples": n_samples if isinstance(n_samples, int) else f"~{n_samples}",
                "n_features": n_features,
                "n_metadata_cols": len(metadata_cols),
                "metadata_columns": metadata_cols,
                "feature_columns": numeric_cols[:20],  # åªæ˜¾ç¤ºå‰20ä¸ªç‰¹å¾åˆ—å
                "total_feature_columns": len(numeric_cols),
                "missing_rate": round(missing_rate, 2),
                "data_range": data_range,
                "potential_groups": potential_groups,
                # å‰ç«¯å¯ç”¨çš„æ‘˜è¦æ•°æ®
                "data": {
                    "summary": {
                        "n_samples": n_samples if isinstance(n_samples, int) else f"~{n_samples}",
                        "n_features": n_features,
                        "missing_rate": round(missing_rate, 2),
                        "data_range": data_range,
                        "is_sampled": is_sampled
                    }
                }
            }
            
            logger.info(f"âœ… [Tabular Inspector] Success: {n_samples} samples, {n_features} features, missing_rate={missing_rate:.2f}%")
            return result
            
        except Exception as e:
            logger.error(f"âŒ [Tabular Inspector] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_type": "tabular"
            }
    
    def _inspect_anndata(
        self,
        file_path: str
    ) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•ç»†èƒæ•°æ®ï¼ˆH5AD, MTX, 10x Genomicsï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        try:
            import scanpy as sc
            
            filepath = Path(file_path)
            
            logger.info(f"ğŸ” [Anndata Inspector] File: {file_path}")
            
            # å¤„ç†ç›®å½•ï¼ˆ10x Genomicsï¼‰
            if filepath.is_dir():
                dir_contents = os.listdir(filepath)
                if any(f in dir_contents for f in ['matrix.mtx', 'matrix.mtx.gz']):
                    # 10x æ ¼å¼éœ€è¦å®Œæ•´åŠ è½½
                    logger.info("   ğŸ“– Loading 10x Genomics directory...")
                    adata = sc.read_10x_mtx(filepath)
                else:
                    return {
                        "status": "error",
                        "error": f"Unknown directory format: {file_path}",
                        "file_type": "directory"
                    }
            # å¤„ç† H5AD æ–‡ä»¶
            elif filepath.suffix == '.h5ad':
                # ä½¿ç”¨ backed='r' æ¨¡å¼ï¼ˆåªè¯»ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®åˆ°å†…å­˜ï¼‰
                logger.info("   ğŸ“– Loading H5AD file (backed mode)...")
                try:
                    adata = sc.read_h5ad(file_path, backed='r')
                except:
                    # å¦‚æœ backed æ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼
                    logger.warning("   âš ï¸ Backed mode failed, using normal mode")
                    adata = sc.read_h5ad(file_path)
            # å¤„ç† MTX æ–‡ä»¶
            else:
                logger.info("   ğŸ“– Loading MTX file...")
                adata = sc.read(file_path)
            
            # æå–åŸºæœ¬ä¿¡æ¯
            n_obs = adata.n_obs
            n_vars = adata.n_vars
            obs_keys = list(adata.obs.columns) if hasattr(adata.obs, 'columns') else []
            var_keys = list(adata.var.columns) if hasattr(adata.var, 'columns') else []
            
            # è®¡ç®—ç¨€ç–åº¦
            if hasattr(adata.X, 'nnz'):
                # ç¨€ç–çŸ©é˜µ
                total_cells = n_obs * n_vars
                sparsity = (1 - adata.X.nnz / total_cells) * 100 if total_cells > 0 else 0
            else:
                # å¯†é›†çŸ©é˜µ
                sparsity = 0
            
            # æ£€æŸ¥æ•°æ®å€¼èŒƒå›´ï¼ˆé‡‡æ ·æ£€æŸ¥ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
            data_range = {}
            try:
                sample_size = min(1000, n_obs * n_vars)
                if sample_size > 0:
                    if hasattr(adata.X, 'toarray'):
                        # ç¨€ç–çŸ©é˜µï¼šé‡‡æ ·æ£€æŸ¥
                        sample_data = adata.X[:min(100, n_obs), :min(100, n_vars)].toarray()
                    else:
                        # å¯†é›†çŸ©é˜µï¼šé‡‡æ ·æ£€æŸ¥
                        sample_data = adata.X[:min(100, n_obs), :min(100, n_vars)]
                    
                    data_range = {
                        "min": float(np.min(sample_data)),
                        "max": float(np.max(sample_data)),
                        "mean": float(np.mean(sample_data)),
                        "median": float(np.median(sample_data))
                    }
            except Exception as e:
                logger.warning(f"   âš ï¸ Could not calculate data range: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰èšç±»ç»“æœ
            has_clusters = 'leiden' in obs_keys or 'louvain' in obs_keys
            has_umap = 'X_umap' in adata.obsm_keys() if hasattr(adata, 'obsm_keys') else False
            
            result = {
                "status": "success",
                "file_path": file_path,
                "file_type": "anndata",
                "n_obs": n_obs,
                "n_vars": n_vars,
                "obs_keys": obs_keys,
                "var_keys": var_keys,
                "sparsity": round(sparsity, 2),
                "data_range": data_range,
                "has_clusters": has_clusters,
                "has_umap": has_umap,
                # å‰ç«¯å¯ç”¨çš„æ‘˜è¦æ•°æ®
                "data": {
                    "summary": {
                        "n_obs": n_obs,
                        "n_vars": n_vars,
                        "sparsity": round(sparsity, 2),
                        "data_range": data_range,
                        "has_clusters": has_clusters,
                        "has_umap": has_umap
                    }
                }
            }
            
            logger.info(f"âœ… [Anndata Inspector] Success: {n_obs} cells, {n_vars} genes, sparsity={sparsity:.2f}%")
            return result
            
        except ImportError:
            logger.error("âŒ [Anndata Inspector] scanpy not available")
            return {
                "status": "error",
                "error": "scanpy not installed",
                "file_type": "anndata"
            }
        except Exception as e:
            logger.error(f"âŒ [Anndata Inspector] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_type": "anndata"
            }
    
    def _inspect_image(
        self,
        file_path: str
    ) -> Dict[str, Any]:
        """
        æ£€æŸ¥å›¾åƒæ–‡ä»¶ï¼ˆJPG, PNG, TIFFï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        try:
            from PIL import Image
            
            logger.info(f"ğŸ” [Image Inspector] File: {file_path}")
            
            with Image.open(file_path) as img:
                width, height = img.size
                mode = img.mode
                format_name = img.format
                
                # è·å–æ–‡ä»¶å¤§å°
                filepath = Path(file_path)
                file_size_mb = filepath.stat().st_size / (1024 * 1024)
            
            result = {
                "status": "success",
                "file_path": file_path,
                "file_type": "image",
                "file_size_mb": round(file_size_mb, 2),
                "width": width,
                "height": height,
                "mode": mode,
                "format": format_name,
                # å‰ç«¯å¯ç”¨çš„æ‘˜è¦æ•°æ®
                "data": {
                    "summary": {
                        "width": width,
                        "height": height,
                        "mode": mode,
                        "format": format_name,
                        "file_size_mb": round(file_size_mb, 2)
                    }
                }
            }
            
            logger.info(f"âœ… [Image Inspector] Success: {width}x{height}, mode={mode}, format={format_name}")
            return result
            
        except ImportError:
            logger.error("âŒ [Image Inspector] PIL not available")
            return {
                "status": "error",
                "error": "PIL (Pillow) not installed",
                "file_type": "image"
            }
        except Exception as e:
            logger.error(f"âŒ [Image Inspector] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_type": "image"
            }
    
    # ================= ä¿ç•™æ—§æ–¹æ³•ä»¥ä¿æŒå…¼å®¹æ€§ =================
    
    def _is_gzipped(self, filepath: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸º gzip å‹ç¼©"""
        if filepath.is_dir():
            return False
        try:
            with open(filepath, 'rb') as f:
                return f.read(2) == b'\x1f\x8b'
        except:
            return False
    
    def _read_head(self, filepath: Path, lines: int = 5) -> list:
        """å®‰å…¨è¯»å–æ–‡ä»¶å‰å‡ è¡Œ"""
        if filepath.is_dir():
            return []
        try:
            if self._is_gzipped(filepath):
                with gzip.open(filepath, 'rt') as f:
                    return [next(f).strip() for _ in range(lines) if True]
            else:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    return [next(f).strip() for _ in range(lines) if True]
        except Exception:
            return []
    
    def generate_metadata(self, filename: str) -> Optional[Dict]:
        """
        ä¸»åŠ¨æ£€æµ‹ï¼šåˆ†æå•ä¸ªæ–‡ä»¶å¹¶ä¿å­˜ .meta.jsonï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        
        Args:
            filename: æ–‡ä»¶åæˆ–ç›®å½•å
        
        Returns:
            å…ƒæ•°æ®å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        filepath = self.upload_dir / filename
        if not filepath.exists():
            return None
        
        # ä½¿ç”¨æ–°çš„ inspect_file æ–¹æ³•
        result = self.inspect_file(str(filepath))
        
        if result.get("status") == "success":
            # ä¿å­˜å…ƒæ•°æ®
            meta_path = filepath.with_suffix(filepath.suffix + '.meta.json')
            try:
                with open(meta_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return result
    
    def get_metadata(self, filename: str) -> Optional[Dict]:
        """
        è·å–æ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆå¦‚æœå·²ç”Ÿæˆï¼‰
        
        Args:
            filename: æ–‡ä»¶å
        
        Returns:
            å…ƒæ•°æ®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        filepath = self.upload_dir / filename
        meta_path = filepath.with_suffix(filepath.suffix + '.meta.json')
        
        if meta_path.exists():
            try:
                with open(meta_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return None
        return None
