"""
æ–‡ä»¶æ£€æµ‹å™¨ - Universal Eyes of the System
å¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥å™¨ï¼Œæ”¯æŒè¡¨æ ¼æ•°æ®ã€å•ç»†èƒæ•°æ®ã€å›¾åƒç­‰
"""
import os
import json
import gzip
import logging
from pathlib import Path
from typing import Dict, Optional, Any, List, Tuple
import numpy as np

logger = logging.getLogger(__name__)


class FileInspector:
    """
    æ–‡ä»¶æ£€æµ‹å™¨ - ç³»ç»Ÿçš„"é€šç”¨çœ¼ç›"
    
    æ”¯æŒå¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥ï¼š
    - Tabular: CSV, TSV, TXT, XLSX
    - Single-Cell: H5AD, MTX, 10x Genomics
    - Image: JPG, PNG, TIFF
    """
    
    # æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼ˆMBï¼‰
    LARGE_FILE_THRESHOLD_MB = 200
    SAMPLE_SIZE_LARGE_FILE = 10000
    
    def __init__(self, upload_dir: str):
        """
        åˆå§‹åŒ–æ–‡ä»¶æ£€æµ‹å™¨
        
        Args:
            upload_dir: ä¸Šä¼ æ–‡ä»¶ç›®å½•
        """
        self.upload_dir = Path(upload_dir)
        # å°è¯•åˆ›å»ºç›®å½•ï¼Œå¦‚æœå¤±è´¥åˆ™è®°å½•è­¦å‘Šï¼ˆå¯èƒ½åœ¨å®¹å™¨å¤–æµ‹è¯•ï¼‰
        try:
            self.upload_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError) as e:
            logger.warning(f"âš ï¸ æ— æ³•åˆ›å»ºä¸Šä¼ ç›®å½• {self.upload_dir}: {e}ã€‚å°†åœ¨è¿è¡Œæ—¶å°è¯•æŸ¥æ‰¾æ–‡ä»¶ã€‚")
        
        # å¸¸è§ Docker æŒ‚è½½è·¯å¾„åˆ—è¡¨ï¼ˆç”¨äºæ™ºèƒ½è·¯å¾„è§£æï¼‰
        self.common_mount_paths = [
            "/app/uploads",
            "/app/data/uploads",
            "/app/data",
            "/workspace/uploads",
            "./uploads",
            "./data"
        ]
    
    def _resolve_actual_path(self, file_path: str) -> Tuple[Optional[str], List[str]]:
        """
        æ™ºèƒ½è·¯å¾„è§£æï¼šå°è¯•åœ¨å¤šä¸ªå¸¸è§è·¯å¾„ä¸­æŸ¥æ‰¾æ–‡ä»¶
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ã€ç»å¯¹è·¯å¾„æˆ–ä»…æ–‡ä»¶åï¼‰
        
        Returns:
            (actual_path, searched_paths): 
            - actual_path: æ‰¾åˆ°çš„å®é™…è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™ä¸º None
            - searched_paths: å·²æœç´¢çš„è·¯å¾„åˆ—è¡¨ï¼ˆç”¨äºé”™è¯¯æŠ¥å‘Šï¼‰
        """
        import os
        from pathlib import Path
        
        searched_paths = []
        
        # Step 1: æ£€æŸ¥åŸå§‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        original_path = Path(file_path)
        if original_path.exists():
            searched_paths.append(str(original_path.resolve()))
            return str(original_path.resolve()), searched_paths
        
        # å¦‚æœåŸå§‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè®°å½•å®ƒ
        if original_path.is_absolute():
            searched_paths.append(str(original_path))
        else:
            # å°è¯•ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
            cwd_path = Path(os.getcwd()) / original_path
            searched_paths.append(str(cwd_path.resolve()))
        
        # Step 2: æå–æ–‡ä»¶å
        filename = original_path.name
        if not filename:
            # å¦‚æœè·¯å¾„æ˜¯ç›®å½•æˆ–æ— æ•ˆï¼Œè¿”å› None
            return None, searched_paths
        
        # Step 3: åœ¨å¸¸è§æŒ‚è½½è·¯å¾„ä¸­æœç´¢
        for mount_path in self.common_mount_paths:
            mount_path_obj = Path(mount_path)
            
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if not mount_path_obj.is_absolute():
                mount_path_obj = Path(os.getcwd()) / mount_path_obj
            
            # å°è¯•è§£æè·¯å¾„
            try:
                resolved_mount = mount_path_obj.resolve()
                candidate_path = resolved_mount / filename
                
                searched_paths.append(str(candidate_path))
                
                if candidate_path.exists() and candidate_path.is_file():
                    logger.info(f"âœ… [Smart Path Resolution] Found file at: {candidate_path}")
                    return str(candidate_path), searched_paths
            except (OSError, ValueError) as e:
                # è·¯å¾„æ— æ•ˆï¼Œè·³è¿‡
                logger.debug(f"âš ï¸ [Smart Path Resolution] Invalid path {mount_path}: {e}")
                continue
        
        # Step 4: å¦‚æœä»æœªæ‰¾åˆ°ï¼Œè¿”å› None å’Œå·²æœç´¢çš„è·¯å¾„åˆ—è¡¨
        logger.warning(f"âŒ [Smart Path Resolution] File not found: {filename}")
        logger.warning(f"   Searched in {len(searched_paths)} locations")
        return None, searched_paths
    
    def inspect_file(self, file_path: str) -> Dict[str, Any]:
        """
        å¤šæ¨¡æ€æ–‡ä»¶æ£€æŸ¥ä¸»å…¥å£ï¼ˆåˆ†å‘å™¨ï¼‰
        
        ğŸ”¥ å‡çº§ï¼šä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æï¼Œè‡ªåŠ¨åœ¨å¤šä¸ªå¸¸è§è·¯å¾„ä¸­æŸ¥æ‰¾æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸ï¼ŒåŒ…å«ç»å¯¹è·¯å¾„ï¼ˆfile_pathå­—æ®µï¼‰
        """
        import os
        
        # ğŸ”¥ Step 1: ä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æ
        actual_path, searched_paths = self._resolve_actual_path(file_path)
        
        if actual_path is None:
            # ğŸ”¥ CRITICAL: è¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œåˆ—å‡ºæ‰€æœ‰æœç´¢è¿‡çš„è·¯å¾„
            current_cwd = os.getcwd()
            error_msg = (
                f"File not found: '{file_path}'\n\n"
                f"**Searched locations ({len(searched_paths)}):**\n"
                + "\n".join(f"  - {path}" for path in searched_paths[:10])  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                + f"\n\n**Current working directory:** {current_cwd}\n"
                f"**Upload directory (configured):** {self.upload_dir}\n"
                f"**Environment UPLOAD_DIR:** {os.getenv('UPLOAD_DIR', 'Not set')}"
            )
            
            logger.error(f"âŒ [FileInspector] {error_msg}")
            
            return {
                "status": "error",
                "success": False,  # ğŸ”¥ æ·»åŠ  success å­—æ®µç”¨äºå‰ç«¯æ£€æŸ¥
                "error": error_msg,
                "file_type": "unknown",
                "file_path": file_path,  # è¿”å›åŸå§‹è·¯å¾„
                "searched_paths": searched_paths,  # è°ƒè¯•ä¿¡æ¯
                "current_cwd": current_cwd
            }
        
        # Step 2: ä½¿ç”¨æ‰¾åˆ°çš„å®é™…è·¯å¾„ç»§ç»­å¤„ç†
        filepath = Path(actual_path)
        absolute_path = str(filepath.resolve())
        
        logger.info(f"âœ… [FileInspector] Using resolved path: {absolute_path}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹å¹¶åˆ†å‘åˆ°ç›¸åº”çš„æ£€æŸ¥å™¨ï¼ˆä¼ é€’ç»å¯¹è·¯å¾„ï¼‰
        if filepath.is_dir():
            # 10x Genomics ç›®å½•
            return self._inspect_anndata(absolute_path)
        
        file_ext = filepath.suffix.lower()
        
        # Tabular æ•°æ®
        if file_ext in ['.csv', '.tsv', '.txt', '.xlsx']:
            return self._inspect_tabular(absolute_path)
        
        # Single-Cell æ•°æ®
        elif file_ext in ['.h5ad', '.mtx'] or file_ext.endswith('.mtx.gz'):
            return self._inspect_anndata(absolute_path)
        
        # å›¾åƒæ•°æ®
        elif file_ext in ['.jpg', '.jpeg', '.png', '.tiff', '.tif']:
            return self._inspect_image(absolute_path)
        
        else:
            return {
                "status": "error",
                "error": f"Unsupported file type: {file_ext}",
                "file_type": "unknown",
                "file_path": absolute_path  # è¿”å›ç»å¯¹è·¯å¾„
            }
    
    def _inspect_tabular(
        self,
        file_path: str
    ) -> Dict[str, Any]:
        """
        æ£€æŸ¥è¡¨æ ¼æ•°æ®ï¼ˆCSV, TSV, TXT, XLSXï¼‰
        
        ç­–ç•¥ï¼š
        - å°æ–‡ä»¶ï¼ˆ<200MBï¼‰ï¼šå®Œæ•´è¯»å–ä»¥è·å¾—å‡†ç¡®ç»Ÿè®¡
        - å¤§æ–‡ä»¶ï¼ˆ>200MBï¼‰ï¼šé‡‡æ ·10000è¡Œé˜²æ­¢OOM
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬ï¼š
            - file_path: ç»å¯¹è·¯å¾„
            - columns: åˆ—ååˆ—è¡¨
            - shape: (rows, cols) å…ƒç»„
            - head: å‰10è¡Œï¼ˆmarkdownæ ¼å¼ï¼‰
            - separator: åˆ†éš”ç¬¦ï¼ˆ',' æˆ– '\t'ï¼‰
        """
        import pandas as pd
        
        try:
            # ğŸ”¥ Step 1: è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            filepath = Path(file_path)
            if not filepath.is_absolute():
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•ç›¸å¯¹äº upload_dir
                filepath = self.upload_dir / filepath
            absolute_path = str(filepath.resolve())
            
            if not filepath.exists():
                return {
                    "status": "error",
                    "error": f"File not found: {absolute_path}",
                    "file_type": "tabular"
                }
            
            file_size_mb = filepath.stat().st_size / (1024 * 1024)
            
            logger.info(f"ğŸ” [Tabular Inspector] File: {absolute_path}, Size: {file_size_mb:.2f} MB")
            
            # ğŸ”¥ Step 2: æ£€æµ‹åˆ†éš”ç¬¦ï¼ˆå…ˆè¯»ç¬¬ä¸€è¡Œï¼‰
            separator = ','
            try:
                with open(absolute_path, 'r', encoding='utf-8', errors='ignore') as f:
                    first_line = f.readline()
                    if '\t' in first_line:
                        separator = '\t'
                    elif ',' in first_line:
                        separator = ','
                    elif ';' in first_line:
                        separator = ';'
            except:
                pass  # ä½¿ç”¨é»˜è®¤åˆ†éš”ç¬¦
            
            # å†³å®šè¯»å–ç­–ç•¥
            if file_size_mb < self.LARGE_FILE_THRESHOLD_MB:
                # å°æ–‡ä»¶ï¼šå®Œæ•´è¯»å–
                logger.info(f"   ğŸ“– Reading full file (size < {self.LARGE_FILE_THRESHOLD_MB}MB)")
                df = pd.read_csv(absolute_path, sep=separator)
                is_sampled = False
            else:
                # å¤§æ–‡ä»¶ï¼šé‡‡æ ·è¯»å–
                logger.info(f"   ğŸ“– Sampling {self.SAMPLE_SIZE_LARGE_FILE} rows (size >= {self.LARGE_FILE_THRESHOLD_MB}MB)")
                df = pd.read_csv(absolute_path, sep=separator, nrows=self.SAMPLE_SIZE_LARGE_FILE)
                is_sampled = True
                # ä¼°ç®—æ€»è¡Œæ•°
                try:
                    with open(absolute_path, 'r', encoding='utf-8', errors='ignore') as f:
                        total_lines = sum(1 for _ in f) - 1  # å‡å»è¡¨å¤´
                except:
                    total_lines = None
            
            # è¯†åˆ«åˆ—ç±»å‹
            metadata_cols = []
            numeric_cols = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    numeric_cols.append(col)
                else:
                    metadata_cols.append(col)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            n_samples = len(df) if not is_sampled else (total_lines if total_lines else len(df))
            n_features = len(numeric_cols)
            
            # ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆå…³é”®ï¼šéœ€è¦å®Œæ•´æ•°æ®æˆ–è¶³å¤Ÿå¤§çš„é‡‡æ ·ï¼‰
            if len(numeric_cols) > 0:
                numeric_data = df[numeric_cols]
                total_cells = numeric_data.size
                missing_cells = numeric_data.isnull().sum().sum()
                missing_rate = (missing_cells / total_cells * 100) if total_cells > 0 else 0
            else:
                missing_rate = 0
            
            # æ•°æ®èŒƒå›´ï¼ˆå…³é”®ï¼šç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ Log2 å˜æ¢ï¼‰
            data_range = {}
            if len(numeric_cols) > 0:
                numeric_data = df[numeric_cols]
                data_range = {
                    "min": float(numeric_data.min().min()),
                    "max": float(numeric_data.max().max()),
                    "mean": float(numeric_data.mean().mean()),
                    "median": float(numeric_data.median().median())
                }
            
            # è¯†åˆ«æ½œåœ¨çš„åˆ†ç»„åˆ—ï¼ˆå”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ï¼‰
            potential_groups = {}
            for col in metadata_cols:
                unique_count = df[col].nunique()
                if unique_count > 1 and unique_count <= min(20, len(df) // 2):
                    potential_groups[col] = {
                        "n_unique": int(unique_count),
                        "values": df[col].unique().tolist()[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
                    }
            
            # ğŸ”¥ Step 3: æå–å‰10è¡Œä½œä¸º headï¼ˆmarkdownæ ¼å¼ï¼‰
            head_df = df.head(10)
            head_json = head_df.to_dict(orient='records')
            
            # ğŸ”¥ ä¿®å¤ï¼šå¤„ç† tabulate ä¾èµ–ç¼ºå¤±é—®é¢˜
            try:
                head_markdown = head_df.to_markdown(index=False)
            except ImportError:
                # å¦‚æœ tabulate ä¸å¯ç”¨ï¼Œç”Ÿæˆç®€å•çš„æ–‡æœ¬è¡¨æ ¼
                logger.warning("âš ï¸ tabulate ä¸å¯ç”¨ï¼Œä½¿ç”¨æ–‡æœ¬æ ¼å¼æ›¿ä»£ markdown")
                try:
                    # ä½¿ç”¨ pandas çš„ to_string æ–¹æ³•ç”Ÿæˆæ–‡æœ¬è¡¨æ ¼
                    head_markdown = head_df.to_string(index=False, max_rows=10)
                except Exception as e:
                    # å¦‚æœ to_string ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„ CSV æ ¼å¼
                    logger.warning(f"âš ï¸ to_string å¤±è´¥ï¼Œä½¿ç”¨ CSV æ ¼å¼: {e}")
                    head_markdown = head_df.to_csv(index=False)
            
            # æ„å»ºç»“æœï¼ˆåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼Œä½œä¸ºå•ä¸€æ•°æ®æºï¼‰
            result = {
                "status": "success",
                "file_path": absolute_path,  # ğŸ”¥ ç»å¯¹è·¯å¾„
                "file_type": "tabular",
                "file_size_mb": round(file_size_mb, 2),
                "is_sampled": is_sampled,
                "separator": separator,  # ğŸ”¥ åˆ†éš”ç¬¦
                "columns": list(df.columns),  # ğŸ”¥ æ‰€æœ‰åˆ—å
                "shape": {
                    "rows": n_samples if isinstance(n_samples, int) else (total_lines if total_lines else len(df)),
                    "cols": len(df.columns)
                },
                "head": {  # ğŸ”¥ å‰10è¡Œæ•°æ®
                    "markdown": head_markdown,
                    "json": head_json
                },
                "n_samples": n_samples if isinstance(n_samples, int) else f"~{n_samples}",
                "n_features": n_features,
                "n_metadata_cols": len(metadata_cols),
                "metadata_columns": metadata_cols,
                "feature_columns": numeric_cols[:20],  # åªæ˜¾ç¤ºå‰20ä¸ªç‰¹å¾åˆ—å
                "total_feature_columns": len(numeric_cols),
                "missing_rate": round(missing_rate, 2),
                "data_range": data_range,
                "potential_groups": potential_groups,
                # å‰ç«¯å¯ç”¨çš„æ‘˜è¦æ•°æ®
                "data": {
                    "summary": {
                        "n_samples": n_samples if isinstance(n_samples, int) else f"~{n_samples}",
                        "n_features": n_features,
                        "missing_rate": round(missing_rate, 2),
                        "data_range": data_range,
                        "is_sampled": is_sampled
                    }
                }
            }
            
            logger.info(f"âœ… [Tabular Inspector] Success: {n_samples} samples, {n_features} features, missing_rate={missing_rate:.2f}%")
            return result
            
        except Exception as e:
            logger.error(f"âŒ [Tabular Inspector] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_type": "tabular"
            }
    
    def _inspect_anndata(
        self,
        file_path: str
    ) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•ç»†èƒæ•°æ®ï¼ˆH5AD, MTX, 10x Genomicsï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        try:
            import scanpy as sc
            
            filepath = Path(file_path)
            
            logger.info(f"ğŸ” [Anndata Inspector] File: {file_path}")
            
            # å¤„ç†ç›®å½•ï¼ˆ10x Genomicsï¼‰
            if filepath.is_dir():
                dir_contents = os.listdir(filepath)
                if any(f in dir_contents for f in ['matrix.mtx', 'matrix.mtx.gz']):
                    # 10x æ ¼å¼éœ€è¦å®Œæ•´åŠ è½½
                    logger.info("   ğŸ“– Loading 10x Genomics directory...")
                    adata = sc.read_10x_mtx(filepath)
                else:
                    return {
                        "status": "error",
                        "error": f"Unknown directory format: {file_path}",
                        "file_type": "directory"
                    }
            # å¤„ç† H5AD æ–‡ä»¶
            elif filepath.suffix == '.h5ad':
                # ä½¿ç”¨ backed='r' æ¨¡å¼ï¼ˆåªè¯»ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®åˆ°å†…å­˜ï¼‰
                logger.info("   ğŸ“– Loading H5AD file (backed mode)...")
                try:
                    adata = sc.read_h5ad(file_path, backed='r')
                except:
                    # å¦‚æœ backed æ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼
                    logger.warning("   âš ï¸ Backed mode failed, using normal mode")
                    adata = sc.read_h5ad(file_path)
            # å¤„ç† MTX æ–‡ä»¶
            else:
                logger.info("   ğŸ“– Loading MTX file...")
                adata = sc.read(file_path)
            
            # æå–åŸºæœ¬ä¿¡æ¯
            n_obs = adata.n_obs
            n_vars = adata.n_vars
            obs_keys = list(adata.obs.columns) if hasattr(adata.obs, 'columns') else []
            var_keys = list(adata.var.columns) if hasattr(adata.var, 'columns') else []
            
            # è®¡ç®—ç¨€ç–åº¦
            if hasattr(adata.X, 'nnz'):
                # ç¨€ç–çŸ©é˜µ
                total_cells = n_obs * n_vars
                sparsity = (1 - adata.X.nnz / total_cells) * 100 if total_cells > 0 else 0
            else:
                # å¯†é›†çŸ©é˜µ
                sparsity = 0
            
            # æ£€æŸ¥æ•°æ®å€¼èŒƒå›´ï¼ˆé‡‡æ ·æ£€æŸ¥ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
            data_range = {}
            try:
                sample_size = min(1000, n_obs * n_vars)
                if sample_size > 0:
                    if hasattr(adata.X, 'toarray'):
                        # ç¨€ç–çŸ©é˜µï¼šé‡‡æ ·æ£€æŸ¥
                        sample_data = adata.X[:min(100, n_obs), :min(100, n_vars)].toarray()
                    else:
                        # å¯†é›†çŸ©é˜µï¼šé‡‡æ ·æ£€æŸ¥
                        sample_data = adata.X[:min(100, n_obs), :min(100, n_vars)]
                    
                    data_range = {
                        "min": float(np.min(sample_data)),
                        "max": float(np.max(sample_data)),
                        "mean": float(np.mean(sample_data)),
                        "median": float(np.median(sample_data))
                    }
            except Exception as e:
                logger.warning(f"   âš ï¸ Could not calculate data range: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰èšç±»ç»“æœ
            has_clusters = 'leiden' in obs_keys or 'louvain' in obs_keys
            has_umap = 'X_umap' in adata.obsm_keys() if hasattr(adata, 'obsm_keys') else False
            
            result = {
                "status": "success",
                "file_path": file_path,
                "file_type": "anndata",
                "n_obs": n_obs,
                "n_vars": n_vars,
                "obs_keys": obs_keys,
                "var_keys": var_keys,
                "sparsity": round(sparsity, 2),
                "data_range": data_range,
                "has_clusters": has_clusters,
                "has_umap": has_umap,
                # å‰ç«¯å¯ç”¨çš„æ‘˜è¦æ•°æ®
                "data": {
                    "summary": {
                        "n_obs": n_obs,
                        "n_vars": n_vars,
                        "sparsity": round(sparsity, 2),
                        "data_range": data_range,
                        "has_clusters": has_clusters,
                        "has_umap": has_umap
                    }
                }
            }
            
            logger.info(f"âœ… [Anndata Inspector] Success: {n_obs} cells, {n_vars} genes, sparsity={sparsity:.2f}%")
            return result
            
        except ImportError:
            logger.error("âŒ [Anndata Inspector] scanpy not available")
            return {
                "status": "error",
                "error": "scanpy not installed",
                "file_type": "anndata"
            }
        except Exception as e:
            logger.error(f"âŒ [Anndata Inspector] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_type": "anndata"
            }
    
    def _inspect_image(
        self,
        file_path: str
    ) -> Dict[str, Any]:
        """
        æ£€æŸ¥å›¾åƒæ–‡ä»¶ï¼ˆJPG, PNG, TIFFï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        try:
            from PIL import Image
            
            logger.info(f"ğŸ” [Image Inspector] File: {file_path}")
            
            with Image.open(file_path) as img:
                width, height = img.size
                mode = img.mode
                format_name = img.format
                
                # è·å–æ–‡ä»¶å¤§å°
                filepath = Path(file_path)
                file_size_mb = filepath.stat().st_size / (1024 * 1024)
            
            result = {
                "status": "success",
                "file_path": file_path,
                "file_type": "image",
                "file_size_mb": round(file_size_mb, 2),
                "width": width,
                "height": height,
                "mode": mode,
                "format": format_name,
                # å‰ç«¯å¯ç”¨çš„æ‘˜è¦æ•°æ®
                "data": {
                    "summary": {
                        "width": width,
                        "height": height,
                        "mode": mode,
                        "format": format_name,
                        "file_size_mb": round(file_size_mb, 2)
                    }
                }
            }
            
            logger.info(f"âœ… [Image Inspector] Success: {width}x{height}, mode={mode}, format={format_name}")
            return result
            
        except ImportError:
            logger.error("âŒ [Image Inspector] PIL not available")
            return {
                "status": "error",
                "error": "PIL (Pillow) not installed",
                "file_type": "image"
            }
        except Exception as e:
            logger.error(f"âŒ [Image Inspector] Failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "file_type": "image"
            }
    
    # ================= ä¿ç•™æ—§æ–¹æ³•ä»¥ä¿æŒå…¼å®¹æ€§ =================
    
    def _is_gzipped(self, filepath: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸º gzip å‹ç¼©"""
        if filepath.is_dir():
            return False
        try:
            with open(filepath, 'rb') as f:
                return f.read(2) == b'\x1f\x8b'
        except:
            return False
    
    def _read_head(self, filepath: Path, lines: int = 5) -> list:
        """å®‰å…¨è¯»å–æ–‡ä»¶å‰å‡ è¡Œ"""
        if filepath.is_dir():
            return []
        try:
            if self._is_gzipped(filepath):
                with gzip.open(filepath, 'rt') as f:
                    return [next(f).strip() for _ in range(lines) if True]
            else:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    return [next(f).strip() for _ in range(lines) if True]
        except Exception:
            return []
    
    def generate_metadata(self, filename: str) -> Optional[Dict]:
        """
        ä¸»åŠ¨æ£€æµ‹ï¼šåˆ†æå•ä¸ªæ–‡ä»¶å¹¶ä¿å­˜ .meta.jsonï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        
        Args:
            filename: æ–‡ä»¶åæˆ–ç›®å½•å
        
        Returns:
            å…ƒæ•°æ®å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        filepath = self.upload_dir / filename
        if not filepath.exists():
            return None
        
        # ä½¿ç”¨æ–°çš„ inspect_file æ–¹æ³•
        result = self.inspect_file(str(filepath))
        
        if result.get("status") == "success":
            # ä¿å­˜å…ƒæ•°æ®
            meta_path = filepath.with_suffix(filepath.suffix + '.meta.json')
            try:
                with open(meta_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return result
    
    def get_metadata(self, filename: str) -> Optional[Dict]:
        """
        è·å–æ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆå¦‚æœå·²ç”Ÿæˆï¼‰
        
        Args:
            filename: æ–‡ä»¶å
        
        Returns:
            å…ƒæ•°æ®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        filepath = self.upload_dir / filename
        meta_path = filepath.with_suffix(filepath.suffix + '.meta.json')
        
        if meta_path.exists():
            try:
                with open(meta_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return None
        return None
