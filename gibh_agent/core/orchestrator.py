"""
Agent ç¼–æ’å™¨ - å®æ—¶æµå¼å¤„ç†

æä¾›ç»Ÿä¸€çš„æµå¼å¤„ç†æ¥å£ï¼Œå®æ—¶è¾“å‡ºçŠ¶æ€æ›´æ–°ã€æ€è€ƒè¿‡ç¨‹å’Œç»“æœã€‚

ğŸ”¥ AGENTIC UPGRADE:
é›†æˆ QueryRewriterã€Clarifier å’Œ Reflector å®ç°æ™ºèƒ½æŸ¥è¯¢å¤„ç†ã€‚
"""
import json
import logging
import asyncio
import os
from typing import Dict, Any, List, Optional, AsyncIterator
from pathlib import Path

from .agentic import QueryRewriter, Clarifier, Reflector
from .file_inspector import FileInspector
from .llm_client import LLMClient
from .workflows import WorkflowRegistry

logger = logging.getLogger(__name__)


class AgentOrchestrator:
    """
    Agent ç¼–æ’å™¨
    
    èŒè´£ï¼š
    1. ç»Ÿä¸€ç®¡ç† Agent çš„æµå¼å¤„ç†æµç¨‹
    2. å®æ—¶è¾“å‡ºçŠ¶æ€æ›´æ–°ã€æ€è€ƒè¿‡ç¨‹å’Œç»“æœ
    3. æä¾›æ¸…æ™°çš„æ‰§è¡Œæ­¥éª¤å¯è§æ€§
    
    ğŸ”¥ AGENTIC UPGRADE:
    - QueryRewriter: æŸ¥è¯¢é‡å†™ï¼ˆæ¨¡ç³Š -> ç²¾ç¡®ï¼‰
    - Clarifier: ä¸»åŠ¨æ¾„æ¸…ï¼ˆè¯¢é—®ç¼ºå¤±ä¿¡æ¯ï¼‰
    - Reflector: è‡ªæˆ‘åæ€ï¼ˆæ£€æŸ¥å’Œçº æ­£è®¡åˆ’ï¼‰
    """
    
    def __init__(self, agent, upload_dir: str = "/app/uploads"):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨
        
        Args:
            agent: Agent å®ä¾‹ï¼ˆGIBHAgentï¼‰
            upload_dir: ä¸Šä¼ æ–‡ä»¶ç›®å½•
        """
        self.agent = agent
        self.upload_dir = Path(upload_dir)
        
        # ğŸ”¥ åˆå§‹åŒ– Agentic ç»„ä»¶
        # è·å– LLM å®¢æˆ·ç«¯ï¼ˆä» agent ä¸­è·å–ï¼‰
        llm_client = self._get_llm_client()
        if llm_client:
            self.query_rewriter = QueryRewriter(llm_client)
            self.clarifier = Clarifier(llm_client)
            self.reflector = Reflector(llm_client)
        else:
            logger.warning("âš ï¸ LLM å®¢æˆ·ç«¯æœªæ‰¾åˆ°ï¼ŒAgentic ç»„ä»¶å°†ä¸å¯ç”¨")
            self.query_rewriter = None
            self.clarifier = None
            self.reflector = None
        
        # åˆå§‹åŒ–æ–‡ä»¶æ£€æŸ¥å™¨
        self.file_inspector = FileInspector(str(self.upload_dir))
        
        # ğŸ”¥ ARCHITECTURAL MERGE: ç»‘å®šåˆ° WorkflowRegistry
        self.workflow_registry = WorkflowRegistry()
        
        # ğŸ”¥ å¯¹è¯çŠ¶æ€ç®¡ç†
        self.conversation_state: Dict[str, Any] = {}
    
    def _get_llm_client(self) -> Optional[LLMClient]:
        """ä» agent ä¸­è·å– LLM å®¢æˆ·ç«¯"""
        try:
            if hasattr(self.agent, 'agents') and self.agent.agents:
                # å°è¯•ä»ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“è·å– LLM client
                first_agent = list(self.agent.agents.values())[0]
                if hasattr(first_agent, 'llm_client'):
                    return first_agent.llm_client
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ è·å– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    async def stream_process(
        self,
        query: str,
        files: List[Dict[str, str]] = None,
        history: List[Dict[str, str]] = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """
        æµå¼å¤„ç†æŸ¥è¯¢
        
        å®æ—¶è¾“å‡ºï¼š
        1. çŠ¶æ€æ›´æ–°ï¼ˆstatusï¼‰
        2. æ€è€ƒè¿‡ç¨‹ï¼ˆthoughtï¼‰
        3. è¯Šæ–­æ•°æ®ï¼ˆdiagnosisï¼‰
        4. å·¥ä½œæµè®¡åˆ’ï¼ˆworkflowï¼‰
        5. æœ€ç»ˆå“åº”ï¼ˆmessageï¼‰
        6. å®Œæˆä¿¡å·ï¼ˆdoneï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            history: å¯¹è¯å†å²
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            SSE æ ¼å¼çš„äº‹ä»¶å­—ç¬¦ä¸²: "data: {json}\n\n"
        """
        files = files or []
        history = history or []
        
        # ğŸ”¥ CRITICAL REGRESSION FIX: Direct Execution Path
        # If the request contains a confirmed workflow_data, EXECUTE it immediately.
        # Do NOT re-plan. Do NOT re-inspect.
        workflow_data = kwargs.get("workflow_data")
        if workflow_data:
            logger.info("ğŸš€ [Orchestrator] æ¥æ”¶åˆ°æ‰§è¡ŒæŒ‡ä»¤ï¼Œè¿›å…¥ç›´æ¥æ‰§è¡Œæ¨¡å¼")
            logger.info(f"ğŸš€ [Orchestrator] workflow_data ç±»å‹: {type(workflow_data)}")
            
            try:
                # Parse workflow_data if it's a string
                if isinstance(workflow_data, str):
                    import json
                    workflow_data = json.loads(workflow_data)
                
                # Extract workflow config
                workflow_config = workflow_data.get("workflow_data") or workflow_data
                steps = workflow_config.get("steps", [])
                
                if not steps or len(steps) == 0:
                    logger.error("âŒ [Orchestrator] workflow_data ä¸­æ²¡æœ‰æ­¥éª¤")
                    yield self._format_sse("error", {
                        "error": "å·¥ä½œæµé…ç½®æ— æ•ˆ",
                        "message": "å·¥ä½œæµæ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°æ­¥éª¤"
                    })
                    return
                
                logger.info(f"âœ… [Orchestrator] å‡†å¤‡æ‰§è¡Œ {len(steps)} ä¸ªæ­¥éª¤")
                
                # 1. Initialize Execution Engine
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨åˆå§‹åŒ–æ‰§è¡Œå¼•æ“...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                from .executor import WorkflowExecutor
                # ğŸ”¥ CRITICAL REGRESSION FIX: Pass upload_dir to executor for path resolution
                upload_dir = getattr(self, 'upload_dir', Path(os.getenv("UPLOAD_DIR", "/app/uploads")))
                upload_dir_str = str(upload_dir) if isinstance(upload_dir, Path) else upload_dir
                executor = WorkflowExecutor(upload_dir=upload_dir_str)
                
                # 2. Extract file paths
                file_paths = workflow_data.get("file_paths", [])
                if not file_paths and files:
                    # Extract from files parameter
                    file_paths = [f.get("path") or f.get("file_path") or f.get("name") for f in files if f]
                    file_paths = [p for p in file_paths if p]
                
                logger.info(f"ğŸ“ [Orchestrator] æ–‡ä»¶è·¯å¾„: {file_paths}")
                
                # 3. Execute Steps
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨æ‰§è¡Œåˆ†æå·¥å…·...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                # Execute workflow
                results = executor.execute_workflow(
                    workflow_data=workflow_config,
                    file_paths=file_paths,
                    agent=self.agent
                )
                
                logger.info(f"âœ… [Orchestrator] å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œç»“æœ: {type(results)}")
                
                # ğŸ”¥ CRITICAL REGRESSION FIX: Check for async_job_started status
                steps_details = results.get("steps_details", [])
                has_async_job = False
                async_step_detail = None
                
                for step_detail in steps_details:
                    if step_detail.get("status") == "async_job_started":
                        has_async_job = True
                        async_step_detail = step_detail
                        logger.info(f"ğŸš€ [Orchestrator] æ£€æµ‹åˆ°å¼‚æ­¥ä½œä¸š: {step_detail.get('step_id')}, job_id: {step_detail.get('job_id')}")
                        break
                
                # ğŸ”¥ CRITICAL: If async job started, yield status and STOP (do not continue)
                if has_async_job:
                    logger.info("ğŸš€ [Orchestrator] å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨ï¼Œåœæ­¢æ‰§è¡Œæµç¨‹")
                    yield self._format_sse("status", {
                        "content": f"å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨: {async_step_detail.get('step_id', 'Unknown')}",
                        "state": "async_job_started"
                    })
                    await asyncio.sleep(0.01)
                    
                    # Yield async job status
                    async_response = {
                        "async_job": {
                            "step_id": async_step_detail.get("step_id"),
                            "job_id": async_step_detail.get("job_id"),
                            "status": "async_job_started",
                            "message": async_step_detail.get("summary", "å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨ï¼Œç­‰å¾…å®Œæˆ")
                        },
                        "steps_details": steps_details
                    }
                    
                    yield self._format_sse("result", async_response)
                    yield self._format_sse("status", {
                        "content": "ç­‰å¾…å¼‚æ­¥ä½œä¸šå®Œæˆ...",
                        "state": "waiting"
                    })
                    await asyncio.sleep(0.01)
                    yield self._format_sse("done", {"status": "async_job_started"})
                    return  # STOP HERE - Do not continue to next steps
                
                # 4. Generate Summary (if agent available and no async job)
                if self.agent and hasattr(self.agent, '_generate_analysis_summary'):
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨ç”Ÿæˆä¸“å®¶è§£è¯»...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    # Detect domain from workflow name or steps
                    domain_name = "Metabolomics"  # Default
                    workflow_name = workflow_config.get("workflow_name", "")
                    if "RNA" in workflow_name or "rna" in workflow_name.lower():
                        domain_name = "RNA"
                    
                    try:
                        summary = await self.agent._generate_analysis_summary(results, domain_name)
                    except Exception as e:
                        logger.warning(f"âš ï¸ [Orchestrator] ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
                        summary = "åˆ†æå®Œæˆ"
                else:
                    summary = "åˆ†æå®Œæˆ"
                
                # 5. Yield Final Result
                final_response = {
                    "report_data": {
                        "steps_details": steps_details,
                        "diagnosis": summary,
                        "workflow_name": workflow_config.get("workflow_name", "å·¥ä½œæµ")
                    }
                }
                
                yield self._format_sse("result", final_response)
                yield self._format_sse("status", {
                    "content": "æ‰§è¡Œå®Œæˆ",
                    "state": "completed"
                })
                await asyncio.sleep(0.01)
                yield self._format_sse("done", {"status": "success"})
                return  # STOP HERE - Do not continue to planning
                
            except Exception as e:
                logger.error(f"âŒ [Orchestrator] ç›´æ¥æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                yield self._format_sse("error", {
                    "error": str(e),
                    "message": f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
                })
                return
        
        # ğŸ”¥ CRITICAL DEBUG: è®°å½•æ¥æ”¶åˆ°çš„åŸå§‹æ–‡ä»¶æ•°æ®
        logger.info(f"ğŸ”¥ DEBUG: Orchestrator received files raw: {files}, type: {type(files)}")
        logger.info(f"ğŸ”¥ DEBUG: files length: {len(files) if files else 0}")
        if files:
            for i, f in enumerate(files):
                logger.info(f"ğŸ”¥ DEBUG: files[{i}]: {f}, type: {type(f)}")
        
        # ğŸ”¥ BUG FIX: ä» kwargs ä¸­æå– session_id å’Œ user_id
        session_id = kwargs.get("session_id") or "default"
        user_id = kwargs.get("user_id") or "guest"
        
        # ğŸ”¥ ARCHITECTURAL MERGE: æ£€æŸ¥å¯¹è¯çŠ¶æ€ï¼ˆå¤„ç†æ¾„æ¸…å›å¤å’Œæ‰§è¡Œæ„å›¾ï¼‰
        session_state = self.conversation_state.get(session_id, {})
        awaiting_clarification = session_state.get("awaiting_clarification", False)
        previous_query = session_state.get("previous_query")
        previous_refined_query = session_state.get("previous_refined_query")
        pending_plan = session_state.get("pending_plan")  # ğŸ”¥ URGENT FIX: æ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„å·¥ä½œæµè®¡åˆ’
        pending_modality = session_state.get("pending_modality")  # ğŸ”¥ CRITICAL: æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„æ¨¡æ€ï¼ˆæ¢å¤æ¡ä»¶ï¼‰
        
        # ğŸ”¥ URGENT FIX: æ£€æŸ¥æ‰§è¡Œæ„å›¾ï¼ˆ"Proceed", "ç»§ç»­", "æ‰§è¡Œ"ç­‰ï¼‰
        execution_keywords = ["proceed", "ç»§ç»­", "æ‰§è¡Œ", "go ahead", "run it", "å¼€å§‹", "execute"]
        query_lower = query.lower().strip()
        is_execution_intent = any(kw in query_lower for kw in execution_keywords)
        
        # å¦‚æœæœ‰å¾…æ‰§è¡Œè®¡åˆ’ä¸”ç”¨æˆ·ç¡®è®¤æ‰§è¡Œï¼Œç›´æ¥æ‰§è¡Œå·¥ä½œæµ
        if pending_plan and is_execution_intent:
            logger.info(f"âœ… [Orchestrator] æ£€æµ‹åˆ°æ‰§è¡Œæ„å›¾ï¼Œç›´æ¥æ‰§è¡Œå¾…æ‰§è¡Œçš„å·¥ä½œæµè®¡åˆ’")
            # æ¸…é™¤å¾…æ‰§è¡Œè®¡åˆ’çŠ¶æ€
            self.conversation_state[session_id] = {}
            # ç›´æ¥è°ƒç”¨æ‰§è¡Œé€»è¾‘ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ‰§è¡Œæ¥å£è°ƒæ•´ï¼‰
            # æš‚æ—¶è·³è¿‡ FileInspector å’Œ Diagnosisï¼Œç›´æ¥è¿›å…¥æ‰§è¡Œ
            yield self._format_sse("status", {
                "content": "æ­£åœ¨æ‰§è¡Œå·¥ä½œæµ...",
                "state": "running"
            })
            await asyncio.sleep(0.01)
            # è¿™é‡Œåº”è¯¥è°ƒç”¨æ‰§è¡Œå™¨ï¼Œä½†ä¸ºäº†ä¸ç ´åç°æœ‰æµç¨‹ï¼Œæˆ‘ä»¬ç»§ç»­æ­£å¸¸æµç¨‹
            # å®é™…æ‰§è¡Œä¼šåœ¨å‰ç«¯ç‚¹å‡»"æ‰§è¡Œ"æŒ‰é’®æ—¶è§¦å‘
        
        try:
            # Step 1: ç«‹å³è¾“å‡ºå¼€å§‹çŠ¶æ€
            yield self._format_sse("status", {
                "content": "æ­£åœ¨æ¥æ”¶è¯·æ±‚...",
                "state": "start"
            })
            await asyncio.sleep(0.01)  # å¼ºåˆ¶ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œç¡®ä¿ç«‹å³å‘é€
            
            # ğŸ”¥ Step 2: Query Rewritingï¼ˆæŸ¥è¯¢é‡å†™ï¼‰
            # å¦‚æœæ˜¯æ¾„æ¸…å›å¤ï¼Œåˆå¹¶æŸ¥è¯¢å¹¶è·³è¿‡é‡å†™
            if awaiting_clarification and previous_query:
                refined_query = f"{previous_refined_query or previous_query}ã€‚ç”¨æˆ·å›å¤ï¼š{query}"
                logger.info(f"âœ… [Orchestrator] å¤„ç†æ¾„æ¸…å›å¤: '{query}' -> åˆå¹¶æŸ¥è¯¢")
                # æ¸…é™¤æ¾„æ¸…çŠ¶æ€
                self.conversation_state[session_id] = {}
            else:
                refined_query = query
                if self.query_rewriter:
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨ä¼˜åŒ–æŸ¥è¯¢è¯­å¥...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    refined_query = await self.query_rewriter.rewrite(query, history)
                    logger.info(f"âœ… [Orchestrator] æŸ¥è¯¢é‡å†™: '{query}' -> '{refined_query}'")
                else:
                    logger.info("â„¹ï¸ [Orchestrator] QueryRewriter ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
            
            # ğŸ”¥ CRITICAL FIX: Step 3.0: Normalize Files (BEFORE Resume Check)
            # Normalize files to a list of valid file dictionaries
            valid_files = []
            if files:
                logger.info(f"ğŸ” [Orchestrator] å¼€å§‹è§„èŒƒåŒ–æ–‡ä»¶ï¼ŒåŸå§‹ files ç±»å‹: {type(files)}, é•¿åº¦: {len(files)}")
                for i, f in enumerate(files):
                    logger.info(f"ğŸ” [Orchestrator] å¤„ç†æ–‡ä»¶ [{i}]: {f}, ç±»å‹: {type(f)}")
                    file_dict = None
                    if isinstance(f, dict):
                        # Already a dictionary
                        path = f.get("path") or f.get("file_path")
                        name = f.get("name") or f.get("file_name") or ""
                        logger.info(f"ğŸ” [Orchestrator] å­—å…¸æ ¼å¼ - path: {path}, name: {name}")
                        if path:
                            # ğŸ”¥ CRITICAL: éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•åœ¨ upload_dir ä¸­æŸ¥æ‰¾ï¼‰
                            path_obj = Path(path)
                            if not path_obj.is_absolute():
                                path_obj = Path(self.upload_dir) / path_obj
                            elif not path_obj.exists():
                                # ç»å¯¹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨ upload_dir ä¸­æŸ¥æ‰¾æ–‡ä»¶å
                                filename = path_obj.name
                                potential_path = Path(self.upload_dir) / filename
                                if potential_path.exists():
                                    path_obj = potential_path
                                    logger.info(f"âœ… [Orchestrator] åœ¨ upload_dir ä¸­æ‰¾åˆ°æ–‡ä»¶: {path_obj}")
                            
                            file_dict = {
                                "name": name or path_obj.name,
                                "path": str(path_obj)
                            }
                            logger.info(f"âœ… [Orchestrator] è§„èŒƒåŒ–æ–‡ä»¶: {file_dict}")
                    elif isinstance(f, str):
                        # String path
                        path_obj = Path(f)
                        if not path_obj.is_absolute():
                            path_obj = Path(self.upload_dir) / path_obj
                        file_dict = {
                            "name": path_obj.name,
                            "path": str(path_obj)
                        }
                        logger.info(f"âœ… [Orchestrator] å­—ç¬¦ä¸²è·¯å¾„è§„èŒƒåŒ–: {file_dict}")
                    elif hasattr(f, "path"):
                        # Pydantic model or object with path attribute
                        path = f.path if hasattr(f, "path") else str(f)
                        name = getattr(f, "name", "") or getattr(f, "file_name", "") or os.path.basename(path)
                        path_obj = Path(path)
                        if not path_obj.is_absolute():
                            path_obj = Path(self.upload_dir) / path_obj
                        file_dict = {
                            "name": name,
                            "path": str(path_obj)
                        }
                        logger.info(f"âœ… [Orchestrator] å¯¹è±¡æ ¼å¼è§„èŒƒåŒ–: {file_dict}")
                    
                    if file_dict:
                        valid_files.append(file_dict)
            
            logger.info(f"âœ… [Orchestrator] è§„èŒƒåŒ–åçš„æ–‡ä»¶åˆ—è¡¨: {valid_files}, æ•°é‡: {len(valid_files)}")
            
            # ğŸ”¥ CRITICAL: Use normalized files for the rest of the logic
            files = valid_files
            
            # ğŸ”¥ URGENT FIX: Step 3.0: Resume Priority Check (BEFORE Intent Analysis)
            # Check if this is a RESUME action: file uploaded + pending modality exists
            has_files = len(valid_files) > 0
            logger.info(f"ğŸ” [Orchestrator] æ–‡ä»¶æ£€æµ‹ç»“æœ: has_files={has_files}, valid_filesæ•°é‡={len(valid_files)}")
            is_resume_action = has_files and pending_modality is not None
            
            # Initialize planner variable (will be set in either branch)
            planner = None
            
            if is_resume_action:
                logger.info(f"ğŸš€ [Orchestrator] æ£€æµ‹åˆ°æ¢å¤æ“ä½œ: æ–‡ä»¶å·²ä¸Šä¼  + å¾…å¤„ç†æ¨¡æ€={pending_modality}")
                logger.info(f"ğŸš€ [Orchestrator] å¼ºåˆ¶è¿›å…¥æ‰§è¡Œæ¨¡å¼ï¼Œè·³è¿‡æ„å›¾åˆ†æ")
                
                # Force the domain to match the pending plan (ignore query re-analysis)
                domain_name = pending_modality
                target_steps = []  # Use full SOP for resume
                
                # Clear pending state (we're resuming now)
                session_state.pop("pending_modality", None)
                self.conversation_state[session_id] = session_state
                
                # Get workflow instance
                workflow = self.workflow_registry.get_workflow(domain_name)
                if not workflow:
                    raise ValueError(f"æ— æ³•è·å–å·¥ä½œæµ: {domain_name}")
                
                # Use full workflow for resume
                target_steps = list(workflow.steps_dag.keys())
                
                logger.info(f"âœ… [Orchestrator] æ¢å¤æ¨¡å¼: domain={domain_name}, target_steps={target_steps}")
                # Skip to file inspection (Branch B) - don't analyze intent again
            else:
                # ğŸ”¥ CRITICAL REFACTOR: Step 3 - ALWAYS Analyze Intent First (Dynamic Scoping)
                # Step 3.1: Analyze Intent (ALWAYS FIRST) - Determine modality and target_steps
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                # Initialize planner for intent analysis
                from .planner import SOPPlanner
                from .tool_retriever import ToolRetriever
                
                llm_client = self._get_llm_client()
                if not llm_client:
                    raise ValueError("LLM å®¢æˆ·ç«¯ä¸å¯ç”¨")
                
                tool_retriever = ToolRetriever()
                planner = SOPPlanner(tool_retriever, llm_client)
                
                # Analyze intent: classify domain and determine target_steps
                intent_result = await planner._classify_intent(refined_query, None)
                domain_name = intent_result.get("domain_name")
                
                # Validate domain
                if not domain_name or not self.workflow_registry.is_supported(domain_name):
                    logger.warning(f"âš ï¸ [Orchestrator] æ— æ³•è¯†åˆ«åŸŸå: {domain_name}")
                    domain_name = "Metabolomics"  # é»˜è®¤å€¼
                
                # Get workflow instance for intent analysis
                workflow = self.workflow_registry.get_workflow(domain_name)
                if not workflow:
                    raise ValueError(f"æ— æ³•è·å–å·¥ä½œæµ: {domain_name}")
                
                # Analyze user intent to determine target_steps
                target_steps = await planner._analyze_user_intent(refined_query, workflow)
                
                # Ensure target_steps is not empty (fallback to full workflow)
                if not target_steps:
                    query_lower = refined_query.lower()
                    vague_keywords = ["analyze this", "full analysis", "å®Œæ•´åˆ†æ", "å…¨éƒ¨", "all", "complete"]
                    if any(kw in query_lower for kw in vague_keywords):
                        target_steps = list(workflow.steps_dag.keys())
                    else:
                        # Fallback to keyword matching
                        from .planner import SOPPlanner
                        target_steps = planner._fallback_intent_analysis(refined_query, list(workflow.steps_dag.keys()))
                        if not target_steps:
                            target_steps = list(workflow.steps_dag.keys())
                
                logger.info(f"âœ… [Orchestrator] æ„å›¾åˆ†æå®Œæˆ: domain={domain_name}, target_steps={target_steps}")
            
            # ğŸ”¥ SYSTEM REFACTOR: Step 3.2: Check Files (The Branching Point)
            # Priority: Files check determines execution mode
            # Note: has_files already checked above for resume action
            
            # ğŸ”¥ CRITICAL: è¯¦ç»†æ—¥å¿—ï¼Œç¡®ä¿æ–‡ä»¶æ£€æŸ¥æ­£ç¡®
            logger.info(f"ğŸ” [Orchestrator] æ–‡ä»¶æ£€æŸ¥: files={files}, has_files={has_files}")
            if files:
                logger.info(f"ğŸ” [Orchestrator] files ç±»å‹: {type(files)}, é•¿åº¦: {len(files) if hasattr(files, '__len__') else 'N/A'}")
                for i, f in enumerate(files):
                    logger.info(f"  [{i}] {f}")
            else:
                logger.warning("âš ï¸ [Orchestrator] files ä¸ºç©ºæˆ– None")
            
            yield self._format_sse("status", {
                "content": "æ­£åœ¨æ£€æµ‹æ–‡ä»¶è¾“å…¥...",
                "state": "running"
            })
            await asyncio.sleep(0.01)
            
            # ============================================================
            # BRANCH A: Plan-First Mode (No Files)
            # ============================================================
            if not has_files:
                logger.info("âš ï¸ [Orchestrator] åˆ†æ”¯ A: Plan-First æ¨¡å¼ï¼ˆæ— æ–‡ä»¶ï¼‰")
                logger.info("âš ï¸ [Orchestrator] è¿›å…¥é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¼šç”Ÿæˆè¯Šæ–­æŠ¥å‘Š")
                
                yield self._format_sse("status", {
                    "content": "æœªæ£€æµ‹åˆ°æ–‡ä»¶ï¼Œè¿›å…¥æ–¹æ¡ˆé¢„è§ˆæ¨¡å¼...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                try:
                    # Step A1: Generate Template Workflow with target_steps
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨æ ¹æ®æ‚¨çš„éœ€æ±‚å®šåˆ¶æµç¨‹...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    # ğŸ”¥ CRITICAL: Use the same target_steps analyzed above
                    # Path B: PREVIEW MODE - Explicitly tell planner this IS a template
                    template_result = await planner.generate_plan(
                        user_query=refined_query,
                        file_metadata=None,  # æ˜ç¡®ä¼ é€’ None
                        category_filter=None,
                        domain_name=domain_name,  # ä½¿ç”¨å·²åˆ†æçš„åŸŸå
                        target_steps=target_steps,  # ğŸ”¥ CRITICAL: ä½¿ç”¨å·²åˆ†æçš„ç›®æ ‡æ­¥éª¤
                        is_template=True  # ğŸ”¥ CRITICAL: Explicitly IS a template
                    )
                    
                    logger.info(f"âœ… [Orchestrator] æ¨¡æ¿ç”Ÿæˆå®Œæˆ: {len(target_steps)} ä¸ªç›®æ ‡æ­¥éª¤")
                    
                    # ğŸ”¥ CRITICAL: Save pending_modality for resume detection
                    session_state["pending_modality"] = domain_name
                    self.conversation_state[session_id] = session_state
                    logger.info(f"ğŸ’¾ [Orchestrator] å·²ä¿å­˜å¾…å¤„ç†æ¨¡æ€: {domain_name} (session_id={session_id})")
                    
                    # Step A2: Yield Template Card
                    workflow_data = template_result.get("workflow_data") or template_result
                    if workflow_data:
                        steps_count = len(workflow_data.get("steps", []))
                        workflow_event_data = {
                            "workflow_config": workflow_data,
                            "workflow_data": workflow_data,
                            "template_mode": True  # ğŸ”¥ CRITICAL: æ˜ç¡®æ ‡è®°ä¸ºæ¨¡æ¿æ¨¡å¼
                        }
                        yield self._format_sse("workflow", workflow_event_data)
                        await asyncio.sleep(0.01)
                    
                    # ğŸ”¥ CRITICAL: Generate message with modality and step count
                    modality_display = "ä»£è°¢ç»„å­¦" if domain_name == "Metabolomics" else "è½¬å½•ç»„"
                    yield self._format_sse("message", {
                        "content": f"å·²ä¸ºæ‚¨è§„åˆ’ **{modality_display}** åˆ†ææµç¨‹ï¼ˆåŒ…å« {steps_count} ä¸ªæ­¥éª¤ï¼‰ã€‚è¯·ä¸Šä¼ æ•°æ®ä»¥æ¿€æ´»ã€‚"
                    })
                    await asyncio.sleep(0.01)
                    
                    # è¾“å‡ºç»“æœäº‹ä»¶
                    yield self._format_sse("result", {
                        "workflow_config": workflow_data,
                        "template_mode": True
                    })
                    
                    # ğŸ”¥ CRITICAL: STOP HERE - ä¸ç»§ç»­æ‰§è¡Œ
                    yield self._format_sse("status", {
                        "content": "æ–¹æ¡ˆæ¨¡ç‰ˆå·²ç”Ÿæˆï¼Œç­‰å¾…ä¸Šä¼ ...",
                        "state": "completed"
                    })
                    await asyncio.sleep(0.01)
                    
                    yield self._format_sse("done", {"status": "success"})
                    return  # ç«‹å³è¿”å›ï¼Œä¸ç»§ç»­æ‰§è¡Œ
                    
                except Exception as e:
                    logger.error(f"âŒ [Orchestrator] Plan-First æ¨¡å¼å¤±è´¥: {e}", exc_info=True)
                    yield self._format_sse("error", {
                        "error": str(e),
                        "message": f"æ¨¡æ¿ç”Ÿæˆå¤±è´¥: {str(e)}"
                    })
                    return
            
            # ============================================================
            # PATH A: CLASSIC EXECUTION (The "Old Way") - Files Detected
            # ============================================================
            else:
                logger.info("ğŸš€ [Orchestrator] Path A: æ–‡ä»¶æ£€æµ‹åˆ°ã€‚å¼ºåˆ¶æ‰§è¡Œæ¨¡å¼ï¼ˆç»å…¸æ‰§è¡Œè·¯å¾„ï¼‰")
                
                # A1. File Inspection - Extract file path and inspect
                first_file = files[0]
                if isinstance(first_file, dict):
                    file_path = first_file.get("path") or first_file.get("file_path") or first_file.get("name")
                elif isinstance(first_file, str):
                    file_path = first_file
                else:
                    file_path = str(first_file)
                
                logger.info(f"ğŸ” [Orchestrator] Path A: æå–æ–‡ä»¶è·¯å¾„: {file_path}")
                
                if not file_path:
                    logger.error("âŒ [Orchestrator] Path A: æ— æ³•æå–æ–‡ä»¶è·¯å¾„")
                    yield self._format_sse("error", {
                        "error": "æ–‡ä»¶è·¯å¾„æ— æ•ˆ",
                        "message": "æ— æ³•ä»æ–‡ä»¶å¯¹è±¡ä¸­æå–è·¯å¾„"
                    })
                    return
                
                yield self._format_sse("status", {
                    "content": f"æ£€æµ‹åˆ°æ–‡ä»¶ï¼Œæ­£åœ¨è¿›è¡Œæ•°æ®ä½“æ£€...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                # Inspect file
                file_metadata = None
                try:
                    file_metadata = self.file_inspector.inspect_file(file_path)
                    logger.info(f"âœ… [Orchestrator] Path A: æ–‡ä»¶æ£€æŸ¥å®Œæˆ: {file_path}")
                    
                    if file_metadata and file_metadata.get("status") == "success":
                        # Extract statistics
                        n_samples = file_metadata.get("n_samples") or file_metadata.get("n_obs") or file_metadata.get("shape", {}).get("rows", 0)
                        n_features = file_metadata.get("n_features") or file_metadata.get("n_vars") or file_metadata.get("shape", {}).get("cols", 0)
                        
                        # Build diagnosis message
                        if domain_name == "Metabolomics":
                            diagnosis_message = f"""### ğŸ“Š æ•°æ®ä½“æ£€æŠ¥å‘Š

**æ•°æ®è§„æ¨¡**:
- **æ ·æœ¬æ•°**: {n_samples} ä¸ª
- **ä»£è°¢ç‰©æ•°**: {n_features} ä¸ª

**æ•°æ®ç‰¹å¾**:
- æ–‡ä»¶ç±»å‹: {file_metadata.get('file_type', 'æœªçŸ¥')}
- æ–‡ä»¶å¤§å°: {file_metadata.get('file_size_mb', 'N/A')} MB

**æ•°æ®è´¨é‡**:
- ç¼ºå¤±å€¼ç‡: {file_metadata.get('missing_rate', 'N/A')}%
- æ•°æ®èŒƒå›´: {file_metadata.get('data_range', {}).get('min', 'N/A')} ~ {file_metadata.get('data_range', {}).get('max', 'N/A')}

**ä¸‹ä¸€æ­¥**: å·²ä¸ºæ‚¨è§„åˆ’åˆ†ææµç¨‹ï¼Œè¯·ç¡®è®¤æ‰§è¡Œã€‚"""
                        else:  # RNA
                            diagnosis_message = f"""### ğŸ“Š æ•°æ®ä½“æ£€æŠ¥å‘Š

**æ•°æ®è§„æ¨¡**:
- **ç»†èƒæ•°**: {n_samples} ä¸ª
- **åŸºå› æ•°**: {n_features} ä¸ª

**æ•°æ®ç‰¹å¾**:
- æ–‡ä»¶ç±»å‹: {file_metadata.get('file_type', 'æœªçŸ¥')}
- ç¨€ç–åº¦: {file_metadata.get('sparsity', 'N/A')}

**æ•°æ®è´¨é‡**: æ•°æ®å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹åˆ†æã€‚

**ä¸‹ä¸€æ­¥**: å·²ä¸ºæ‚¨è§„åˆ’åˆ†ææµç¨‹ï¼Œè¯·ç¡®è®¤æ‰§è¡Œã€‚"""
                        
                        yield self._format_sse("diagnosis", {
                            "message": diagnosis_message,
                            "n_samples": n_samples,
                            "n_features": n_features,
                            "file_type": file_metadata.get('file_type'),
                            "status": "data_ready"
                        })
                        await asyncio.sleep(0.01)
                except Exception as e:
                    logger.error(f"âŒ [Orchestrator] Path A: æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
                    yield self._format_sse("error", {
                        "error": str(e),
                        "message": f"æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {str(e)}"
                    })
                    return
                
                # A2. Plan (With Metadata) - CRITICAL: Explicitly tell planner this is NOT a template
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨æ ¹æ®æ‚¨çš„éœ€æ±‚å®šåˆ¶æµç¨‹...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                if planner is None:
                    from .planner import SOPPlanner
                    from .tool_retriever import ToolRetriever
                    llm_client = self._get_llm_client()
                    if not llm_client:
                        raise ValueError("LLM å®¢æˆ·ç«¯ä¸å¯ç”¨")
                    tool_retriever = ToolRetriever()
                    planner = SOPPlanner(tool_retriever, llm_client)
                
                logger.info(f"ğŸ” [Orchestrator] Path A: è°ƒç”¨ planner.generate_plan (is_template=False)")
                logger.info(f"  - file_metadata å­˜åœ¨: {file_metadata is not None}")
                logger.info(f"  - domain_name: {domain_name}")
                logger.info(f"  - target_steps: {target_steps}")
                if file_metadata:
                    logger.info(f"  - file_metadata.file_path: {file_metadata.get('file_path', 'N/A')}")
                
                result = await planner.generate_plan(
                    user_query=refined_query,
                    file_metadata=file_metadata,  # ğŸ”¥ CRITICAL: file_metadata exists
                    category_filter=None,
                    domain_name=domain_name,
                    target_steps=target_steps,
                    is_template=False  # ğŸ”¥ CRITICAL: Explicitly NOT a template
                )
                
                logger.info(f"âœ… [Orchestrator] Path A: å·¥ä½œæµè§„åˆ’å®Œæˆ")
                logger.info(f"âœ… [Orchestrator] Path A: è¿”å›ç»“æœ template_mode: {result.get('template_mode', 'N/A')}")
                
                # A3. Force Validation
                if isinstance(result, dict):
                    # FORCE OVERRIDE: Explicitly set template_mode = False
                    if result.get("template_mode"):
                        logger.error("âŒ [Orchestrator] Path A: é€»è¾‘é”™è¯¯ - Planner è¿”å› template_mode=True  despite file presence. å¼ºåˆ¶è¦†ç›–ã€‚")
                    result["template_mode"] = False
                    if "workflow_data" in result:
                        result["workflow_data"]["template_mode"] = False
                    
                    # Validate Steps
                    workflow_data = result.get("workflow_data") or result
                    steps = workflow_data.get("steps", [])
                    if not steps or len(steps) == 0:
                        logger.warning(f"âš ï¸ [Orchestrator] Path A: Planner è¿”å›ç©ºæ­¥éª¤ï¼Œå›é€€åˆ°ç¡¬ç¼–ç  SOP")
                        # Regenerate using hardcoded SOP
                        try:
                            workflow = self.workflow_registry.get_workflow(domain_name)
                            if not workflow:
                                raise ValueError(f"æ— æ³•è·å–å·¥ä½œæµ: {domain_name}")
                            
                            if domain_name == "Metabolomics":
                                hardcoded_result = planner._generate_metabolomics_plan(file_metadata)
                            else:
                                hardcoded_result = workflow.generate_template(
                                    target_steps=target_steps or list(workflow.steps_dag.keys()),
                                    file_metadata=file_metadata
                                )
                                hardcoded_result = planner._fill_parameters(hardcoded_result, file_metadata, workflow, template_mode=False)
                            
                            result = {
                                "type": "workflow_config",
                                "workflow_data": hardcoded_result.get("workflow_data") or hardcoded_result,
                                "template_mode": False
                            }
                            logger.info(f"âœ… [Orchestrator] Path A: ä½¿ç”¨ç¡¬ç¼–ç  SOP ç”Ÿæˆå·¥ä½œæµ: {len(result.get('workflow_data', {}).get('steps', []))} ä¸ªæ­¥éª¤")
                        except Exception as e:
                            logger.error(f"âŒ [Orchestrator] Path A: ç¡¬ç¼–ç  SOP ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
                    
                    # Yield workflow event
                    workflow_data = result.get("workflow_data") or result
                    if workflow_data:
                        yield self._format_sse("workflow", {
                            "workflow_config": workflow_data,
                            "template_mode": False  # ğŸ”¥ CRITICAL: Always False in Path A
                        })
                        await asyncio.sleep(0.01)
                    
                    yield self._format_sse("result", {
                        "workflow_config": workflow_data,
                        "template_mode": False
                    })
                
                yield self._format_sse("status", {
                    "content": "å‡†å¤‡å°±ç»ªï¼Œè¯·ç¡®è®¤æ‰§è¡Œã€‚",
                    "state": "completed"
                })
                await asyncio.sleep(0.01)
                
                yield self._format_sse("done", {"status": "success"})
                return  # ğŸ”¥ CRITICAL: Stop here for Path A
            
        except Exception as e:
            logger.error(f"âŒ æµå¼å¤„ç†å¤±è´¥: {e}", exc_info=True)
            yield self._format_sse("status", {
                "content": f"å¤„ç†å¤±è´¥: {str(e)}",
                "state": "error"
            })
            yield self._format_sse("error", {
                "error": str(e),
                "message": f"å¤„ç†å¤±è´¥: {str(e)}"
            })
    
    def _format_sse(self, event_type: str, data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ– SSE äº‹ä»¶
        
        Args:
            event_type: äº‹ä»¶ç±»å‹ï¼ˆstatus, thought, message, diagnosis, workflow, done, errorï¼‰
            data: äº‹ä»¶æ•°æ®
            
        Returns:
            SSE æ ¼å¼å­—ç¬¦ä¸²: "event: {type}\ndata: {json}\n\n"
        """
        json_data = json.dumps(data, ensure_ascii=False)
        return f"event: {event_type}\ndata: {json_data}\n\n"

