"""
Agent ç¼–æ’å™¨ - å®æ—¶æµå¼å¤„ç†

æä¾›ç»Ÿä¸€çš„æµå¼å¤„ç†æ¥å£ï¼Œå®æ—¶è¾“å‡ºçŠ¶æ€æ›´æ–°ã€æ€è€ƒè¿‡ç¨‹å’Œç»“æœã€‚

ğŸ”¥ AGENTIC UPGRADE:
é›†æˆ QueryRewriterã€Clarifier å’Œ Reflector å®ç°æ™ºèƒ½æŸ¥è¯¢å¤„ç†ã€‚
"""
import json
import logging
import asyncio
import os
from typing import Dict, Any, List, Optional, AsyncIterator
from pathlib import Path

from .agentic import QueryRewriter, Clarifier, Reflector
from .file_inspector import FileInspector
from .llm_client import LLMClient
from .workflows import WorkflowRegistry

logger = logging.getLogger(__name__)


class AgentOrchestrator:
    """
    Agent ç¼–æ’å™¨
    
    èŒè´£ï¼š
    1. ç»Ÿä¸€ç®¡ç† Agent çš„æµå¼å¤„ç†æµç¨‹
    2. å®æ—¶è¾“å‡ºçŠ¶æ€æ›´æ–°ã€æ€è€ƒè¿‡ç¨‹å’Œç»“æœ
    3. æä¾›æ¸…æ™°çš„æ‰§è¡Œæ­¥éª¤å¯è§æ€§
    
    ğŸ”¥ AGENTIC UPGRADE:
    - QueryRewriter: æŸ¥è¯¢é‡å†™ï¼ˆæ¨¡ç³Š -> ç²¾ç¡®ï¼‰
    - Clarifier: ä¸»åŠ¨æ¾„æ¸…ï¼ˆè¯¢é—®ç¼ºå¤±ä¿¡æ¯ï¼‰
    - Reflector: è‡ªæˆ‘åæ€ï¼ˆæ£€æŸ¥å’Œçº æ­£è®¡åˆ’ï¼‰
    """
    
    def __init__(self, agent, upload_dir: str = "/app/uploads"):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨
        
        Args:
            agent: Agent å®ä¾‹ï¼ˆGIBHAgentï¼‰
            upload_dir: ä¸Šä¼ æ–‡ä»¶ç›®å½•
        """
        self.agent = agent
        self.upload_dir = Path(upload_dir)
        
        # ğŸ”¥ åˆå§‹åŒ– Agentic ç»„ä»¶
        # è·å– LLM å®¢æˆ·ç«¯ï¼ˆä» agent ä¸­è·å–ï¼‰
        llm_client = self._get_llm_client()
        if llm_client:
            self.query_rewriter = QueryRewriter(llm_client)
            self.clarifier = Clarifier(llm_client)
            self.reflector = Reflector(llm_client)
        else:
            logger.warning("âš ï¸ LLM å®¢æˆ·ç«¯æœªæ‰¾åˆ°ï¼ŒAgentic ç»„ä»¶å°†ä¸å¯ç”¨")
            self.query_rewriter = None
            self.clarifier = None
            self.reflector = None
        
        # åˆå§‹åŒ–æ–‡ä»¶æ£€æŸ¥å™¨
        self.file_inspector = FileInspector(str(self.upload_dir))
        
        # ğŸ”¥ ARCHITECTURAL MERGE: ç»‘å®šåˆ° WorkflowRegistry
        self.workflow_registry = WorkflowRegistry()
        
        # ğŸ”¥ å¯¹è¯çŠ¶æ€ç®¡ç†
        self.conversation_state: Dict[str, Any] = {}
    
    def _get_llm_client(self) -> Optional[LLMClient]:
        """ä» agent ä¸­è·å– LLM å®¢æˆ·ç«¯"""
        try:
            if hasattr(self.agent, 'agents') and self.agent.agents:
                # å°è¯•ä»ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“è·å– LLM client
                first_agent = list(self.agent.agents.values())[0]
                if hasattr(first_agent, 'llm_client'):
                    return first_agent.llm_client
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ è·å– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    async def _classify_global_intent(self, query: str, files: List[Dict[str, str]] = None) -> str:
        """
        ğŸ”¥ PHASE 1: Classify global intent (Chat vs Task)
        
        Returns:
            "chat" for general conversation
            "task" for bioinformatics analysis tasks
        """
        # Quick heuristic: If files are present, it's likely a task
        if files and len(files) > 0:
            return "task"
        
        # Quick keyword check for obvious tasks
        task_keywords = [
            "analyze", "analysis", "analyze", "åˆ†æ", "å¤„ç†", "è®¡ç®—", "ç»Ÿè®¡",
            "pca", "differential", "pathway", "enrichment", "visualize", "å¯è§†åŒ–",
            "metabolomics", "transcriptomics", "rna", "ä»£è°¢ç»„", "è½¬å½•ç»„",
            "workflow", "pipeline", "å·¥ä½œæµ", "æµç¨‹"
        ]
        query_lower = query.lower()
        if any(kw in query_lower for kw in task_keywords):
            return "task"
        
        # Use LLM for ambiguous cases
        try:
            llm_client = self._get_llm_client()
            if not llm_client:
                # Fallback: if no LLM, treat as chat for safety
                return "chat"
            
            prompt = f"""ç”¨æˆ·è¾“å…¥: "{query}"

è¯·åˆ¤æ–­è¿™æ˜¯ï¼š
1. ä¸€èˆ¬èŠå¤©å¯¹è¯ï¼ˆé—®å€™ã€è¯¢é—®ã€é—²èŠç­‰ï¼‰
2. ç”Ÿç‰©ä¿¡æ¯å­¦åˆ†æä»»åŠ¡ï¼ˆæ•°æ®åˆ†æã€å·¥ä½œæµæ‰§è¡Œç­‰ï¼‰

åªè¿”å›JSONæ ¼å¼: {{"type": "chat"}} æˆ– {{"type": "task"}}
ä¸è¦è¿”å›å…¶ä»–å†…å®¹ã€‚"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ„å›¾åˆ†ç±»åŠ©æ‰‹ã€‚åªè¿”å›JSONæ ¼å¼çš„æ„å›¾ç±»å‹ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            completion = await llm_client.achat(messages, temperature=0.1, max_tokens=50)
            response = completion.choices[0].message.content.strip()
            
            # Parse JSON response
            import json
            try:
                # Remove markdown code blocks if present
                if "```" in response:
                    response = response.split("```")[1]
                    if response.startswith("json"):
                        response = response[4:]
                response = response.strip()
                
                result = json.loads(response)
                intent_type = result.get("type", "chat")
                return intent_type if intent_type in ["chat", "task"] else "chat"
            except json.JSONDecodeError:
                # If JSON parsing fails, check response text
                if "task" in response.lower():
                    return "task"
                return "chat"
                
        except Exception as e:
            logger.error(f"âŒ [Orchestrator] LLMæ„å›¾åˆ†ç±»å¤±è´¥: {e}", exc_info=True)
            # Fallback: treat as chat for safety
            return "chat"
    
    async def stream_process(
        self,
        query: str,
        files: List[Dict[str, str]] = None,
        history: List[Dict[str, str]] = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """
        æµå¼å¤„ç†æŸ¥è¯¢
        
        å®æ—¶è¾“å‡ºï¼š
        1. çŠ¶æ€æ›´æ–°ï¼ˆstatusï¼‰
        2. æ€è€ƒè¿‡ç¨‹ï¼ˆthoughtï¼‰
        3. è¯Šæ–­æ•°æ®ï¼ˆdiagnosisï¼‰
        4. å·¥ä½œæµè®¡åˆ’ï¼ˆworkflowï¼‰
        5. æœ€ç»ˆå“åº”ï¼ˆmessageï¼‰
        6. å®Œæˆä¿¡å·ï¼ˆdoneï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            history: å¯¹è¯å†å²
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            SSE æ ¼å¼çš„äº‹ä»¶å­—ç¬¦ä¸²: "data: {json}\n\n"
        """
        files = files or []
        history = history or []
        
        # ğŸ”¥ PHASE 1: Layer 0 - Global Intent Routing (Chat vs Task)
        # Check if this is general chat or a bioinformatics task BEFORE any file inspection or planning
        try:
            yield self._format_sse("status", {
                "content": "æ­£åœ¨ç†è§£æ‚¨çš„æ„å›¾...",
                "state": "analyzing"
            })
            await asyncio.sleep(0.01)
            
            intent_type = await self._classify_global_intent(query, files)
            logger.info(f"ğŸ” [Orchestrator] å…¨å±€æ„å›¾åˆ†ç±»: {intent_type}")
            
            if intent_type == "chat":
                # ğŸ”¥ CHAT MODE: Stream LLM response directly, skip all file/planning logic
                logger.info("ğŸ’¬ [Orchestrator] è¿›å…¥èŠå¤©æ¨¡å¼ï¼Œè·³è¿‡æ–‡ä»¶æ£€æŸ¥å’Œè§„åˆ’")
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨æ€è€ƒ...",
                    "state": "thinking"
                })
                await asyncio.sleep(0.01)
                
                # Stream LLM response
                llm_client = self._get_llm_client()
                if llm_client:
                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€‚ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚"},
                        {"role": "user", "content": query}
                    ]
                    
                    # Add history context if available
                    if history:
                        for h in history[-5:]:  # Last 5 messages
                            if isinstance(h, dict):
                                role = h.get("role", "user")
                                content = h.get("content", h.get("message", ""))
                                if content:
                                    messages.append({"role": role, "content": content})
                    
                    message_buffer = ""
                    async for chunk in llm_client.astream(messages, temperature=0.7, max_tokens=1000):
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if delta and delta.content:
                                message_buffer += delta.content
                                yield self._format_sse("message", {
                                    "content": message_buffer
                                })
                                await asyncio.sleep(0.01)
                    
                    yield self._format_sse("status", {
                        "content": "å›ç­”å®Œæˆ",
                        "state": "completed"
                    })
                    await asyncio.sleep(0.01)
                    yield self._format_sse("done", {"status": "success"})
                    return
                else:
                    # Fallback if LLM not available
                    yield self._format_sse("message", {
                        "content": "æŠ±æ­‰ï¼ŒLLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
                    })
                    yield self._format_sse("done", {"status": "success"})
                    return
            
            # ğŸ”¥ TASK MODE: Continue with existing logic (file check -> plan -> execute)
            logger.info("ğŸ”¬ [Orchestrator] è¿›å…¥ä»»åŠ¡æ¨¡å¼ï¼Œç»§ç»­æ–‡ä»¶æ£€æŸ¥å’Œè§„åˆ’æµç¨‹")
            
        except Exception as e:
            logger.error(f"âŒ [Orchestrator] å…¨å±€æ„å›¾åˆ†ç±»å¤±è´¥: {e}", exc_info=True)
            # Continue with task mode as fallback
            logger.warning("âš ï¸ [Orchestrator] æ„å›¾åˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤è¿›å…¥ä»»åŠ¡æ¨¡å¼")
        
        # ğŸ”¥ CRITICAL REGRESSION FIX: Direct Execution Path
        # If the request contains a confirmed workflow_data, EXECUTE it immediately.
        # Do NOT re-plan. Do NOT re-inspect.
        workflow_data = kwargs.get("workflow_data")
        if workflow_data:
            logger.info("ğŸš€ [Orchestrator] æ¥æ”¶åˆ°æ‰§è¡ŒæŒ‡ä»¤ï¼Œè¿›å…¥ç›´æ¥æ‰§è¡Œæ¨¡å¼")
            logger.info(f"ğŸš€ [Orchestrator] workflow_data ç±»å‹: {type(workflow_data)}")
            
            try:
                # Parse workflow_data if it's a string
                if isinstance(workflow_data, str):
                    import json
                    workflow_data = json.loads(workflow_data)
                
                # Extract workflow config
                workflow_config = workflow_data.get("workflow_data") or workflow_data
                steps = workflow_config.get("steps", [])
                
                if not steps or len(steps) == 0:
                    logger.error("âŒ [Orchestrator] workflow_data ä¸­æ²¡æœ‰æ­¥éª¤")
                    yield self._format_sse("error", {
                        "error": "å·¥ä½œæµé…ç½®æ— æ•ˆ",
                        "message": "å·¥ä½œæµæ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°æ­¥éª¤"
                    })
                    return
                
                logger.info(f"âœ… [Orchestrator] å‡†å¤‡æ‰§è¡Œ {len(steps)} ä¸ªæ­¥éª¤")
                
                # 1. Initialize Execution Engine
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨åˆå§‹åŒ–æ‰§è¡Œå¼•æ“...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                from .executor import WorkflowExecutor
                # ğŸ”¥ CRITICAL REGRESSION FIX: Pass upload_dir to executor for path resolution
                upload_dir = getattr(self, 'upload_dir', Path(os.getenv("UPLOAD_DIR", "/app/uploads")))
                upload_dir_str = str(upload_dir) if isinstance(upload_dir, Path) else upload_dir
                executor = WorkflowExecutor(upload_dir=upload_dir_str)
                
                # 2. Extract file paths
                file_paths = workflow_data.get("file_paths", [])
                if not file_paths and files:
                    # Extract from files parameter
                    file_paths = [f.get("path") or f.get("file_path") or f.get("name") for f in files if f]
                    file_paths = [p for p in file_paths if p]
                
                logger.info(f"ğŸ“ [Orchestrator] æ–‡ä»¶è·¯å¾„: {file_paths}")
                
                # ğŸ”¥ TASK 1: Execute Steps with specific step names in logs
                steps = workflow_config.get("steps", [])
                
                # Yield status for each step before execution
                for i, step in enumerate(steps, 1):
                    # ğŸ”¥ TASK 1: Debug - Log step data to see why name might be missing
                    logger.info(f"ğŸ” [Orchestrator] Step Data: {step}")
                    
                    # ğŸ”¥ TASK 1: Fix - Try multiple fields to get step name
                    step_name = (
                        step.get("name") or 
                        step.get("step_name") or 
                        step.get("id") or 
                        step.get("step_id") or 
                        f"æ­¥éª¤ {i}"
                    )
                    tool_id = step.get("tool_id", "")
                    
                    # Skip visualize_pca (it's merged into pca_analysis)
                    if tool_id == "visualize_pca" or step.get("step_id") == "visualize_pca":
                        continue
                    
                    # ğŸ”¥ ä¿®å¤ï¼šä»å·¥å…·æ³¨å†Œè¡¨è·å–å·¥å…·åç§°ï¼Œæ˜¾ç¤ºå…·ä½“å·¥å…·åç§°
                    tool_display_name = step_name
                    if tool_id:
                        try:
                            from gibh_agent.core.tool_registry import ToolRegistry
                            registry = ToolRegistry()
                            tool_metadata = registry.get_metadata(tool_id)
                            if tool_metadata:
                                # ä½¿ç”¨å·¥å…·çš„æè¿°ä½œä¸ºæ˜¾ç¤ºåç§°ï¼ˆæ›´å‹å¥½ï¼‰
                                tool_display_name = tool_metadata.description if tool_metadata.description else step_name
                                # å¦‚æœæè¿°å¤ªé•¿ï¼Œæˆªæ–­
                                if len(tool_display_name) > 50:
                                    tool_display_name = tool_display_name[:50] + "..."
                            else:
                                # å¦‚æœå·¥å…·æ³¨å†Œè¡¨ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨step_name
                                tool_display_name = step_name
                        except Exception as e:
                            logger.debug(f"âš ï¸ [Orchestrator] æ— æ³•ä»å·¥å…·æ³¨å†Œè¡¨è·å–å·¥å…·åç§°: {e}ï¼Œä½¿ç”¨é»˜è®¤åç§°")
                            tool_display_name = step_name
                    
                    yield self._format_sse("status", {
                        "content": f"æ­£åœ¨æ‰§è¡Œæ­¥éª¤: {tool_display_name}...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                
                # Execute workflow (this will actually execute all steps)
                results = executor.execute_workflow(
                    workflow_data=workflow_config,
                    file_paths=file_paths,
                    agent=self.agent
                )
                
                logger.info(f"âœ… [Orchestrator] å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œç»“æœ: {type(results)}")
                
                # ğŸ”¥ TASK 3: Store executor reference for later use (to get output_dir)
                executor_output_dir = getattr(executor, 'output_dir', None)
                
                # ğŸ”¥ CRITICAL REGRESSION FIX: Check for async_job_started status
                steps_details = results.get("steps_details", [])
                has_async_job = False
                async_step_detail = None
                
                for step_detail in steps_details:
                    if step_detail.get("status") == "async_job_started":
                        has_async_job = True
                        async_step_detail = step_detail
                        logger.info(f"ğŸš€ [Orchestrator] æ£€æµ‹åˆ°å¼‚æ­¥ä½œä¸š: {step_detail.get('step_id')}, job_id: {step_detail.get('job_id')}")
                        break
                
                # ğŸ”¥ CRITICAL: If async job started, yield status and STOP (do not continue)
                if has_async_job:
                    logger.info("ğŸš€ [Orchestrator] å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨ï¼Œåœæ­¢æ‰§è¡Œæµç¨‹")
                    yield self._format_sse("status", {
                        "content": f"å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨: {async_step_detail.get('step_id', 'Unknown')}",
                        "state": "async_job_started"
                    })
                    await asyncio.sleep(0.01)
                    
                    # Yield async job status
                    async_response = {
                        "async_job": {
                            "step_id": async_step_detail.get("step_id"),
                            "job_id": async_step_detail.get("job_id"),
                            "status": "async_job_started",
                            "message": async_step_detail.get("summary", "å¼‚æ­¥ä½œä¸šå·²å¯åŠ¨ï¼Œç­‰å¾…å®Œæˆ")
                        },
                        "steps_details": steps_details
                    }
                    
                    yield self._format_sse("result", async_response)
                    yield self._format_sse("status", {
                        "content": "ç­‰å¾…å¼‚æ­¥ä½œä¸šå®Œæˆ...",
                        "state": "waiting"
                    })
                    await asyncio.sleep(0.01)
                    yield self._format_sse("done", {"status": "async_job_started"})
                    return  # STOP HERE - Do not continue to next steps
                
                # 4. Generate Summary (if agent available and no async job)
                # ğŸ”¥ CRITICAL FIX: ALWAYS generate summary, regardless of workflow status
                # Even if some steps failed, we should summarize what succeeded
                # ğŸ”¥ ä¿®å¤ï¼šä»GIBHAgent.agentsä¸­é€‰æ‹©åˆé€‚çš„é¢†åŸŸæ™ºèƒ½ä½“
                target_agent = None
                if self.agent:
                    # æ£€æµ‹é¢†åŸŸç±»å‹ï¼Œé€‰æ‹©å¯¹åº”çš„æ™ºèƒ½ä½“
                    domain_name = "Metabolomics"  # Default
                    workflow_name = workflow_config.get("workflow_name", "")
                    if "RNA" in workflow_name or "rna" in workflow_name.lower():
                        domain_name = "RNA"
                        # æ£€æŸ¥æ­¥éª¤ä¸­çš„å·¥å…·IDæ¥ç¡®å®šé¢†åŸŸ
                        for step in steps:
                            tool_id = step.get("tool_id", "").lower()
                            if "rna" in tool_id or "cellranger" in tool_id or "scanpy" in tool_id:
                                domain_name = "RNA"
                                break
                            elif "metabolomics" in tool_id or "pca" in tool_id or "pls" in tool_id:
                                domain_name = "Metabolomics"
                                break
                    
                    # æ ¹æ®é¢†åŸŸé€‰æ‹©æ™ºèƒ½ä½“
                    if hasattr(self.agent, 'agents') and self.agent.agents:
                        if domain_name == "RNA" and "rna_agent" in self.agent.agents:
                            target_agent = self.agent.agents["rna_agent"]
                        elif domain_name == "Metabolomics" and "metabolomics_agent" in self.agent.agents:
                            target_agent = self.agent.agents["metabolomics_agent"]
                        else:
                            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ™ºèƒ½ä½“ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
                            target_agent = list(self.agent.agents.values())[0] if self.agent.agents else None
                    
                    logger.info(f"ğŸ¯ [Orchestrator] é€‰æ‹©æ™ºèƒ½ä½“: {domain_name}, target_agent: {target_agent.__class__.__name__ if target_agent else 'None'}")
                
                if target_agent and hasattr(target_agent, '_generate_analysis_summary'):
                    import time
                    start_time = time.time()
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨ç”Ÿæˆä¸“å®¶è§£è¯»æŠ¥å‘Š...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    # ğŸ”¥ CRITICAL FIX: Check if there are failed/warning steps
                    failed_steps = [s for s in steps_details if s.get("status") == "error"]
                    warning_steps = [s for s in steps_details if s.get("status") == "warning"]
                    successful_steps = [s for s in steps_details if s.get("status") == "success"]
                    
                    # ğŸ”¥ CRITICAL FIX: Generate summary as long as we have ANY steps (success, warning, or error)
                    # This ensures diagnosis is always generated, even if some steps failed
                    if len(steps_details) > 0:
                        # Build context for summary generation
                        summary_context = {
                            "has_failures": len(failed_steps) > 0,
                            "has_warnings": len(warning_steps) > 0,
                            "failed_steps": failed_steps,
                            "warning_steps": warning_steps,
                            "successful_steps": successful_steps,
                            "workflow_status": results.get("status", "unknown")
                        }
                        
                        try:
                            # ğŸ”¥ TASK 3: Extract output_dir from results or executor to pass to Reporter
                            output_dir = results.get("output_dir") or results.get("output_path")
                            if not output_dir:
                                output_dir = executor_output_dir  # Use stored executor reference
                            
                            logger.info(f"ğŸ“‚ [Orchestrator] ä¼ é€’output_dirç»™Reporter: {output_dir}")
                            logger.info(f"ğŸš€ [Orchestrator] å¼€å§‹è°ƒç”¨LLMç”ŸæˆAIä¸“å®¶åˆ†ææŠ¥å‘Šï¼Œé¢†åŸŸ: {domain_name}")
                            
                            # ğŸ”¥ ä¿®å¤ï¼šä»resultsæˆ–steps_detailsä¸­æå–steps_resultsåˆ—è¡¨
                            steps_results = results.get("steps_results", [])
                            if not steps_results:
                                # ä»steps_detailsä¸­æå–step_result
                                steps_results = []
                                for step_detail in steps_details:
                                    if "step_result" in step_detail:
                                        steps_results.append(step_detail["step_result"])
                                    elif "status" in step_detail:
                                        # å¦‚æœæ²¡æœ‰step_resultï¼Œæ„å»ºä¸€ä¸ªåŸºæœ¬çš„step_result
                                        steps_results.append({
                                            "step_name": step_detail.get("name", step_detail.get("step_id", "Unknown")),
                                            "status": step_detail.get("status", "unknown"),
                                            "data": step_detail.get("data", {})
                                        })
                            
                            logger.info(f"ğŸ“Š [Orchestrator] æå–åˆ° {len(steps_results)} ä¸ªæ­¥éª¤ç»“æœç”¨äºç”ŸæˆæŠ¥å‘Š")
                            
                            # ğŸ”¥ TASK 2: Force LLM call - _generate_analysis_summary now always returns structured content
                            summary = await target_agent._generate_analysis_summary(
                                steps_results=steps_results,  # ğŸ”¥ ä¿®å¤ï¼šä¼ é€’æ­£ç¡®çš„å‚æ•°
                                omics_type=domain_name,  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
                                workflow_name=workflow_config.get("workflow_name", "å·¥ä½œæµ"),
                                summary_context=summary_context,
                                output_dir=output_dir  # ğŸ”¥ TASK 3: Pass output_dir to Reporter
                            )
                            
                            elapsed_time = time.time() - start_time
                            logger.info(f"âœ… [Orchestrator] AIä¸“å®¶åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼Œé•¿åº¦: {len(summary) if summary else 0}å­—ç¬¦")
                            logger.debug(f"ğŸ” [Orchestrator] summary é¢„è§ˆ: {summary[:300] if summary else 'None'}...")
                            
                            # ğŸ”¥ TASK: æ£€æŸ¥æ˜¯å¦æœ‰LLMé”™è¯¯ï¼Œå¦‚æœæœ‰åˆ™é€šè¿‡SSEå‘é€è¯¦ç»†é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯
                            if hasattr(target_agent, 'context') and "last_llm_error" in target_agent.context:
                                llm_error_info = target_agent.context.pop("last_llm_error")  # å–å‡ºåæ¸…é™¤
                                logger.warning(f"âš ï¸ [Orchestrator] æ£€æµ‹åˆ°LLMè°ƒç”¨é”™è¯¯ï¼Œå‘é€è¯¦ç»†é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯")
                                yield self._format_sse("error", {
                                    "error": llm_error_info.get("error_message", "LLMè°ƒç”¨å¤±è´¥"),
                                    "message": f"LLMè°ƒç”¨å¤±è´¥: {llm_error_info.get('error_message', 'æœªçŸ¥é”™è¯¯')}",
                                    "error_type": llm_error_info.get("error_type", "Unknown"),
                                    "details": llm_error_info.get("error_details", ""),
                                    "context": llm_error_info.get("context", {}),
                                    "possible_causes": llm_error_info.get("possible_causes", []),
                                    "debug_info": llm_error_info.get("error_details", "")  # å…¼å®¹å‰ç«¯å­—æ®µå
                                })
                                await asyncio.sleep(0.01)
                            
                            # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥summaryæ˜¯å¦åŒ…å«çœŸæ­£çš„ç”Ÿä¿¡åˆ†æå†…å®¹ï¼Œè€Œä¸æ˜¯é”™è¯¯ä¿¡æ¯
                            # å¦‚æœsummaryæ˜¯é”™è¯¯ä¿¡æ¯æˆ–è¿‡çŸ­ï¼Œæ‰ä½¿ç”¨åå¤‡æ–¹æ¡ˆ
                            if not summary:
                                logger.warning(f"âš ï¸ [Orchestrator] summaryä¸ºNoneï¼Œä½¿ç”¨ç»“æ„åŒ–åå¤‡")
                                summary = f"""## åˆ†æç»“æœæ‘˜è¦

æœ¬æ¬¡åˆ†æå®Œæˆäº† {len(successful_steps)} ä¸ªæ­¥éª¤ã€‚è¯·æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†å›¾è¡¨å’Œç»Ÿè®¡ç»“æœä»¥è·å–æ›´æ·±å…¥çš„ç”Ÿç‰©å­¦è§£é‡Šã€‚

### å…³é”®å‘ç°
- æˆåŠŸæ­¥éª¤: {len(successful_steps)}/{len(steps_details)}
- è¯·æŸ¥çœ‹æ‰§è¡Œç»“æœä¸­çš„å›¾è¡¨å’Œæ•°æ®è¡¨æ ¼è·å–è¯¦ç»†åˆ†æã€‚"""
                            elif len(summary.strip()) < 50:
                                logger.warning(f"âš ï¸ [Orchestrator] æ‘˜è¦è¿‡çŸ­ï¼ˆ{len(summary.strip())}å­—ç¬¦ï¼‰ï¼Œä½¿ç”¨ç»“æ„åŒ–åå¤‡")
                                summary = f"""## åˆ†æç»“æœæ‘˜è¦

æœ¬æ¬¡åˆ†æå®Œæˆäº† {len(successful_steps)} ä¸ªæ­¥éª¤ã€‚è¯·æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†å›¾è¡¨å’Œç»Ÿè®¡ç»“æœä»¥è·å–æ›´æ·±å…¥çš„ç”Ÿç‰©å­¦è§£é‡Šã€‚

### å…³é”®å‘ç°
- æˆåŠŸæ­¥éª¤: {len(successful_steps)}/{len(steps_details)}
- è¯·æŸ¥çœ‹æ‰§è¡Œç»“æœä¸­çš„å›¾è¡¨å’Œæ•°æ®è¡¨æ ¼è·å–è¯¦ç»†åˆ†æã€‚"""
                            elif "âš ï¸" in summary or "AIä¸“å®¶åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥" in summary or "åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥" in summary:
                                # å¦‚æœsummaryæ˜¯é”™è¯¯ä¿¡æ¯ï¼Œä¿ç•™å®ƒï¼ˆç”¨æˆ·éœ€è¦çœ‹åˆ°é”™è¯¯ï¼‰
                                logger.warning(f"âš ï¸ [Orchestrator] summaryåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œä¿ç•™åŸå†…å®¹")
                                # ä¸æ›¿æ¢ï¼Œè®©ç”¨æˆ·çœ‹åˆ°é”™è¯¯ä¿¡æ¯
                            else:
                                # summaryæ˜¯æœ‰æ•ˆçš„ç”Ÿä¿¡åˆ†æå†…å®¹ï¼Œç›´æ¥ä½¿ç”¨
                                logger.info(f"âœ… [Orchestrator] summaryæ˜¯æœ‰æ•ˆçš„ç”Ÿä¿¡åˆ†æå†…å®¹ï¼Œé•¿åº¦: {len(summary)}å­—ç¬¦")
                            
                            # ğŸ”¥ PHASE 2: Generate quality evaluation
                            evaluation = None
                            if summary and target_agent and hasattr(target_agent, '_evaluate_analysis_quality'):
                                try:
                                    steps_results = results.get("steps_results", [])
                                    if not steps_results:
                                        # Extract from steps_details
                                        steps_results = []
                                        for step_detail in steps_details:
                                            if "step_result" in step_detail:
                                                steps_results.append(step_detail["step_result"])
                                    
                                    evaluation = await target_agent._evaluate_analysis_quality(
                                        steps_results,
                                        summary,
                                        workflow_config.get("workflow_name", "å·¥ä½œæµ")
                                    )
                                    logger.info(f"âœ… [Orchestrator] è´¨é‡è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {evaluation.get('score', 'N/A')}")
                                except Exception as e:
                                    logger.warning(f"âš ï¸ [Orchestrator] è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
                            
                        except Exception as e:
                            logger.error(f"âŒ [Orchestrator] ç”Ÿæˆæ‘˜è¦å¼‚å¸¸: {e}", exc_info=True)
                            # ğŸ”¥ TASK 2: Use structured fallback instead of simple list
                            summary = f"""## åˆ†æç»“æœæ‘˜è¦

æœ¬æ¬¡åˆ†æå®Œæˆäº† {len(successful_steps)} ä¸ªæ­¥éª¤ã€‚è¯·æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†å›¾è¡¨å’Œç»Ÿè®¡ç»“æœä»¥è·å–æ›´æ·±å…¥çš„ç”Ÿç‰©å­¦è§£é‡Šã€‚

### å…³é”®å‘ç°
- æˆåŠŸæ­¥éª¤: {len(successful_steps)}/{len(steps_details)}
- è¯·æŸ¥çœ‹æ‰§è¡Œç»“æœä¸­çš„å›¾è¡¨å’Œæ•°æ®è¡¨æ ¼è·å–è¯¦ç»†åˆ†æã€‚"""
                            evaluation = None
                    else:
                        summary = "åˆ†æå®Œæˆï¼ˆæ— æ­¥éª¤æ‰§è¡Œï¼‰"
                        evaluation = None
                else:
                    # ğŸ”¥ CRITICAL FIX: Generate basic summary even without agent
                    failed_steps = [s for s in steps_details if s.get("status") == "error"]
                    warning_steps = [s for s in steps_details if s.get("status") == "warning"]
                    successful_steps = [s for s in steps_details if s.get("status") == "success"]
                    
                    if len(steps_details) > 0:
                        summary = self._generate_fallback_summary(successful_steps, warning_steps, failed_steps, steps_details)
                    else:
                        summary = "åˆ†æå®Œæˆï¼ˆæ— æ­¥éª¤æ‰§è¡Œï¼‰"
                    evaluation = None
                
                # ğŸ”¥ TASK 3: Yield Execution Results FIRST (step_result events)
                # This allows frontend to render the Accordion with step results
                if steps_details and len(steps_details) > 0:
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨æ¸²æŸ“æ‰§è¡Œç»“æœ...",
                        "state": "rendering"
                    })
                    await asyncio.sleep(0.01)
                    
                    # Yield step_result event with execution steps
                    step_result_response = {
                        "report_data": {
                            "steps_details": steps_details,
                            "workflow_name": workflow_config.get("workflow_name", "å·¥ä½œæµ")
                        }
                    }
                    yield self._format_sse("step_result", step_result_response)
                    await asyncio.sleep(0.01)
                
                # ğŸ”¥ TASK 3: THEN Yield Diagnosis Report LAST (diagnosis event)
                # This ensures the Expert Report appears after the execution results
                if summary:
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨ç”Ÿæˆä¸“å®¶è§£è¯»æŠ¥å‘Š...",
                        "state": "generating_report"
                    })
                    await asyncio.sleep(0.01)
                    
                    diagnosis_response = {
                        "report_data": {
                            "diagnosis": summary,
                            "workflow_name": workflow_config.get("workflow_name", "å·¥ä½œæµ")
                        }
                    }
                    
                    # ğŸ”¥ PHASE 2: Add evaluation to diagnosis response
                    if evaluation:
                        diagnosis_response["report_data"]["evaluation"] = evaluation
                    
                    yield self._format_sse("diagnosis", diagnosis_response)
                    await asyncio.sleep(0.01)
                
                # ğŸ”¥ TASK 3: Also yield combined result event for backward compatibility
                final_response = {
                    "report_data": {
                        "steps_details": steps_details,
                        "diagnosis": summary,
                        "workflow_name": workflow_config.get("workflow_name", "å·¥ä½œæµ")
                    }
                }
                
                # ğŸ”¥ PHASE 2: Add evaluation to response
                if evaluation:
                    final_response["report_data"]["evaluation"] = evaluation
                
                yield self._format_sse("result", final_response)
                yield self._format_sse("status", {
                    "content": "æ‰§è¡Œå®Œæˆ",
                    "state": "completed"
                })
                await asyncio.sleep(0.01)
                yield self._format_sse("done", {"status": "success"})
                return  # STOP HERE - Do not continue to planning
                
            except Exception as e:
                logger.error(f"âŒ [Orchestrator] ç›´æ¥æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                yield self._format_sse("error", {
                    "error": str(e),
                    "message": f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
                })
                return
        
        # ğŸ”¥ CRITICAL DEBUG: è®°å½•æ¥æ”¶åˆ°çš„åŸå§‹æ–‡ä»¶æ•°æ®
        logger.info(f"ğŸ”¥ DEBUG: Orchestrator received files raw: {files}, type: {type(files)}")
        logger.info(f"ğŸ”¥ DEBUG: files length: {len(files) if files else 0}")
        if files:
            for i, f in enumerate(files):
                logger.info(f"ğŸ”¥ DEBUG: files[{i}]: {f}, type: {type(f)}")
        
        # ğŸ”¥ BUG FIX: ä» kwargs ä¸­æå– session_id å’Œ user_id
        session_id = kwargs.get("session_id") or "default"
        user_id = kwargs.get("user_id") or "guest"
        
        # ğŸ”¥ ARCHITECTURAL MERGE: æ£€æŸ¥å¯¹è¯çŠ¶æ€ï¼ˆå¤„ç†æ¾„æ¸…å›å¤å’Œæ‰§è¡Œæ„å›¾ï¼‰
        session_state = self.conversation_state.get(session_id, {})
        awaiting_clarification = session_state.get("awaiting_clarification", False)
        previous_query = session_state.get("previous_query")
        previous_refined_query = session_state.get("previous_refined_query")
        pending_plan = session_state.get("pending_plan")  # ğŸ”¥ URGENT FIX: æ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„å·¥ä½œæµè®¡åˆ’
        pending_modality = session_state.get("pending_modality")  # ğŸ”¥ CRITICAL: æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„æ¨¡æ€ï¼ˆæ¢å¤æ¡ä»¶ï¼‰
        
        # ğŸ”¥ URGENT FIX: æ£€æŸ¥æ‰§è¡Œæ„å›¾ï¼ˆ"Proceed", "ç»§ç»­", "æ‰§è¡Œ"ç­‰ï¼‰
        execution_keywords = ["proceed", "ç»§ç»­", "æ‰§è¡Œ", "go ahead", "run it", "å¼€å§‹", "execute"]
        query_lower = query.lower().strip()
        is_execution_intent = any(kw in query_lower for kw in execution_keywords)
        
        # å¦‚æœæœ‰å¾…æ‰§è¡Œè®¡åˆ’ä¸”ç”¨æˆ·ç¡®è®¤æ‰§è¡Œï¼Œç›´æ¥æ‰§è¡Œå·¥ä½œæµ
        if pending_plan and is_execution_intent:
            logger.info(f"âœ… [Orchestrator] æ£€æµ‹åˆ°æ‰§è¡Œæ„å›¾ï¼Œç›´æ¥æ‰§è¡Œå¾…æ‰§è¡Œçš„å·¥ä½œæµè®¡åˆ’")
            # æ¸…é™¤å¾…æ‰§è¡Œè®¡åˆ’çŠ¶æ€
            self.conversation_state[session_id] = {}
            # ç›´æ¥è°ƒç”¨æ‰§è¡Œé€»è¾‘ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ‰§è¡Œæ¥å£è°ƒæ•´ï¼‰
            # æš‚æ—¶è·³è¿‡ FileInspector å’Œ Diagnosisï¼Œç›´æ¥è¿›å…¥æ‰§è¡Œ
            yield self._format_sse("status", {
                "content": "æ­£åœ¨æ‰§è¡Œå·¥ä½œæµ...",
                "state": "running"
            })
            await asyncio.sleep(0.01)
            # è¿™é‡Œåº”è¯¥è°ƒç”¨æ‰§è¡Œå™¨ï¼Œä½†ä¸ºäº†ä¸ç ´åç°æœ‰æµç¨‹ï¼Œæˆ‘ä»¬ç»§ç»­æ­£å¸¸æµç¨‹
            # å®é™…æ‰§è¡Œä¼šåœ¨å‰ç«¯ç‚¹å‡»"æ‰§è¡Œ"æŒ‰é’®æ—¶è§¦å‘
        
        try:
            # Step 1: ç«‹å³è¾“å‡ºå¼€å§‹çŠ¶æ€
            yield self._format_sse("status", {
                "content": "æ­£åœ¨æ¥æ”¶è¯·æ±‚...",
                "state": "start"
            })
            await asyncio.sleep(0.01)  # å¼ºåˆ¶ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œç¡®ä¿ç«‹å³å‘é€
            
            # ğŸ”¥ Step 2: Query Rewritingï¼ˆæŸ¥è¯¢é‡å†™ï¼‰
            # å¦‚æœæ˜¯æ¾„æ¸…å›å¤ï¼Œåˆå¹¶æŸ¥è¯¢å¹¶è·³è¿‡é‡å†™
            if awaiting_clarification and previous_query:
                refined_query = f"{previous_refined_query or previous_query}ã€‚ç”¨æˆ·å›å¤ï¼š{query}"
                logger.info(f"âœ… [Orchestrator] å¤„ç†æ¾„æ¸…å›å¤: '{query}' -> åˆå¹¶æŸ¥è¯¢")
                # æ¸…é™¤æ¾„æ¸…çŠ¶æ€
                self.conversation_state[session_id] = {}
            else:
                refined_query = query
                if self.query_rewriter:
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨ä¼˜åŒ–æŸ¥è¯¢è¯­å¥...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    refined_query = await self.query_rewriter.rewrite(query, history)
                    logger.info(f"âœ… [Orchestrator] æŸ¥è¯¢é‡å†™: '{query}' -> '{refined_query}'")
                else:
                    logger.info("â„¹ï¸ [Orchestrator] QueryRewriter ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
            
            # ğŸ”¥ CRITICAL FIX: Step 3.0: Normalize Files (BEFORE Resume Check)
            # Normalize files to a list of valid file dictionaries
            valid_files = []
            if files:
                logger.info(f"ğŸ” [Orchestrator] å¼€å§‹è§„èŒƒåŒ–æ–‡ä»¶ï¼ŒåŸå§‹ files ç±»å‹: {type(files)}, é•¿åº¦: {len(files)}")
                for i, f in enumerate(files):
                    logger.info(f"ğŸ” [Orchestrator] å¤„ç†æ–‡ä»¶ [{i}]: {f}, ç±»å‹: {type(f)}")
                    file_dict = None
                    if isinstance(f, dict):
                        # Already a dictionary
                        path = f.get("path") or f.get("file_path")
                        name = f.get("name") or f.get("file_name") or ""
                        logger.info(f"ğŸ” [Orchestrator] å­—å…¸æ ¼å¼ - path: {path}, name: {name}")
                        if path:
                            # ğŸ”¥ CRITICAL: éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•åœ¨ upload_dir ä¸­æŸ¥æ‰¾ï¼‰
                            path_obj = Path(path)
                            if not path_obj.is_absolute():
                                path_obj = Path(self.upload_dir) / path_obj
                            elif not path_obj.exists():
                                # ç»å¯¹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨ upload_dir ä¸­æŸ¥æ‰¾æ–‡ä»¶å
                                filename = path_obj.name
                                potential_path = Path(self.upload_dir) / filename
                                if potential_path.exists():
                                    path_obj = potential_path
                                    logger.info(f"âœ… [Orchestrator] åœ¨ upload_dir ä¸­æ‰¾åˆ°æ–‡ä»¶: {path_obj}")
                            
                            file_dict = {
                                "name": name or path_obj.name,
                                "path": str(path_obj)
                            }
                            logger.info(f"âœ… [Orchestrator] è§„èŒƒåŒ–æ–‡ä»¶: {file_dict}")
                    elif isinstance(f, str):
                        # String path
                        path_obj = Path(f)
                        if not path_obj.is_absolute():
                            path_obj = Path(self.upload_dir) / path_obj
                        file_dict = {
                            "name": path_obj.name,
                            "path": str(path_obj)
                        }
                        logger.info(f"âœ… [Orchestrator] å­—ç¬¦ä¸²è·¯å¾„è§„èŒƒåŒ–: {file_dict}")
                    elif hasattr(f, "path"):
                        # Pydantic model or object with path attribute
                        path = f.path if hasattr(f, "path") else str(f)
                        name = getattr(f, "name", "") or getattr(f, "file_name", "") or os.path.basename(path)
                        path_obj = Path(path)
                        if not path_obj.is_absolute():
                            path_obj = Path(self.upload_dir) / path_obj
                        file_dict = {
                            "name": name,
                            "path": str(path_obj)
                        }
                        logger.info(f"âœ… [Orchestrator] å¯¹è±¡æ ¼å¼è§„èŒƒåŒ–: {file_dict}")
                    
                    if file_dict:
                        valid_files.append(file_dict)
            
            logger.info(f"âœ… [Orchestrator] è§„èŒƒåŒ–åçš„æ–‡ä»¶åˆ—è¡¨: {valid_files}, æ•°é‡: {len(valid_files)}")
            
            # ğŸ”¥ CRITICAL: Use normalized files for the rest of the logic
            files = valid_files
            
            # ğŸ”¥ URGENT FIX: Step 3.0: Resume Priority Check (BEFORE Intent Analysis)
            # Check if this is a RESUME action: file uploaded + pending modality exists
            has_files = len(valid_files) > 0
            logger.info(f"ğŸ” [Orchestrator] æ–‡ä»¶æ£€æµ‹ç»“æœ: has_files={has_files}, valid_filesæ•°é‡={len(valid_files)}")
            is_resume_action = has_files and pending_modality is not None
            
            # Initialize planner variable (will be set in either branch)
            planner = None
            
            if is_resume_action:
                logger.info(f"ğŸš€ [Orchestrator] æ£€æµ‹åˆ°æ¢å¤æ“ä½œ: æ–‡ä»¶å·²ä¸Šä¼  + å¾…å¤„ç†æ¨¡æ€={pending_modality}")
                logger.info(f"ğŸš€ [Orchestrator] å¼ºåˆ¶è¿›å…¥æ‰§è¡Œæ¨¡å¼ï¼Œè·³è¿‡æ„å›¾åˆ†æ")
                
                # Force the domain to match the pending plan (ignore query re-analysis)
                domain_name = pending_modality
                target_steps = []  # Use full SOP for resume
                
                # Clear pending state (we're resuming now)
                session_state.pop("pending_modality", None)
                self.conversation_state[session_id] = session_state
                
                # Get workflow instance
                workflow = self.workflow_registry.get_workflow(domain_name)
                if not workflow:
                    raise ValueError(f"æ— æ³•è·å–å·¥ä½œæµ: {domain_name}")
                
                # Use full workflow for resume
                target_steps = list(workflow.steps_dag.keys())
                
                logger.info(f"âœ… [Orchestrator] æ¢å¤æ¨¡å¼: domain={domain_name}, target_steps={target_steps}")
                # Skip to file inspection (Branch B) - don't analyze intent again
            else:
                # ğŸ”¥ CRITICAL REFACTOR: Step 3 - ALWAYS Analyze Intent First (Dynamic Scoping)
                # Step 3.1: Analyze Intent (ALWAYS FIRST) - Determine modality and target_steps
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                # Initialize planner for intent analysis
                from .planner import SOPPlanner
                from .tool_retriever import ToolRetriever
                
                llm_client = self._get_llm_client()
                if not llm_client:
                    raise ValueError("LLM å®¢æˆ·ç«¯ä¸å¯ç”¨")
                
                tool_retriever = ToolRetriever()
                planner = SOPPlanner(tool_retriever, llm_client)
                
                
                # ğŸ”¥ CRITICAL FIX: Pass file_metadata to intent classification for file-type-based routing
                # If files exist, inspect first file to get metadata for intent classification
                file_metadata_for_intent = None
                if files and len(files) > 0:
                    first_file = files[0]
                    logger.info(f"ğŸ” [Orchestrator] å‡†å¤‡æ£€æŸ¥æ–‡ä»¶ç”¨äºæ„å›¾åˆ†ç±»: {first_file}, type={type(first_file)}")
                    if isinstance(first_file, dict):
                        file_path = first_file.get("path") or first_file.get("file_path") or first_file.get("name")
                    elif isinstance(first_file, str):
                        file_path = first_file
                    else:
                        file_path = str(first_file)
                        
                    logger.info(f"ğŸ” [Orchestrator] æå–çš„æ–‡ä»¶è·¯å¾„: {file_path}")
                        
                    if file_path:
                        # Ensure absolute path
                        path_obj = Path(file_path)
                        if not path_obj.is_absolute():
                            path_obj = Path(self.upload_dir) / path_obj
                        file_path = str(path_obj.resolve())
                            
                        logger.info(f"ğŸ” [Orchestrator] è§£æåçš„ç»å¯¹è·¯å¾„: {file_path}")
                            
                        try:
                            # Inspect file to get metadata for intent classification
                            file_metadata_for_intent = self.file_inspector.inspect_file(file_path)
                            logger.info(f"âœ… [Orchestrator] æ–‡ä»¶æ£€æŸ¥æˆåŠŸï¼Œæ–‡ä»¶ç±»å‹: {file_metadata_for_intent.get('file_type', 'unknown')}")
                            logger.info(f"âœ… [Orchestrator] æ–‡ä»¶å…ƒæ•°æ®é”®: {list(file_metadata_for_intent.keys())[:10]}")
                        except Exception as e:
                            logger.error(f"âŒ [Orchestrator] æ–‡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç”¨äºæ„å›¾åˆ†ç±»: {e}", exc_info=True)
                    else:
                        logger.warning(f"âš ï¸ [Orchestrator] æ— æ³•ä»æ–‡ä»¶å¯¹è±¡ä¸­æå–è·¯å¾„")
                else:
                    logger.info(f"â„¹ï¸ [Orchestrator] æ²¡æœ‰æ–‡ä»¶ï¼Œè·³è¿‡æ–‡ä»¶æ£€æŸ¥")
                
                # Analyze intent: classify domain and determine target_steps
                intent_result = await planner._classify_intent(refined_query, file_metadata_for_intent)
                domain_name = intent_result.get("domain_name")
                
                # Validate domain
                if not domain_name or not self.workflow_registry.is_supported(domain_name):
                    logger.warning(f"âš ï¸ [Orchestrator] æ— æ³•è¯†åˆ«åŸŸå: {domain_name}")
                    domain_name = "Metabolomics"  # é»˜è®¤å€¼
                
                # Get workflow instance for intent analysis
                workflow = self.workflow_registry.get_workflow(domain_name)
                if not workflow:
                    raise ValueError(f"æ— æ³•è·å–å·¥ä½œæµ: {domain_name}")
                
                # Analyze user intent to determine target_steps
                target_steps = await planner._analyze_user_intent(refined_query, workflow)
                
                # Ensure target_steps is not empty (fallback to full workflow)
                if not target_steps:
                    query_lower = refined_query.lower()
                    vague_keywords = ["analyze this", "full analysis", "å®Œæ•´åˆ†æ", "å…¨éƒ¨", "all", "complete"]
                    if any(kw in query_lower for kw in vague_keywords):
                        target_steps = list(workflow.steps_dag.keys())
                    else:
                        # Fallback to keyword matching
                        from .planner import SOPPlanner
                        target_steps = planner._fallback_intent_analysis(refined_query, list(workflow.steps_dag.keys()))
                        if not target_steps:
                            target_steps = list(workflow.steps_dag.keys())
                
                logger.info(f"âœ… [Orchestrator] æ„å›¾åˆ†æå®Œæˆ: domain={domain_name}, target_steps={target_steps}")
                
                # ğŸ”¥ SYSTEM REFACTOR: Step 3.2: Check Files (The Branching Point)
                # Priority: Files check determines execution mode
                # Note: has_files already checked above for resume action
                
                # ğŸ”¥ CRITICAL: è¯¦ç»†æ—¥å¿—ï¼Œç¡®ä¿æ–‡ä»¶æ£€æŸ¥æ­£ç¡®
                logger.info(f"ğŸ” [Orchestrator] æ–‡ä»¶æ£€æŸ¥: files={files}, has_files={has_files}")
                if files:
                    logger.info(f"ğŸ” [Orchestrator] files ç±»å‹: {type(files)}, é•¿åº¦: {len(files) if hasattr(files, '__len__') else 'N/A'}")
                    for i, f in enumerate(files):
                        logger.info(f"  [{i}] {f}")
                else:
                    logger.warning("âš ï¸ [Orchestrator] files ä¸ºç©ºæˆ– None")
                
                yield self._format_sse("status", {
                    "content": "æ­£åœ¨æ£€æµ‹æ–‡ä»¶è¾“å…¥...",
                    "state": "running"
                })
                await asyncio.sleep(0.01)
                
                # ============================================================
                # BRANCH A: Plan-First Mode (No Files)
                # ============================================================
                if not has_files:
                    logger.info("âš ï¸ [Orchestrator] åˆ†æ”¯ A: Plan-First æ¨¡å¼ï¼ˆæ— æ–‡ä»¶ï¼‰")
                    logger.info("âš ï¸ [Orchestrator] è¿›å…¥é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¼šç”Ÿæˆè¯Šæ–­æŠ¥å‘Š")
                    
                    yield self._format_sse("status", {
                        "content": "æœªæ£€æµ‹åˆ°æ–‡ä»¶ï¼Œè¿›å…¥æ–¹æ¡ˆé¢„è§ˆæ¨¡å¼...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    try:
                        # Step A1: Generate Template Workflow with target_steps
                        yield self._format_sse("status", {
                            "content": "æ­£åœ¨æ ¹æ®æ‚¨çš„éœ€æ±‚å®šåˆ¶æµç¨‹...",
                            "state": "running"
                        })
                        await asyncio.sleep(0.01)
                        
                        # ğŸ”¥ CRITICAL: Use the same target_steps analyzed above
                        # Path B: PREVIEW MODE - Explicitly tell planner this IS a template
                        template_result = await planner.generate_plan(
                        user_query=refined_query,
                        file_metadata=None,  # æ˜ç¡®ä¼ é€’ None
                        category_filter=None,
                        domain_name=domain_name,  # ä½¿ç”¨å·²åˆ†æçš„åŸŸå
                        target_steps=target_steps,  # ğŸ”¥ CRITICAL: ä½¿ç”¨å·²åˆ†æçš„ç›®æ ‡æ­¥éª¤
                        is_template=True  # ğŸ”¥ CRITICAL: Explicitly IS a template
                        )
                        
                        logger.info(f"âœ… [Orchestrator] æ¨¡æ¿ç”Ÿæˆå®Œæˆ: {len(target_steps)} ä¸ªç›®æ ‡æ­¥éª¤")
                        
                        # ğŸ”¥ CRITICAL: Save pending_modality for resume detection
                        session_state["pending_modality"] = domain_name
                        self.conversation_state[session_id] = session_state
                        logger.info(f"ğŸ’¾ [Orchestrator] å·²ä¿å­˜å¾…å¤„ç†æ¨¡æ€: {domain_name} (session_id={session_id})")
                        
                        # Step A2: Yield Template Card - ONLY if steps are not empty
                        workflow_data = template_result.get("workflow_data") or template_result
                        if workflow_data:
                            steps = workflow_data.get("steps", [])
                            steps_count = len(steps)
                            
                            # ğŸ”¥ TASK 1: Empty Guard - Do NOT yield empty plans
                            if not steps or len(steps) == 0:
                                logger.error(f"âŒ [Orchestrator] Plan-Firstæ¨¡å¼: æ¨¡æ¿å·¥ä½œæµæ­¥éª¤ä¸ºç©ºï¼Œä¸å‘é€workflowäº‹ä»¶")
                                yield self._format_sse("error", {
                                    "error": "æ¨¡æ¿ç”Ÿæˆå¤±è´¥",
                                    "message": "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„å·¥ä½œæµæ¨¡æ¿ï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ"
                                })
                                return
                            
                            logger.info(f"âœ… [Orchestrator] Plan-Firstæ¨¡å¼: å‘é€workflowäº‹ä»¶ï¼ŒåŒ…å« {steps_count} ä¸ªæ­¥éª¤")
                            workflow_event_data = {
                                "workflow_config": workflow_data,
                                "workflow_data": workflow_data,
                                "template_mode": True  # ğŸ”¥ CRITICAL: æ˜ç¡®æ ‡è®°ä¸ºæ¨¡æ¿æ¨¡å¼
                            }
                            yield self._format_sse("workflow", workflow_event_data)
                            await asyncio.sleep(0.01)
                        
                        # ğŸ”¥ CRITICAL: Generate message with modality and step count
                        modality_display = "ä»£è°¢ç»„å­¦" if domain_name == "Metabolomics" else "è½¬å½•ç»„"
                        yield self._format_sse("message", {
                            "content": f"å·²ä¸ºæ‚¨è§„åˆ’ **{modality_display}** åˆ†ææµç¨‹ï¼ˆåŒ…å« {steps_count} ä¸ªæ­¥éª¤ï¼‰ã€‚è¯·ä¸Šä¼ æ•°æ®ä»¥æ¿€æ´»ã€‚"
                        })
                        await asyncio.sleep(0.01)
                        
                        # è¾“å‡ºç»“æœäº‹ä»¶
                        yield self._format_sse("result", {
                            "workflow_config": workflow_data,
                            "template_mode": True
                        })
                        
                        # ğŸ”¥ CRITICAL: STOP HERE - ä¸ç»§ç»­æ‰§è¡Œ
                        yield self._format_sse("status", {
                            "content": "æ–¹æ¡ˆæ¨¡ç‰ˆå·²ç”Ÿæˆï¼Œç­‰å¾…ä¸Šä¼ ...",
                            "state": "completed"
                        })
                        await asyncio.sleep(0.01)
                        
                        yield self._format_sse("done", {"status": "success"})
                        return  # ç«‹å³è¿”å›ï¼Œä¸ç»§ç»­æ‰§è¡Œ
                    except Exception as e:
                        logger.error(f"âŒ [Orchestrator] Plan-First æ¨¡å¼å¤±è´¥: {e}", exc_info=True)
                        yield self._format_sse("error", {
                            "error": str(e),
                            "message": f"æ¨¡æ¿ç”Ÿæˆå¤±è´¥: {str(e)}"
                        })
                        return
                
                # ============================================================
                # PATH A: CLASSIC EXECUTION (The "Old Way") - Files Detected
                # ============================================================
                else:
                    logger.info("ğŸš€ [Orchestrator] Path A: æ–‡ä»¶æ£€æµ‹åˆ°ã€‚å¼ºåˆ¶æ‰§è¡Œæ¨¡å¼ï¼ˆç»å…¸æ‰§è¡Œè·¯å¾„ï¼‰")
                    
                    # A1. File Inspection - Extract file path and inspect
                    first_file = files[0]
                    if isinstance(first_file, dict):
                        file_path = first_file.get("path") or first_file.get("file_path") or first_file.get("name")
                    elif isinstance(first_file, str):
                        file_path = first_file
                    else:
                        file_path = str(first_file)
                    
                    logger.info(f"ğŸ” [Orchestrator] Path A: æå–æ–‡ä»¶è·¯å¾„: {file_path}")
                    
                    if not file_path:
                        logger.error("âŒ [Orchestrator] Path A: æ— æ³•æå–æ–‡ä»¶è·¯å¾„")
                        yield self._format_sse("error", {
                            "error": "æ–‡ä»¶è·¯å¾„æ— æ•ˆ",
                            "message": "æ— æ³•ä»æ–‡ä»¶å¯¹è±¡ä¸­æå–è·¯å¾„"
                        })
                        return
                    
                    yield self._format_sse("status", {
                        "content": f"æ£€æµ‹åˆ°æ–‡ä»¶ï¼Œæ­£åœ¨è¿›è¡Œæ•°æ®ä½“æ£€...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    # Inspect file
                    file_metadata = None
                    try:
                        file_metadata = self.file_inspector.inspect_file(file_path)
                        logger.info(f"âœ… [Orchestrator] Path A: æ–‡ä»¶æ£€æŸ¥å®Œæˆ: {file_path}")
                        
                        if file_metadata and file_metadata.get("status") == "success":
                            # Extract statistics
                            n_samples = file_metadata.get("n_samples") or file_metadata.get("n_obs") or file_metadata.get("shape", {}).get("rows", 0)
                            n_features = file_metadata.get("n_features") or file_metadata.get("n_vars") or file_metadata.get("shape", {}).get("cols", 0)
                            
                            # Build diagnosis message
                            if domain_name == "Metabolomics":
                                diagnosis_message = f"""### ğŸ“Š æ•°æ®ä½“æ£€æŠ¥å‘Š

**æ•°æ®è§„æ¨¡**:
- **æ ·æœ¬æ•°**: {n_samples} ä¸ª
- **ä»£è°¢ç‰©æ•°**: {n_features} ä¸ª

**æ•°æ®ç‰¹å¾**:
- æ–‡ä»¶ç±»å‹: {file_metadata.get('file_type', 'æœªçŸ¥')}
- æ–‡ä»¶å¤§å°: {file_metadata.get('file_size_mb', 'N/A')} MB

**æ•°æ®è´¨é‡**:
- ç¼ºå¤±å€¼ç‡: {file_metadata.get('missing_rate', 'N/A')}%
- æ•°æ®èŒƒå›´: {file_metadata.get('data_range', {}).get('min', 'N/A')} ~ {file_metadata.get('data_range', {}).get('max', 'N/A')}

**ä¸‹ä¸€æ­¥**: å·²ä¸ºæ‚¨è§„åˆ’åˆ†ææµç¨‹ï¼Œè¯·ç¡®è®¤æ‰§è¡Œã€‚"""
                            else:  # RNA
                                diagnosis_message = f"""### ğŸ“Š æ•°æ®ä½“æ£€æŠ¥å‘Š

**æ•°æ®è§„æ¨¡**:
- **ç»†èƒæ•°**: {n_samples} ä¸ª
- **åŸºå› æ•°**: {n_features} ä¸ª

**æ•°æ®ç‰¹å¾**:
- æ–‡ä»¶ç±»å‹: {file_metadata.get('file_type', 'æœªçŸ¥')}
- ç¨€ç–åº¦: {file_metadata.get('sparsity', 'N/A')}

**æ•°æ®è´¨é‡**: æ•°æ®å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹åˆ†æã€‚

**ä¸‹ä¸€æ­¥**: å·²ä¸ºæ‚¨è§„åˆ’åˆ†ææµç¨‹ï¼Œè¯·ç¡®è®¤æ‰§è¡Œã€‚"""
                            
                            yield self._format_sse("diagnosis", {
                                "message": diagnosis_message,
                                "n_samples": n_samples,
                                "n_features": n_features,
                                "file_type": file_metadata.get('file_type'),
                                "status": "data_ready"
                            })
                            await asyncio.sleep(0.01)
                    except Exception as e:
                        logger.error(f"âŒ [Orchestrator] Path A: æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
                        yield self._format_sse("error", {
                            "error": str(e),
                            "message": f"æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {str(e)}"
                        })
                        return
                    
                    # A2. Plan (With Metadata) - CRITICAL: Explicitly tell planner this is NOT a template
                    yield self._format_sse("status", {
                        "content": "æ­£åœ¨æ ¹æ®æ‚¨çš„éœ€æ±‚å®šåˆ¶æµç¨‹...",
                        "state": "running"
                    })
                    await asyncio.sleep(0.01)
                    
                    if planner is None:
                        from .planner import SOPPlanner
                        from .tool_retriever import ToolRetriever
                        llm_client = self._get_llm_client()
                        if not llm_client:
                            raise ValueError("LLM å®¢æˆ·ç«¯ä¸å¯ç”¨")
                        tool_retriever = ToolRetriever()
                        planner = SOPPlanner(tool_retriever, llm_client)
                    
                    logger.info(f"ğŸ” [Orchestrator] Path A: è°ƒç”¨ planner.generate_plan (is_template=False)")
                    logger.info(f"  - file_metadata å­˜åœ¨: {file_metadata is not None}")
                    logger.info(f"  - domain_name: {domain_name}")
                    logger.info(f"  - target_steps: {target_steps}")
                    if file_metadata:
                        logger.info(f"  - file_metadata.file_path: {file_metadata.get('file_path', 'N/A')}")
                    
                    result = await planner.generate_plan(
                        user_query=refined_query,
                        file_metadata=file_metadata,  # ğŸ”¥ CRITICAL: file_metadata exists
                        category_filter=None,
                        domain_name=domain_name,
                        target_steps=target_steps,
                        is_template=False  # ğŸ”¥ CRITICAL: Explicitly NOT a template
                    )
                    
                    logger.info(f"âœ… [Orchestrator] Path A: å·¥ä½œæµè§„åˆ’å®Œæˆ")
                    logger.info(f"âœ… [Orchestrator] Path A: è¿”å›ç»“æœ template_mode: {result.get('template_mode', 'N/A')}")
                    
                    # ğŸ”¥ TASK: æ£€æŸ¥æ˜¯å¦æœ‰LLMé”™è¯¯ï¼ˆæ•°æ®è¯Šæ–­é˜¶æ®µï¼‰ï¼Œå¦‚æœæœ‰åˆ™é€šè¿‡SSEå‘é€è¯¦ç»†é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯
                    if hasattr(self.agent, 'context') and "last_llm_error" in self.agent.context:
                        llm_error_info = self.agent.context.pop("last_llm_error")  # å–å‡ºåæ¸…é™¤
                        logger.warning(f"âš ï¸ [Orchestrator] æ£€æµ‹åˆ°LLMè°ƒç”¨é”™è¯¯ï¼ˆæ•°æ®è¯Šæ–­é˜¶æ®µï¼‰ï¼Œå‘é€è¯¦ç»†é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯")
                        yield self._format_sse("error", {
                            "error": llm_error_info.get("error_message", "LLMè°ƒç”¨å¤±è´¥"),
                            "message": f"æ•°æ®è¯Šæ–­LLMè°ƒç”¨å¤±è´¥: {llm_error_info.get('error_message', 'æœªçŸ¥é”™è¯¯')}",
                            "error_type": llm_error_info.get("error_type", "Unknown"),
                            "details": llm_error_info.get("error_details", ""),
                            "context": llm_error_info.get("context", {}),
                            "possible_causes": llm_error_info.get("possible_causes", []),
                            "debug_info": llm_error_info.get("error_details", "")  # å…¼å®¹å‰ç«¯å­—æ®µå
                        })
                        await asyncio.sleep(0.01)
                        
                    # A3. Force Validation
                    if isinstance(result, dict):
                        # FORCE OVERRIDE: Explicitly set template_mode = False
                        if result.get("template_mode"):
                            logger.error("âŒ [Orchestrator] Path A: é€»è¾‘é”™è¯¯ - Planner è¿”å› template_mode=True  despite file presence. å¼ºåˆ¶è¦†ç›–ã€‚")
                        result["template_mode"] = False
                        if "workflow_data" in result:
                            result["workflow_data"]["template_mode"] = False
                        
                        # Validate Steps
                        workflow_data = result.get("workflow_data") or result
                        steps = workflow_data.get("steps", [])
                        if not steps or len(steps) == 0:
                            logger.warning(f"âš ï¸ [Orchestrator] Path A: Planner è¿”å›ç©ºæ­¥éª¤ï¼Œå›é€€åˆ°ç¡¬ç¼–ç  SOP")
                            # Regenerate using hardcoded SOP
                            try:
                                workflow = self.workflow_registry.get_workflow(domain_name)
                                if not workflow:
                                    raise ValueError(f"æ— æ³•è·å–å·¥ä½œæµ: {domain_name}")
                                
                                if domain_name == "Metabolomics":
                                    hardcoded_result = planner._generate_metabolomics_plan(file_metadata)
                                else:
                                    hardcoded_result = workflow.generate_template(
                                        target_steps=target_steps or list(workflow.steps_dag.keys()),
                                        file_metadata=file_metadata
                                    )
                                    hardcoded_result = planner._fill_parameters(hardcoded_result, file_metadata, workflow, template_mode=False)
                                
                                result = {
                                    "type": "workflow_config",
                                    "workflow_data": hardcoded_result.get("workflow_data") or hardcoded_result,
                                    "template_mode": False
                                }
                                logger.info(f"âœ… [Orchestrator] Path A: ä½¿ç”¨ç¡¬ç¼–ç  SOP ç”Ÿæˆå·¥ä½œæµ: {len(result.get('workflow_data', {}).get('steps', []))} ä¸ªæ­¥éª¤")
                            except Exception as e:
                                logger.error(f"âŒ [Orchestrator] Path A: ç¡¬ç¼–ç  SOP ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
                        
                        # ğŸ”¥ TASK 1: Yield workflow event - ONLY if steps are not empty
                        workflow_data = result.get("workflow_data") or result
                        
                        # ğŸ”¥ CRITICAL: Extract steps BEFORE yielding workflow event
                        steps = workflow_data.get("steps", []) if workflow_data else []
                        has_valid_plan = bool(workflow_data and steps and len(steps) > 0)
                        
                        # ğŸ”¥ TASK 1: Empty Guard - Do NOT yield empty plans
                        if not has_valid_plan:
                            logger.error(f"âŒ [Orchestrator] Path A: å·¥ä½œæµè§„åˆ’å¤±è´¥ï¼Œæ­¥éª¤ä¸ºç©ºã€‚ä¸å‘é€workflowäº‹ä»¶ã€‚")
                            logger.error(f"   - workflow_dataå­˜åœ¨: {workflow_data is not None}")
                            logger.error(f"   - stepsé•¿åº¦: {len(steps) if steps else 0}")
                            yield self._format_sse("error", {
                                "error": "å·¥ä½œæµè§„åˆ’å¤±è´¥",
                                "message": "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„å·¥ä½œæµæ­¥éª¤ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ"
                            })
                            return
                        
                        # ğŸ”¥ CRITICAL: In Path A, files are guaranteed to exist (we're in the else branch)
                        has_files = True  # Path A means files were detected
                        
                        # Debug logging BEFORE execution decision
                        logger.info(f"ğŸ” [Orchestrator] DEBUG: Query='{refined_query}', Files={len(files) if files else 0}, Plan Generated={has_valid_plan}, Steps={len(steps) if steps else 0}")
                        logger.info(f"ğŸ” [Orchestrator] DEBUG: workflow_data keys={list(workflow_data.keys()) if workflow_data else 'None'}")
                        if workflow_data:
                            logger.info(f"ğŸ” [Orchestrator] DEBUG: workflow_data.steps exists={('steps' in workflow_data)}, steps type={type(steps)}, steps length={len(steps) if steps else 0}")
                        
                        # ğŸ”¥ TASK 1: Yield workflow event ONLY ONCE, at the very end of planning block
                        logger.info(f"âœ… [Orchestrator] Path A: å‘é€workflowäº‹ä»¶ï¼ŒåŒ…å« {len(steps)} ä¸ªæ­¥éª¤")
                        yield self._format_sse("workflow", {
                            "workflow_config": workflow_data,
                            "template_mode": False  # ğŸ”¥ CRITICAL: Always False in Path A
                        })
                        await asyncio.sleep(0.01)
                    
                        # Yield result event with workflow config
                        yield self._format_sse("result", {
                            "workflow_config": workflow_data,
                            "template_mode": False
                        })
                        await asyncio.sleep(0.01)
                        
                        yield self._format_sse("status", {
                            "content": "å·¥ä½œæµè§„åˆ’å®Œæˆï¼Œè¯·ç¡®è®¤æ‰§è¡Œã€‚",
                            "state": "completed"
                        })
                        await asyncio.sleep(0.01)
                        
                        yield self._format_sse("done", {"status": "success"})
                        return  # ğŸ”¥ CRITICAL: STOP HERE - Do NOT auto-execute
                        
                        # ğŸ”¥ REMOVED: Auto-execution logic
                        # The workflow should stop at planning stage and wait for explicit execution request
                        # Execution will be triggered by a second request with workflow_data parameter
                    else:
                        # ğŸ”¥ CRITICAL: If result is not a dict, log error but still try to execute if workflow_data exists
                        logger.error(f"âŒ [Orchestrator] Path A: result ä¸æ˜¯å­—å…¸ç±»å‹: {type(result)}")
                        if workflow_data:
                            logger.info(f"ğŸš€ [Orchestrator] Path A: result ä¸æ˜¯å­—å…¸ï¼Œä½† workflow_data å­˜åœ¨ï¼Œå°è¯•æ‰§è¡Œ")
                            # Try to execute with workflow_data
                            should_auto_execute = True
                            # ... (same execution logic as above) ...
                            # For now, just log and return error
                            yield self._format_sse("error", {
                                "error": "å·¥ä½œæµè§„åˆ’ç»“æœæ ¼å¼é”™è¯¯",
                                "message": f"è§„åˆ’ç»“æœä¸æ˜¯å­—å…¸ç±»å‹: {type(result)}"
                            })
                            return
            
        except Exception as e:
            logger.error(f"âŒ æµå¼å¤„ç†å¤±è´¥: {e}", exc_info=True)
            yield self._format_sse("status", {
                "content": f"å¤„ç†å¤±è´¥: {str(e)}",
                "state": "error"
            })
            yield self._format_sse("error", {
                "error": str(e),
                "message": f"å¤„ç†å¤±è´¥: {str(e)}"
            })
    
    def _generate_fallback_summary(
        self, 
        successful_steps: List[Dict[str, Any]], 
        warning_steps: List[Dict[str, Any]], 
        failed_steps: List[Dict[str, Any]], 
        all_steps: List[Dict[str, Any]]
    ) -> str:
        """
        ç”Ÿæˆåå¤‡æ‘˜è¦ï¼ˆå½“ AI ç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            successful_steps: æˆåŠŸæ­¥éª¤åˆ—è¡¨
            warning_steps: è­¦å‘Šæ­¥éª¤åˆ—è¡¨
            failed_steps: å¤±è´¥æ­¥éª¤åˆ—è¡¨
            all_steps: æ‰€æœ‰æ­¥éª¤åˆ—è¡¨
        
        Returns:
            Markdown æ ¼å¼çš„æ‘˜è¦
        """
        summary_parts = []
        
        if successful_steps:
            summary_parts.append(f"âœ… **æˆåŠŸæ­¥éª¤** ({len(successful_steps)}/{len(all_steps)}):")
            for step in successful_steps:
                step_name = step.get('name', step.get('step_id', 'Unknown'))
                summary_parts.append(f"- {step_name}")
        
        if warning_steps:
            summary_parts.append(f"\nâš ï¸ **è·³è¿‡æ­¥éª¤** ({len(warning_steps)}/{len(all_steps)}):")
            for step in warning_steps:
                step_name = step.get('name', step.get('step_id', 'Unknown'))
                reason = step.get('message') or step.get('skipped_reason') or 'æ­¥éª¤è¢«è·³è¿‡'
                summary_parts.append(f"- {step_name}: {reason}")
        
        if failed_steps:
            summary_parts.append(f"\nâŒ **å¤±è´¥æ­¥éª¤** ({len(failed_steps)}/{len(all_steps)}):")
            for step in failed_steps:
                step_name = step.get('name', step.get('step_id', 'Unknown'))
                error_msg = step.get("error") or step.get("summary", "æœªçŸ¥é”™è¯¯")
                summary_parts.append(f"- {step_name}: {error_msg}")
        
        if not summary_parts:
            return "åˆ†æå®Œæˆ"
        
        return "\n".join(summary_parts)
    
    def _format_sse(self, event_type: str, data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ– SSE äº‹ä»¶
        
        Args:
            event_type: äº‹ä»¶ç±»å‹ï¼ˆstatus, thought, message, diagnosis, workflow, done, errorï¼‰
            data: äº‹ä»¶æ•°æ®
            
        Returns:
            SSE æ ¼å¼å­—ç¬¦ä¸²: "event: {type}\ndata: {json}\n\n"
        """
        json_data = json.dumps(data, ensure_ascii=False)
        return f"event: {event_type}\ndata: {json_data}\n\n"

