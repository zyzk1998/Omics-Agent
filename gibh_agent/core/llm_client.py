"""
ç»Ÿä¸€ LLM å®¢æˆ·ç«¯
æ”¯æŒæœ¬åœ°ï¼ˆvLLM/Ollamaï¼‰å’Œäº‘ç«¯ï¼ˆDeepSeek-V3/SiliconFlowï¼‰æ— ç¼åˆ‡æ¢
ä½¿ç”¨ OpenAI SDK æ ‡å‡†æ¥å£
"""
from typing import Optional, AsyncIterator, Dict, Any
from openai import OpenAI, AsyncOpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk
import os
import re
import logging

logger = logging.getLogger(__name__)


class LLMClient:
    """
    ç»Ÿä¸€ LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯æ¨¡å‹åˆ‡æ¢
    
    ä½¿ç”¨æ–¹å¼ï¼š
        # æœ¬åœ°æ¨¡å‹ï¼ˆvLLMï¼‰
        client = LLMClient(
            base_url="http://localhost:8000/v1",
            api_key="EMPTY",
            model="qwen3-vl"
        )
        
        # äº‘ç«¯æ¨¡å‹ï¼ˆDeepSeekï¼‰
        client = LLMClient(
            base_url="https://api.deepseek.com/v1",
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            model="deepseek-chat"
        )
    """
    
    def __init__(
        self,
        base_url: str,
        api_key: str = "EMPTY",
        model: str = "gpt-3.5-turbo",
        temperature: float = 0.7,
        max_tokens: int = 2048,
        timeout: float = 60.0
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            base_url: API åŸºç¡€ URLï¼ˆæœ¬åœ°æˆ–äº‘ç«¯ï¼‰
            api_key: API å¯†é’¥ï¼ˆæœ¬åœ°æ¨¡å‹å¯ä¸º "EMPTY"ï¼‰
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        
        # åˆå§‹åŒ–åŒæ­¥å’Œå¼‚æ­¥å®¢æˆ·ç«¯
        self._sync_client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=timeout
        )
        self._async_client = AsyncOpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=timeout
        )
    
    def chat(
        self,
        messages: list,
        stream: bool = False,
        **kwargs
    ) -> ChatCompletion:
        """
        åŒæ­¥èŠå¤©è°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"role": "user", "content": "..."}]
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰
        
        Returns:
            ChatCompletion å¯¹è±¡
        """
        import logging
        import json
        logger = logging.getLogger(__name__)
        
        params = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens": kwargs.get("max_tokens", self.max_tokens),
            "stream": stream
        }
        params.update(kwargs)
        
        completion = self._sync_client.chat.completions.create(**params)
        
        # ğŸ”¥ Task 2: å¼ºåˆ¶è®°å½•åŸå§‹ JSON å“åº”
        try:
            # å°è¯•åºåˆ—åŒ–æ•´ä¸ªå“åº”å¯¹è±¡
            if hasattr(completion, 'model_dump'):
                log_payload = json.dumps(completion.model_dump(), default=str, ensure_ascii=False)
            elif hasattr(completion, 'dict'):
                log_payload = json.dumps(completion.dict(), default=str, ensure_ascii=False)
            else:
                # æå–å…³é”®ä¿¡æ¯
                response_data = {
                    "id": getattr(completion, 'id', None),
                    "model": getattr(completion, 'model', None),
                    "choices": []
                }
                if hasattr(completion, 'choices') and completion.choices:
                    for choice in completion.choices:
                        choice_data = {
                            "index": getattr(choice, 'index', None),
                            "message": {}
                        }
                        if hasattr(choice, 'message'):
                            msg = choice.message
                            choice_data["message"] = {
                                "role": getattr(msg, 'role', None),
                                "content": getattr(msg, 'content', None)
                            }
                        response_data["choices"].append(choice_data)
                log_payload = json.dumps(response_data, default=str, ensure_ascii=False)
            
            logger.info(f"ğŸ”¥ [LLM_RAW_DUMP] {log_payload}")
        except Exception as e:
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œè‡³å°‘è®°å½•å­—ç¬¦ä¸²è¡¨ç¤º
            logger.info(f"ğŸ”¥ [LLM_RAW_DUMP] {str(completion)}")
            logger.warning(f"âš ï¸ æ— æ³•åºåˆ—åŒ–å“åº”å¯¹è±¡: {e}")
        
        return completion
    
    async def achat(
        self,
        messages: list,
        stream: bool = False,
        **kwargs
    ) -> ChatCompletion:
        """
        å¼‚æ­¥èŠå¤©è°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ChatCompletion å¯¹è±¡
        """
        import logging
        logger = logging.getLogger(__name__)
        
        params = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens": kwargs.get("max_tokens", self.max_tokens),
            "stream": stream
        }
        params.update(kwargs)
        
        import json
        
        completion = await self._async_client.chat.completions.create(**params)
        
        # ğŸ”¥ Task 2: å¼ºåˆ¶è®°å½•åŸå§‹ JSON å“åº”
        try:
            # å°è¯•åºåˆ—åŒ–æ•´ä¸ªå“åº”å¯¹è±¡
            if hasattr(completion, 'model_dump'):
                log_payload = json.dumps(completion.model_dump(), default=str, ensure_ascii=False)
            elif hasattr(completion, 'dict'):
                log_payload = json.dumps(completion.dict(), default=str, ensure_ascii=False)
            else:
                # æå–å…³é”®ä¿¡æ¯
                response_data = {
                    "id": getattr(completion, 'id', None),
                    "model": getattr(completion, 'model', None),
                    "choices": []
                }
                if hasattr(completion, 'choices') and completion.choices:
                    for choice in completion.choices:
                        choice_data = {
                            "index": getattr(choice, 'index', None),
                            "message": {}
                        }
                        if hasattr(choice, 'message'):
                            msg = choice.message
                            choice_data["message"] = {
                                "role": getattr(msg, 'role', None),
                                "content": getattr(msg, 'content', None)
                            }
                        response_data["choices"].append(choice_data)
                log_payload = json.dumps(response_data, default=str, ensure_ascii=False)
            
            logger.info(f"ğŸ”¥ [LLM_RAW_DUMP] {log_payload}")
        except Exception as e:
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œè‡³å°‘è®°å½•å­—ç¬¦ä¸²è¡¨ç¤º
            logger.info(f"ğŸ”¥ [LLM_RAW_DUMP] {str(completion)}")
            logger.warning(f"âš ï¸ æ— æ³•åºåˆ—åŒ–å“åº”å¯¹è±¡: {e}")
        
        return completion
    
    async def astream(
        self,
        messages: list,
        **kwargs
    ) -> AsyncIterator[ChatCompletionChunk]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
        
        Yields:
            ChatCompletionChunk å¯¹è±¡
        """
        import logging
        logger = logging.getLogger(__name__)
        
        params = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens": kwargs.get("max_tokens", self.max_tokens),
            "stream": True
        }
        params.update(kwargs)
        
        import json
        
        # ğŸ”¥ Task 2: æ”¶é›†æµå¼å“åº”å¹¶è®°å½•å®Œæ•´ JSON
        collected_content = []
        collected_chunks = []
        async for chunk in await self._async_client.chat.completions.create(**params):
            if chunk.choices and chunk.choices[0].delta.content:
                collected_content.append(chunk.choices[0].delta.content)
            # æ”¶é›†å®Œæ•´çš„ chunk å¯¹è±¡ç”¨äº JSON åºåˆ—åŒ–
            collected_chunks.append(chunk)
            yield chunk
        
        # è®°å½•å®Œæ•´çš„æµå¼å“åº”å†…å®¹ï¼ˆJSON æ ¼å¼ï¼‰
        if collected_chunks:
            try:
                # æ„å»ºå®Œæ•´çš„å“åº”å¯¹è±¡è¡¨ç¤º
                stream_data = {
                    "type": "stream",
                    "content": "".join(collected_content),
                    "chunks_count": len(collected_chunks)
                }
                # å°è¯•åºåˆ—åŒ–æœ€åä¸€ä¸ª chunk çš„å®Œæ•´ä¿¡æ¯
                if collected_chunks:
                    last_chunk = collected_chunks[-1]
                    if hasattr(last_chunk, 'model_dump'):
                        stream_data["last_chunk"] = last_chunk.model_dump()
                    else:
                        stream_data["last_chunk"] = {
                            "id": getattr(last_chunk, 'id', None),
                            "model": getattr(last_chunk, 'model', None)
                        }
                
                log_payload = json.dumps(stream_data, default=str, ensure_ascii=False)
                logger.info(f"ğŸ”¥ [LLM_RAW_DUMP] {log_payload}")
            except Exception as e:
                # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œè‡³å°‘è®°å½•æ–‡æœ¬å†…å®¹
                raw_content = "".join(collected_content)
                logger.info(f"ğŸ”¥ [LLM_RAW_DUMP] {raw_content}")
                logger.warning(f"âš ï¸ æ— æ³•åºåˆ—åŒ–æµå¼å“åº”: {e}")
    
    def get_content(self, completion: ChatCompletion) -> str:
        """ä» ChatCompletion ä¸­æå–å†…å®¹"""
        return completion.choices[0].message.content
    
    def extract_think_and_content(self, completion: ChatCompletion) -> tuple[str, str]:
        """
        ä»å“åº”ä¸­æå– think è¿‡ç¨‹å’Œå®é™…å†…å®¹
        
        Returns:
            (think_content, actual_content) å…ƒç»„
        """
        content = completion.choices[0].message.content or ""
        
        # ä¼˜å…ˆè§£æ DeepSeek-R1 çš„ <think> æ ‡ç­¾ï¼Œç„¶åæ”¯æŒå…¶ä»–å˜ä½“ï¼ˆå‘åå…¼å®¹ï¼‰
        think_patterns = [
            (r'<think>(.*?)</think>', re.DOTALL),  # DeepSeek-R1 æ ‡å‡†æ ¼å¼ï¼ˆä¼˜å…ˆï¼‰
            (r'<think>(.*?)</think>', re.DOTALL),  # æ—§åè®®æ ¼å¼
            (r'<reasoning>(.*?)</reasoning>', re.DOTALL),
            (r'<thought>(.*?)</thought>', re.DOTALL),
            (r'<thinking>(.*?)</thinking>', re.DOTALL),
        ]
        
        think_content = ""
        actual_content = content
        
        for pattern, flags in think_patterns:
            matches = re.findall(pattern, content, flags)
            if matches:
                think_content = "\n\n".join(matches)
                # ç§»é™¤ think æ ‡ç­¾ï¼Œä¿ç•™å®é™…å†…å®¹
                actual_content = re.sub(pattern, '', content, flags=flags).strip()
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ ¼å¼çš„æ€è€ƒè¿‡ç¨‹
        if not think_content and "æ€è€ƒ" in content[:100]:
            # å°è¯•æå–æ€è€ƒéƒ¨åˆ†
            lines = content.split('\n')
            think_lines = []
            content_lines = []
            in_think = False
            
            for line in lines:
                if any(keyword in line for keyword in ["æ€è€ƒ", "åˆ†æ", "è€ƒè™‘", "æ¨ç†"]):
                    in_think = True
                if in_think and line.strip():
                    think_lines.append(line)
                else:
                    content_lines.append(line)
                    in_think = False
            
            if think_lines:
                think_content = '\n'.join(think_lines)
                actual_content = '\n'.join(content_lines).strip()
        
        return think_content, actual_content
    
    async def get_stream_content(self, stream: AsyncIterator[ChatCompletionChunk]) -> AsyncIterator[str]:
        """ä»æµå¼å“åº”ä¸­æå–å†…å®¹"""
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


class LLMClientFactory:
    """LLM å®¢æˆ·ç«¯å·¥å‚ï¼Œæ ¹æ®é…ç½®åˆ›å»ºå®¢æˆ·ç«¯"""
    
    @staticmethod
    def create_default() -> LLMClient:
        """
        ğŸ”¥ TASK 1: åˆ›å»ºé»˜è®¤LLMå®¢æˆ·ç«¯ï¼Œç»Ÿä¸€ä½¿ç”¨ç¡…åŸºæµåŠ¨DeepSeek API
        
        ä¼˜å…ˆçº§ï¼š
        1. SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ï¼ˆç¡…åŸºæµåŠ¨ï¼‰
        2. å¦‚æœæœªè®¾ç½®ï¼ŒæŠ›å‡ºé”™è¯¯ï¼ˆä¸å†å›é€€åˆ°æœ¬åœ°LLMï¼‰
        
        Returns:
            LLMClient å®ä¾‹ï¼ˆç¡…åŸºæµåŠ¨DeepSeek APIï¼‰
        """
        # ğŸ”¥ TASK 1: ç»Ÿä¸€ä½¿ç”¨ç¡…åŸºæµåŠ¨API
        api_key = os.getenv("SILICONFLOW_API_KEY")
        if not api_key:
            error_msg = (
                "âŒ [LLMClientFactory] SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼\n"
                "æœ¬é¡¹ç›®ç»Ÿä¸€ä½¿ç”¨ç¡…åŸºæµåŠ¨DeepSeek APIï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š\n"
                "  export SILICONFLOW_API_KEY='your_api_key_here'\n"
                "æˆ–åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ ï¼š\n"
                "  SILICONFLOW_API_KEY=your_api_key_here"
            )
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # ä½¿ç”¨ç¡…åŸºæµåŠ¨API
        base_url = "https://api.siliconflow.cn/v1"
        model = os.getenv("SILICONFLOW_MODEL", "deepseek-ai/DeepSeek-R1")
        
        logger.info(f"ğŸ”— [LLMClientFactory] åˆ›å»ºç¡…åŸºæµåŠ¨LLMå®¢æˆ·ç«¯: {base_url} (model: {model})")
        logger.info(f"   API Key: {'***' + api_key[-4:] if len(api_key) > 4 else '***'}")
        
        return LLMClient(
            base_url=base_url,
            api_key=api_key,
            model=model,
            temperature=0.7,
            max_tokens=4096
        )
    
    @staticmethod
    def create_from_config(config: Dict[str, Any]) -> LLMClient:
        """
        ä»é…ç½®å­—å…¸åˆ›å»ºå®¢æˆ·ç«¯
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« base_url, api_key, model ç­‰
        
        Returns:
            LLMClient å®ä¾‹
        """
        return LLMClient(
            base_url=config.get("base_url"),
            api_key=config.get("api_key", "EMPTY"),
            model=config.get("model", "gpt-3.5-turbo"),
            temperature=config.get("temperature", 0.7),
            max_tokens=config.get("max_tokens", 2048),
            timeout=config.get("timeout", 60.0)
        )
    
    @staticmethod
    def create_local_vllm(model: str = "qwen3-vl", base_url: str = None) -> LLMClient:
        """åˆ›å»ºæœ¬åœ° vLLM å®¢æˆ·ç«¯"""
        if base_url is None:
            base_url = os.getenv("VLLM_URL", "http://localhost:8000/v1")
        return LLMClient(
            base_url=base_url,
            api_key="EMPTY",
            model=model
        )
    
    @staticmethod
    def create_cloud_deepseek(api_key: str = None, model: str = "deepseek-chat") -> LLMClient:
        """åˆ›å»º DeepSeek äº‘ç«¯å®¢æˆ·ç«¯"""
        if api_key is None:
            api_key = os.getenv("DEEPSEEK_API_KEY", "")
        return LLMClient(
            base_url="https://api.deepseek.com/v1",
            api_key=api_key,
            model=model
        )
    
    @staticmethod
    def create_cloud_siliconflow(api_key: str = None, model: str = None) -> LLMClient:
        """åˆ›å»º SiliconFlow äº‘ç«¯å®¢æˆ·ç«¯ï¼ˆç¡…åŸºæµåŠ¨ï¼‰"""
        if api_key is None:
            api_key = os.getenv("SILICONFLOW_API_KEY", "")
        if model is None:
            # é»˜è®¤ä½¿ç”¨ DeepSeek-R1ï¼ˆæ”¯æŒæ€è€ƒè¿‡ç¨‹æµå¼è¾“å‡ºï¼‰ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ SILICONFLOW_MODEL è¦†ç›–
            model = os.getenv("SILICONFLOW_MODEL", "deepseek-ai/DeepSeek-R1")
        return LLMClient(
            base_url="https://api.siliconflow.cn/v1",
            api_key=api_key,
            model=model,
            temperature=0.7,
            max_tokens=4096
        )

